{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.3, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(70):\n",
    "        image = cv2.imread(f\"../training/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            train_label = np.zeros([len(classes)])\n",
    "            train_label[class_index] = 1\n",
    "            train_data.append(landmarks)\n",
    "            train_labels.append(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(train_data, train_labels)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_labels = []\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(20):\n",
    "        image = cv2.imread(f\"../validation/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            val_label = np.zeros([len(classes)])\n",
    "            val_label[class_index] = 1\n",
    "            val_data.append(landmarks)\n",
    "            val_labels.append(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(val_data, val_labels)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(curr_model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 0.0026394419591724725\n",
      "  batch 20 loss: 0.0029047839161421506\n",
      "  batch 30 loss: 0.0021298019515825217\n",
      "  batch 40 loss: 0.003093775585648473\n",
      "  batch 50 loss: 0.0026070380735518485\n",
      "  batch 60 loss: 0.0034350211441051214\n",
      "  batch 70 loss: 0.0030692430261296975\n",
      "  batch 80 loss: 0.0020734137127647045\n",
      "LOSS train 0.0020734137127647045 valid 0.0040474376794190905\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.003629777159630976\n",
      "  batch 20 loss: 0.001876000685251711\n",
      "  batch 30 loss: 0.003721128241977567\n",
      "  batch 40 loss: 0.0039708421749310215\n",
      "  batch 50 loss: 0.0021996103411993317\n",
      "  batch 60 loss: 0.00229879035941849\n",
      "  batch 70 loss: 0.0029280942662580855\n",
      "  batch 80 loss: 0.002478487038035837\n",
      "LOSS train 0.002478487038035837 valid 0.00267898780755786\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.003862616045012146\n",
      "  batch 20 loss: 0.002561762181017002\n",
      "  batch 30 loss: 0.0015381543212924953\n",
      "  batch 40 loss: 0.002336064077348965\n",
      "  batch 50 loss: 0.003309473959780007\n",
      "  batch 60 loss: 0.003249885203422309\n",
      "  batch 70 loss: 0.003192635398636412\n",
      "  batch 80 loss: 0.0026264331286256493\n",
      "LOSS train 0.0026264331286256493 valid 0.003362986742922658\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.0026996383037385385\n",
      "  batch 20 loss: 0.003133420653728081\n",
      "  batch 30 loss: 0.002846027058672007\n",
      "  batch 40 loss: 0.002041615463610924\n",
      "  batch 50 loss: 0.003821876420317949\n",
      "  batch 60 loss: 0.002168063227190942\n",
      "  batch 70 loss: 0.001859884478631102\n",
      "  batch 80 loss: 0.002981569811254303\n",
      "LOSS train 0.002981569811254303 valid 0.004097409024489025\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.0029442986398862557\n",
      "  batch 20 loss: 0.0025572176462219432\n",
      "  batch 30 loss: 0.0032600495092196978\n",
      "  batch 40 loss: 0.002654472152244125\n",
      "  batch 50 loss: 0.0020472863909162696\n",
      "  batch 60 loss: 0.0019969369954196737\n",
      "  batch 70 loss: 0.0026091614264259987\n",
      "  batch 80 loss: 0.003703675513975213\n",
      "LOSS train 0.003703675513975213 valid 0.002742372051543498\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.0018067208596221462\n",
      "  batch 20 loss: 0.003611753873951784\n",
      "  batch 30 loss: 0.0025483778122065816\n",
      "  batch 40 loss: 0.0035374062817936647\n",
      "  batch 50 loss: 0.0027911470843832832\n",
      "  batch 60 loss: 0.0009097283811456691\n",
      "  batch 70 loss: 0.0019269785740789302\n",
      "  batch 80 loss: 0.004584950321577708\n",
      "LOSS train 0.004584950321577708 valid 0.003499353523475293\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.002240722273188567\n",
      "  batch 20 loss: 0.003160962259050848\n",
      "  batch 30 loss: 0.002889378698273504\n",
      "  batch 40 loss: 0.003246927515419884\n",
      "  batch 50 loss: 0.0026358819344750374\n",
      "  batch 60 loss: 0.0017622392182602199\n",
      "  batch 70 loss: 0.0024957861818620587\n",
      "  batch 80 loss: 0.0026953445904268846\n",
      "LOSS train 0.0026953445904268846 valid 0.0038736403042275926\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.0028468597941810002\n",
      "  batch 20 loss: 0.003083144910260671\n",
      "  batch 30 loss: 0.0025579492501719868\n",
      "  batch 40 loss: 0.003100306693318089\n",
      "  batch 50 loss: 0.0020096556925977894\n",
      "  batch 60 loss: 0.00238283584948249\n",
      "  batch 70 loss: 0.0015879006890372692\n",
      "  batch 80 loss: 0.003066293743654569\n",
      "LOSS train 0.003066293743654569 valid 0.003180862828721729\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.0024431750515759632\n",
      "  batch 20 loss: 0.002638164812674404\n",
      "  batch 30 loss: 0.0027893054895685053\n",
      "  batch 40 loss: 0.0029666466671187663\n",
      "  batch 50 loss: 0.001989966059886683\n",
      "  batch 60 loss: 0.0030824782474041966\n",
      "  batch 70 loss: 0.0033780699301360073\n",
      "  batch 80 loss: 0.0017519323602982694\n",
      "LOSS train 0.0017519323602982694 valid 0.0032261830603238194\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.003027445412101315\n",
      "  batch 20 loss: 0.0031213681309736783\n",
      "  batch 30 loss: 0.002934262176336233\n",
      "  batch 40 loss: 0.003049852926301355\n",
      "  batch 50 loss: 0.0020940741017057007\n",
      "  batch 60 loss: 0.002144043984083055\n",
      "  batch 70 loss: 0.00266374664884097\n",
      "  batch 80 loss: 0.002355294490303095\n",
      "LOSS train 0.002355294490303095 valid 0.0033034121751916246\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.003070916252409006\n",
      "  batch 20 loss: 0.0026276702568793554\n",
      "  batch 30 loss: 0.0016752450934291118\n",
      "  batch 40 loss: 0.0022075810374758476\n",
      "  batch 50 loss: 0.0021227113862778424\n",
      "  batch 60 loss: 0.0036157758865556387\n",
      "  batch 70 loss: 0.002486266065432119\n",
      "  batch 80 loss: 0.0032066917293150253\n",
      "LOSS train 0.0032066917293150253 valid 0.0035823230635469373\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.0018900621344641877\n",
      "  batch 20 loss: 0.003922941260475455\n",
      "  batch 30 loss: 0.0036478566847222282\n",
      "  batch 40 loss: 0.0020897125815054094\n",
      "  batch 50 loss: 0.0018289036141140968\n",
      "  batch 60 loss: 0.0023822936342412504\n",
      "  batch 70 loss: 0.0025968160031425215\n",
      "  batch 80 loss: 0.002636710135857356\n",
      "LOSS train 0.002636710135857356 valid 0.003088301044263062\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.0028182619163317213\n",
      "  batch 20 loss: 0.00343896231099734\n",
      "  batch 30 loss: 0.002397354576453381\n",
      "  batch 40 loss: 0.0031312951094605523\n",
      "  batch 50 loss: 0.002393504572398797\n",
      "  batch 60 loss: 0.001983798816036142\n",
      "  batch 70 loss: 0.00239461373555514\n",
      "  batch 80 loss: 0.0020378850305633025\n",
      "LOSS train 0.0020378850305633025 valid 0.0030499959084590955\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.0025709377403927646\n",
      "  batch 20 loss: 0.0016802830316464679\n",
      "  batch 30 loss: 0.0020948577136095992\n",
      "  batch 40 loss: 0.00320551211162865\n",
      "  batch 50 loss: 0.0016207820161980636\n",
      "  batch 60 loss: 0.002871699793422522\n",
      "  batch 70 loss: 0.0033082785584269913\n",
      "  batch 80 loss: 0.002714832647370713\n",
      "LOSS train 0.002714832647370713 valid 0.002997293891839945\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.002504521251012193\n",
      "  batch 20 loss: 0.002648970078644197\n",
      "  batch 30 loss: 0.0015161635099389058\n",
      "  batch 40 loss: 0.0032371159134982007\n",
      "  batch 50 loss: 0.001814382693328298\n",
      "  batch 60 loss: 0.0024833610180849063\n",
      "  batch 70 loss: 0.0032200316437069887\n",
      "  batch 80 loss: 0.002749265309512339\n",
      "LOSS train 0.002749265309512339 valid 0.003620829268675152\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.002301142271335266\n",
      "  batch 20 loss: 0.0028088787021147256\n",
      "  batch 30 loss: 0.0031849278274648897\n",
      "  batch 40 loss: 0.0022783889127140355\n",
      "  batch 50 loss: 0.0028034356085981926\n",
      "  batch 60 loss: 0.001506694272563891\n",
      "  batch 70 loss: 0.0028291888700550773\n",
      "  batch 80 loss: 0.0026217752519073654\n",
      "LOSS train 0.0026217752519073654 valid 0.0038651211993419566\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.0027966459388380827\n",
      "  batch 20 loss: 0.001862021823797022\n",
      "  batch 30 loss: 0.003368802538557247\n",
      "  batch 40 loss: 0.0021511789579108155\n",
      "  batch 50 loss: 0.002387617498925465\n",
      "  batch 60 loss: 0.003084110527663597\n",
      "  batch 70 loss: 0.0024906453058292756\n",
      "  batch 80 loss: 0.0022956830076964253\n",
      "LOSS train 0.0022956830076964253 valid 0.003211237317264022\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.0021029382361803075\n",
      "  batch 20 loss: 0.002863363190738255\n",
      "  batch 30 loss: 0.0032962074723855038\n",
      "  batch 40 loss: 0.002058365284563024\n",
      "  batch 50 loss: 0.0032222145291370906\n",
      "  batch 60 loss: 0.0024344216742974824\n",
      "  batch 70 loss: 0.0017922012232816087\n",
      "  batch 80 loss: 0.0018770107000364079\n",
      "LOSS train 0.0018770107000364079 valid 0.0038058077787900403\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.0026408489255459245\n",
      "  batch 20 loss: 0.0033579715827499967\n",
      "  batch 30 loss: 0.0018691516376975414\n",
      "  batch 40 loss: 0.0021200055506255923\n",
      "  batch 50 loss: 0.0017050362895929538\n",
      "  batch 60 loss: 0.002775192654212333\n",
      "  batch 70 loss: 0.00331533159223909\n",
      "  batch 80 loss: 0.0025034155982211815\n",
      "LOSS train 0.0025034155982211815 valid 0.00265991472209862\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.0022315747087418457\n",
      "  batch 20 loss: 0.0021922660163113507\n",
      "  batch 30 loss: 0.001966240914748596\n",
      "  batch 40 loss: 0.0027736846179095666\n",
      "  batch 50 loss: 0.0021752931666469523\n",
      "  batch 60 loss: 0.002288225470863381\n",
      "  batch 70 loss: 0.0031246422951653587\n",
      "  batch 80 loss: 0.00291650150093119\n",
      "LOSS train 0.00291650150093119 valid 0.003416563811842934\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.0030958614541077623\n",
      "  batch 20 loss: 0.00211249049818889\n",
      "  batch 30 loss: 0.0022024876710929676\n",
      "  batch 40 loss: 0.0023372315106598763\n",
      "  batch 50 loss: 0.0034229076356382393\n",
      "  batch 60 loss: 0.0017792857838117015\n",
      "  batch 70 loss: 0.002364405604498643\n",
      "  batch 80 loss: 0.002320556847280386\n",
      "LOSS train 0.002320556847280386 valid 0.0025679823610698805\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.0019982237870408426\n",
      "  batch 20 loss: 0.00280654709731607\n",
      "  batch 30 loss: 0.0015730658355209925\n",
      "  batch 40 loss: 0.00260903602865028\n",
      "  batch 50 loss: 0.0029199850995837552\n",
      "  batch 60 loss: 0.0027266362722798477\n",
      "  batch 70 loss: 0.0016513754471702668\n",
      "  batch 80 loss: 0.0027659951049713526\n",
      "LOSS train 0.0027659951049713526 valid 0.003139272191292548\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 0.0028346353493816424\n",
      "  batch 20 loss: 0.0022048644020856044\n",
      "  batch 30 loss: 0.0028667598677316166\n",
      "  batch 40 loss: 0.0018878482585250821\n",
      "  batch 50 loss: 0.0030229497023924524\n",
      "  batch 60 loss: 0.0027450708387164015\n",
      "  batch 70 loss: 0.0016684958334508338\n",
      "  batch 80 loss: 0.001787140391684261\n",
      "LOSS train 0.001787140391684261 valid 0.0024596493694753006\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 0.002053377118716071\n",
      "  batch 20 loss: 0.0015696143805371322\n",
      "  batch 30 loss: 0.0014606851734924931\n",
      "  batch 40 loss: 0.0020284038453269202\n",
      "  batch 50 loss: 0.002968160818818433\n",
      "  batch 60 loss: 0.0032622465565964377\n",
      "  batch 70 loss: 0.00223245835572925\n",
      "  batch 80 loss: 0.002711477282468877\n",
      "LOSS train 0.002711477282468877 valid 0.002911919144262356\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 0.0017960051170575753\n",
      "  batch 20 loss: 0.002647153005682412\n",
      "  batch 30 loss: 0.0021433613501358195\n",
      "  batch 40 loss: 0.0017467706326442568\n",
      "  batch 50 loss: 0.0023999755856380033\n",
      "  batch 60 loss: 0.0027354100106890657\n",
      "  batch 70 loss: 0.002784358452709057\n",
      "  batch 80 loss: 0.002501241601385118\n",
      "LOSS train 0.002501241601385118 valid 0.003060216751018743\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 0.002221543599546294\n",
      "  batch 20 loss: 0.0024000266034363447\n",
      "  batch 30 loss: 0.0024005274211504\n",
      "  batch 40 loss: 0.0018423408318653856\n",
      "  batch 50 loss: 0.002903142961622507\n",
      "  batch 60 loss: 0.0027032388897623603\n",
      "  batch 70 loss: 0.0021836796783645694\n",
      "  batch 80 loss: 0.002153943922257895\n",
      "LOSS train 0.002153943922257895 valid 0.004132264614017913\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 0.002259996374596085\n",
      "  batch 20 loss: 0.0020131910831537427\n",
      "  batch 30 loss: 0.001750440721934865\n",
      "  batch 40 loss: 0.00233051988701618\n",
      "  batch 50 loss: 0.001780205558122816\n",
      "  batch 60 loss: 0.002195656771243648\n",
      "  batch 70 loss: 0.002084593594679518\n",
      "  batch 80 loss: 0.004082081601018217\n",
      "LOSS train 0.004082081601018217 valid 0.004366148665394576\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 0.0029078569356670416\n",
      "  batch 20 loss: 0.0027307952889032094\n",
      "  batch 30 loss: 0.0023339884013580557\n",
      "  batch 40 loss: 0.0020917204726060845\n",
      "  batch 50 loss: 0.002733657505723386\n",
      "  batch 60 loss: 0.0026527936132879405\n",
      "  batch 70 loss: 0.001698700162660316\n",
      "  batch 80 loss: 0.0020343280718861934\n",
      "LOSS train 0.0020343280718861934 valid 0.003149063859546004\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 0.001979982557168114\n",
      "  batch 20 loss: 0.0024296354437638003\n",
      "  batch 30 loss: 0.0016684576044212918\n",
      "  batch 40 loss: 0.002309797386237733\n",
      "  batch 50 loss: 0.002295589261780151\n",
      "  batch 60 loss: 0.002350552445420817\n",
      "  batch 70 loss: 0.001349097104014163\n",
      "  batch 80 loss: 0.0037424510371465657\n",
      "LOSS train 0.0037424510371465657 valid 0.0029416977546316047\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 0.0021251692505984466\n",
      "  batch 20 loss: 0.0016121394578931358\n",
      "  batch 30 loss: 0.0025021019419227742\n",
      "  batch 40 loss: 0.002103764201297054\n",
      "  batch 50 loss: 0.0034850181240926757\n",
      "  batch 60 loss: 0.0023926783733031697\n",
      "  batch 70 loss: 0.0019666039514504518\n",
      "  batch 80 loss: 0.0027095793288935965\n",
      "LOSS train 0.0027095793288935965 valid 0.00311091452983419\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 0.0016457649134167696\n",
      "  batch 20 loss: 0.002015449599502972\n",
      "  batch 30 loss: 0.002440337014513716\n",
      "  batch 40 loss: 0.0027193667424171507\n",
      "  batch 50 loss: 0.0022793838975644575\n",
      "  batch 60 loss: 0.0026076769453538874\n",
      "  batch 70 loss: 0.002118891407690171\n",
      "  batch 80 loss: 0.0026966481210934034\n",
      "LOSS train 0.0026966481210934034 valid 0.0029713685904334854\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 0.0019050139923933785\n",
      "  batch 20 loss: 0.002493320573000801\n",
      "  batch 30 loss: 0.001914298681128912\n",
      "  batch 40 loss: 0.0023965916228462446\n",
      "  batch 50 loss: 0.0026319007605479783\n",
      "  batch 60 loss: 0.0026063432824230404\n",
      "  batch 70 loss: 0.0017191815642604523\n",
      "  batch 80 loss: 0.003046200238372876\n",
      "LOSS train 0.003046200238372876 valid 0.0028884135966563917\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 0.002815391351418839\n",
      "  batch 20 loss: 0.0018631003720201988\n",
      "  batch 30 loss: 0.0023441722754341753\n",
      "  batch 40 loss: 0.0023066010189722874\n",
      "  batch 50 loss: 0.002389348844280903\n",
      "  batch 60 loss: 0.0027865719787314447\n",
      "  batch 70 loss: 0.0019814577898273457\n",
      "  batch 80 loss: 0.0021515912949780615\n",
      "LOSS train 0.0021515912949780615 valid 0.0028830660417952458\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 0.002656394429880038\n",
      "  batch 20 loss: 0.002485843357135309\n",
      "  batch 30 loss: 0.002095833926227897\n",
      "  batch 40 loss: 0.0017697382411483887\n",
      "  batch 50 loss: 0.002692427683155074\n",
      "  batch 60 loss: 0.0022738161930760726\n",
      "  batch 70 loss: 0.0018406145499284322\n",
      "  batch 80 loss: 0.0020038371492887563\n",
      "LOSS train 0.0020038371492887563 valid 0.002554713549825465\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 0.002680626938547448\n",
      "  batch 20 loss: 0.0021042002117042102\n",
      "  batch 30 loss: 0.0019551841714132933\n",
      "  batch 40 loss: 0.002254610121553924\n",
      "  batch 50 loss: 0.002170465737503946\n",
      "  batch 60 loss: 0.0024128346861630235\n",
      "  batch 70 loss: 0.0020179831883524457\n",
      "  batch 80 loss: 0.002145050187414199\n",
      "LOSS train 0.002145050187414199 valid 0.0030829138953868095\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 0.0021097064971968395\n",
      "  batch 20 loss: 0.0022340618683870162\n",
      "  batch 30 loss: 0.0010593105871066655\n",
      "  batch 40 loss: 0.002596077263888219\n",
      "  batch 50 loss: 0.002814236555104799\n",
      "  batch 60 loss: 0.0018380108236328851\n",
      "  batch 70 loss: 0.0025040442678914586\n",
      "  batch 80 loss: 0.0024625205035590627\n",
      "LOSS train 0.0024625205035590627 valid 0.002752353626610784\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 0.0017151673310763726\n",
      "  batch 20 loss: 0.002194607819706107\n",
      "  batch 30 loss: 0.0020619593029095997\n",
      "  batch 40 loss: 0.0017793208227089962\n",
      "  batch 50 loss: 0.0015745969283784689\n",
      "  batch 60 loss: 0.0031629967805656633\n",
      "  batch 70 loss: 0.0027653100812585762\n",
      "  batch 80 loss: 0.002305371233558162\n",
      "LOSS train 0.002305371233558162 valid 0.0031673129326736673\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 0.001718725549494593\n",
      "  batch 20 loss: 0.0019562156357096684\n",
      "  batch 30 loss: 0.002084444656975393\n",
      "  batch 40 loss: 0.0023674102821814812\n",
      "  batch 50 loss: 0.0021247160662369426\n",
      "  batch 60 loss: 0.0023515799046890606\n",
      "  batch 70 loss: 0.002395994381424771\n",
      "  batch 80 loss: 0.002233109753035478\n",
      "LOSS train 0.002233109753035478 valid 0.0027420552714374933\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 0.002406030781082791\n",
      "  batch 20 loss: 0.0024796737451424633\n",
      "  batch 30 loss: 0.002269537315794423\n",
      "  batch 40 loss: 0.0014250881179123098\n",
      "  batch 50 loss: 0.0027427693966501467\n",
      "  batch 60 loss: 0.001523852059631281\n",
      "  batch 70 loss: 0.00336340032448561\n",
      "  batch 80 loss: 0.0012685128255270684\n",
      "LOSS train 0.0012685128255270684 valid 0.0028613841523929297\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.0027830905661630823\n",
      "  batch 20 loss: 0.00150006370859046\n",
      "  batch 30 loss: 0.002265904159639831\n",
      "  batch 40 loss: 0.002458942036901135\n",
      "  batch 50 loss: 0.001591114252278203\n",
      "  batch 60 loss: 0.0019422247075340238\n",
      "  batch 70 loss: 0.001995877582680805\n",
      "  batch 80 loss: 0.0022364540425542146\n",
      "LOSS train 0.0022364540425542146 valid 0.0036427945568175348\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.003251726708458591\n",
      "  batch 20 loss: 0.001461451811906045\n",
      "  batch 30 loss: 0.0015139871333758493\n",
      "  batch 40 loss: 0.0026544078055621865\n",
      "  batch 50 loss: 0.0022257154070302932\n",
      "  batch 60 loss: 0.001823514514717317\n",
      "  batch 70 loss: 0.002531757703729909\n",
      "  batch 80 loss: 0.0017843420548729228\n",
      "LOSS train 0.0017843420548729228 valid 0.0033949084666528508\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 0.0021562189516998844\n",
      "  batch 20 loss: 0.002232507010774043\n",
      "  batch 30 loss: 0.002178328930176576\n",
      "  batch 40 loss: 0.0014735109079765606\n",
      "  batch 50 loss: 0.0025737621296798353\n",
      "  batch 60 loss: 0.0027080901348881525\n",
      "  batch 70 loss: 0.001963205021036174\n",
      "  batch 80 loss: 0.0017015596421060763\n",
      "LOSS train 0.0017015596421060763 valid 0.002508481317127007\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.0024858481457386006\n",
      "  batch 20 loss: 0.003612002850650242\n",
      "  batch 30 loss: 0.0014640692756756834\n",
      "  batch 40 loss: 0.0014879521277407549\n",
      "  batch 50 loss: 0.0020518115177537767\n",
      "  batch 60 loss: 0.0017864075617126218\n",
      "  batch 70 loss: 0.002140957626079398\n",
      "  batch 80 loss: 0.0019189913902096123\n",
      "LOSS train 0.0019189913902096123 valid 0.0034016812752815893\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.002694284179290207\n",
      "  batch 20 loss: 0.001776264033503594\n",
      "  batch 30 loss: 0.0017636513635466144\n",
      "  batch 40 loss: 0.0022253284357702796\n",
      "  batch 50 loss: 0.001651731522542832\n",
      "  batch 60 loss: 0.002383145875126047\n",
      "  batch 70 loss: 0.001500239476513343\n",
      "  batch 80 loss: 0.002384829202981109\n",
      "LOSS train 0.002384829202981109 valid 0.002607982439430998\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.002171133684657889\n",
      "  batch 20 loss: 0.0015355966110973895\n",
      "  batch 30 loss: 0.0024256046830714697\n",
      "  batch 40 loss: 0.0023603274410675112\n",
      "  batch 50 loss: 0.0016301287849273649\n",
      "  batch 60 loss: 0.002435432721199504\n",
      "  batch 70 loss: 0.0024666292213396447\n",
      "  batch 80 loss: 0.0020425086441491656\n",
      "LOSS train 0.0020425086441491656 valid 0.0027682918639766285\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.0019720051517310823\n",
      "  batch 20 loss: 0.0017616409278161881\n",
      "  batch 30 loss: 0.0017263711041891837\n",
      "  batch 40 loss: 0.0029278121842253313\n",
      "  batch 50 loss: 0.0020900926573290237\n",
      "  batch 60 loss: 0.0022313941626180165\n",
      "  batch 70 loss: 0.0018624497343694203\n",
      "  batch 80 loss: 0.002272415760808144\n",
      "LOSS train 0.002272415760808144 valid 0.002599157070999354\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.002210757680984443\n",
      "  batch 20 loss: 0.0026946057420673243\n",
      "  batch 30 loss: 0.0018126745093354657\n",
      "  batch 40 loss: 0.0018453523601010603\n",
      "  batch 50 loss: 0.001707750329035207\n",
      "  batch 60 loss: 0.0019958190348347673\n",
      "  batch 70 loss: 0.0021699107395534156\n",
      "  batch 80 loss: 0.00121691779622779\n",
      "LOSS train 0.00121691779622779 valid 0.002734064688875151\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.0017882662893953238\n",
      "  batch 20 loss: 0.0013942327215488604\n",
      "  batch 30 loss: 0.001354206215967224\n",
      "  batch 40 loss: 0.0014825645862174496\n",
      "  batch 50 loss: 0.0022581968495615002\n",
      "  batch 60 loss: 0.0030488270290334185\n",
      "  batch 70 loss: 0.0020582204328661647\n",
      "  batch 80 loss: 0.0022111664183512405\n",
      "LOSS train 0.0022111664183512405 valid 0.0030189050135777506\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.0016915487864366697\n",
      "  batch 20 loss: 0.0015555012283471116\n",
      "  batch 30 loss: 0.001802651582954695\n",
      "  batch 40 loss: 0.00240613636595981\n",
      "  batch 50 loss: 0.0023517774512583855\n",
      "  batch 60 loss: 0.0012657269734972943\n",
      "  batch 70 loss: 0.002627616792324261\n",
      "  batch 80 loss: 0.0025388273829094034\n",
      "LOSS train 0.0025388273829094034 valid 0.002632441590167218\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.0017385403936032162\n",
      "  batch 20 loss: 0.0011121965257245847\n",
      "  batch 30 loss: 0.0024893647845146915\n",
      "  batch 40 loss: 0.002150039979733265\n",
      "  batch 50 loss: 0.002013422817742594\n",
      "  batch 60 loss: 0.0020997310885377376\n",
      "  batch 70 loss: 0.0018543992057857394\n",
      "  batch 80 loss: 0.0021548752760622847\n",
      "LOSS train 0.0021548752760622847 valid 0.00305378600950462\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.001477168011433605\n",
      "  batch 20 loss: 0.0025483422916522613\n",
      "  batch 30 loss: 0.0013097747133201665\n",
      "  batch 40 loss: 0.001364006218796021\n",
      "  batch 50 loss: 0.001762558935308789\n",
      "  batch 60 loss: 0.0029834558304855817\n",
      "  batch 70 loss: 0.0027496951603779964\n",
      "  batch 80 loss: 0.001708818810629964\n",
      "LOSS train 0.001708818810629964 valid 0.0030918831715371197\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.0020616390968257294\n",
      "  batch 20 loss: 0.0018489375956505683\n",
      "  batch 30 loss: 0.0017546499409604622\n",
      "  batch 40 loss: 0.0022909536417955677\n",
      "  batch 50 loss: 0.0019288343226435245\n",
      "  batch 60 loss: 0.0031164241976284757\n",
      "  batch 70 loss: 0.0015105552432942205\n",
      "  batch 80 loss: 0.0012697425960993768\n",
      "LOSS train 0.0012697425960993768 valid 0.002695230615299806\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.0017185065030048464\n",
      "  batch 20 loss: 0.001933207085903632\n",
      "  batch 30 loss: 0.0022903851896330708\n",
      "  batch 40 loss: 0.00195136374750291\n",
      "  batch 50 loss: 0.0017187605027800146\n",
      "  batch 60 loss: 0.0016839073395999548\n",
      "  batch 70 loss: 0.0022970323595018272\n",
      "  batch 80 loss: 0.0020602465678052793\n",
      "LOSS train 0.0020602465678052793 valid 0.0028925538068324385\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.002688722740992944\n",
      "  batch 20 loss: 0.0021029627956181685\n",
      "  batch 30 loss: 0.0014495713888436513\n",
      "  batch 40 loss: 0.0016833111193818695\n",
      "  batch 50 loss: 0.0026954056291288\n",
      "  batch 60 loss: 0.0018987699134981994\n",
      "  batch 70 loss: 0.001804643943796691\n",
      "  batch 80 loss: 0.0017314633197656804\n",
      "LOSS train 0.0017314633197656804 valid 0.0027144348902311322\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.002394179488373993\n",
      "  batch 20 loss: 0.0027751516887178696\n",
      "  batch 30 loss: 0.0020149440019167743\n",
      "  batch 40 loss: 0.0011357124322898926\n",
      "  batch 50 loss: 0.00272765622157749\n",
      "  batch 60 loss: 0.0021238544785887824\n",
      "  batch 70 loss: 0.0013847173680801462\n",
      "  batch 80 loss: 0.0016750575912908515\n",
      "LOSS train 0.0016750575912908515 valid 0.0025577791084742786\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.0020871921530670077\n",
      "  batch 20 loss: 0.0024220951826009695\n",
      "  batch 30 loss: 0.002080618221043551\n",
      "  batch 40 loss: 0.0013289908014712637\n",
      "  batch 50 loss: 0.0023243711816121502\n",
      "  batch 60 loss: 0.0022386671430012937\n",
      "  batch 70 loss: 0.0018611036120319113\n",
      "  batch 80 loss: 0.0020243824722228966\n",
      "LOSS train 0.0020243824722228966 valid 0.002369580823178694\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 0.0014177703322843626\n",
      "  batch 20 loss: 0.0013785428878009044\n",
      "  batch 30 loss: 0.0018188961028329231\n",
      "  batch 40 loss: 0.0025723548757468962\n",
      "  batch 50 loss: 0.002214764946700143\n",
      "  batch 60 loss: 0.0029890367934399366\n",
      "  batch 70 loss: 0.0018374120713701813\n",
      "  batch 80 loss: 0.001586343896838116\n",
      "LOSS train 0.001586343896838116 valid 0.003162779413305543\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.0019834853278553055\n",
      "  batch 20 loss: 0.0022618332563183684\n",
      "  batch 30 loss: 0.0017211475172416613\n",
      "  batch 40 loss: 0.0027643896415042945\n",
      "  batch 50 loss: 0.0022304246582052654\n",
      "  batch 60 loss: 0.0012267392479998307\n",
      "  batch 70 loss: 0.001947520226542565\n",
      "  batch 80 loss: 0.0015307262398437161\n",
      "LOSS train 0.0015307262398437161 valid 0.002628147764598907\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.0015709470835872708\n",
      "  batch 20 loss: 0.0016111210635813221\n",
      "  batch 30 loss: 0.0025552180915838106\n",
      "  batch 40 loss: 0.0026219351172699135\n",
      "  batch 50 loss: 0.0014464748414866335\n",
      "  batch 60 loss: 0.0020469260752861373\n",
      "  batch 70 loss: 0.001828448933758864\n",
      "  batch 80 loss: 0.0014553980631717423\n",
      "LOSS train 0.0014553980631717423 valid 0.002426476548462233\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.001967178835548111\n",
      "  batch 20 loss: 0.002477618592422459\n",
      "  batch 30 loss: 0.0013207247224272579\n",
      "  batch 40 loss: 0.001261413566101055\n",
      "  batch 50 loss: 0.002344768578916501\n",
      "  batch 60 loss: 0.0016305254920098377\n",
      "  batch 70 loss: 0.0022018605127982482\n",
      "  batch 80 loss: 0.002056301460538634\n",
      "LOSS train 0.002056301460538634 valid 0.003056837989233827\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.0020490120175395534\n",
      "  batch 20 loss: 0.001739300889585138\n",
      "  batch 30 loss: 0.0020961826971131357\n",
      "  batch 40 loss: 0.0018266151143279785\n",
      "  batch 50 loss: 0.002265828028635042\n",
      "  batch 60 loss: 0.0017341687427801845\n",
      "  batch 70 loss: 0.002703622123055993\n",
      "  batch 80 loss: 0.0011386767202907321\n",
      "LOSS train 0.0011386767202907321 valid 0.0024464576016907814\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.0029303229747711158\n",
      "  batch 20 loss: 0.0023060330704595343\n",
      "  batch 30 loss: 0.0015644361059230505\n",
      "  batch 40 loss: 0.0020866571424562608\n",
      "  batch 50 loss: 0.001645697442245364\n",
      "  batch 60 loss: 0.0014565119224471345\n",
      "  batch 70 loss: 0.002044508457572647\n",
      "  batch 80 loss: 0.0013432914050667932\n",
      "LOSS train 0.0013432914050667932 valid 0.00264554386081727\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.0015974206070609397\n",
      "  batch 20 loss: 0.0019814717752524302\n",
      "  batch 30 loss: 0.0018852173588925325\n",
      "  batch 40 loss: 0.001866559092945863\n",
      "  batch 50 loss: 0.0025817472324206393\n",
      "  batch 60 loss: 0.002222334112957469\n",
      "  batch 70 loss: 0.0010127014655154198\n",
      "  batch 80 loss: 0.002127269347499805\n",
      "LOSS train 0.002127269347499805 valid 0.002666898017951098\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.0014781468907131058\n",
      "  batch 20 loss: 0.0014796565019423724\n",
      "  batch 30 loss: 0.002231512913476763\n",
      "  batch 40 loss: 0.0018037848354197194\n",
      "  batch 50 loss: 0.0017597206315599578\n",
      "  batch 60 loss: 0.0021370013896103044\n",
      "  batch 70 loss: 0.0014118693174395957\n",
      "  batch 80 loss: 0.002835239581509086\n",
      "LOSS train 0.002835239581509086 valid 0.00301544982953601\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.0022354735941235047\n",
      "  batch 20 loss: 0.002115701730519959\n",
      "  batch 30 loss: 0.002306921533829609\n",
      "  batch 40 loss: 0.0019046539595137801\n",
      "  batch 50 loss: 0.0015829411443291975\n",
      "  batch 60 loss: 0.0018739037812451897\n",
      "  batch 70 loss: 0.001733203352307555\n",
      "  batch 80 loss: 0.0010394482214792333\n",
      "LOSS train 0.0010394482214792333 valid 0.0023982733189677674\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.002002715624502116\n",
      "  batch 20 loss: 0.0014132510809304222\n",
      "  batch 30 loss: 0.0023439927545837237\n",
      "  batch 40 loss: 0.001501508004923835\n",
      "  batch 50 loss: 0.0016458906863363154\n",
      "  batch 60 loss: 0.0014478518536179762\n",
      "  batch 70 loss: 0.0024104000066586194\n",
      "  batch 80 loss: 0.002026763521485009\n",
      "LOSS train 0.002026763521485009 valid 0.002635469305268998\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.0021540088415918034\n",
      "  batch 20 loss: 0.0021745563538388525\n",
      "  batch 30 loss: 0.0016786623319603677\n",
      "  batch 40 loss: 0.0012907577574083006\n",
      "  batch 50 loss: 0.0015810506562161208\n",
      "  batch 60 loss: 0.0019622917217475334\n",
      "  batch 70 loss: 0.002341134480548135\n",
      "  batch 80 loss: 0.0018441651868101872\n",
      "LOSS train 0.0018441651868101872 valid 0.0030411941477359507\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.0016983283924673742\n",
      "  batch 20 loss: 0.00144105919911226\n",
      "  batch 30 loss: 0.0021838017651930387\n",
      "  batch 40 loss: 0.0019182095646556262\n",
      "  batch 50 loss: 0.0017468323713956124\n",
      "  batch 60 loss: 0.0017379704379891336\n",
      "  batch 70 loss: 0.001780872129461386\n",
      "  batch 80 loss: 0.0015433094290415283\n",
      "LOSS train 0.0015433094290415283 valid 0.002430975901961574\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.0015231793612315414\n",
      "  batch 20 loss: 0.0017709096573412352\n",
      "  batch 30 loss: 0.0017540663736838268\n",
      "  batch 40 loss: 0.001982450918990253\n",
      "  batch 50 loss: 0.0015990428840837012\n",
      "  batch 60 loss: 0.0018970591495076406\n",
      "  batch 70 loss: 0.0024355815514809365\n",
      "  batch 80 loss: 0.0018771359563402256\n",
      "LOSS train 0.0018771359563402256 valid 0.0027654492821511667\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.001802897447282703\n",
      "  batch 20 loss: 0.0014778422823610526\n",
      "  batch 30 loss: 0.001797662114722698\n",
      "  batch 40 loss: 0.002361588712824414\n",
      "  batch 50 loss: 0.001300065707982867\n",
      "  batch 60 loss: 0.0013476454226974965\n",
      "  batch 70 loss: 0.002140276811860531\n",
      "  batch 80 loss: 0.0020700467300343917\n",
      "LOSS train 0.0020700467300343917 valid 0.0020897614044952206\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.0018042236984513239\n",
      "  batch 20 loss: 0.0017069226741568855\n",
      "  batch 30 loss: 0.0022971500932101206\n",
      "  batch 40 loss: 0.00152928479133152\n",
      "  batch 50 loss: 0.0022768468779304383\n",
      "  batch 60 loss: 0.0012543608329337985\n",
      "  batch 70 loss: 0.0012188632638014951\n",
      "  batch 80 loss: 0.0020229568608215232\n",
      "LOSS train 0.0020229568608215232 valid 0.003546865643675119\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.0016835050335089364\n",
      "  batch 20 loss: 0.0014291443710703789\n",
      "  batch 30 loss: 0.0020379634069627173\n",
      "  batch 40 loss: 0.0015454594317702685\n",
      "  batch 50 loss: 0.002541032448323222\n",
      "  batch 60 loss: 0.0019329907679775716\n",
      "  batch 70 loss: 0.0013350492540212145\n",
      "  batch 80 loss: 0.0020869070258299873\n",
      "LOSS train 0.0020869070258299873 valid 0.0028256439222514016\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.0011187018956206884\n",
      "  batch 20 loss: 0.002279095441542722\n",
      "  batch 30 loss: 0.0012539023548697515\n",
      "  batch 40 loss: 0.0017722696049531806\n",
      "  batch 50 loss: 0.001964155557311642\n",
      "  batch 60 loss: 0.001985923104973608\n",
      "  batch 70 loss: 0.002402599907327385\n",
      "  batch 80 loss: 0.0014635332792238386\n",
      "LOSS train 0.0014635332792238386 valid 0.0026955332468878625\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.0012877062161919639\n",
      "  batch 20 loss: 0.0019990653292609297\n",
      "  batch 30 loss: 0.002221073738337509\n",
      "  batch 40 loss: 0.0017717865249778698\n",
      "  batch 50 loss: 0.0020582591872539526\n",
      "  batch 60 loss: 0.0014074897803197928\n",
      "  batch 70 loss: 0.0013584364434109375\n",
      "  batch 80 loss: 0.0016730124175865057\n",
      "LOSS train 0.0016730124175865057 valid 0.0017412730497744632\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.0018016007643041122\n",
      "  batch 20 loss: 0.0012745914479580732\n",
      "  batch 30 loss: 0.0024844877510759034\n",
      "  batch 40 loss: 0.0015425869183559372\n",
      "  batch 50 loss: 0.0017080598366931098\n",
      "  batch 60 loss: 0.0018166588075246182\n",
      "  batch 70 loss: 0.0021714803843678964\n",
      "  batch 80 loss: 0.0015534177458334853\n",
      "LOSS train 0.0015534177458334853 valid 0.0025252095949690554\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.0018190743168247537\n",
      "  batch 20 loss: 0.0017466652166433505\n",
      "  batch 30 loss: 0.001328428769176071\n",
      "  batch 40 loss: 0.0018653786994718758\n",
      "  batch 50 loss: 0.0020757825351324756\n",
      "  batch 60 loss: 0.0020128525069367242\n",
      "  batch 70 loss: 0.0015119044314360508\n",
      "  batch 80 loss: 0.0014910224687810115\n",
      "LOSS train 0.0014910224687810115 valid 0.002415148301379304\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 0.0022204515636985887\n",
      "  batch 20 loss: 0.0011891816312072478\n",
      "  batch 30 loss: 0.001218309147520813\n",
      "  batch 40 loss: 0.0017128990729133875\n",
      "  batch 50 loss: 0.0019811163634471996\n",
      "  batch 60 loss: 0.0018956774032517388\n",
      "  batch 70 loss: 0.0016583256831950166\n",
      "  batch 80 loss: 0.0016564423646514114\n",
      "LOSS train 0.0016564423646514114 valid 0.002408252298419029\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 0.0018790537386394134\n",
      "  batch 20 loss: 0.0024008801608545126\n",
      "  batch 30 loss: 0.0013050727753181946\n",
      "  batch 40 loss: 0.0016513741378389568\n",
      "  batch 50 loss: 0.001793677363707502\n",
      "  batch 60 loss: 0.0016929698303556507\n",
      "  batch 70 loss: 0.001061747090108156\n",
      "  batch 80 loss: 0.0019995415120547476\n",
      "LOSS train 0.0019995415120547476 valid 0.0025276158061387833\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 0.0014276799053504873\n",
      "  batch 20 loss: 0.002018943762186609\n",
      "  batch 30 loss: 0.0015368162205959379\n",
      "  batch 40 loss: 0.0018900976205827647\n",
      "  batch 50 loss: 0.0018003583670633816\n",
      "  batch 60 loss: 0.001369048059916622\n",
      "  batch 70 loss: 0.0018012524491268778\n",
      "  batch 80 loss: 0.001674763449261718\n",
      "LOSS train 0.001674763449261718 valid 0.0027685100667167715\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 0.0016058540119956888\n",
      "  batch 20 loss: 0.0013996877480281001\n",
      "  batch 30 loss: 0.000916686619518714\n",
      "  batch 40 loss: 0.001987168616187773\n",
      "  batch 50 loss: 0.002320395091419414\n",
      "  batch 60 loss: 0.0015936346216960829\n",
      "  batch 70 loss: 0.0016454716341854692\n",
      "  batch 80 loss: 0.0021289293594804804\n",
      "LOSS train 0.0021289293594804804 valid 0.002750957013095103\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 0.0012349987464062907\n",
      "  batch 20 loss: 0.001259083054014809\n",
      "  batch 30 loss: 0.0018271518288145217\n",
      "  batch 40 loss: 0.002008568969336011\n",
      "  batch 50 loss: 0.0017484302812761144\n",
      "  batch 60 loss: 0.001531975107332073\n",
      "  batch 70 loss: 0.0025900194096152517\n",
      "  batch 80 loss: 0.0018907811420831421\n",
      "LOSS train 0.0018907811420831421 valid 0.0026700747983431938\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 0.001135013026487286\n",
      "  batch 20 loss: 0.001377346614435737\n",
      "  batch 30 loss: 0.001561277207429157\n",
      "  batch 40 loss: 0.0014402897658328585\n",
      "  batch 50 loss: 0.0011991514750661735\n",
      "  batch 60 loss: 0.0016349106018992642\n",
      "  batch 70 loss: 0.00295273942597305\n",
      "  batch 80 loss: 0.002214577780205218\n",
      "LOSS train 0.002214577780205218 valid 0.0023795616935512955\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 0.0022649482825158884\n",
      "  batch 20 loss: 0.0012793998415475017\n",
      "  batch 30 loss: 0.00156116721499302\n",
      "  batch 40 loss: 0.001725275607338972\n",
      "  batch 50 loss: 0.0017578671681803826\n",
      "  batch 60 loss: 0.00204106241142199\n",
      "  batch 70 loss: 0.0013425353950083263\n",
      "  batch 80 loss: 0.0016231565649718505\n",
      "LOSS train 0.0016231565649718505 valid 0.002422608911565476\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 0.0015705144862408816\n",
      "  batch 20 loss: 0.001496157076610416\n",
      "  batch 30 loss: 0.0018278593019715572\n",
      "  batch 40 loss: 0.001163488005107638\n",
      "  batch 50 loss: 0.001282233576012004\n",
      "  batch 60 loss: 0.0013665697545661715\n",
      "  batch 70 loss: 0.0022271102699789936\n",
      "  batch 80 loss: 0.001889887243692101\n",
      "LOSS train 0.001889887243692101 valid 0.0023112939503607778\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 0.0015216066723382938\n",
      "  batch 20 loss: 0.0018570329271938135\n",
      "  batch 30 loss: 0.001702507727623015\n",
      "  batch 40 loss: 0.0009442470905241862\n",
      "  batch 50 loss: 0.0019209173644242129\n",
      "  batch 60 loss: 0.00113913091466884\n",
      "  batch 70 loss: 0.0015056607657186305\n",
      "  batch 80 loss: 0.0017671816891152048\n",
      "LOSS train 0.0017671816891152048 valid 0.0030073933514995588\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 0.001811348669968993\n",
      "  batch 20 loss: 0.0016944676373213952\n",
      "  batch 30 loss: 0.0009937107423752423\n",
      "  batch 40 loss: 0.0020632355904353973\n",
      "  batch 50 loss: 0.0014604376492343362\n",
      "  batch 60 loss: 0.0021297907288385432\n",
      "  batch 70 loss: 0.0014053861477464125\n",
      "  batch 80 loss: 0.0021298414491411678\n",
      "LOSS train 0.0021298414491411678 valid 0.0022453496292109774\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 0.001547828937975737\n",
      "  batch 20 loss: 0.0014417570651744428\n",
      "  batch 30 loss: 0.001672377335182773\n",
      "  batch 40 loss: 0.002496874489384027\n",
      "  batch 50 loss: 0.0009326266612049494\n",
      "  batch 60 loss: 0.0015052291510471605\n",
      "  batch 70 loss: 0.001301285133570218\n",
      "  batch 80 loss: 0.0020526140730396493\n",
      "LOSS train 0.0020526140730396493 valid 0.0024871035183605273\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 0.0013326049729016631\n",
      "  batch 20 loss: 0.0017609705443078383\n",
      "  batch 30 loss: 0.0017878248348665692\n",
      "  batch 40 loss: 0.0018897496675094772\n",
      "  batch 50 loss: 0.0015225728649397752\n",
      "  batch 60 loss: 0.0013606925205579047\n",
      "  batch 70 loss: 0.0017227017740538031\n",
      "  batch 80 loss: 0.0016776178956035893\n",
      "LOSS train 0.0016776178956035893 valid 0.0027646464445479067\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 0.002237594007340249\n",
      "  batch 20 loss: 0.0015821064253714213\n",
      "  batch 30 loss: 0.0012497824121112445\n",
      "  batch 40 loss: 0.002019727160734419\n",
      "  batch 50 loss: 0.0016574930323713488\n",
      "  batch 60 loss: 0.001032517176622605\n",
      "  batch 70 loss: 0.0020703707742995904\n",
      "  batch 80 loss: 0.0016931708034576332\n",
      "LOSS train 0.0016931708034576332 valid 0.00260893249417677\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 0.0025629648727999665\n",
      "  batch 20 loss: 0.000930309234104243\n",
      "  batch 30 loss: 0.0018200257699731992\n",
      "  batch 40 loss: 0.0018120749134482139\n",
      "  batch 50 loss: 0.0017831646016531976\n",
      "  batch 60 loss: 0.0017728536959111807\n",
      "  batch 70 loss: 0.0013764051913085495\n",
      "  batch 80 loss: 0.0013555242036943581\n",
      "LOSS train 0.0013555242036943581 valid 0.0023910108919335473\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 0.0013276967768092618\n",
      "  batch 20 loss: 0.0018733354872949803\n",
      "  batch 30 loss: 0.0014386599261342781\n",
      "  batch 40 loss: 0.0015367869061037708\n",
      "  batch 50 loss: 0.001821339980671155\n",
      "  batch 60 loss: 0.0018042712353917522\n",
      "  batch 70 loss: 0.001327393490828399\n",
      "  batch 80 loss: 0.0021688156364007226\n",
      "LOSS train 0.0021688156364007226 valid 0.002674830950109026\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 0.0009913760790936977\n",
      "  batch 20 loss: 0.0016143966521383391\n",
      "  batch 30 loss: 0.002361565398592802\n",
      "  batch 40 loss: 0.0017800122700464271\n",
      "  batch 50 loss: 0.002154498876996058\n",
      "  batch 60 loss: 0.001252944582239479\n",
      "  batch 70 loss: 0.0017435425299481722\n",
      "  batch 80 loss: 0.0012329454640962467\n",
      "LOSS train 0.0012329454640962467 valid 0.0024232914245021675\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 0.002021403957382972\n",
      "  batch 20 loss: 0.0013137674784218234\n",
      "  batch 30 loss: 0.0018879114701917388\n",
      "  batch 40 loss: 0.0013832812905775426\n",
      "  batch 50 loss: 0.0013201281213696347\n",
      "  batch 60 loss: 0.0018511088881837169\n",
      "  batch 70 loss: 0.001664698040872281\n",
      "  batch 80 loss: 0.0011047454229810682\n",
      "LOSS train 0.0011047454229810682 valid 0.002078546853390435\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 0.0015192741624389327\n",
      "  batch 20 loss: 0.0018467151218146682\n",
      "  batch 30 loss: 0.001928694987691415\n",
      "  batch 40 loss: 0.0015854883050678836\n",
      "  batch 50 loss: 0.001029938959163701\n",
      "  batch 60 loss: 0.0011626417905745256\n",
      "  batch 70 loss: 0.0022325309025632125\n",
      "  batch 80 loss: 0.001926459120409163\n",
      "LOSS train 0.001926459120409163 valid 0.00227286088762412\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 0.00220360121406884\n",
      "  batch 20 loss: 0.0010505551874189223\n",
      "  batch 30 loss: 0.0010685025675627458\n",
      "  batch 40 loss: 0.0017171300164534387\n",
      "  batch 50 loss: 0.001739521859491333\n",
      "  batch 60 loss: 0.00160412328646089\n",
      "  batch 70 loss: 0.0022445738053704645\n",
      "  batch 80 loss: 0.0013688396233646927\n",
      "LOSS train 0.0013688396233646927 valid 0.002257357727003182\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 0.001412522084297052\n",
      "  batch 20 loss: 0.0015940203928153096\n",
      "  batch 30 loss: 0.0013944073374773324\n",
      "  batch 40 loss: 0.0020648600553613504\n",
      "  batch 50 loss: 0.0017110102223682588\n",
      "  batch 60 loss: 0.0014886564881862797\n",
      "  batch 70 loss: 0.0015792685814631114\n",
      "  batch 80 loss: 0.0016222643660739778\n",
      "LOSS train 0.0016222643660739778 valid 0.0024399591273868283\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 0.0012455490534875936\n",
      "  batch 20 loss: 0.0010541399079897929\n",
      "  batch 30 loss: 0.0022086615782257015\n",
      "  batch 40 loss: 0.00241750945970125\n",
      "  batch 50 loss: 0.0012068200136070573\n",
      "  batch 60 loss: 0.001628603895244396\n",
      "  batch 70 loss: 0.0013848811182697318\n",
      "  batch 80 loss: 0.0014941953286552235\n",
      "LOSS train 0.0014941953286552235 valid 0.002855208392775239\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 0.0015924627395463632\n",
      "  batch 20 loss: 0.0010382527117712924\n",
      "  batch 30 loss: 0.0023544946011043065\n",
      "  batch 40 loss: 0.000982929514884745\n",
      "  batch 50 loss: 0.0014612951688718566\n",
      "  batch 60 loss: 0.0013807520807517903\n",
      "  batch 70 loss: 0.002002909034933964\n",
      "  batch 80 loss: 0.0016742751655328902\n",
      "LOSS train 0.0016742751655328902 valid 0.0031556001667195233\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 0.0011627785958182812\n",
      "  batch 20 loss: 0.0021120153456081424\n",
      "  batch 30 loss: 0.0010570187391749641\n",
      "  batch 40 loss: 0.0017567285104178154\n",
      "  batch 50 loss: 0.0013544108451128522\n",
      "  batch 60 loss: 0.0021793872622765777\n",
      "  batch 70 loss: 0.0016808633288121655\n",
      "  batch 80 loss: 0.0013399476187601067\n",
      "LOSS train 0.0013399476187601067 valid 0.0030243743201026517\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 0.0014587300520361168\n",
      "  batch 20 loss: 0.0016957750990002295\n",
      "  batch 30 loss: 0.0010809592799262192\n",
      "  batch 40 loss: 0.0027318995376163003\n",
      "  batch 50 loss: 0.0017048132202660327\n",
      "  batch 60 loss: 0.0019504712654395463\n",
      "  batch 70 loss: 0.0013837744120337447\n",
      "  batch 80 loss: 0.0008269108389981738\n",
      "LOSS train 0.0008269108389981738 valid 0.0025711288194452207\n",
      "EPOCH 101:\n",
      "  batch 10 loss: 0.0013235703823283984\n",
      "  batch 20 loss: 0.0012638865833309865\n",
      "  batch 30 loss: 0.0011231207843195533\n",
      "  batch 40 loss: 0.0018042399692149047\n",
      "  batch 50 loss: 0.001268192151923131\n",
      "  batch 60 loss: 0.002226198824428138\n",
      "  batch 70 loss: 0.001641641769947455\n",
      "  batch 80 loss: 0.0017007073122670135\n",
      "LOSS train 0.0017007073122670135 valid 0.0021977406040332424\n",
      "EPOCH 102:\n",
      "  batch 10 loss: 0.0014887053519430538\n",
      "  batch 20 loss: 0.0008137949335036865\n",
      "  batch 30 loss: 0.0021793302607193256\n",
      "  batch 40 loss: 0.00128681813946514\n",
      "  batch 50 loss: 0.002043670920602381\n",
      "  batch 60 loss: 0.0014180160272587727\n",
      "  batch 70 loss: 0.0011337725360760942\n",
      "  batch 80 loss: 0.001439593028226227\n",
      "LOSS train 0.001439593028226227 valid 0.002080459834778594\n",
      "EPOCH 103:\n",
      "  batch 10 loss: 0.0014677723363547557\n",
      "  batch 20 loss: 0.0016242033309765702\n",
      "  batch 30 loss: 0.001927125679958408\n",
      "  batch 40 loss: 0.0018598425486061388\n",
      "  batch 50 loss: 0.0015821225970285013\n",
      "  batch 60 loss: 0.0013094376670665041\n",
      "  batch 70 loss: 0.0014136938672663745\n",
      "  batch 80 loss: 0.0015083501611172778\n",
      "LOSS train 0.0015083501611172778 valid 0.0023507099904691133\n",
      "EPOCH 104:\n",
      "  batch 10 loss: 0.0014471808107032302\n",
      "  batch 20 loss: 0.0013849206679765302\n",
      "  batch 30 loss: 0.001635362570561938\n",
      "  batch 40 loss: 0.001819209956056511\n",
      "  batch 50 loss: 0.001881584452075913\n",
      "  batch 60 loss: 0.001297545882079021\n",
      "  batch 70 loss: 0.0010867273098881468\n",
      "  batch 80 loss: 0.001618166520052\n",
      "LOSS train 0.001618166520052 valid 0.002213255323804333\n",
      "EPOCH 105:\n",
      "  batch 10 loss: 0.001103679880901609\n",
      "  batch 20 loss: 0.002510177838053096\n",
      "  batch 30 loss: 0.0010672168545568183\n",
      "  batch 40 loss: 0.0014222011573906458\n",
      "  batch 50 loss: 0.0014631779445096527\n",
      "  batch 60 loss: 0.0019807091530083198\n",
      "  batch 70 loss: 0.0011815720150252674\n",
      "  batch 80 loss: 0.0015432490073294502\n",
      "LOSS train 0.0015432490073294502 valid 0.002168298890210281\n",
      "EPOCH 106:\n",
      "  batch 10 loss: 0.0017604544218443152\n",
      "  batch 20 loss: 0.0016304116506887568\n",
      "  batch 30 loss: 0.0017061153035001554\n",
      "  batch 40 loss: 0.0006172901697368615\n",
      "  batch 50 loss: 0.0017275953236890018\n",
      "  batch 60 loss: 0.0014588843720844125\n",
      "  batch 70 loss: 0.0011979090212832944\n",
      "  batch 80 loss: 0.0018642038784264514\n",
      "LOSS train 0.0018642038784264514 valid 0.0021656015192002085\n",
      "EPOCH 107:\n",
      "  batch 10 loss: 0.002478235218552527\n",
      "  batch 20 loss: 0.0013591048564251195\n",
      "  batch 30 loss: 0.0010144889605442132\n",
      "  batch 40 loss: 0.0012209619797772576\n",
      "  batch 50 loss: 0.0019649776538926745\n",
      "  batch 60 loss: 0.0008958368556307051\n",
      "  batch 70 loss: 0.001315854856113674\n",
      "  batch 80 loss: 0.0016978227890831477\n",
      "LOSS train 0.0016978227890831477 valid 0.0024473678315644065\n",
      "EPOCH 108:\n",
      "  batch 10 loss: 0.001110893807390312\n",
      "  batch 20 loss: 0.001990625338805785\n",
      "  batch 30 loss: 0.0016502676458117094\n",
      "  batch 40 loss: 0.0014831719034134494\n",
      "  batch 50 loss: 0.0016112147803141851\n",
      "  batch 60 loss: 0.001041517946600834\n",
      "  batch 70 loss: 0.0017805259025692522\n",
      "  batch 80 loss: 0.0012087474528414076\n",
      "LOSS train 0.0012087474528414076 valid 0.002453983683626575\n",
      "EPOCH 109:\n",
      "  batch 10 loss: 0.001611385845831137\n",
      "  batch 20 loss: 0.0009925074790601228\n",
      "  batch 30 loss: 0.001089727344236735\n",
      "  batch 40 loss: 0.0013874918391763914\n",
      "  batch 50 loss: 0.0014333423852008309\n",
      "  batch 60 loss: 0.0014750339660679402\n",
      "  batch 70 loss: 0.0021193075922326444\n",
      "  batch 80 loss: 0.001944371063837025\n",
      "LOSS train 0.001944371063837025 valid 0.0021515254933001417\n",
      "EPOCH 110:\n",
      "  batch 10 loss: 0.0011564998052676855\n",
      "  batch 20 loss: 0.0007623317309082723\n",
      "  batch 30 loss: 0.0013850426913393221\n",
      "  batch 40 loss: 0.0019417075969556663\n",
      "  batch 50 loss: 0.0014666650923572887\n",
      "  batch 60 loss: 0.0017831492658501702\n",
      "  batch 70 loss: 0.0011534445845882146\n",
      "  batch 80 loss: 0.001864817931550533\n",
      "LOSS train 0.001864817931550533 valid 0.002333327786636801\n",
      "EPOCH 111:\n",
      "  batch 10 loss: 0.0014737913805845438\n",
      "  batch 20 loss: 0.0018733181456127568\n",
      "  batch 30 loss: 0.0013398166374940957\n",
      "  batch 40 loss: 0.0014151837527151656\n",
      "  batch 50 loss: 0.0010435333055909268\n",
      "  batch 60 loss: 0.001708926438425351\n",
      "  batch 70 loss: 0.0016784063127943227\n",
      "  batch 80 loss: 0.0013410833959994761\n",
      "LOSS train 0.0013410833959994761 valid 0.002050503823888903\n",
      "EPOCH 112:\n",
      "  batch 10 loss: 0.001251125026868749\n",
      "  batch 20 loss: 0.0016943025262037281\n",
      "  batch 30 loss: 0.001026344156224468\n",
      "  batch 40 loss: 0.0016643128721625545\n",
      "  batch 50 loss: 0.001560948528094741\n",
      "  batch 60 loss: 0.0015781258607432847\n",
      "  batch 70 loss: 0.002106824346628855\n",
      "  batch 80 loss: 0.0013436389602702547\n",
      "LOSS train 0.0013436389602702547 valid 0.0019706014548091844\n",
      "EPOCH 113:\n",
      "  batch 10 loss: 0.0012193672598414195\n",
      "  batch 20 loss: 0.0015858354729630264\n",
      "  batch 30 loss: 0.001212112133481469\n",
      "  batch 40 loss: 0.0013640517726742018\n",
      "  batch 50 loss: 0.0017186282118530015\n",
      "  batch 60 loss: 0.0009852833622403523\n",
      "  batch 70 loss: 0.0018084193612139643\n",
      "  batch 80 loss: 0.0013144216417856568\n",
      "LOSS train 0.0013144216417856568 valid 0.0017672335348152047\n",
      "EPOCH 114:\n",
      "  batch 10 loss: 0.002080871831026343\n",
      "  batch 20 loss: 0.0011033086245561207\n",
      "  batch 30 loss: 0.002101010640797085\n",
      "  batch 40 loss: 0.0018619555946543186\n",
      "  batch 50 loss: 0.001183920838417407\n",
      "  batch 60 loss: 0.001060456342725047\n",
      "  batch 70 loss: 0.0008719134784939797\n",
      "  batch 80 loss: 0.0018182786326462973\n",
      "LOSS train 0.0018182786326462973 valid 0.0021035696581520823\n",
      "EPOCH 115:\n",
      "  batch 10 loss: 0.001476003390655478\n",
      "  batch 20 loss: 0.0011111403157372025\n",
      "  batch 30 loss: 0.0011746558843469756\n",
      "  batch 40 loss: 0.001788278677031485\n",
      "  batch 50 loss: 0.0017327954643292288\n",
      "  batch 60 loss: 0.0013909854560154144\n",
      "  batch 70 loss: 0.0013044412051158361\n",
      "  batch 80 loss: 0.00164436003825017\n",
      "LOSS train 0.00164436003825017 valid 0.0020359309990135442\n",
      "EPOCH 116:\n",
      "  batch 10 loss: 0.0011788358275282463\n",
      "  batch 20 loss: 0.0012014075106719702\n",
      "  batch 30 loss: 0.0012809562858763003\n",
      "  batch 40 loss: 0.0020704615598219787\n",
      "  batch 50 loss: 0.001582315387224753\n",
      "  batch 60 loss: 0.0017833005440706985\n",
      "  batch 70 loss: 0.0013060605872510678\n",
      "  batch 80 loss: 0.0013484683306273838\n",
      "LOSS train 0.0013484683306273838 valid 0.001931925006961137\n",
      "EPOCH 117:\n",
      "  batch 10 loss: 0.0013215171890351486\n",
      "  batch 20 loss: 0.0015581234522073829\n",
      "  batch 30 loss: 0.0021826599301959957\n",
      "  batch 40 loss: 0.0017217780832879726\n",
      "  batch 50 loss: 0.001015469522104695\n",
      "  batch 60 loss: 0.0014387949671572642\n",
      "  batch 70 loss: 0.0011663044872818773\n",
      "  batch 80 loss: 0.0013482490650631007\n",
      "LOSS train 0.0013482490650631007 valid 0.002269372281966753\n",
      "EPOCH 118:\n",
      "  batch 10 loss: 0.001506936976892348\n",
      "  batch 20 loss: 0.0015499423020457926\n",
      "  batch 30 loss: 0.0011079875788141181\n",
      "  batch 40 loss: 0.0011746426113575126\n",
      "  batch 50 loss: 0.0008987343114540636\n",
      "  batch 60 loss: 0.001606122467444493\n",
      "  batch 70 loss: 0.0013867372726394933\n",
      "  batch 80 loss: 0.0017870901656863226\n",
      "LOSS train 0.0017870901656863226 valid 0.0021025211632741047\n",
      "EPOCH 119:\n",
      "  batch 10 loss: 0.0011191808087801292\n",
      "  batch 20 loss: 0.001931979320221444\n",
      "  batch 30 loss: 0.0012757493367189453\n",
      "  batch 40 loss: 0.0010373037283159191\n",
      "  batch 50 loss: 0.0009230200523802524\n",
      "  batch 60 loss: 0.0023352460858632185\n",
      "  batch 70 loss: 0.0009743918309936817\n",
      "  batch 80 loss: 0.0018399677559216344\n",
      "LOSS train 0.0018399677559216344 valid 0.0021044020638555593\n",
      "EPOCH 120:\n",
      "  batch 10 loss: 0.002071468039980573\n",
      "  batch 20 loss: 0.0012750103485927866\n",
      "  batch 30 loss: 0.0011080133510404267\n",
      "  batch 40 loss: 0.0011391710708664959\n",
      "  batch 50 loss: 0.0011330213408825784\n",
      "  batch 60 loss: 0.0011291812868023498\n",
      "  batch 70 loss: 0.00200791411086243\n",
      "  batch 80 loss: 0.0017425184442799945\n",
      "LOSS train 0.0017425184442799945 valid 0.002335881834765132\n",
      "EPOCH 121:\n",
      "  batch 10 loss: 0.0014688481607493032\n",
      "  batch 20 loss: 0.0014658077936928748\n",
      "  batch 30 loss: 0.0008824121692214248\n",
      "  batch 40 loss: 0.0014854082751469377\n",
      "  batch 50 loss: 0.0015206704121908388\n",
      "  batch 60 loss: 0.0017819325175878475\n",
      "  batch 70 loss: 0.0015105061871452108\n",
      "  batch 80 loss: 0.0013940262326059382\n",
      "LOSS train 0.0013940262326059382 valid 0.0022605012791063928\n",
      "EPOCH 122:\n",
      "  batch 10 loss: 0.000931991471298943\n",
      "  batch 20 loss: 0.001240073624148863\n",
      "  batch 30 loss: 0.0015153122514675488\n",
      "  batch 40 loss: 0.0016357202611629873\n",
      "  batch 50 loss: 0.0011522547295555797\n",
      "  batch 60 loss: 0.001799897618172963\n",
      "  batch 70 loss: 0.0016076143049417624\n",
      "  batch 80 loss: 0.0012543496488831352\n",
      "LOSS train 0.0012543496488831352 valid 0.002310003686047821\n",
      "EPOCH 123:\n",
      "  batch 10 loss: 0.0013828190107233241\n",
      "  batch 20 loss: 0.0011712569735095712\n",
      "  batch 30 loss: 0.0013516462945290185\n",
      "  batch 40 loss: 0.0020114565984272305\n",
      "  batch 50 loss: 0.0016089847117200407\n",
      "  batch 60 loss: 0.0011960434884599636\n",
      "  batch 70 loss: 0.0012400024885664608\n",
      "  batch 80 loss: 0.0012323891420919608\n",
      "LOSS train 0.0012323891420919608 valid 0.0021770720786207674\n",
      "EPOCH 124:\n",
      "  batch 10 loss: 0.00139692973707497\n",
      "  batch 20 loss: 0.0012119126369952936\n",
      "  batch 30 loss: 0.001495777067839299\n",
      "  batch 40 loss: 0.0013955214039469866\n",
      "  batch 50 loss: 0.0015438253710726713\n",
      "  batch 60 loss: 0.0015090097607810549\n",
      "  batch 70 loss: 0.0014198690997261564\n",
      "  batch 80 loss: 0.001217057979283709\n",
      "LOSS train 0.001217057979283709 valid 0.0020110362598279607\n",
      "EPOCH 125:\n",
      "  batch 10 loss: 0.0014499564060997728\n",
      "  batch 20 loss: 0.0011177525926882482\n",
      "  batch 30 loss: 0.001000459621070604\n",
      "  batch 40 loss: 0.0015158836897512628\n",
      "  batch 50 loss: 0.0015613867549859606\n",
      "  batch 60 loss: 0.0016825798356023824\n",
      "  batch 70 loss: 0.001637988443883387\n",
      "  batch 80 loss: 0.0014785631367885798\n",
      "LOSS train 0.0014785631367885798 valid 0.002032587398675787\n",
      "EPOCH 126:\n",
      "  batch 10 loss: 0.0016208418726762375\n",
      "  batch 20 loss: 0.001041210982316443\n",
      "  batch 30 loss: 0.0012466205132682262\n",
      "  batch 40 loss: 0.0014444971078546587\n",
      "  batch 50 loss: 0.0018488489687456422\n",
      "  batch 60 loss: 0.0007261286593802652\n",
      "  batch 70 loss: 0.0014036896383686325\n",
      "  batch 80 loss: 0.001607315262543807\n",
      "LOSS train 0.001607315262543807 valid 0.0022197835466658943\n",
      "EPOCH 127:\n",
      "  batch 10 loss: 0.0014720801740395473\n",
      "  batch 20 loss: 0.001479869707918624\n",
      "  batch 30 loss: 0.000981995132809743\n",
      "  batch 40 loss: 0.0018195011808302298\n",
      "  batch 50 loss: 0.00174651718210157\n",
      "  batch 60 loss: 0.0010076834517576572\n",
      "  batch 70 loss: 0.0014216209166988847\n",
      "  batch 80 loss: 0.0013933064507398285\n",
      "LOSS train 0.0013933064507398285 valid 0.0022910884186239855\n",
      "EPOCH 128:\n",
      "  batch 10 loss: 0.0014970511363628703\n",
      "  batch 20 loss: 0.0012098943494265769\n",
      "  batch 30 loss: 0.0018232460210981572\n",
      "  batch 40 loss: 0.0011647598430045037\n",
      "  batch 50 loss: 0.0014094903801378678\n",
      "  batch 60 loss: 0.0015137776413212123\n",
      "  batch 70 loss: 0.0008596663215939771\n",
      "  batch 80 loss: 0.0014485788080833117\n",
      "LOSS train 0.0014485788080833117 valid 0.002127283086838361\n",
      "EPOCH 129:\n",
      "  batch 10 loss: 0.0014342443690679829\n",
      "  batch 20 loss: 0.001478234786890198\n",
      "  batch 30 loss: 0.0016456793880365694\n",
      "  batch 40 loss: 0.0010300200519964165\n",
      "  batch 50 loss: 0.0010070952176818083\n",
      "  batch 60 loss: 0.001036558229259299\n",
      "  batch 70 loss: 0.001489049871622683\n",
      "  batch 80 loss: 0.00171346464240969\n",
      "LOSS train 0.00171346464240969 valid 0.002281544196671348\n",
      "EPOCH 130:\n",
      "  batch 10 loss: 0.001149622237778658\n",
      "  batch 20 loss: 0.0012763726556727305\n",
      "  batch 30 loss: 0.00143432786142057\n",
      "  batch 40 loss: 0.001625318334788517\n",
      "  batch 50 loss: 0.0014340159725691136\n",
      "  batch 60 loss: 0.0013812003839177578\n",
      "  batch 70 loss: 0.0015336327020577301\n",
      "  batch 80 loss: 0.001275598052041005\n",
      "LOSS train 0.001275598052041005 valid 0.00201581800060012\n",
      "EPOCH 131:\n",
      "  batch 10 loss: 0.0012170922599636924\n",
      "  batch 20 loss: 0.001090266322688649\n",
      "  batch 30 loss: 0.0013745630010475907\n",
      "  batch 40 loss: 0.0014470473451524413\n",
      "  batch 50 loss: 0.0017583734545382867\n",
      "  batch 60 loss: 0.001108613484103671\n",
      "  batch 70 loss: 0.0013255287681317896\n",
      "  batch 80 loss: 0.0013642146063432392\n",
      "LOSS train 0.0013642146063432392 valid 0.002195361717576816\n",
      "EPOCH 132:\n",
      "  batch 10 loss: 0.001545138234382648\n",
      "  batch 20 loss: 0.001678015358038465\n",
      "  batch 30 loss: 0.0010898345117539066\n",
      "  batch 40 loss: 0.0010306839523309464\n",
      "  batch 50 loss: 0.0010436828176068503\n",
      "  batch 60 loss: 0.001471904464006002\n",
      "  batch 70 loss: 0.0016982701534061563\n",
      "  batch 80 loss: 0.001476773048176483\n",
      "LOSS train 0.001476773048176483 valid 0.0022196713189441652\n",
      "EPOCH 133:\n",
      "  batch 10 loss: 0.0017354868204108697\n",
      "  batch 20 loss: 0.0015782443921125377\n",
      "  batch 30 loss: 0.0012004051074143262\n",
      "  batch 40 loss: 0.001052952345742142\n",
      "  batch 50 loss: 0.0014699517416715934\n",
      "  batch 60 loss: 0.0014817075315363582\n",
      "  batch 70 loss: 0.001174918550395887\n",
      "  batch 80 loss: 0.0011183045600432705\n",
      "LOSS train 0.0011183045600432705 valid 0.00221950512443982\n",
      "EPOCH 134:\n",
      "  batch 10 loss: 0.0015311714531776488\n",
      "  batch 20 loss: 0.001621604845439606\n",
      "  batch 30 loss: 0.0017565405975176417\n",
      "  batch 40 loss: 0.0010300264357624655\n",
      "  batch 50 loss: 0.0007313271864632043\n",
      "  batch 60 loss: 0.0012071990451886449\n",
      "  batch 70 loss: 0.0015530764368691052\n",
      "  batch 80 loss: 0.0013372205490441047\n",
      "LOSS train 0.0013372205490441047 valid 0.0021454007160082254\n",
      "EPOCH 135:\n",
      "  batch 10 loss: 0.0014471814389878546\n",
      "  batch 20 loss: 0.0011625401334129037\n",
      "  batch 30 loss: 0.0019957648930358117\n",
      "  batch 40 loss: 0.0015295302639515285\n",
      "  batch 50 loss: 0.0011877602051470148\n",
      "  batch 60 loss: 0.0010704992420755843\n",
      "  batch 70 loss: 0.0013430150797546503\n",
      "  batch 80 loss: 0.0008068759756270083\n",
      "LOSS train 0.0008068759756270083 valid 0.0015981289895353257\n",
      "EPOCH 136:\n",
      "  batch 10 loss: 0.0009931480379179903\n",
      "  batch 20 loss: 0.0015046065919193552\n",
      "  batch 30 loss: 0.0008296656684024129\n",
      "  batch 40 loss: 0.001693826949696131\n",
      "  batch 50 loss: 0.001959385545285386\n",
      "  batch 60 loss: 0.0008092247001854957\n",
      "  batch 70 loss: 0.001305986184684116\n",
      "  batch 80 loss: 0.0013861355805545373\n",
      "LOSS train 0.0013861355805545373 valid 0.0021424563864638913\n",
      "EPOCH 137:\n",
      "  batch 10 loss: 0.0009144695324209806\n",
      "  batch 20 loss: 0.0009218013502390931\n",
      "  batch 30 loss: 0.0013566471122118174\n",
      "  batch 40 loss: 0.002136074826057666\n",
      "  batch 50 loss: 0.0011495698045905557\n",
      "  batch 60 loss: 0.0015745348496523093\n",
      "  batch 70 loss: 0.0015027107159028218\n",
      "  batch 80 loss: 0.001236002832297345\n",
      "LOSS train 0.001236002832297345 valid 0.0022343794015705497\n",
      "EPOCH 138:\n",
      "  batch 10 loss: 0.0014211150449000344\n",
      "  batch 20 loss: 0.0008936412000991823\n",
      "  batch 30 loss: 0.001600050607379444\n",
      "  batch 40 loss: 0.0018674016991212738\n",
      "  batch 50 loss: 0.0011652101277263682\n",
      "  batch 60 loss: 0.0014401848207739932\n",
      "  batch 70 loss: 0.000954880327049068\n",
      "  batch 80 loss: 0.0012605330292899453\n",
      "LOSS train 0.0012605330292899453 valid 0.0022523792400079403\n",
      "EPOCH 139:\n",
      "  batch 10 loss: 0.0016452394787734193\n",
      "  batch 20 loss: 0.001366082349682074\n",
      "  batch 30 loss: 0.0013040266667985634\n",
      "  batch 40 loss: 0.001446955309643272\n",
      "  batch 50 loss: 0.0011039841265699124\n",
      "  batch 60 loss: 0.00112409154308466\n",
      "  batch 70 loss: 0.0012435534326812104\n",
      "  batch 80 loss: 0.0013012729083584419\n",
      "LOSS train 0.0013012729083584419 valid 0.0019271200903722273\n",
      "EPOCH 140:\n",
      "  batch 10 loss: 0.0015457127055697128\n",
      "  batch 20 loss: 0.0017476175573733598\n",
      "  batch 30 loss: 0.001723721443283921\n",
      "  batch 40 loss: 0.0008275222755742106\n",
      "  batch 50 loss: 0.0014250723769805517\n",
      "  batch 60 loss: 0.0011677706512045916\n",
      "  batch 70 loss: 0.0011744346095269975\n",
      "  batch 80 loss: 0.001073839405739818\n",
      "LOSS train 0.001073839405739818 valid 0.0017432973652057625\n",
      "EPOCH 141:\n",
      "  batch 10 loss: 0.001137854119417625\n",
      "  batch 20 loss: 0.0016682192849543753\n",
      "  batch 30 loss: 0.0011436633832090592\n",
      "  batch 40 loss: 0.00160368887891309\n",
      "  batch 50 loss: 0.0009488165577920427\n",
      "  batch 60 loss: 0.0013409421086862494\n",
      "  batch 70 loss: 0.0011282054895559667\n",
      "  batch 80 loss: 0.0013392115644933257\n",
      "LOSS train 0.0013392115644933257 valid 0.002180871704363199\n",
      "EPOCH 142:\n",
      "  batch 10 loss: 0.000918277378991661\n",
      "  batch 20 loss: 0.0010786082492131755\n",
      "  batch 30 loss: 0.0014985141911495249\n",
      "  batch 40 loss: 0.0010416539638583799\n",
      "  batch 50 loss: 0.0013621234605352584\n",
      "  batch 60 loss: 0.0016030632825732027\n",
      "  batch 70 loss: 0.00152505222424395\n",
      "  batch 80 loss: 0.0016598738394577595\n",
      "LOSS train 0.0016598738394577595 valid 0.0024665441715933413\n",
      "EPOCH 143:\n",
      "  batch 10 loss: 0.0010738508239285238\n",
      "  batch 20 loss: 0.0014512624578571832\n",
      "  batch 30 loss: 0.0014788850348509187\n",
      "  batch 40 loss: 0.001267437888850509\n",
      "  batch 50 loss: 0.0014934307665328106\n",
      "  batch 60 loss: 0.0011670231279481413\n",
      "  batch 70 loss: 0.0009775868843235002\n",
      "  batch 80 loss: 0.0013953426211514852\n",
      "LOSS train 0.0013953426211514852 valid 0.0020297393367263794\n",
      "EPOCH 144:\n",
      "  batch 10 loss: 0.0007745205834282842\n",
      "  batch 20 loss: 0.0014666924645666767\n",
      "  batch 30 loss: 0.0014557202187688745\n",
      "  batch 40 loss: 0.00148518427183717\n",
      "  batch 50 loss: 0.0014149025943993366\n",
      "  batch 60 loss: 0.0012094607935352996\n",
      "  batch 70 loss: 0.0015622277707279864\n",
      "  batch 80 loss: 0.0009366233320406536\n",
      "LOSS train 0.0009366233320406536 valid 0.0018401691612007199\n",
      "EPOCH 145:\n",
      "  batch 10 loss: 0.0010514046540777144\n",
      "  batch 20 loss: 0.0009161438857972825\n",
      "  batch 30 loss: 0.0012838124834388509\n",
      "  batch 40 loss: 0.0016564057570008118\n",
      "  batch 50 loss: 0.0011185100528166459\n",
      "  batch 60 loss: 0.0017773334893661286\n",
      "  batch 70 loss: 0.0009376317614794516\n",
      "  batch 80 loss: 0.0013243109996551538\n",
      "LOSS train 0.0013243109996551538 valid 0.0018599739846195007\n",
      "EPOCH 146:\n",
      "  batch 10 loss: 0.0014915953658885428\n",
      "  batch 20 loss: 0.001292221748360589\n",
      "  batch 30 loss: 0.0008458651920761895\n",
      "  batch 40 loss: 0.0009665848596796423\n",
      "  batch 50 loss: 0.0011034634267389266\n",
      "  batch 60 loss: 0.0010972405404231721\n",
      "  batch 70 loss: 0.0019248070730355949\n",
      "  batch 80 loss: 0.001328518643759935\n",
      "LOSS train 0.001328518643759935 valid 0.0026167361023499325\n",
      "EPOCH 147:\n",
      "  batch 10 loss: 0.0013296163943948614\n",
      "  batch 20 loss: 0.0015709616777542124\n",
      "  batch 30 loss: 0.0013761998599420623\n",
      "  batch 40 loss: 0.00155997618990682\n",
      "  batch 50 loss: 0.0014796166366068063\n",
      "  batch 60 loss: 0.0008255662977489919\n",
      "  batch 70 loss: 0.0009227473990677026\n",
      "  batch 80 loss: 0.0008923829635023139\n",
      "LOSS train 0.0008923829635023139 valid 0.002016606364600193\n",
      "EPOCH 148:\n",
      "  batch 10 loss: 0.0013669947865196264\n",
      "  batch 20 loss: 0.0011716212089197598\n",
      "  batch 30 loss: 0.001252208895368767\n",
      "  batch 40 loss: 0.0012052418160976685\n",
      "  batch 50 loss: 0.001265864002783701\n",
      "  batch 60 loss: 0.0012502461984581713\n",
      "  batch 70 loss: 0.0012435652264514373\n",
      "  batch 80 loss: 0.0013077518316237046\n",
      "LOSS train 0.0013077518316237046 valid 0.0018697380296316623\n",
      "EPOCH 149:\n",
      "  batch 10 loss: 0.001213095608261483\n",
      "  batch 20 loss: 0.000873321801992688\n",
      "  batch 30 loss: 0.0012667967490131104\n",
      "  batch 40 loss: 0.0015360131803788591\n",
      "  batch 50 loss: 0.0010235780347954914\n",
      "  batch 60 loss: 0.0011892737911210816\n",
      "  batch 70 loss: 0.0009795642859955934\n",
      "  batch 80 loss: 0.0016392220453724348\n",
      "LOSS train 0.0016392220453724348 valid 0.0023682593941794037\n",
      "EPOCH 150:\n",
      "  batch 10 loss: 0.0011714646931864081\n",
      "  batch 20 loss: 0.0012218053933906957\n",
      "  batch 30 loss: 0.0008414198817831675\n",
      "  batch 40 loss: 0.0017673351736220866\n",
      "  batch 50 loss: 0.0013107686520243077\n",
      "  batch 60 loss: 0.0014187162055748104\n",
      "  batch 70 loss: 0.0014306578348737276\n",
      "  batch 80 loss: 0.0010761289783772555\n",
      "LOSS train 0.0010761289783772555 valid 0.0020303917494811684\n",
      "EPOCH 151:\n",
      "  batch 10 loss: 0.0014266507266825102\n",
      "  batch 20 loss: 0.0016065075478763902\n",
      "  batch 30 loss: 0.001469552994916512\n",
      "  batch 40 loss: 0.0010505958625799394\n",
      "  batch 50 loss: 0.0009949737429906236\n",
      "  batch 60 loss: 0.0008100928135036156\n",
      "  batch 70 loss: 0.0011001146021527575\n",
      "  batch 80 loss: 0.0012620137739872916\n",
      "LOSS train 0.0012620137739872916 valid 0.0019816347947562464\n",
      "EPOCH 152:\n",
      "  batch 10 loss: 0.0010575431865675\n",
      "  batch 20 loss: 0.0012598734475830042\n",
      "  batch 30 loss: 0.0006499328804807191\n",
      "  batch 40 loss: 0.0011737674611680404\n",
      "  batch 50 loss: 0.0006692317722297503\n",
      "  batch 60 loss: 0.0011834771559335877\n",
      "  batch 70 loss: 0.0021140760375203626\n",
      "  batch 80 loss: 0.001608509821488724\n",
      "LOSS train 0.001608509821488724 valid 0.0021525452593505177\n",
      "EPOCH 153:\n",
      "  batch 10 loss: 0.0016090218621286566\n",
      "  batch 20 loss: 0.0014993603997936588\n",
      "  batch 30 loss: 0.0008150891928949023\n",
      "  batch 40 loss: 0.0011971859711820798\n",
      "  batch 50 loss: 0.001168637797206884\n",
      "  batch 60 loss: 0.0015237417713024116\n",
      "  batch 70 loss: 0.0010805268751596486\n",
      "  batch 80 loss: 0.001019851358358892\n",
      "LOSS train 0.001019851358358892 valid 0.0020111610582262073\n",
      "EPOCH 154:\n",
      "  batch 10 loss: 0.0017109972458229095\n",
      "  batch 20 loss: 0.001204349180727604\n",
      "  batch 30 loss: 0.001188589253348482\n",
      "  batch 40 loss: 0.0011676404964106268\n",
      "  batch 50 loss: 0.0015851402581461115\n",
      "  batch 60 loss: 0.001470982219234429\n",
      "  batch 70 loss: 0.0007373382183459398\n",
      "  batch 80 loss: 0.0009219735604176549\n",
      "LOSS train 0.0009219735604176549 valid 0.0019018415789105348\n",
      "EPOCH 155:\n",
      "  batch 10 loss: 0.001170607003535906\n",
      "  batch 20 loss: 0.0011942317313753392\n",
      "  batch 30 loss: 0.0015087204181526205\n",
      "  batch 40 loss: 0.0009634193101305755\n",
      "  batch 50 loss: 0.0014085840416441897\n",
      "  batch 60 loss: 0.0008548951705165564\n",
      "  batch 70 loss: 0.0015856255707831224\n",
      "  batch 80 loss: 0.0011222460755732299\n",
      "LOSS train 0.0011222460755732299 valid 0.0023300691600661593\n",
      "EPOCH 156:\n",
      "  batch 10 loss: 0.000989010562528847\n",
      "  batch 20 loss: 0.0009270415774722096\n",
      "  batch 30 loss: 0.001361128514500365\n",
      "  batch 40 loss: 0.0012615351710508094\n",
      "  batch 50 loss: 0.0013987497923778846\n",
      "  batch 60 loss: 0.0011493891248846922\n",
      "  batch 70 loss: 0.0016477738367825622\n",
      "  batch 80 loss: 0.0011544829690137703\n",
      "LOSS train 0.0011544829690137703 valid 0.002287205204274869\n",
      "EPOCH 157:\n",
      "  batch 10 loss: 0.0014156501207367\n",
      "  batch 20 loss: 0.0016134295850463332\n",
      "  batch 30 loss: 0.0014071864455388549\n",
      "  batch 40 loss: 0.0007580771361972438\n",
      "  batch 50 loss: 0.001555407388104868\n",
      "  batch 60 loss: 0.0007590919570759525\n",
      "  batch 70 loss: 0.0010306048957602343\n",
      "  batch 80 loss: 0.0012468729301133407\n",
      "LOSS train 0.0012468729301133407 valid 0.002039927061423441\n",
      "EPOCH 158:\n",
      "  batch 10 loss: 0.0011988992830197277\n",
      "  batch 20 loss: 0.001480076577337286\n",
      "  batch 30 loss: 0.0012020216149039697\n",
      "  batch 40 loss: 0.0005750958747398726\n",
      "  batch 50 loss: 0.0012643439813075474\n",
      "  batch 60 loss: 0.001515984271037496\n",
      "  batch 70 loss: 0.0012414453456500497\n",
      "  batch 80 loss: 0.0013365233171214186\n",
      "LOSS train 0.0013365233171214186 valid 0.0018610590446496644\n",
      "EPOCH 159:\n",
      "  batch 10 loss: 0.0013824314482747014\n",
      "  batch 20 loss: 0.0014482099173108054\n",
      "  batch 30 loss: 0.0008727277322918781\n",
      "  batch 40 loss: 0.001332918702507868\n",
      "  batch 50 loss: 0.0013093840376541266\n",
      "  batch 60 loss: 0.0012521548353049638\n",
      "  batch 70 loss: 0.000913600049190677\n",
      "  batch 80 loss: 0.0015087173847291523\n",
      "LOSS train 0.0015087173847291523 valid 0.0019272588476451347\n",
      "EPOCH 160:\n",
      "  batch 10 loss: 0.0016849516630543348\n",
      "  batch 20 loss: 0.0014313489093865429\n",
      "  batch 30 loss: 0.0013108928657629803\n",
      "  batch 40 loss: 0.0015353688423090262\n",
      "  batch 50 loss: 0.001069406715561172\n",
      "  batch 60 loss: 0.0005939016231650384\n",
      "  batch 70 loss: 0.0009551727557891354\n",
      "  batch 80 loss: 0.0013582932852528983\n",
      "LOSS train 0.0013582932852528983 valid 0.0018926304103842994\n",
      "EPOCH 161:\n",
      "  batch 10 loss: 0.001428026575899821\n",
      "  batch 20 loss: 0.0009688820411440702\n",
      "  batch 30 loss: 0.0008920386798934032\n",
      "  batch 40 loss: 0.0013491144141084988\n",
      "  batch 50 loss: 0.0007018463044289547\n",
      "  batch 60 loss: 0.0013037846242042405\n",
      "  batch 70 loss: 0.0021599532695688595\n",
      "  batch 80 loss: 0.0008721556648424667\n",
      "LOSS train 0.0008721556648424667 valid 0.0022608502436423806\n",
      "EPOCH 162:\n",
      "  batch 10 loss: 0.0014084424494399173\n",
      "  batch 20 loss: 0.0007666558821455283\n",
      "  batch 30 loss: 0.001285996122646793\n",
      "  batch 40 loss: 0.0010375835280683533\n",
      "  batch 50 loss: 0.0009843142752345101\n",
      "  batch 60 loss: 0.0012575381044996447\n",
      "  batch 70 loss: 0.001973789350449806\n",
      "  batch 80 loss: 0.0009459291178018248\n",
      "LOSS train 0.0009459291178018248 valid 0.0022025874443534123\n",
      "EPOCH 163:\n",
      "  batch 10 loss: 0.0008838591259291206\n",
      "  batch 20 loss: 0.0014046065069209135\n",
      "  batch 30 loss: 0.0011182770854873069\n",
      "  batch 40 loss: 0.001119464285795857\n",
      "  batch 50 loss: 0.0007137303536126182\n",
      "  batch 60 loss: 0.001505294185068351\n",
      "  batch 70 loss: 0.0015153030763826792\n",
      "  batch 80 loss: 0.001353378051180698\n",
      "LOSS train 0.001353378051180698 valid 0.002130249297279079\n",
      "EPOCH 164:\n",
      "  batch 10 loss: 0.001075650278056628\n",
      "  batch 20 loss: 0.0012323628125557207\n",
      "  batch 30 loss: 0.0009841484736057282\n",
      "  batch 40 loss: 0.0008381483192948736\n",
      "  batch 50 loss: 0.0010128892819352587\n",
      "  batch 60 loss: 0.0014377652683549512\n",
      "  batch 70 loss: 0.0010562085123183352\n",
      "  batch 80 loss: 0.0014814385200907054\n",
      "LOSS train 0.0014814385200907054 valid 0.0021063085913010583\n",
      "EPOCH 165:\n",
      "  batch 10 loss: 0.0015315437314029623\n",
      "  batch 20 loss: 0.0006561546643524708\n",
      "  batch 30 loss: 0.0009564329945305871\n",
      "  batch 40 loss: 0.0013258200449143942\n",
      "  batch 50 loss: 0.0010325286914905974\n",
      "  batch 60 loss: 0.0010767019541333411\n",
      "  batch 70 loss: 0.0014264261955077018\n",
      "  batch 80 loss: 0.0014086287253235242\n",
      "LOSS train 0.0014086287253235242 valid 0.0020462567893082453\n",
      "EPOCH 166:\n",
      "  batch 10 loss: 0.0008440069875064182\n",
      "  batch 20 loss: 0.0017975212784875794\n",
      "  batch 30 loss: 0.0011132489203191653\n",
      "  batch 40 loss: 0.0013543682685394742\n",
      "  batch 50 loss: 0.0013171908052697745\n",
      "  batch 60 loss: 0.0011455113907800295\n",
      "  batch 70 loss: 0.0007841035019168884\n",
      "  batch 80 loss: 0.0014890951858888002\n",
      "LOSS train 0.0014890951858888002 valid 0.0018943702304750332\n",
      "EPOCH 167:\n",
      "  batch 10 loss: 0.0011715304591348285\n",
      "  batch 20 loss: 0.0010233028718971582\n",
      "  batch 30 loss: 0.001238477386135628\n",
      "  batch 40 loss: 0.0013721456500832118\n",
      "  batch 50 loss: 0.0008478693902645773\n",
      "  batch 60 loss: 0.0012615408928979832\n",
      "  batch 70 loss: 0.0008486730228923989\n",
      "  batch 80 loss: 0.0013313047751694284\n",
      "LOSS train 0.0013313047751694284 valid 0.0020080541261904726\n",
      "EPOCH 168:\n",
      "  batch 10 loss: 0.0020363309971457967\n",
      "  batch 20 loss: 0.0008693684085017139\n",
      "  batch 30 loss: 0.0010632008055040387\n",
      "  batch 40 loss: 0.0011231491137891681\n",
      "  batch 50 loss: 0.0007957873212205869\n",
      "  batch 60 loss: 0.0007087710179575879\n",
      "  batch 70 loss: 0.0017021643841218293\n",
      "  batch 80 loss: 0.0011070288102075665\n",
      "LOSS train 0.0011070288102075665 valid 0.001857273351015465\n",
      "EPOCH 169:\n",
      "  batch 10 loss: 0.0010229904129118949\n",
      "  batch 20 loss: 0.0009215049790782359\n",
      "  batch 30 loss: 0.0012327857571790446\n",
      "  batch 40 loss: 0.001358493946048611\n",
      "  batch 50 loss: 0.001457821153093164\n",
      "  batch 60 loss: 0.0011757479340872123\n",
      "  batch 70 loss: 0.0014511188674475762\n",
      "  batch 80 loss: 0.0007966885243121169\n",
      "LOSS train 0.0007966885243121169 valid 0.001923603764867039\n",
      "EPOCH 170:\n",
      "  batch 10 loss: 0.0005663677467396156\n",
      "  batch 20 loss: 0.0006651084642868455\n",
      "  batch 30 loss: 0.0010859018554015166\n",
      "  batch 40 loss: 0.0016297683956281617\n",
      "  batch 50 loss: 0.0014275194174729223\n",
      "  batch 60 loss: 0.0011158696693939873\n",
      "  batch 70 loss: 0.0017269297725931664\n",
      "  batch 80 loss: 0.0010195860903024823\n",
      "LOSS train 0.0010195860903024823 valid 0.002172562631221808\n",
      "EPOCH 171:\n",
      "  batch 10 loss: 0.0011763802252062305\n",
      "  batch 20 loss: 0.0009266674432296895\n",
      "  batch 30 loss: 0.0010522823483938737\n",
      "  batch 40 loss: 0.000943053390278692\n",
      "  batch 50 loss: 0.0011748428640458997\n",
      "  batch 60 loss: 0.000997049591802579\n",
      "  batch 70 loss: 0.0012242389240100238\n",
      "  batch 80 loss: 0.0017437484705851603\n",
      "LOSS train 0.0017437484705851603 valid 0.0018603416927999205\n",
      "EPOCH 172:\n",
      "  batch 10 loss: 0.0009103060048175849\n",
      "  batch 20 loss: 0.0014155862353163683\n",
      "  batch 30 loss: 0.0010434293380967574\n",
      "  batch 40 loss: 0.0011937891740330998\n",
      "  batch 50 loss: 0.0016108671034885448\n",
      "  batch 60 loss: 0.0006663464876339731\n",
      "  batch 70 loss: 0.0008106440281693495\n",
      "  batch 80 loss: 0.0014996220302435858\n",
      "LOSS train 0.0014996220302435858 valid 0.002371938284331918\n",
      "EPOCH 173:\n",
      "  batch 10 loss: 0.001627717215615121\n",
      "  batch 20 loss: 0.0013599213159295686\n",
      "  batch 30 loss: 0.0011485089346081167\n",
      "  batch 40 loss: 0.0010425002701424547\n",
      "  batch 50 loss: 0.0008995687851552248\n",
      "  batch 60 loss: 0.000977354510882833\n",
      "  batch 70 loss: 0.0014104006109278088\n",
      "  batch 80 loss: 0.0008213659153398112\n",
      "LOSS train 0.0008213659153398112 valid 0.001875695996959621\n",
      "EPOCH 174:\n",
      "  batch 10 loss: 0.0007242905898436902\n",
      "  batch 20 loss: 0.0014544489291438366\n",
      "  batch 30 loss: 0.0009049357851949935\n",
      "  batch 40 loss: 0.0009099875389495082\n",
      "  batch 50 loss: 0.001548962112872232\n",
      "  batch 60 loss: 0.0008188916202925611\n",
      "  batch 70 loss: 0.0012961219451995021\n",
      "  batch 80 loss: 0.0014384900101049424\n",
      "LOSS train 0.0014384900101049424 valid 0.0018532497826527106\n",
      "EPOCH 175:\n",
      "  batch 10 loss: 0.001076365830715531\n",
      "  batch 20 loss: 0.0010560578851709578\n",
      "  batch 30 loss: 0.0010866172037822253\n",
      "  batch 40 loss: 0.001477130066848531\n",
      "  batch 50 loss: 0.0010891238435590366\n",
      "  batch 60 loss: 0.0012968577969331818\n",
      "  batch 70 loss: 0.001099103992351047\n",
      "  batch 80 loss: 0.0006997080246662791\n",
      "LOSS train 0.0006997080246662791 valid 0.0020791695852904013\n",
      "EPOCH 176:\n",
      "  batch 10 loss: 0.001075177260156579\n",
      "  batch 20 loss: 0.0010995427781040235\n",
      "  batch 30 loss: 0.0012454554973260201\n",
      "  batch 40 loss: 0.0009967702135071476\n",
      "  batch 50 loss: 0.0011718916154393356\n",
      "  batch 60 loss: 0.0019377819571673172\n",
      "  batch 70 loss: 0.0004248465672503698\n",
      "  batch 80 loss: 0.0009598750590555482\n",
      "LOSS train 0.0009598750590555482 valid 0.001520459422345084\n",
      "EPOCH 177:\n",
      "  batch 10 loss: 0.0010194997320297715\n",
      "  batch 20 loss: 0.0006705200086969398\n",
      "  batch 30 loss: 0.001869780007845634\n",
      "  batch 40 loss: 0.0009077545865153524\n",
      "  batch 50 loss: 0.001620571931152881\n",
      "  batch 60 loss: 0.0013666029861610697\n",
      "  batch 70 loss: 0.0009555966332953858\n",
      "  batch 80 loss: 0.000704970432667551\n",
      "LOSS train 0.000704970432667551 valid 0.0018331064490848803\n",
      "EPOCH 178:\n",
      "  batch 10 loss: 0.0012295067547881899\n",
      "  batch 20 loss: 0.0009414804799860122\n",
      "  batch 30 loss: 0.0014787070089823829\n",
      "  batch 40 loss: 0.0014793309455683357\n",
      "  batch 50 loss: 0.000655621631793224\n",
      "  batch 60 loss: 0.0012110763512509948\n",
      "  batch 70 loss: 0.0014318975965466052\n",
      "  batch 80 loss: 0.0007030607249760123\n",
      "LOSS train 0.0007030607249760123 valid 0.0019443560315949072\n",
      "EPOCH 179:\n",
      "  batch 10 loss: 0.00135175929293041\n",
      "  batch 20 loss: 0.0012182032192413316\n",
      "  batch 30 loss: 0.0008248241887883978\n",
      "  batch 40 loss: 0.0011210970934286025\n",
      "  batch 50 loss: 0.001026016609432645\n",
      "  batch 60 loss: 0.0011836753591182969\n",
      "  batch 70 loss: 0.0011799845653797548\n",
      "  batch 80 loss: 0.0009958779714338561\n",
      "LOSS train 0.0009958779714338561 valid 0.002583145023399993\n",
      "EPOCH 180:\n",
      "  batch 10 loss: 0.0013752533507556564\n",
      "  batch 20 loss: 0.0011558259076650756\n",
      "  batch 30 loss: 0.0012040752259508737\n",
      "  batch 40 loss: 0.0014184747792342022\n",
      "  batch 50 loss: 0.0008269612826666162\n",
      "  batch 60 loss: 0.0010151728571003104\n",
      "  batch 70 loss: 0.0008411341378405269\n",
      "  batch 80 loss: 0.001231362331449759\n",
      "LOSS train 0.001231362331449759 valid 0.00229776746290554\n",
      "EPOCH 181:\n",
      "  batch 10 loss: 0.0009073830769239066\n",
      "  batch 20 loss: 0.0007151955730194004\n",
      "  batch 30 loss: 0.001168009274834958\n",
      "  batch 40 loss: 0.0012932652475001305\n",
      "  batch 50 loss: 0.001350501132651516\n",
      "  batch 60 loss: 0.0009284694380426117\n",
      "  batch 70 loss: 0.0008525757650318155\n",
      "  batch 80 loss: 0.0017370474771979615\n",
      "LOSS train 0.0017370474771979615 valid 0.00211752062436517\n",
      "EPOCH 182:\n",
      "  batch 10 loss: 0.0008850841996888903\n",
      "  batch 20 loss: 0.001032209032530318\n",
      "  batch 30 loss: 0.0016277437828193797\n",
      "  batch 40 loss: 0.0008503063132422994\n",
      "  batch 50 loss: 0.0006891015026468494\n",
      "  batch 60 loss: 0.0010985397241540795\n",
      "  batch 70 loss: 0.0010371555895403617\n",
      "  batch 80 loss: 0.00162138477889755\n",
      "LOSS train 0.00162138477889755 valid 0.001947483025855945\n",
      "EPOCH 183:\n",
      "  batch 10 loss: 0.0010322100081111784\n",
      "  batch 20 loss: 0.0008006205929177668\n",
      "  batch 30 loss: 0.0012641779174686007\n",
      "  batch 40 loss: 0.0016150761825656445\n",
      "  batch 50 loss: 0.0010224605715450253\n",
      "  batch 60 loss: 0.001320780611513328\n",
      "  batch 70 loss: 0.0009060040643589673\n",
      "  batch 80 loss: 0.0008735162485777436\n",
      "LOSS train 0.0008735162485777436 valid 0.0020084634831982837\n",
      "EPOCH 184:\n",
      "  batch 10 loss: 0.0009474621392655536\n",
      "  batch 20 loss: 0.0013350401089724073\n",
      "  batch 30 loss: 0.00118476391362492\n",
      "  batch 40 loss: 0.0013107617308946828\n",
      "  batch 50 loss: 0.0008729822421742028\n",
      "  batch 60 loss: 0.0012712399092265514\n",
      "  batch 70 loss: 0.001111398562451882\n",
      "  batch 80 loss: 0.0006127623238910473\n",
      "LOSS train 0.0006127623238910473 valid 0.0018606528886402885\n",
      "EPOCH 185:\n",
      "  batch 10 loss: 0.0007884585653101794\n",
      "  batch 20 loss: 0.0013875767270803863\n",
      "  batch 30 loss: 0.0011411860878297376\n",
      "  batch 40 loss: 0.0006565207435301091\n",
      "  batch 50 loss: 0.0009439096956413096\n",
      "  batch 60 loss: 0.0009008709298313988\n",
      "  batch 70 loss: 0.001383807553366978\n",
      "  batch 80 loss: 0.00137240633929423\n",
      "LOSS train 0.00137240633929423 valid 0.002047910941723785\n",
      "EPOCH 186:\n",
      "  batch 10 loss: 0.0011242957824151745\n",
      "  batch 20 loss: 0.0007722263970833865\n",
      "  batch 30 loss: 0.0008438948149319004\n",
      "  batch 40 loss: 0.001029996184047377\n",
      "  batch 50 loss: 0.0012739410379367655\n",
      "  batch 60 loss: 0.0013348070724475748\n",
      "  batch 70 loss: 0.0011382662114215237\n",
      "  batch 80 loss: 0.0010656505226933177\n",
      "LOSS train 0.0010656505226933177 valid 0.0019262100110609027\n",
      "EPOCH 187:\n",
      "  batch 10 loss: 0.0012132839665355277\n",
      "  batch 20 loss: 0.00099088463937278\n",
      "  batch 30 loss: 0.0007659155170841814\n",
      "  batch 40 loss: 0.0007690027469323013\n",
      "  batch 50 loss: 0.0011519225560448377\n",
      "  batch 60 loss: 0.0010559037768643974\n",
      "  batch 70 loss: 0.0018582761350160125\n",
      "  batch 80 loss: 0.0010785480722233843\n",
      "LOSS train 0.0010785480722233843 valid 0.0018157612166851322\n",
      "EPOCH 188:\n",
      "  batch 10 loss: 0.0013122669161873546\n",
      "  batch 20 loss: 0.0012378367344695108\n",
      "  batch 30 loss: 0.000980450841470315\n",
      "  batch 40 loss: 0.001088491778253342\n",
      "  batch 50 loss: 0.0011406299081727412\n",
      "  batch 60 loss: 0.0007057805656245364\n",
      "  batch 70 loss: 0.0008189928465867525\n",
      "  batch 80 loss: 0.0015305529398517593\n",
      "LOSS train 0.0015305529398517593 valid 0.002025073344161683\n",
      "EPOCH 189:\n",
      "  batch 10 loss: 0.0011351795769769524\n",
      "  batch 20 loss: 0.0011370751446975191\n",
      "  batch 30 loss: 0.001061906305261573\n",
      "  batch 40 loss: 0.0012082125763868135\n",
      "  batch 50 loss: 0.0007704706313717225\n",
      "  batch 60 loss: 0.0009859003909582498\n",
      "  batch 70 loss: 0.0010974435723937858\n",
      "  batch 80 loss: 0.0009149799173826523\n",
      "LOSS train 0.0009149799173826523 valid 0.001938221642449207\n",
      "EPOCH 190:\n",
      "  batch 10 loss: 0.0010510334788193631\n",
      "  batch 20 loss: 0.0009592103297194398\n",
      "  batch 30 loss: 0.0012866489250171753\n",
      "  batch 40 loss: 0.000862170247631866\n",
      "  batch 50 loss: 0.0010692295659112006\n",
      "  batch 60 loss: 0.0010481234778580983\n",
      "  batch 70 loss: 0.0015799281813258404\n",
      "  batch 80 loss: 0.0007041261921131081\n",
      "LOSS train 0.0007041261921131081 valid 0.0019290970723250212\n",
      "EPOCH 191:\n",
      "  batch 10 loss: 0.0016368220151747436\n",
      "  batch 20 loss: 0.0009522670601654681\n",
      "  batch 30 loss: 0.0008292090330684232\n",
      "  batch 40 loss: 0.0008841443894823442\n",
      "  batch 50 loss: 0.001006576989129826\n",
      "  batch 60 loss: 0.0009478129689966863\n",
      "  batch 70 loss: 0.0012244103366555238\n",
      "  batch 80 loss: 0.001062647806458017\n",
      "LOSS train 0.001062647806458017 valid 0.0019725577662893555\n",
      "EPOCH 192:\n",
      "  batch 10 loss: 0.0010839809406576252\n",
      "  batch 20 loss: 0.0010235539005805094\n",
      "  batch 30 loss: 0.0008490547835435791\n",
      "  batch 40 loss: 0.0011749488390535134\n",
      "  batch 50 loss: 0.0011688915678462308\n",
      "  batch 60 loss: 0.0009654940618929686\n",
      "  batch 70 loss: 0.0010506659047621269\n",
      "  batch 80 loss: 0.0013078754750466715\n",
      "LOSS train 0.0013078754750466715 valid 0.001889296441549959\n",
      "EPOCH 193:\n",
      "  batch 10 loss: 0.0010290625157921341\n",
      "  batch 20 loss: 0.001302762959028314\n",
      "  batch 30 loss: 0.0008826359859142486\n",
      "  batch 40 loss: 0.0010889066200661546\n",
      "  batch 50 loss: 0.0009192494237112214\n",
      "  batch 60 loss: 0.0014369629263001117\n",
      "  batch 70 loss: 0.000992162922557327\n",
      "  batch 80 loss: 0.0007076652156570162\n",
      "LOSS train 0.0007076652156570162 valid 0.0018690698574255293\n",
      "EPOCH 194:\n",
      "  batch 10 loss: 0.0014436253055237103\n",
      "  batch 20 loss: 0.0010462910321280105\n",
      "  batch 30 loss: 0.0008306458878109879\n",
      "  batch 40 loss: 0.0008703081689844794\n",
      "  batch 50 loss: 0.0010976012511264344\n",
      "  batch 60 loss: 0.0010207213078814448\n",
      "  batch 70 loss: 0.0012645451379057703\n",
      "  batch 80 loss: 0.000892092487077889\n",
      "LOSS train 0.000892092487077889 valid 0.0016470432134019575\n",
      "EPOCH 195:\n",
      "  batch 10 loss: 0.001490244995667922\n",
      "  batch 20 loss: 0.0011906678939340054\n",
      "  batch 30 loss: 0.0008576465458361327\n",
      "  batch 40 loss: 0.0011446635245448533\n",
      "  batch 50 loss: 0.0008369922672443409\n",
      "  batch 60 loss: 0.0005008450253541242\n",
      "  batch 70 loss: 0.0014678229229701857\n",
      "  batch 80 loss: 0.001004009846377585\n",
      "LOSS train 0.001004009846377585 valid 0.0018245946117849598\n",
      "EPOCH 196:\n",
      "  batch 10 loss: 0.0013127381704123309\n",
      "  batch 20 loss: 0.000705843366802128\n",
      "  batch 30 loss: 0.0009916472905985075\n",
      "  batch 40 loss: 0.0013976001236528646\n",
      "  batch 50 loss: 0.0009090557998945314\n",
      "  batch 60 loss: 0.0009124673607715294\n",
      "  batch 70 loss: 0.0010752197056376645\n",
      "  batch 80 loss: 0.0008415758441628896\n",
      "LOSS train 0.0008415758441628896 valid 0.0018753634660606622\n",
      "EPOCH 197:\n",
      "  batch 10 loss: 0.0006517423450446813\n",
      "  batch 20 loss: 0.0007488672442633515\n",
      "  batch 30 loss: 0.0009883831983131587\n",
      "  batch 40 loss: 0.0011735104164301902\n",
      "  batch 50 loss: 0.0007605622346261498\n",
      "  batch 60 loss: 0.0014036820024273312\n",
      "  batch 70 loss: 0.0015463677287584688\n",
      "  batch 80 loss: 0.0010789557044802224\n",
      "LOSS train 0.0010789557044802224 valid 0.001968442533193411\n",
      "EPOCH 198:\n",
      "  batch 10 loss: 0.0011410157930583863\n",
      "  batch 20 loss: 0.0011879823382770383\n",
      "  batch 30 loss: 0.0010642488043288267\n",
      "  batch 40 loss: 0.0008114260536672191\n",
      "  batch 50 loss: 0.0013495386751287697\n",
      "  batch 60 loss: 0.0012718659753460314\n",
      "  batch 70 loss: 0.0008992903344420711\n",
      "  batch 80 loss: 0.0009000318417008657\n",
      "LOSS train 0.0009000318417008657 valid 0.0019451464884787128\n",
      "EPOCH 199:\n",
      "  batch 10 loss: 0.0008835099219538734\n",
      "  batch 20 loss: 0.0014091813048253243\n",
      "  batch 30 loss: 0.0010039098528039857\n",
      "  batch 40 loss: 0.000940873954732524\n",
      "  batch 50 loss: 0.0009545687549291415\n",
      "  batch 60 loss: 0.0013972682418824434\n",
      "  batch 70 loss: 0.0010155292785668734\n",
      "  batch 80 loss: 0.0009678476689970239\n",
      "LOSS train 0.0009678476689970239 valid 0.0017810100607994173\n",
      "EPOCH 200:\n",
      "  batch 10 loss: 0.0009122883452675979\n",
      "  batch 20 loss: 0.0010521125110130925\n",
      "  batch 30 loss: 0.0008967433763729105\n",
      "  batch 40 loss: 0.001480348560397715\n",
      "  batch 50 loss: 0.0012419950950032898\n",
      "  batch 60 loss: 0.0010043701132701698\n",
      "  batch 70 loss: 0.0010172484515408088\n",
      "  batch 80 loss: 0.0008129443695850114\n",
      "LOSS train 0.0008129443695850114 valid 0.0018705637721723178\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch(curr_model=model)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total = len(validation_set)\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        inputs, labels = vdata\n",
    "        outputs = torch.argmax(model(inputs), dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct_count += (outputs==labels).sum().item()\n",
    "        # correct_count += (outputs==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../models/model6.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278350515463918"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_data.view(-1,21*2), train_labels)\n",
    "y_pred = knn.predict(val_data.view(-1,21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 21, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RidgeClassifierCV()\n",
    "clf.fit(train_data.view(-1, 21*2), train_labels)\n",
    "y_pred = clf.predict(val_data.view(-1, 21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_6527/1044316036.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6817, 0.0807, 0.0736, 0.0839, 0.0802],\n",
       "        [0.5953, 0.0871, 0.1020, 0.1140, 0.1016],\n",
       "        [0.6809, 0.0781, 0.0679, 0.0831, 0.0901],\n",
       "        [0.6449, 0.0783, 0.0792, 0.0871, 0.1105],\n",
       "        [0.6436, 0.0777, 0.0825, 0.0876, 0.1086],\n",
       "        [0.5774, 0.0947, 0.0956, 0.1239, 0.1083],\n",
       "        [0.5115, 0.0937, 0.0987, 0.1656, 0.1306],\n",
       "        [0.4998, 0.1001, 0.0840, 0.1717, 0.1444],\n",
       "        [0.6768, 0.0651, 0.0778, 0.0808, 0.0994],\n",
       "        [0.7222, 0.0787, 0.0629, 0.0701, 0.0661],\n",
       "        [0.7228, 0.0590, 0.0769, 0.0705, 0.0708],\n",
       "        [0.6058, 0.0801, 0.0881, 0.1046, 0.1215],\n",
       "        [0.5946, 0.0750, 0.0937, 0.1189, 0.1178],\n",
       "        [0.4962, 0.1252, 0.0958, 0.1549, 0.1279],\n",
       "        [0.5400, 0.1057, 0.0932, 0.1275, 0.1336],\n",
       "        [0.6375, 0.1178, 0.0775, 0.0829, 0.0844],\n",
       "        [0.6289, 0.0775, 0.0811, 0.1140, 0.0985],\n",
       "        [0.6558, 0.0717, 0.0818, 0.0909, 0.0998],\n",
       "        [0.6551, 0.0666, 0.0837, 0.0982, 0.0964],\n",
       "        [0.0692, 0.6421, 0.0877, 0.1231, 0.0780],\n",
       "        [0.0709, 0.6230, 0.0911, 0.1339, 0.0812],\n",
       "        [0.0713, 0.6259, 0.0887, 0.1374, 0.0766],\n",
       "        [0.0731, 0.6298, 0.0872, 0.1398, 0.0701],\n",
       "        [0.0836, 0.6127, 0.0964, 0.1132, 0.0941],\n",
       "        [0.0836, 0.5978, 0.0984, 0.1219, 0.0983],\n",
       "        [0.0737, 0.5998, 0.0957, 0.1305, 0.1002],\n",
       "        [0.0759, 0.6073, 0.0940, 0.1117, 0.1111],\n",
       "        [0.0637, 0.6749, 0.0802, 0.0768, 0.1044],\n",
       "        [0.0840, 0.6363, 0.0915, 0.0746, 0.1136],\n",
       "        [0.0836, 0.6447, 0.0907, 0.0694, 0.1116],\n",
       "        [0.0888, 0.6488, 0.0910, 0.0679, 0.1035],\n",
       "        [0.0871, 0.6840, 0.0805, 0.0661, 0.0823],\n",
       "        [0.0699, 0.6930, 0.0732, 0.0743, 0.0896],\n",
       "        [0.0963, 0.5992, 0.1002, 0.0809, 0.1235],\n",
       "        [0.0959, 0.5948, 0.1039, 0.0852, 0.1202],\n",
       "        [0.0941, 0.6012, 0.0999, 0.0767, 0.1282],\n",
       "        [0.0969, 0.5862, 0.1051, 0.0799, 0.1319],\n",
       "        [0.0812, 0.0810, 0.6525, 0.0663, 0.1190],\n",
       "        [0.0866, 0.0844, 0.6427, 0.0729, 0.1134],\n",
       "        [0.0903, 0.0872, 0.6412, 0.0718, 0.1095],\n",
       "        [0.0913, 0.0883, 0.6387, 0.0689, 0.1127],\n",
       "        [0.0952, 0.0853, 0.6478, 0.0540, 0.1177],\n",
       "        [0.0962, 0.0852, 0.6467, 0.0499, 0.1220],\n",
       "        [0.0804, 0.0695, 0.6838, 0.0544, 0.1119],\n",
       "        [0.0796, 0.0786, 0.6544, 0.0701, 0.1173],\n",
       "        [0.0836, 0.0841, 0.6444, 0.0763, 0.1116],\n",
       "        [0.0880, 0.0867, 0.6409, 0.0780, 0.1063],\n",
       "        [0.0891, 0.0842, 0.6515, 0.0734, 0.1018],\n",
       "        [0.0894, 0.0837, 0.6530, 0.0719, 0.1021],\n",
       "        [0.0874, 0.0841, 0.6332, 0.1226, 0.0727],\n",
       "        [0.0941, 0.0944, 0.6099, 0.1294, 0.0723],\n",
       "        [0.0888, 0.0941, 0.5737, 0.1660, 0.0773],\n",
       "        [0.0906, 0.0844, 0.6455, 0.1254, 0.0541],\n",
       "        [0.0858, 0.0774, 0.6550, 0.1273, 0.0544],\n",
       "        [0.0856, 0.0805, 0.6373, 0.1363, 0.0604],\n",
       "        [0.0906, 0.0884, 0.6089, 0.1498, 0.0623],\n",
       "        [0.0879, 0.0879, 0.6046, 0.1525, 0.0670],\n",
       "        [0.0923, 0.0867, 0.0797, 0.6535, 0.0878],\n",
       "        [0.0957, 0.0904, 0.0790, 0.6515, 0.0833],\n",
       "        [0.1044, 0.0945, 0.0829, 0.6379, 0.0804],\n",
       "        [0.1005, 0.0921, 0.0815, 0.6418, 0.0840],\n",
       "        [0.0983, 0.0897, 0.0809, 0.6516, 0.0796],\n",
       "        [0.1101, 0.0960, 0.0830, 0.6270, 0.0838],\n",
       "        [0.1286, 0.1032, 0.1016, 0.5771, 0.0895],\n",
       "        [0.0780, 0.0869, 0.1104, 0.6340, 0.0907],\n",
       "        [0.0775, 0.0869, 0.1030, 0.6466, 0.0860],\n",
       "        [0.1045, 0.1047, 0.0964, 0.6075, 0.0867],\n",
       "        [0.1269, 0.1253, 0.1334, 0.5058, 0.1085],\n",
       "        [0.1322, 0.1233, 0.1321, 0.5085, 0.1039],\n",
       "        [0.1046, 0.0892, 0.1067, 0.5955, 0.1040],\n",
       "        [0.0979, 0.1060, 0.1097, 0.5837, 0.1026],\n",
       "        [0.1103, 0.1136, 0.1168, 0.5579, 0.1013],\n",
       "        [0.1103, 0.1100, 0.1144, 0.5670, 0.0982],\n",
       "        [0.1073, 0.1094, 0.1066, 0.5802, 0.0965],\n",
       "        [0.1036, 0.1047, 0.1002, 0.5911, 0.1004],\n",
       "        [0.1067, 0.1124, 0.0982, 0.5870, 0.0957],\n",
       "        [0.1094, 0.1192, 0.1036, 0.5698, 0.0980],\n",
       "        [0.0880, 0.0855, 0.0918, 0.1049, 0.6298],\n",
       "        [0.0838, 0.0797, 0.0874, 0.1029, 0.6461],\n",
       "        [0.0863, 0.0841, 0.0863, 0.1071, 0.6361],\n",
       "        [0.0834, 0.0808, 0.0853, 0.1045, 0.6459],\n",
       "        [0.0873, 0.0822, 0.0982, 0.1077, 0.6245],\n",
       "        [0.1011, 0.0975, 0.1056, 0.1238, 0.5719],\n",
       "        [0.1049, 0.0998, 0.0983, 0.1199, 0.5771],\n",
       "        [0.1184, 0.1066, 0.1037, 0.1005, 0.5709],\n",
       "        [0.1120, 0.1007, 0.0971, 0.1019, 0.5883],\n",
       "        [0.1071, 0.1021, 0.1082, 0.1341, 0.5485],\n",
       "        [0.1080, 0.0999, 0.1012, 0.1313, 0.5597],\n",
       "        [0.0950, 0.0900, 0.0926, 0.1091, 0.6133],\n",
       "        [0.0928, 0.1323, 0.0920, 0.1117, 0.5713],\n",
       "        [0.0940, 0.1307, 0.0883, 0.1018, 0.5852],\n",
       "        [0.1183, 0.1211, 0.0890, 0.1068, 0.5648],\n",
       "        [0.1417, 0.1255, 0.0944, 0.1196, 0.5190],\n",
       "        [0.1463, 0.1143, 0.1095, 0.1156, 0.5142],\n",
       "        [0.1148, 0.1146, 0.0927, 0.1211, 0.5568],\n",
       "        [0.1126, 0.1125, 0.0884, 0.1162, 0.5703],\n",
       "        [0.1227, 0.1053, 0.1000, 0.1206, 0.5513]], dtype=torch.float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_data.view(-1, 21*2), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reetinav/anaconda3/envs/PIC16B/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(multi_class=\"ovr\")\n",
    "clf.fit(train_data.view(-1, 21*2), torch.argmax(train_labels, dim=1))\n",
    "clf.score(val_data.view(-1, 21*2), torch.argmax(val_labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7528, 0.0526, 0.0551, 0.0519, 0.0876],\n",
       "        [0.6623, 0.0536, 0.0913, 0.0680, 0.1248],\n",
       "        [0.7421, 0.0511, 0.0541, 0.0523, 0.1004],\n",
       "        [0.7057, 0.0502, 0.0582, 0.0611, 0.1248],\n",
       "        [0.6898, 0.0542, 0.0641, 0.0646, 0.1273],\n",
       "        [0.6079, 0.0719, 0.0888, 0.0880, 0.1435],\n",
       "        [0.5345, 0.0827, 0.1002, 0.1041, 0.1785],\n",
       "        [0.5266, 0.0871, 0.0854, 0.1148, 0.1861],\n",
       "        [0.7288, 0.0413, 0.0623, 0.0545, 0.1131],\n",
       "        [0.7814, 0.0521, 0.0439, 0.0446, 0.0779],\n",
       "        [0.8104, 0.0328, 0.0484, 0.0363, 0.0720],\n",
       "        [0.7139, 0.0439, 0.0575, 0.1079, 0.0768],\n",
       "        [0.6979, 0.0423, 0.0651, 0.1153, 0.0793],\n",
       "        [0.5230, 0.0924, 0.0954, 0.1829, 0.1062],\n",
       "        [0.5858, 0.0602, 0.0815, 0.1587, 0.1139],\n",
       "        [0.6972, 0.0675, 0.0633, 0.1023, 0.0697],\n",
       "        [0.7065, 0.0484, 0.0617, 0.1076, 0.0757],\n",
       "        [0.7538, 0.0348, 0.0552, 0.0913, 0.0649],\n",
       "        [0.7592, 0.0318, 0.0541, 0.0928, 0.0620],\n",
       "        [0.0342, 0.6723, 0.0858, 0.0939, 0.1137],\n",
       "        [0.0351, 0.6573, 0.0920, 0.0981, 0.1174],\n",
       "        [0.0358, 0.6592, 0.0934, 0.0995, 0.1120],\n",
       "        [0.0370, 0.6632, 0.0997, 0.0967, 0.1033],\n",
       "        [0.0466, 0.6359, 0.0972, 0.0931, 0.1272],\n",
       "        [0.0460, 0.6253, 0.1020, 0.0965, 0.1302],\n",
       "        [0.0381, 0.6393, 0.1065, 0.0934, 0.1227],\n",
       "        [0.0400, 0.6355, 0.1061, 0.0852, 0.1333],\n",
       "        [0.0322, 0.6886, 0.0855, 0.0671, 0.1267],\n",
       "        [0.0396, 0.6722, 0.0977, 0.0995, 0.0909],\n",
       "        [0.0361, 0.6855, 0.1014, 0.0901, 0.0869],\n",
       "        [0.0441, 0.6781, 0.0926, 0.0963, 0.0889],\n",
       "        [0.0419, 0.7072, 0.0820, 0.0910, 0.0779],\n",
       "        [0.0329, 0.7002, 0.0715, 0.1179, 0.0775],\n",
       "        [0.0454, 0.6440, 0.1134, 0.1016, 0.0956],\n",
       "        [0.0434, 0.6418, 0.1193, 0.1013, 0.0942],\n",
       "        [0.0430, 0.6512, 0.1111, 0.0971, 0.0976],\n",
       "        [0.0444, 0.6358, 0.1198, 0.0987, 0.1013],\n",
       "        [0.0250, 0.0642, 0.7968, 0.0224, 0.0915],\n",
       "        [0.0279, 0.0673, 0.7909, 0.0251, 0.0889],\n",
       "        [0.0288, 0.0671, 0.7939, 0.0245, 0.0857],\n",
       "        [0.0286, 0.0665, 0.7936, 0.0226, 0.0887],\n",
       "        [0.0262, 0.0605, 0.8107, 0.0167, 0.0859],\n",
       "        [0.0249, 0.0594, 0.8191, 0.0156, 0.0810],\n",
       "        [0.0219, 0.0483, 0.8297, 0.0150, 0.0850],\n",
       "        [0.0248, 0.0599, 0.7982, 0.0215, 0.0956],\n",
       "        [0.0265, 0.0662, 0.7978, 0.0247, 0.0846],\n",
       "        [0.0266, 0.0676, 0.8105, 0.0260, 0.0692],\n",
       "        [0.0249, 0.0641, 0.8264, 0.0225, 0.0621],\n",
       "        [0.0244, 0.0615, 0.8314, 0.0215, 0.0612],\n",
       "        [0.0329, 0.0799, 0.7396, 0.1204, 0.0273],\n",
       "        [0.0336, 0.0847, 0.7489, 0.1065, 0.0263],\n",
       "        [0.0388, 0.0874, 0.7319, 0.1013, 0.0406],\n",
       "        [0.0302, 0.0689, 0.7843, 0.0963, 0.0203],\n",
       "        [0.0284, 0.0639, 0.7903, 0.0974, 0.0200],\n",
       "        [0.0310, 0.0731, 0.7599, 0.1121, 0.0240],\n",
       "        [0.0343, 0.0859, 0.7280, 0.1252, 0.0266],\n",
       "        [0.0339, 0.0856, 0.7297, 0.1220, 0.0288],\n",
       "        [0.0791, 0.0974, 0.0770, 0.6618, 0.0846],\n",
       "        [0.0806, 0.1006, 0.0759, 0.6619, 0.0810],\n",
       "        [0.0839, 0.1058, 0.0811, 0.6492, 0.0799],\n",
       "        [0.0812, 0.1020, 0.0803, 0.6555, 0.0810],\n",
       "        [0.0796, 0.1017, 0.0821, 0.6574, 0.0791],\n",
       "        [0.0881, 0.1058, 0.0841, 0.6418, 0.0803],\n",
       "        [0.0951, 0.1042, 0.1162, 0.6037, 0.0808],\n",
       "        [0.0680, 0.1059, 0.1149, 0.6291, 0.0820],\n",
       "        [0.0674, 0.1058, 0.1085, 0.6380, 0.0803],\n",
       "        [0.0924, 0.1131, 0.1074, 0.6030, 0.0841],\n",
       "        [0.1143, 0.1253, 0.1531, 0.5061, 0.1011],\n",
       "        [0.1148, 0.1221, 0.1603, 0.5073, 0.0955],\n",
       "        [0.0970, 0.0956, 0.1271, 0.5944, 0.0860],\n",
       "        [0.0930, 0.1133, 0.1146, 0.5767, 0.1023],\n",
       "        [0.0989, 0.1148, 0.1210, 0.5660, 0.0993],\n",
       "        [0.1011, 0.1147, 0.1244, 0.5634, 0.0964],\n",
       "        [0.0971, 0.1211, 0.1236, 0.5648, 0.0934],\n",
       "        [0.0962, 0.1180, 0.1213, 0.5676, 0.0968],\n",
       "        [0.1021, 0.1363, 0.1113, 0.5483, 0.1020],\n",
       "        [0.1033, 0.1387, 0.1118, 0.5431, 0.1031],\n",
       "        [0.0799, 0.1005, 0.1137, 0.1013, 0.6046],\n",
       "        [0.0768, 0.0967, 0.1109, 0.0992, 0.6165],\n",
       "        [0.0777, 0.1019, 0.1098, 0.1004, 0.6102],\n",
       "        [0.0757, 0.0981, 0.1093, 0.1000, 0.6169],\n",
       "        [0.0798, 0.0891, 0.1133, 0.1027, 0.6151],\n",
       "        [0.0896, 0.0956, 0.1115, 0.1174, 0.5858],\n",
       "        [0.0928, 0.1051, 0.1102, 0.1116, 0.5802],\n",
       "        [0.1055, 0.1209, 0.1316, 0.1002, 0.5417],\n",
       "        [0.0985, 0.1152, 0.1257, 0.1003, 0.5603],\n",
       "        [0.0937, 0.0990, 0.1138, 0.1208, 0.5727],\n",
       "        [0.0949, 0.1017, 0.1099, 0.1177, 0.5759],\n",
       "        [0.0836, 0.1046, 0.1199, 0.0977, 0.5941],\n",
       "        [0.0881, 0.2182, 0.1045, 0.0892, 0.5000],\n",
       "        [0.0886, 0.2123, 0.1009, 0.0844, 0.5138],\n",
       "        [0.1178, 0.1436, 0.1019, 0.0864, 0.5503],\n",
       "        [0.1370, 0.1400, 0.1140, 0.0915, 0.5175],\n",
       "        [0.1351, 0.1244, 0.1427, 0.0891, 0.5087],\n",
       "        [0.1163, 0.1313, 0.1094, 0.0947, 0.5483],\n",
       "        [0.1146, 0.1283, 0.0982, 0.0911, 0.5677],\n",
       "        [0.1220, 0.1145, 0.1068, 0.0915, 0.5652]], dtype=torch.float64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
