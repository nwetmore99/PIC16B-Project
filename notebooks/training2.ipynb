{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.3, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(70):\n",
    "        image = cv2.imread(f\"../training/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            train_label = np.zeros([len(classes)])\n",
    "            train_label[class_index] = 1\n",
    "            train_data.append(landmarks)\n",
    "            train_labels.append(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_39568/3266268609.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1695391825142/work/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  train_labels = torch.tensor(train_labels)\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(train_data, train_labels)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_labels = []\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(20):\n",
    "        image = cv2.imread(f\"../validation/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            val_label = np.zeros([len(classes)])\n",
    "            val_label[class_index] = 1\n",
    "            val_data.append(landmarks)\n",
    "            val_labels.append(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(val_data, val_labels)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(curr_model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.6179750233888626\n",
      "  batch 20 loss: 1.637564355134964\n",
      "  batch 30 loss: 1.6156188249588013\n",
      "  batch 40 loss: 1.589386910200119\n",
      "  batch 50 loss: 1.5989562839269638\n",
      "  batch 60 loss: 1.6341707080602645\n",
      "  batch 70 loss: 1.6121572315692902\n",
      "  batch 80 loss: 1.6184952229261398\n",
      "LOSS train 1.6184952229261398 valid 1.6037553989887237\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 1.6107109278440475\n",
      "  batch 20 loss: 1.6050293415784835\n",
      "  batch 30 loss: 1.5994475424289702\n",
      "  batch 40 loss: 1.5954216212034225\n",
      "  batch 50 loss: 1.6147206246852874\n",
      "  batch 60 loss: 1.5917430430650712\n",
      "  batch 70 loss: 1.6003032594919204\n",
      "  batch 80 loss: 1.5947639137506484\n",
      "LOSS train 1.5947639137506484 valid 1.597197924852371\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 1.5936622262001037\n",
      "  batch 20 loss: 1.5950306475162506\n",
      "  batch 30 loss: 1.592026698589325\n",
      "  batch 40 loss: 1.5919599652290344\n",
      "  batch 50 loss: 1.5904400646686554\n",
      "  batch 60 loss: 1.5939450442790986\n",
      "  batch 70 loss: 1.5813089072704316\n",
      "  batch 80 loss: 1.5947670072317124\n",
      "LOSS train 1.5947670072317124 valid 1.5896975553035737\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 1.5853248357772827\n",
      "  batch 20 loss: 1.5867553919553756\n",
      "  batch 30 loss: 1.588665145635605\n",
      "  batch 40 loss: 1.5908859372138977\n",
      "  batch 50 loss: 1.5670086205005647\n",
      "  batch 60 loss: 1.5816807836294173\n",
      "  batch 70 loss: 1.5824969351291656\n",
      "  batch 80 loss: 1.581833976507187\n",
      "LOSS train 1.581833976507187 valid 1.5820139729976654\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 1.5621032804250716\n",
      "  batch 20 loss: 1.5741597622632981\n",
      "  batch 30 loss: 1.5606927037239076\n",
      "  batch 40 loss: 1.5642546981573104\n",
      "  batch 50 loss: 1.5833021849393845\n",
      "  batch 60 loss: 1.568601456284523\n",
      "  batch 70 loss: 1.5623364627361298\n",
      "  batch 80 loss: 1.5842954844236374\n",
      "LOSS train 1.5842954844236374 valid 1.5714179539680482\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 1.5501886636018753\n",
      "  batch 20 loss: 1.5618564367294312\n",
      "  batch 30 loss: 1.570684626698494\n",
      "  batch 40 loss: 1.5511370956897736\n",
      "  batch 50 loss: 1.5576132893562318\n",
      "  batch 60 loss: 1.5561135590076447\n",
      "  batch 70 loss: 1.5780316650867463\n",
      "  batch 80 loss: 1.5401546031236648\n",
      "LOSS train 1.5401546031236648 valid 1.5597713792324066\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 1.53459133207798\n",
      "  batch 20 loss: 1.5668752193450928\n",
      "  batch 30 loss: 1.5449556678533554\n",
      "  batch 40 loss: 1.5356198757886887\n",
      "  batch 50 loss: 1.5504471391439438\n",
      "  batch 60 loss: 1.5129073321819306\n",
      "  batch 70 loss: 1.5630421459674835\n",
      "  batch 80 loss: 1.5330909311771392\n",
      "LOSS train 1.5330909311771392 valid 1.5496915793418884\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 1.533489739894867\n",
      "  batch 20 loss: 1.5072581589221954\n",
      "  batch 30 loss: 1.5147076100111008\n",
      "  batch 40 loss: 1.5440521210432052\n",
      "  batch 50 loss: 1.5222682774066925\n",
      "  batch 60 loss: 1.5552082270383836\n",
      "  batch 70 loss: 1.5179108768701552\n",
      "  batch 80 loss: 1.4897377997636796\n",
      "LOSS train 1.4897377997636796 valid 1.5310387957096099\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 1.5205105125904084\n",
      "  batch 20 loss: 1.489426788687706\n",
      "  batch 30 loss: 1.5246636360883712\n",
      "  batch 40 loss: 1.4954888284206391\n",
      "  batch 50 loss: 1.51121985912323\n",
      "  batch 60 loss: 1.515459743142128\n",
      "  batch 70 loss: 1.439752784371376\n",
      "  batch 80 loss: 1.5034843981266022\n",
      "LOSS train 1.5034843981266022 valid 1.5043016064167023\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 1.4563982725143432\n",
      "  batch 20 loss: 1.4922135680913926\n",
      "  batch 30 loss: 1.4661442697048188\n",
      "  batch 40 loss: 1.4525208860635757\n",
      "  batch 50 loss: 1.4347851380705834\n",
      "  batch 60 loss: 1.5003427028656007\n",
      "  batch 70 loss: 1.488942387700081\n",
      "  batch 80 loss: 1.4410926967859268\n",
      "LOSS train 1.4410926967859268 valid 1.4751037538051606\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 1.3879706367850304\n",
      "  batch 20 loss: 1.402192285656929\n",
      "  batch 30 loss: 1.4046481549739838\n",
      "  batch 40 loss: 1.405382540822029\n",
      "  batch 50 loss: 1.379163059592247\n",
      "  batch 60 loss: 1.4355361580848693\n",
      "  batch 70 loss: 1.501691387593746\n",
      "  batch 80 loss: 1.4621873199939728\n",
      "LOSS train 1.4621873199939728 valid 1.4387698316574096\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 1.410642333328724\n",
      "  batch 20 loss: 1.4049533501267433\n",
      "  batch 30 loss: 1.3845962390303612\n",
      "  batch 40 loss: 1.3589078232645988\n",
      "  batch 50 loss: 1.3615018725395203\n",
      "  batch 60 loss: 1.4226368069648743\n",
      "  batch 70 loss: 1.3610098198056222\n",
      "  batch 80 loss: 1.371322537958622\n",
      "LOSS train 1.371322537958622 valid 1.3892823469638824\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 1.3304257541894913\n",
      "  batch 20 loss: 1.3895657703280448\n",
      "  batch 30 loss: 1.38460683375597\n",
      "  batch 40 loss: 1.3162726595997811\n",
      "  batch 50 loss: 1.180922205746174\n",
      "  batch 60 loss: 1.3421686828136443\n",
      "  batch 70 loss: 1.3053680643439294\n",
      "  batch 80 loss: 1.2963670372962952\n",
      "LOSS train 1.2963670372962952 valid 1.3274112403392793\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 1.262002769112587\n",
      "  batch 20 loss: 1.224262349307537\n",
      "  batch 30 loss: 1.2735095128417016\n",
      "  batch 40 loss: 1.3257011219859123\n",
      "  batch 50 loss: 1.1896991811692714\n",
      "  batch 60 loss: 1.2258340269327164\n",
      "  batch 70 loss: 1.248410599678755\n",
      "  batch 80 loss: 1.2899569526314736\n",
      "LOSS train 1.2899569526314736 valid 1.256950899362564\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 1.1755151994526387\n",
      "  batch 20 loss: 1.2234042927622795\n",
      "  batch 30 loss: 1.2083341419696807\n",
      "  batch 40 loss: 1.1603488728404046\n",
      "  batch 50 loss: 1.1337145499885082\n",
      "  batch 60 loss: 1.1543697826564312\n",
      "  batch 70 loss: 1.193197498470545\n",
      "  batch 80 loss: 1.2207810066640377\n",
      "LOSS train 1.2207810066640377 valid 1.1668736287951469\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 1.1315743386745454\n",
      "  batch 20 loss: 1.1502298049628734\n",
      "  batch 30 loss: 1.1166177652776241\n",
      "  batch 40 loss: 1.1801282055675983\n",
      "  batch 50 loss: 1.109433189406991\n",
      "  batch 60 loss: 1.0758156217634678\n",
      "  batch 70 loss: 1.1274008445441723\n",
      "  batch 80 loss: 1.0092170812189578\n",
      "LOSS train 1.0092170812189578 valid 1.101359507739544\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 1.1790385656058788\n",
      "  batch 20 loss: 1.0260655261576175\n",
      "  batch 30 loss: 1.0230912379920483\n",
      "  batch 40 loss: 0.9403086006641388\n",
      "  batch 50 loss: 0.9537202652543784\n",
      "  batch 60 loss: 1.0946616780012846\n",
      "  batch 70 loss: 0.9484869047999382\n",
      "  batch 80 loss: 1.0264963030815124\n",
      "LOSS train 1.0264963030815124 valid 1.0494582983851433\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 1.0245227381587028\n",
      "  batch 20 loss: 0.8250354900956154\n",
      "  batch 30 loss: 1.0072571884840726\n",
      "  batch 40 loss: 0.9356596179306507\n",
      "  batch 50 loss: 0.9515475045889616\n",
      "  batch 60 loss: 1.0547851838171483\n",
      "  batch 70 loss: 0.9485781192779541\n",
      "  batch 80 loss: 1.0915825590491295\n",
      "LOSS train 1.0915825590491295 valid 1.0605557996034622\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 1.0518584929406642\n",
      "  batch 20 loss: 0.8562587454915047\n",
      "  batch 30 loss: 0.9617144953459501\n",
      "  batch 40 loss: 0.8351289212703705\n",
      "  batch 50 loss: 0.9472157772630453\n",
      "  batch 60 loss: 0.7635670445859433\n",
      "  batch 70 loss: 1.0245419699698686\n",
      "  batch 80 loss: 0.8931472344323993\n",
      "LOSS train 0.8931472344323993 valid 0.8907493459433318\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.9677055004984141\n",
      "  batch 20 loss: 0.8333159172907472\n",
      "  batch 30 loss: 0.8028206469491124\n",
      "  batch 40 loss: 0.8831774754449725\n",
      "  batch 50 loss: 0.9498731352388858\n",
      "  batch 60 loss: 0.9065757438540458\n",
      "  batch 70 loss: 0.7802938412874937\n",
      "  batch 80 loss: 0.7858124705962837\n",
      "LOSS train 0.7858124705962837 valid 0.8435206115245819\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.9635238710790872\n",
      "  batch 20 loss: 0.7815631486475467\n",
      "  batch 30 loss: 0.6400666618719697\n",
      "  batch 40 loss: 0.7131397658959031\n",
      "  batch 50 loss: 0.6618638925254345\n",
      "  batch 60 loss: 0.8962563065811991\n",
      "  batch 70 loss: 0.8933942360803485\n",
      "  batch 80 loss: 0.8460608663037419\n",
      "LOSS train 0.8460608663037419 valid 0.9768341355025768\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.7872661832720041\n",
      "  batch 20 loss: 0.688505882024765\n",
      "  batch 30 loss: 0.8541490806266665\n",
      "  batch 40 loss: 0.8562450112774969\n",
      "  batch 50 loss: 0.8624184717424214\n",
      "  batch 60 loss: 0.6697620125487447\n",
      "  batch 70 loss: 0.7807254646904767\n",
      "  batch 80 loss: 0.7111033371649682\n",
      "LOSS train 0.7111033371649682 valid 0.7001902912184597\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 0.7753054089844227\n",
      "  batch 20 loss: 0.7558162619359792\n",
      "  batch 30 loss: 0.7432805588468909\n",
      "  batch 40 loss: 0.7199786496348679\n",
      "  batch 50 loss: 0.8054958591237664\n",
      "  batch 60 loss: 0.6849666354246438\n",
      "  batch 70 loss: 0.7000069420784711\n",
      "  batch 80 loss: 0.6892880717292428\n",
      "LOSS train 0.6892880717292428 valid 0.6648154397681355\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 0.6261846930719912\n",
      "  batch 20 loss: 0.8289467876777052\n",
      "  batch 30 loss: 0.6933002218604087\n",
      "  batch 40 loss: 0.5571026051416993\n",
      "  batch 50 loss: 0.7492394579574466\n",
      "  batch 60 loss: 0.6943510530516506\n",
      "  batch 70 loss: 0.6942711974494159\n",
      "  batch 80 loss: 0.6948893865570426\n",
      "LOSS train 0.6948893865570426 valid 0.6019027410820127\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 0.7078277659602463\n",
      "  batch 20 loss: 0.6879226434975863\n",
      "  batch 30 loss: 0.6020185492001474\n",
      "  batch 40 loss: 0.6563965230248868\n",
      "  batch 50 loss: 0.6162537759169936\n",
      "  batch 60 loss: 0.5834042445756495\n",
      "  batch 70 loss: 0.666151900216937\n",
      "  batch 80 loss: 0.6697927463334054\n",
      "LOSS train 0.6697927463334054 valid 0.6410916673764586\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 0.5740153556223959\n",
      "  batch 20 loss: 0.685009648045525\n",
      "  batch 30 loss: 0.5068727986887097\n",
      "  batch 40 loss: 0.5929444333072752\n",
      "  batch 50 loss: 0.6608299661427737\n",
      "  batch 60 loss: 0.5838151657022536\n",
      "  batch 70 loss: 0.615919345850125\n",
      "  batch 80 loss: 0.6816541633568705\n",
      "LOSS train 0.6816541633568705 valid 0.6026883203536272\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 0.49049720547627657\n",
      "  batch 20 loss: 0.5938192924018949\n",
      "  batch 30 loss: 0.6167568418662995\n",
      "  batch 40 loss: 0.61484286012128\n",
      "  batch 50 loss: 0.5426152155734598\n",
      "  batch 60 loss: 0.6048652444966137\n",
      "  batch 70 loss: 0.49610704481601714\n",
      "  batch 80 loss: 0.5788450152846053\n",
      "LOSS train 0.5788450152846053 valid 0.47226413681171836\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 0.65398757182993\n",
      "  batch 20 loss: 0.4903650602325797\n",
      "  batch 30 loss: 0.5403506723698228\n",
      "  batch 40 loss: 0.4297047082800418\n",
      "  batch 50 loss: 0.5223052008775995\n",
      "  batch 60 loss: 0.5713907810859382\n",
      "  batch 70 loss: 0.49164673751220106\n",
      "  batch 80 loss: 0.4101632036734372\n",
      "LOSS train 0.4101632036734372 valid 0.4265219367481768\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 0.5141076942440123\n",
      "  batch 20 loss: 0.4779448346234858\n",
      "  batch 30 loss: 0.5660977398976683\n",
      "  batch 40 loss: 0.5566161795984954\n",
      "  batch 50 loss: 0.5253353291191161\n",
      "  batch 60 loss: 0.4606088514672592\n",
      "  batch 70 loss: 0.3517891303403303\n",
      "  batch 80 loss: 0.40996960236225277\n",
      "LOSS train 0.40996960236225277 valid 0.39443652775138616\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 0.466096511669457\n",
      "  batch 20 loss: 0.48797136950306597\n",
      "  batch 30 loss: 0.43630460761487483\n",
      "  batch 40 loss: 0.33357142517343163\n",
      "  batch 50 loss: 0.3948093756102026\n",
      "  batch 60 loss: 0.5230672939214855\n",
      "  batch 70 loss: 0.39559928099624814\n",
      "  batch 80 loss: 0.395068412204273\n",
      "LOSS train 0.395068412204273 valid 0.3172313724178821\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 0.35580949459690603\n",
      "  batch 20 loss: 0.4859575117938221\n",
      "  batch 30 loss: 0.35333060203120115\n",
      "  batch 40 loss: 0.46235489319078626\n",
      "  batch 50 loss: 0.40389380347914994\n",
      "  batch 60 loss: 0.42106833518482745\n",
      "  batch 70 loss: 0.33174046580679717\n",
      "  batch 80 loss: 0.36660466094035654\n",
      "LOSS train 0.36660466094035654 valid 0.31902175924740733\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 0.3792696027085185\n",
      "  batch 20 loss: 0.2933063135482371\n",
      "  batch 30 loss: 0.3805799645371735\n",
      "  batch 40 loss: 0.3539710831362754\n",
      "  batch 50 loss: 0.39330827896483245\n",
      "  batch 60 loss: 0.3379038258572109\n",
      "  batch 70 loss: 0.4963209770619869\n",
      "  batch 80 loss: 0.27548960195854305\n",
      "LOSS train 0.27548960195854305 valid 0.2847410921473056\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 0.31402095123194157\n",
      "  batch 20 loss: 0.3346258235629648\n",
      "  batch 30 loss: 0.34426115844398736\n",
      "  batch 40 loss: 0.3641393911791965\n",
      "  batch 50 loss: 0.2762065231334418\n",
      "  batch 60 loss: 0.29022897768300027\n",
      "  batch 70 loss: 0.3180569097166881\n",
      "  batch 80 loss: 0.33100307097192855\n",
      "LOSS train 0.33100307097192855 valid 0.21701185390353203\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 0.2903343944810331\n",
      "  batch 20 loss: 0.2871655979892239\n",
      "  batch 30 loss: 0.25383414186071607\n",
      "  batch 40 loss: 0.25222224004101007\n",
      "  batch 50 loss: 0.27659339816309514\n",
      "  batch 60 loss: 0.3504215482040308\n",
      "  batch 70 loss: 0.24795426722848787\n",
      "  batch 80 loss: 0.28611306925304236\n",
      "LOSS train 0.28611306925304236 valid 0.4240562852844596\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 0.30980213205330076\n",
      "  batch 20 loss: 0.2420750042423606\n",
      "  batch 30 loss: 0.28362135258503257\n",
      "  batch 40 loss: 0.24831466214964165\n",
      "  batch 50 loss: 0.2851421664934605\n",
      "  batch 60 loss: 0.1900402144063264\n",
      "  batch 70 loss: 0.279865110386163\n",
      "  batch 80 loss: 0.2932556051178835\n",
      "LOSS train 0.2932556051178835 valid 0.508854634705931\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 0.34786318365950136\n",
      "  batch 20 loss: 0.25889445734210315\n",
      "  batch 30 loss: 0.2258991748210974\n",
      "  batch 40 loss: 0.2348374695633538\n",
      "  batch 50 loss: 0.20504797948524356\n",
      "  batch 60 loss: 0.19751444045687094\n",
      "  batch 70 loss: 0.242295702744741\n",
      "  batch 80 loss: 0.21025886824354528\n",
      "LOSS train 0.21025886824354528 valid 0.5323829661495983\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 0.2816673230729066\n",
      "  batch 20 loss: 0.1972200697986409\n",
      "  batch 30 loss: 0.21345384824089705\n",
      "  batch 40 loss: 0.1974731619702652\n",
      "  batch 50 loss: 0.2528859998332337\n",
      "  batch 60 loss: 0.2217892271117307\n",
      "  batch 70 loss: 0.23392598946229554\n",
      "  batch 80 loss: 0.14835349802160636\n",
      "LOSS train 0.14835349802160636 valid 0.11522011402994395\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 0.25619858225109055\n",
      "  batch 20 loss: 0.2019207096192986\n",
      "  batch 30 loss: 0.1870545932208188\n",
      "  batch 40 loss: 0.27220330710988494\n",
      "  batch 50 loss: 0.14955367187503726\n",
      "  batch 60 loss: 0.12226895825006068\n",
      "  batch 70 loss: 0.17666100575588645\n",
      "  batch 80 loss: 0.134523157030344\n",
      "LOSS train 0.134523157030344 valid 0.1470032935659401\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 0.17970039551146327\n",
      "  batch 20 loss: 0.1368665214860812\n",
      "  batch 30 loss: 0.17364631417440252\n",
      "  batch 40 loss: 0.14309194321976976\n",
      "  batch 50 loss: 0.1752731257583946\n",
      "  batch 60 loss: 0.19521593974204735\n",
      "  batch 70 loss: 0.13766592968604527\n",
      "  batch 80 loss: 0.1831539903068915\n",
      "LOSS train 0.1831539903068915 valid 0.09324175259796902\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.15473186940071174\n",
      "  batch 20 loss: 0.22311899717897177\n",
      "  batch 30 loss: 0.17240559099591338\n",
      "  batch 40 loss: 0.14304549758089707\n",
      "  batch 50 loss: 0.15063165542669593\n",
      "  batch 60 loss: 0.13666065594879911\n",
      "  batch 70 loss: 0.1393011787615251\n",
      "  batch 80 loss: 0.13210098585113883\n",
      "LOSS train 0.13210098585113883 valid 0.09610437800176441\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.1394560229149647\n",
      "  batch 20 loss: 0.13288730966160073\n",
      "  batch 30 loss: 0.1622343515045941\n",
      "  batch 40 loss: 0.15007712405058554\n",
      "  batch 50 loss: 0.14640999580733477\n",
      "  batch 60 loss: 0.11584941920591518\n",
      "  batch 70 loss: 0.132995320612099\n",
      "  batch 80 loss: 0.12165287536336108\n",
      "LOSS train 0.12165287536336108 valid 0.11193271648138761\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 0.14866958904312924\n",
      "  batch 20 loss: 0.15743777599418535\n",
      "  batch 30 loss: 0.09607412116602063\n",
      "  batch 40 loss: 0.13318704680423252\n",
      "  batch 50 loss: 0.12276311696623451\n",
      "  batch 60 loss: 0.13250227480893956\n",
      "  batch 70 loss: 0.10300842477008701\n",
      "  batch 80 loss: 0.10449569876655004\n",
      "LOSS train 0.10449569876655004 valid 0.10470192156732082\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.16390360563527792\n",
      "  batch 20 loss: 0.11399630802916363\n",
      "  batch 30 loss: 0.08998493142426015\n",
      "  batch 40 loss: 0.12741330283461139\n",
      "  batch 50 loss: 0.06898944310378283\n",
      "  batch 60 loss: 0.14132435912033542\n",
      "  batch 70 loss: 0.1272769471455831\n",
      "  batch 80 loss: 0.1373182665440254\n",
      "LOSS train 0.1373182665440254 valid 0.07525200170697645\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.10517301593208686\n",
      "  batch 20 loss: 0.1095553599880077\n",
      "  batch 30 loss: 0.09663402987062\n",
      "  batch 40 loss: 0.09256668416783213\n",
      "  batch 50 loss: 0.09228719833190553\n",
      "  batch 60 loss: 0.13896198907168583\n",
      "  batch 70 loss: 0.12962717110058292\n",
      "  batch 80 loss: 0.11244457701686769\n",
      "LOSS train 0.11244457701686769 valid 0.06824083029758185\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.08443905682070181\n",
      "  batch 20 loss: 0.08611111590871587\n",
      "  batch 30 loss: 0.10294870442012324\n",
      "  batch 40 loss: 0.0800364604452625\n",
      "  batch 50 loss: 0.11801103067409713\n",
      "  batch 60 loss: 0.08863573449198156\n",
      "  batch 70 loss: 0.10894771103048698\n",
      "  batch 80 loss: 0.10511305425607134\n",
      "LOSS train 0.10511305425607134 valid 0.04513693463988602\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.09229442328214646\n",
      "  batch 20 loss: 0.0526138368411921\n",
      "  batch 30 loss: 0.12702378723188304\n",
      "  batch 40 loss: 0.08514410986099392\n",
      "  batch 50 loss: 0.09578818076406606\n",
      "  batch 60 loss: 0.09133790071937256\n",
      "  batch 70 loss: 0.07859619092778303\n",
      "  batch 80 loss: 0.07601076445716899\n",
      "LOSS train 0.07601076445716899 valid 0.0677320864587091\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.11154408812290058\n",
      "  batch 20 loss: 0.08651361619122326\n",
      "  batch 30 loss: 0.08349834234104492\n",
      "  batch 40 loss: 0.10123672859626823\n",
      "  batch 50 loss: 0.07309432334150187\n",
      "  batch 60 loss: 0.08014706196263433\n",
      "  batch 70 loss: 0.07506311119068414\n",
      "  batch 80 loss: 0.10744192996644415\n",
      "LOSS train 0.10744192996644415 valid 0.04521973638795316\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.07471732422709465\n",
      "  batch 20 loss: 0.09161326870089397\n",
      "  batch 30 loss: 0.07996579583268612\n",
      "  batch 40 loss: 0.08153313444345259\n",
      "  batch 50 loss: 0.045180901914136484\n",
      "  batch 60 loss: 0.07431029388390016\n",
      "  batch 70 loss: 0.07551242138724774\n",
      "  batch 80 loss: 0.07056096436572261\n",
      "LOSS train 0.07056096436572261 valid 0.04965672232210636\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.07226378874038346\n",
      "  batch 20 loss: 0.05842067730845883\n",
      "  batch 30 loss: 0.08192324645933695\n",
      "  batch 40 loss: 0.0657326326239854\n",
      "  batch 50 loss: 0.08422976873116568\n",
      "  batch 60 loss: 0.08239122904487886\n",
      "  batch 70 loss: 0.05325196244521067\n",
      "  batch 80 loss: 0.07741249255486764\n",
      "LOSS train 0.07741249255486764 valid 0.034218172067776324\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.07341533644357696\n",
      "  batch 20 loss: 0.07131874332844745\n",
      "  batch 30 loss: 0.03784191464364994\n",
      "  batch 40 loss: 0.054190815758192915\n",
      "  batch 50 loss: 0.07030218383879401\n",
      "  batch 60 loss: 0.06638094608788378\n",
      "  batch 70 loss: 0.07271082770312205\n",
      "  batch 80 loss: 0.06487136229989118\n",
      "LOSS train 0.06487136229989118 valid 0.041613636882975695\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.047437121658003886\n",
      "  batch 20 loss: 0.06648706031264737\n",
      "  batch 30 loss: 0.06643954460741952\n",
      "  batch 40 loss: 0.057780033070594074\n",
      "  batch 50 loss: 0.06371880223159679\n",
      "  batch 60 loss: 0.08753518695302773\n",
      "  batch 70 loss: 0.055086864848271945\n",
      "  batch 80 loss: 0.03623188431665767\n",
      "LOSS train 0.03623188431665767 valid 0.03628937073051929\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.06203833909821697\n",
      "  batch 20 loss: 0.09017163781099953\n",
      "  batch 30 loss: 0.04645930689293891\n",
      "  batch 40 loss: 0.04804131810960825\n",
      "  batch 50 loss: 0.04509459791588597\n",
      "  batch 60 loss: 0.07908647853764705\n",
      "  batch 70 loss: 0.06270920276292599\n",
      "  batch 80 loss: 0.05470449881977402\n",
      "LOSS train 0.05470449881977402 valid 0.04264921997440979\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.060883485624799505\n",
      "  batch 20 loss: 0.05498155697423499\n",
      "  batch 30 loss: 0.05114068463153672\n",
      "  batch 40 loss: 0.02784885773580754\n",
      "  batch 50 loss: 0.0812620859258459\n",
      "  batch 60 loss: 0.06592570959619479\n",
      "  batch 70 loss: 0.048957587976474315\n",
      "  batch 80 loss: 0.04958097389608156\n",
      "LOSS train 0.04958097389608156 valid 0.0292777216329705\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.04520902248332277\n",
      "  batch 20 loss: 0.06486300375836436\n",
      "  batch 30 loss: 0.05402012122503948\n",
      "  batch 40 loss: 0.043382771326287185\n",
      "  batch 50 loss: 0.046407471515703944\n",
      "  batch 60 loss: 0.05465608596859965\n",
      "  batch 70 loss: 0.04876330879924353\n",
      "  batch 80 loss: 0.04184300170745701\n",
      "LOSS train 0.04184300170745701 valid 0.02846766416914761\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.03634543943335302\n",
      "  batch 20 loss: 0.05066969510662602\n",
      "  batch 30 loss: 0.04418308140593581\n",
      "  batch 40 loss: 0.06346599449170753\n",
      "  batch 50 loss: 0.036627365632739384\n",
      "  batch 60 loss: 0.04432863185647875\n",
      "  batch 70 loss: 0.040762758307391775\n",
      "  batch 80 loss: 0.06305572018609382\n",
      "LOSS train 0.06305572018609382 valid 0.026572607106063514\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.048975021517253484\n",
      "  batch 20 loss: 0.051494642881152686\n",
      "  batch 30 loss: 0.05116945630288683\n",
      "  batch 40 loss: 0.048373898287536576\n",
      "  batch 50 loss: 0.043957107304595414\n",
      "  batch 60 loss: 0.042311513985623606\n",
      "  batch 70 loss: 0.028777445081504992\n",
      "  batch 80 loss: 0.05382999089342775\n",
      "LOSS train 0.05382999089342775 valid 0.029331795290345325\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 0.02330808392725885\n",
      "  batch 20 loss: 0.05762187274522148\n",
      "  batch 30 loss: 0.04168244635075098\n",
      "  batch 40 loss: 0.04237750358588528\n",
      "  batch 50 loss: 0.039692447945708406\n",
      "  batch 60 loss: 0.07050428204092896\n",
      "  batch 70 loss: 0.0407356282841647\n",
      "  batch 80 loss: 0.029170540055201855\n",
      "LOSS train 0.029170540055201855 valid 0.03273928349488415\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.049692262249300255\n",
      "  batch 20 loss: 0.04000883482367499\n",
      "  batch 30 loss: 0.029434508658596314\n",
      "  batch 40 loss: 0.05002815518528223\n",
      "  batch 50 loss: 0.03946759833779652\n",
      "  batch 60 loss: 0.04666334221983561\n",
      "  batch 70 loss: 0.03643967081588926\n",
      "  batch 80 loss: 0.04849296039610636\n",
      "LOSS train 0.04849296039610636 valid 0.019407039518700913\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.027937834574549923\n",
      "  batch 20 loss: 0.04067950049211504\n",
      "  batch 30 loss: 0.03631214454071596\n",
      "  batch 40 loss: 0.027794545223878232\n",
      "  batch 50 loss: 0.042116920006810686\n",
      "  batch 60 loss: 0.03246678855502978\n",
      "  batch 70 loss: 0.049681944024632683\n",
      "  batch 80 loss: 0.05252657459932379\n",
      "LOSS train 0.05252657459932379 valid 0.01890226068906486\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.03816528415918583\n",
      "  batch 20 loss: 0.030996870415401646\n",
      "  batch 30 loss: 0.03213639586756471\n",
      "  batch 40 loss: 0.07968936690303963\n",
      "  batch 50 loss: 0.03599989857902983\n",
      "  batch 60 loss: 0.03483286548143951\n",
      "  batch 70 loss: 0.022260691339033654\n",
      "  batch 80 loss: 0.0307764286728343\n",
      "LOSS train 0.0307764286728343 valid 0.015077879574964754\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.03224724763422273\n",
      "  batch 20 loss: 0.03205177739946521\n",
      "  batch 30 loss: 0.05204318199685076\n",
      "  batch 40 loss: 0.034406852099346\n",
      "  batch 50 loss: 0.0299454801963293\n",
      "  batch 60 loss: 0.0395749921357492\n",
      "  batch 70 loss: 0.021565658826148137\n",
      "  batch 80 loss: 0.04503089818172157\n",
      "LOSS train 0.04503089818172157 valid 0.01802365118812304\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.03698692783655133\n",
      "  batch 20 loss: 0.026887385608279146\n",
      "  batch 30 loss: 0.024027513324108442\n",
      "  batch 40 loss: 0.04461266083744704\n",
      "  batch 50 loss: 0.030961854584165848\n",
      "  batch 60 loss: 0.029456019945791924\n",
      "  batch 70 loss: 0.033152445507585074\n",
      "  batch 80 loss: 0.026121156876615714\n",
      "LOSS train 0.026121156876615714 valid 0.01532713373133447\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.032682590500917284\n",
      "  batch 20 loss: 0.022723775659687816\n",
      "  batch 30 loss: 0.030664529179921372\n",
      "  batch 40 loss: 0.019548419657803608\n",
      "  batch 50 loss: 0.038642386186256775\n",
      "  batch 60 loss: 0.03454407817771425\n",
      "  batch 70 loss: 0.03214427239217912\n",
      "  batch 80 loss: 0.037339945236453786\n",
      "LOSS train 0.037339945236453786 valid 0.01592512351053301\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.03796610253630206\n",
      "  batch 20 loss: 0.018904722067236434\n",
      "  batch 30 loss: 0.029425357254513073\n",
      "  batch 40 loss: 0.04547212565666996\n",
      "  batch 50 loss: 0.022075364439660915\n",
      "  batch 60 loss: 0.022394924531545256\n",
      "  batch 70 loss: 0.03580063888293807\n",
      "  batch 80 loss: 0.03178011951385997\n",
      "LOSS train 0.03178011951385997 valid 0.020120694935321808\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.019904267525998876\n",
      "  batch 20 loss: 0.032023262239817996\n",
      "  batch 30 loss: 0.025527756800875066\n",
      "  batch 40 loss: 0.026206590217771008\n",
      "  batch 50 loss: 0.021400601080677005\n",
      "  batch 60 loss: 0.032397673532250336\n",
      "  batch 70 loss: 0.016377670622023287\n",
      "  batch 80 loss: 0.04752253906917758\n",
      "LOSS train 0.04752253906917758 valid 0.01728858375863638\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.027592679126246366\n",
      "  batch 20 loss: 0.040377405823528535\n",
      "  batch 30 loss: 0.017601758224191143\n",
      "  batch 40 loss: 0.02593545670970343\n",
      "  batch 50 loss: 0.01610803714429494\n",
      "  batch 60 loss: 0.031261465961142675\n",
      "  batch 70 loss: 0.02773872409889009\n",
      "  batch 80 loss: 0.029642629812587983\n",
      "LOSS train 0.029642629812587983 valid 0.018754969721776434\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.04277374391967896\n",
      "  batch 20 loss: 0.01458542203035904\n",
      "  batch 30 loss: 0.02351366086222697\n",
      "  batch 40 loss: 0.024051142018288375\n",
      "  batch 50 loss: 0.024335293770127463\n",
      "  batch 60 loss: 0.024009025048144395\n",
      "  batch 70 loss: 0.02162429382588016\n",
      "  batch 80 loss: 0.036711158448451894\n",
      "LOSS train 0.036711158448451894 valid 0.020714555748854765\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.017418308305786924\n",
      "  batch 20 loss: 0.03530424983910052\n",
      "  batch 30 loss: 0.026849241572199388\n",
      "  batch 40 loss: 0.02180061192048015\n",
      "  batch 50 loss: 0.02144442932039965\n",
      "  batch 60 loss: 0.022619482971640535\n",
      "  batch 70 loss: 0.026120149298367323\n",
      "  batch 80 loss: 0.024221944432065358\n",
      "LOSS train 0.024221944432065358 valid 0.019099368220195173\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.02451173223598744\n",
      "  batch 20 loss: 0.02259500572035904\n",
      "  batch 30 loss: 0.02295071933622239\n",
      "  batch 40 loss: 0.02011473744205432\n",
      "  batch 50 loss: 0.015859333795378915\n",
      "  batch 60 loss: 0.021863834598480025\n",
      "  batch 70 loss: 0.01931246239691973\n",
      "  batch 80 loss: 0.030564387772028568\n",
      "LOSS train 0.030564387772028568 valid 0.022537265949649735\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.024942918019223725\n",
      "  batch 20 loss: 0.01630901536118472\n",
      "  batch 30 loss: 0.023827833207906223\n",
      "  batch 40 loss: 0.02659455990724382\n",
      "  batch 50 loss: 0.02628902687720256\n",
      "  batch 60 loss: 0.023805675224866717\n",
      "  batch 70 loss: 0.01742420641530771\n",
      "  batch 80 loss: 0.018892197084642248\n",
      "LOSS train 0.018892197084642248 valid 0.051541754730278624\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.023100402564159593\n",
      "  batch 20 loss: 0.020516464031243232\n",
      "  batch 30 loss: 0.03217641187366098\n",
      "  batch 40 loss: 0.026260361044842286\n",
      "  batch 50 loss: 0.027761822920001577\n",
      "  batch 60 loss: 0.01626006799779134\n",
      "  batch 70 loss: 0.02174882803665241\n",
      "  batch 80 loss: 0.013066144545882707\n",
      "LOSS train 0.013066144545882707 valid 0.014041959817986935\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.01814129051272175\n",
      "  batch 20 loss: 0.016760612278449116\n",
      "  batch 30 loss: 0.021956874709212572\n",
      "  batch 40 loss: 0.025609545768384125\n",
      "  batch 50 loss: 0.02943587896443205\n",
      "  batch 60 loss: 0.025175004216725937\n",
      "  batch 70 loss: 0.018301824069203575\n",
      "  batch 80 loss: 0.017544331811222946\n",
      "LOSS train 0.017544331811222946 valid 0.01373439067916479\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.020131357686477714\n",
      "  batch 20 loss: 0.022617506666574628\n",
      "  batch 30 loss: 0.018079716911597644\n",
      "  batch 40 loss: 0.02312831246163114\n",
      "  batch 50 loss: 0.019383512087370037\n",
      "  batch 60 loss: 0.027585673951398347\n",
      "  batch 70 loss: 0.017525469528482062\n",
      "  batch 80 loss: 0.015515950708504533\n",
      "LOSS train 0.015515950708504533 valid 0.011930810594931245\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.03165303213900188\n",
      "  batch 20 loss: 0.017485839224536903\n",
      "  batch 30 loss: 0.016393833425536287\n",
      "  batch 40 loss: 0.023383842186740368\n",
      "  batch 50 loss: 0.012408652802696452\n",
      "  batch 60 loss: 0.022392338743884466\n",
      "  batch 70 loss: 0.019048564308468484\n",
      "  batch 80 loss: 0.010904594522799016\n",
      "LOSS train 0.010904594522799016 valid 0.010729180628259201\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.010295207620220025\n",
      "  batch 20 loss: 0.025798768739332446\n",
      "  batch 30 loss: 0.028970475579990308\n",
      "  batch 40 loss: 0.01605262162993313\n",
      "  batch 50 loss: 0.020476486886036583\n",
      "  batch 60 loss: 0.018166141808615067\n",
      "  batch 70 loss: 0.01808151684999757\n",
      "  batch 80 loss: 0.012612517240268063\n",
      "LOSS train 0.012612517240268063 valid 0.015363483398687094\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.01676938990349299\n",
      "  batch 20 loss: 0.020009885155013764\n",
      "  batch 30 loss: 0.02077341003823676\n",
      "  batch 40 loss: 0.024937143531860783\n",
      "  batch 50 loss: 0.021678900269034785\n",
      "  batch 60 loss: 0.02123554302379489\n",
      "  batch 70 loss: 0.010498036469653017\n",
      "  batch 80 loss: 0.009420918042815174\n",
      "LOSS train 0.009420918042815174 valid 0.010415822703798767\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 0.023036223804956533\n",
      "  batch 20 loss: 0.020861987533862703\n",
      "  batch 30 loss: 0.009829604366677814\n",
      "  batch 40 loss: 0.017254894984216663\n",
      "  batch 50 loss: 0.018951812248997158\n",
      "  batch 60 loss: 0.014139122243068413\n",
      "  batch 70 loss: 0.019541965220560086\n",
      "  batch 80 loss: 0.0206874814171897\n",
      "LOSS train 0.0206874814171897 valid 0.029042370913084595\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 0.021764008994068718\n",
      "  batch 20 loss: 0.022388283743930516\n",
      "  batch 30 loss: 0.010611605413942016\n",
      "  batch 40 loss: 0.021550611501152162\n",
      "  batch 50 loss: 0.020076214028085813\n",
      "  batch 60 loss: 0.015711409196956082\n",
      "  batch 70 loss: 0.015036065310414414\n",
      "  batch 80 loss: 0.01328631778342242\n",
      "LOSS train 0.01328631778342242 valid 0.01376738160091918\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 0.014448171400363207\n",
      "  batch 20 loss: 0.010981509531120537\n",
      "  batch 30 loss: 0.018888201433946962\n",
      "  batch 40 loss: 0.018995750297108316\n",
      "  batch 50 loss: 0.00995706392568536\n",
      "  batch 60 loss: 0.01776599147997331\n",
      "  batch 70 loss: 0.021763620922865812\n",
      "  batch 80 loss: 0.02133739271594095\n",
      "LOSS train 0.02133739271594095 valid 0.009214295213168952\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 0.015406144418011535\n",
      "  batch 20 loss: 0.01919360374013195\n",
      "  batch 30 loss: 0.016841437608673005\n",
      "  batch 40 loss: 0.019344565504434287\n",
      "  batch 50 loss: 0.014954029248474398\n",
      "  batch 60 loss: 0.01605298553186003\n",
      "  batch 70 loss: 0.01392728102982801\n",
      "  batch 80 loss: 0.01018676302228414\n",
      "LOSS train 0.01018676302228414 valid 0.014120330462465063\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 0.021040989835819347\n",
      "  batch 20 loss: 0.019302060425980018\n",
      "  batch 30 loss: 0.017509666184923844\n",
      "  batch 40 loss: 0.015897729004063877\n",
      "  batch 50 loss: 0.018229290701128775\n",
      "  batch 60 loss: 0.011150738320793607\n",
      "  batch 70 loss: 0.014266218951161137\n",
      "  batch 80 loss: 0.009544267610181123\n",
      "LOSS train 0.009544267610181123 valid 0.00804615056636976\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 0.021894882561900884\n",
      "  batch 20 loss: 0.021704490119736873\n",
      "  batch 30 loss: 0.010537144759655347\n",
      "  batch 40 loss: 0.016874042072959127\n",
      "  batch 50 loss: 0.015138365548409638\n",
      "  batch 60 loss: 0.014209020947600948\n",
      "  batch 70 loss: 0.012492302850296256\n",
      "  batch 80 loss: 0.012206027828506194\n",
      "LOSS train 0.012206027828506194 valid 0.007394696114934049\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 0.01569636520180211\n",
      "  batch 20 loss: 0.01017446999940148\n",
      "  batch 30 loss: 0.015535408757932601\n",
      "  batch 40 loss: 0.016356140449715895\n",
      "  batch 50 loss: 0.015958966057223734\n",
      "  batch 60 loss: 0.014142473105312092\n",
      "  batch 70 loss: 0.01450970154874085\n",
      "  batch 80 loss: 0.015147661449736916\n",
      "LOSS train 0.015147661449736916 valid 0.006788743041688576\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 0.011564224662288325\n",
      "  batch 20 loss: 0.013752803917304846\n",
      "  batch 30 loss: 0.01114616257509624\n",
      "  batch 40 loss: 0.015442337867352763\n",
      "  batch 50 loss: 0.016727366256236564\n",
      "  batch 60 loss: 0.012682192122883861\n",
      "  batch 70 loss: 0.013895713269812404\n",
      "  batch 80 loss: 0.021356472290790407\n",
      "LOSS train 0.021356472290790407 valid 0.009998441244242713\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 0.013504662955529056\n",
      "  batch 20 loss: 0.010355664169037482\n",
      "  batch 30 loss: 0.012732430298274266\n",
      "  batch 40 loss: 0.01139468769179075\n",
      "  batch 50 loss: 0.016959380889238672\n",
      "  batch 60 loss: 0.010818477992143017\n",
      "  batch 70 loss: 0.017538300973501463\n",
      "  batch 80 loss: 0.015327401819740771\n",
      "LOSS train 0.015327401819740771 valid 0.014560286006890237\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 0.011727543963934295\n",
      "  batch 20 loss: 0.01408442187203036\n",
      "  batch 30 loss: 0.011728059157758253\n",
      "  batch 40 loss: 0.010873105940117967\n",
      "  batch 50 loss: 0.018450500474500586\n",
      "  batch 60 loss: 0.015091142906749155\n",
      "  batch 70 loss: 0.007348753213591408\n",
      "  batch 80 loss: 0.022679566207443713\n",
      "LOSS train 0.022679566207443713 valid 0.00803633135947166\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 0.015943086101833613\n",
      "  batch 20 loss: 0.01157827655551955\n",
      "  batch 30 loss: 0.011585818780440604\n",
      "  batch 40 loss: 0.0076428335214586696\n",
      "  batch 50 loss: 0.012012367732313577\n",
      "  batch 60 loss: 0.008666946266384912\n",
      "  batch 70 loss: 0.016042614635443896\n",
      "  batch 80 loss: 0.014673787783249282\n",
      "LOSS train 0.014673787783249282 valid 0.011638041789701674\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 0.013563045676710317\n",
      "  batch 20 loss: 0.014769538566179108\n",
      "  batch 30 loss: 0.01023556764175737\n",
      "  batch 40 loss: 0.012876859567040811\n",
      "  batch 50 loss: 0.01318490333869704\n",
      "  batch 60 loss: 0.00825818674347829\n",
      "  batch 70 loss: 0.01401265428539773\n",
      "  batch 80 loss: 0.014467882750250283\n",
      "LOSS train 0.014467882750250283 valid 0.009977787012758199\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 0.010691973392385989\n",
      "  batch 20 loss: 0.011716447886647075\n",
      "  batch 30 loss: 0.010175850352970884\n",
      "  batch 40 loss: 0.014591275990460418\n",
      "  batch 50 loss: 0.016048372731165727\n",
      "  batch 60 loss: 0.008663723752397346\n",
      "  batch 70 loss: 0.013550217265219544\n",
      "  batch 80 loss: 0.014919219673538464\n",
      "LOSS train 0.014919219673538464 valid 0.0079026712886116\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 0.007672657737202826\n",
      "  batch 20 loss: 0.01862252417522541\n",
      "  batch 30 loss: 0.01159996747992409\n",
      "  batch 40 loss: 0.015267331670838758\n",
      "  batch 50 loss: 0.011914437208906748\n",
      "  batch 60 loss: 0.010702124257295508\n",
      "  batch 70 loss: 0.011152445373227238\n",
      "  batch 80 loss: 0.0134444849598367\n",
      "LOSS train 0.0134444849598367 valid 0.009925933197082485\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 0.014478220159435295\n",
      "  batch 20 loss: 0.009841744542791275\n",
      "  batch 30 loss: 0.014281285426659452\n",
      "  batch 40 loss: 0.012747106105598505\n",
      "  batch 50 loss: 0.01634014933806611\n",
      "  batch 60 loss: 0.01243493457659497\n",
      "  batch 70 loss: 0.010571394546423108\n",
      "  batch 80 loss: 0.008526732828613603\n",
      "LOSS train 0.008526732828613603 valid 0.006853661101631587\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 0.01271129875240149\n",
      "  batch 20 loss: 0.008582422896142815\n",
      "  batch 30 loss: 0.011303253567530192\n",
      "  batch 40 loss: 0.009049313557625283\n",
      "  batch 50 loss: 0.012306370805526967\n",
      "  batch 60 loss: 0.009677675266721053\n",
      "  batch 70 loss: 0.019270067529396327\n",
      "  batch 80 loss: 0.012576492239895742\n",
      "LOSS train 0.012576492239895742 valid 0.009650534329703078\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 0.009192641539266333\n",
      "  batch 20 loss: 0.013839402557641734\n",
      "  batch 30 loss: 0.011212835447804537\n",
      "  batch 40 loss: 0.0066403272838215345\n",
      "  batch 50 loss: 0.014975321743077075\n",
      "  batch 60 loss: 0.0106086499086814\n",
      "  batch 70 loss: 0.011763079342927085\n",
      "  batch 80 loss: 0.011950043939577881\n",
      "LOSS train 0.011950043939577881 valid 0.008495896113308844\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 0.012397498349128\n",
      "  batch 20 loss: 0.016279635230603162\n",
      "  batch 30 loss: 0.007321434922414482\n",
      "  batch 40 loss: 0.012277741306388635\n",
      "  batch 50 loss: 0.013230985762857018\n",
      "  batch 60 loss: 0.011132120829097402\n",
      "  batch 70 loss: 0.010000773211686464\n",
      "  batch 80 loss: 0.007352008173620561\n",
      "LOSS train 0.007352008173620561 valid 0.006213776305667124\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 0.010548642483263393\n",
      "  batch 20 loss: 0.012114241627568845\n",
      "  batch 30 loss: 0.012439210375487165\n",
      "  batch 40 loss: 0.006511211886754608\n",
      "  batch 50 loss: 0.011783643312446657\n",
      "  batch 60 loss: 0.009029091374395648\n",
      "  batch 70 loss: 0.01592539212979318\n",
      "  batch 80 loss: 0.010910072764636425\n",
      "LOSS train 0.010910072764636425 valid 0.010029090974421706\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 0.008393899626389612\n",
      "  batch 20 loss: 0.01313663532200735\n",
      "  batch 30 loss: 0.01179499035642948\n",
      "  batch 40 loss: 0.009936691074653937\n",
      "  batch 50 loss: 0.014034440486466338\n",
      "  batch 60 loss: 0.007604755008651409\n",
      "  batch 70 loss: 0.010234162526467116\n",
      "  batch 80 loss: 0.007828241952665849\n",
      "LOSS train 0.007828241952665849 valid 0.01051647540036356\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 0.0114946570714892\n",
      "  batch 20 loss: 0.011566929261425685\n",
      "  batch 30 loss: 0.008932464772078675\n",
      "  batch 40 loss: 0.0068115809393930245\n",
      "  batch 50 loss: 0.015387729665962979\n",
      "  batch 60 loss: 0.009277865132935404\n",
      "  batch 70 loss: 0.009333020326812403\n",
      "  batch 80 loss: 0.010003243013125029\n",
      "LOSS train 0.010003243013125029 valid 0.009239582165610044\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 0.009353555609050091\n",
      "  batch 20 loss: 0.010899601130950032\n",
      "  batch 30 loss: 0.012492131143517327\n",
      "  batch 40 loss: 0.008819299690367188\n",
      "  batch 50 loss: 0.008854531618817418\n",
      "  batch 60 loss: 0.00919883393598866\n",
      "  batch 70 loss: 0.01087243792735535\n",
      "  batch 80 loss: 0.012105782955768519\n",
      "LOSS train 0.012105782955768519 valid 0.004850082647171803\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 0.011260315846448066\n",
      "  batch 20 loss: 0.005608077027500258\n",
      "  batch 30 loss: 0.011657086295599584\n",
      "  batch 40 loss: 0.0069157610707407\n",
      "  batch 50 loss: 0.01449233856110368\n",
      "  batch 60 loss: 0.008287755082710646\n",
      "  batch 70 loss: 0.00928614478179952\n",
      "  batch 80 loss: 0.014525022484303918\n",
      "LOSS train 0.014525022484303918 valid 0.00723320459597744\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 0.007658428567447117\n",
      "  batch 20 loss: 0.012773020347594866\n",
      "  batch 30 loss: 0.009685052331042244\n",
      "  batch 40 loss: 0.006187018616583373\n",
      "  batch 50 loss: 0.007072063483064994\n",
      "  batch 60 loss: 0.008702029932283039\n",
      "  batch 70 loss: 0.009975768644653726\n",
      "  batch 80 loss: 0.013654781243894831\n",
      "LOSS train 0.013654781243894831 valid 0.010030118569848128\n",
      "EPOCH 101:\n",
      "  batch 10 loss: 0.004883762092867982\n",
      "  batch 20 loss: 0.014837145259298268\n",
      "  batch 30 loss: 0.006234368540754076\n",
      "  batch 40 loss: 0.00877640529506607\n",
      "  batch 50 loss: 0.010765225686918711\n",
      "  batch 60 loss: 0.008303951076959493\n",
      "  batch 70 loss: 0.010179530474488274\n",
      "  batch 80 loss: 0.00981161997460731\n",
      "LOSS train 0.00981161997460731 valid 0.008035034579224885\n",
      "EPOCH 102:\n",
      "  batch 10 loss: 0.009103345364019333\n",
      "  batch 20 loss: 0.009237459056930675\n",
      "  batch 30 loss: 0.00542959265603713\n",
      "  batch 40 loss: 0.009804300852920278\n",
      "  batch 50 loss: 0.007633461253317364\n",
      "  batch 60 loss: 0.00970761758799199\n",
      "  batch 70 loss: 0.013719487098023819\n",
      "  batch 80 loss: 0.00969361544957792\n",
      "LOSS train 0.00969361544957792 valid 0.011978158658894244\n",
      "EPOCH 103:\n",
      "  batch 10 loss: 0.009663347291643731\n",
      "  batch 20 loss: 0.006387286859717278\n",
      "  batch 30 loss: 0.013334973089513369\n",
      "  batch 40 loss: 0.00692970222189615\n",
      "  batch 50 loss: 0.010653767865187546\n",
      "  batch 60 loss: 0.008863525371270952\n",
      "  batch 70 loss: 0.009351965064342948\n",
      "  batch 80 loss: 0.009091374660602013\n",
      "LOSS train 0.009091374660602013 valid 0.00884963802076527\n",
      "EPOCH 104:\n",
      "  batch 10 loss: 0.011007890548899012\n",
      "  batch 20 loss: 0.01102612158683769\n",
      "  batch 30 loss: 0.007616324800437724\n",
      "  batch 40 loss: 0.00924964977075433\n",
      "  batch 50 loss: 0.010885578864326817\n",
      "  batch 60 loss: 0.009229742081515724\n",
      "  batch 70 loss: 0.008845150120214385\n",
      "  batch 80 loss: 0.004837018982470909\n",
      "LOSS train 0.004837018982470909 valid 0.0048370142660860435\n",
      "EPOCH 105:\n",
      "  batch 10 loss: 0.006144878664781572\n",
      "  batch 20 loss: 0.009563847008212178\n",
      "  batch 30 loss: 0.00950976297572197\n",
      "  batch 40 loss: 0.009325719746993855\n",
      "  batch 50 loss: 0.008680507641474832\n",
      "  batch 60 loss: 0.013678753741078253\n",
      "  batch 70 loss: 0.008393545668150182\n",
      "  batch 80 loss: 0.005865797806109185\n",
      "LOSS train 0.005865797806109185 valid 0.007407486297161085\n",
      "EPOCH 106:\n",
      "  batch 10 loss: 0.008836806022191012\n",
      "  batch 20 loss: 0.0070215625510172686\n",
      "  batch 30 loss: 0.006713396914710757\n",
      "  batch 40 loss: 0.012878492671188724\n",
      "  batch 50 loss: 0.006218751359119779\n",
      "  batch 60 loss: 0.005550806627798011\n",
      "  batch 70 loss: 0.011024421674119367\n",
      "  batch 80 loss: 0.008280941431803513\n",
      "LOSS train 0.008280941431803513 valid 0.007823794933065073\n",
      "EPOCH 107:\n",
      "  batch 10 loss: 0.007199139697877399\n",
      "  batch 20 loss: 0.012012417115693097\n",
      "  batch 30 loss: 0.006977883802392171\n",
      "  batch 40 loss: 0.0035777454208073324\n",
      "  batch 50 loss: 0.006516750673654315\n",
      "  batch 60 loss: 0.007672830862429691\n",
      "  batch 70 loss: 0.011066769264471078\n",
      "  batch 80 loss: 0.009819604019139661\n",
      "LOSS train 0.009819604019139661 valid 0.011302095286082477\n",
      "EPOCH 108:\n",
      "  batch 10 loss: 0.012191091566273826\n",
      "  batch 20 loss: 0.008930986343511905\n",
      "  batch 30 loss: 0.008200204344575468\n",
      "  batch 40 loss: 0.006823644985706779\n",
      "  batch 50 loss: 0.00672410262068297\n",
      "  batch 60 loss: 0.010226397814585652\n",
      "  batch 70 loss: 0.004943378122152353\n",
      "  batch 80 loss: 0.01012871911025286\n",
      "LOSS train 0.01012871911025286 valid 0.007434969822643325\n",
      "EPOCH 109:\n",
      "  batch 10 loss: 0.010130746392860601\n",
      "  batch 20 loss: 0.010852221495224513\n",
      "  batch 30 loss: 0.006888384819831117\n",
      "  batch 40 loss: 0.00909978446579771\n",
      "  batch 50 loss: 0.009049713465719833\n",
      "  batch 60 loss: 0.008539012103210553\n",
      "  batch 70 loss: 0.00741484823411156\n",
      "  batch 80 loss: 0.006668444920433103\n",
      "LOSS train 0.006668444920433103 valid 0.00488493626966374\n",
      "EPOCH 110:\n",
      "  batch 10 loss: 0.006185321255179588\n",
      "  batch 20 loss: 0.010586094696554938\n",
      "  batch 30 loss: 0.0073458004413623715\n",
      "  batch 40 loss: 0.0063492598006632765\n",
      "  batch 50 loss: 0.007972807276837557\n",
      "  batch 60 loss: 0.007357135576967266\n",
      "  batch 70 loss: 0.010249320859475119\n",
      "  batch 80 loss: 0.007455358415245428\n",
      "LOSS train 0.007455358415245428 valid 0.0095440243340272\n",
      "EPOCH 111:\n",
      "  batch 10 loss: 0.007982492391238338\n",
      "  batch 20 loss: 0.010148079857754056\n",
      "  batch 30 loss: 0.00861225981843745\n",
      "  batch 40 loss: 0.010361621740412375\n",
      "  batch 50 loss: 0.006089169679034967\n",
      "  batch 60 loss: 0.008065968286246061\n",
      "  batch 70 loss: 0.0076638206892312155\n",
      "  batch 80 loss: 0.005902470439286845\n",
      "LOSS train 0.005902470439286845 valid 0.005061200543059386\n",
      "EPOCH 112:\n",
      "  batch 10 loss: 0.007182248646586231\n",
      "  batch 20 loss: 0.00593678253244434\n",
      "  batch 30 loss: 0.010692961332460981\n",
      "  batch 40 loss: 0.009068112169734377\n",
      "  batch 50 loss: 0.006186996014821489\n",
      "  batch 60 loss: 0.010300013691812637\n",
      "  batch 70 loss: 0.008085922829741321\n",
      "  batch 80 loss: 0.006064543452885119\n",
      "LOSS train 0.006064543452885119 valid 0.005674286667344859\n",
      "EPOCH 113:\n",
      "  batch 10 loss: 0.009035634960491734\n",
      "  batch 20 loss: 0.011427495461430227\n",
      "  batch 30 loss: 0.006815390943120292\n",
      "  batch 40 loss: 0.007838128636103647\n",
      "  batch 50 loss: 0.005895272808447771\n",
      "  batch 60 loss: 0.007263598792633275\n",
      "  batch 70 loss: 0.004645817828532017\n",
      "  batch 80 loss: 0.008258081815984041\n",
      "LOSS train 0.008258081815984041 valid 0.005152722381171771\n",
      "EPOCH 114:\n",
      "  batch 10 loss: 0.008036777918641746\n",
      "  batch 20 loss: 0.008613207697362669\n",
      "  batch 30 loss: 0.008040426869956718\n",
      "  batch 40 loss: 0.004291259378624091\n",
      "  batch 50 loss: 0.006793690695485566\n",
      "  batch 60 loss: 0.007424316431388434\n",
      "  batch 70 loss: 0.006273396499818773\n",
      "  batch 80 loss: 0.008126407072086294\n",
      "LOSS train 0.008126407072086294 valid 0.007447909246111522\n",
      "EPOCH 115:\n",
      "  batch 10 loss: 0.006539089135549148\n",
      "  batch 20 loss: 0.008766111532531795\n",
      "  batch 30 loss: 0.009560764405887313\n",
      "  batch 40 loss: 0.0075622700625899595\n",
      "  batch 50 loss: 0.0079843460875054\n",
      "  batch 60 loss: 0.005022097433629824\n",
      "  batch 70 loss: 0.005415634482596942\n",
      "  batch 80 loss: 0.008217256422449281\n",
      "LOSS train 0.008217256422449281 valid 0.007769310879521072\n",
      "EPOCH 116:\n",
      "  batch 10 loss: 0.005966138435542234\n",
      "  batch 20 loss: 0.007292866962052358\n",
      "  batch 30 loss: 0.0086402731833914\n",
      "  batch 40 loss: 0.0074403575541509785\n",
      "  batch 50 loss: 0.008718451680215367\n",
      "  batch 60 loss: 0.006040340297295188\n",
      "  batch 70 loss: 0.009308292181958677\n",
      "  batch 80 loss: 0.005511049745837227\n",
      "LOSS train 0.005511049745837227 valid 0.006183168193383608\n",
      "EPOCH 117:\n",
      "  batch 10 loss: 0.008634948803228326\n",
      "  batch 20 loss: 0.005459832493943395\n",
      "  batch 30 loss: 0.006830898726911982\n",
      "  batch 40 loss: 0.010064683261953178\n",
      "  batch 50 loss: 0.0068200477633581615\n",
      "  batch 60 loss: 0.005570538588835916\n",
      "  batch 70 loss: 0.006039615180816327\n",
      "  batch 80 loss: 0.006040539781133702\n",
      "LOSS train 0.006040539781133702 valid 0.005237533386389259\n",
      "EPOCH 118:\n",
      "  batch 10 loss: 0.009475107699108776\n",
      "  batch 20 loss: 0.0066237525999895295\n",
      "  batch 30 loss: 0.006594039751144009\n",
      "  batch 40 loss: 0.0046927870967920175\n",
      "  batch 50 loss: 0.008954201082997316\n",
      "  batch 60 loss: 0.006236398208511673\n",
      "  batch 70 loss: 0.006546986882676719\n",
      "  batch 80 loss: 0.008405227806724725\n",
      "LOSS train 0.008405227806724725 valid 0.006623585668348824\n",
      "EPOCH 119:\n",
      "  batch 10 loss: 0.010472578236294794\n",
      "  batch 20 loss: 0.006924313131730741\n",
      "  batch 30 loss: 0.00655213144946174\n",
      "  batch 40 loss: 0.007457307368986222\n",
      "  batch 50 loss: 0.003657533539990254\n",
      "  batch 60 loss: 0.006414936545206728\n",
      "  batch 70 loss: 0.0068611344520832064\n",
      "  batch 80 loss: 0.0074471373139203935\n",
      "LOSS train 0.0074471373139203935 valid 0.004675720201048534\n",
      "EPOCH 120:\n",
      "  batch 10 loss: 0.008179889409984754\n",
      "  batch 20 loss: 0.0074866999274490805\n",
      "  batch 30 loss: 0.009710781063131436\n",
      "  batch 40 loss: 0.004900432974955038\n",
      "  batch 50 loss: 0.005835995826237195\n",
      "  batch 60 loss: 0.006214643154999066\n",
      "  batch 70 loss: 0.004827162388028228\n",
      "  batch 80 loss: 0.009798045260140498\n",
      "LOSS train 0.009798045260140498 valid 0.004278154263156466\n",
      "EPOCH 121:\n",
      "  batch 10 loss: 0.0054504785309291036\n",
      "  batch 20 loss: 0.006358696735333069\n",
      "  batch 30 loss: 0.004345778277729551\n",
      "  batch 40 loss: 0.006600484015780239\n",
      "  batch 50 loss: 0.009688920232838428\n",
      "  batch 60 loss: 0.005274783147888229\n",
      "  batch 70 loss: 0.004644767865465837\n",
      "  batch 80 loss: 0.009851012966100825\n",
      "LOSS train 0.009851012966100825 valid 0.00810665944030916\n",
      "EPOCH 122:\n",
      "  batch 10 loss: 0.007190679441737302\n",
      "  batch 20 loss: 0.005767924658812262\n",
      "  batch 30 loss: 0.0048407456304630616\n",
      "  batch 40 loss: 0.010479333301555016\n",
      "  batch 50 loss: 0.004969242511197081\n",
      "  batch 60 loss: 0.007368820502415474\n",
      "  batch 70 loss: 0.0065731606828194344\n",
      "  batch 80 loss: 0.006418127049801115\n",
      "LOSS train 0.006418127049801115 valid 0.005119256618563668\n",
      "EPOCH 123:\n",
      "  batch 10 loss: 0.007216598449122103\n",
      "  batch 20 loss: 0.006096337561757537\n",
      "  batch 30 loss: 0.006275615012509661\n",
      "  batch 40 loss: 0.00630796123368782\n",
      "  batch 50 loss: 0.007177219316872652\n",
      "  batch 60 loss: 0.005706183321035496\n",
      "  batch 70 loss: 0.00638040857265878\n",
      "  batch 80 loss: 0.005431883042911067\n",
      "LOSS train 0.005431883042911067 valid 0.005547564669759595\n",
      "EPOCH 124:\n",
      "  batch 10 loss: 0.006441636246199778\n",
      "  batch 20 loss: 0.007483155674708542\n",
      "  batch 30 loss: 0.006755919557144807\n",
      "  batch 40 loss: 0.0069281961418710125\n",
      "  batch 50 loss: 0.007401219481198496\n",
      "  batch 60 loss: 0.00363103987847353\n",
      "  batch 70 loss: 0.006971234534285031\n",
      "  batch 80 loss: 0.004717600299227342\n",
      "LOSS train 0.004717600299227342 valid 0.006298093192745\n",
      "EPOCH 125:\n",
      "  batch 10 loss: 0.005583515991020249\n",
      "  batch 20 loss: 0.007323371154416236\n",
      "  batch 30 loss: 0.004287103075239429\n",
      "  batch 40 loss: 0.006325143220146856\n",
      "  batch 50 loss: 0.004248002365056891\n",
      "  batch 60 loss: 0.009378607723192545\n",
      "  batch 70 loss: 0.007640150004772295\n",
      "  batch 80 loss: 0.006392762648374628\n",
      "LOSS train 0.006392762648374628 valid 0.005473813053249614\n",
      "EPOCH 126:\n",
      "  batch 10 loss: 0.007314357464292698\n",
      "  batch 20 loss: 0.00671439789730357\n",
      "  batch 30 loss: 0.0068100436808890665\n",
      "  batch 40 loss: 0.004303163609620242\n",
      "  batch 50 loss: 0.004438473032314505\n",
      "  batch 60 loss: 0.007187361094020161\n",
      "  batch 70 loss: 0.005564761752339109\n",
      "  batch 80 loss: 0.00544234773824428\n",
      "LOSS train 0.00544234773824428 valid 0.005710609234738513\n",
      "EPOCH 127:\n",
      "  batch 10 loss: 0.004622931889116444\n",
      "  batch 20 loss: 0.005603898171193578\n",
      "  batch 30 loss: 0.004799490863661049\n",
      "  batch 40 loss: 0.007215690890825499\n",
      "  batch 50 loss: 0.005445344770305382\n",
      "  batch 60 loss: 0.006358548967727984\n",
      "  batch 70 loss: 0.005938132997471257\n",
      "  batch 80 loss: 0.006272509411792271\n",
      "LOSS train 0.006272509411792271 valid 0.005191000532940962\n",
      "EPOCH 128:\n",
      "  batch 10 loss: 0.0056379341128376835\n",
      "  batch 20 loss: 0.006636449525012722\n",
      "  batch 30 loss: 0.004414353077936539\n",
      "  batch 40 loss: 0.006306020793454081\n",
      "  batch 50 loss: 0.0045382632894870765\n",
      "  batch 60 loss: 0.007104818268635427\n",
      "  batch 70 loss: 0.006527690992516\n",
      "  batch 80 loss: 0.006730276715461514\n",
      "LOSS train 0.006730276715461514 valid 0.0035153998820169363\n",
      "EPOCH 129:\n",
      "  batch 10 loss: 0.003960544347046379\n",
      "  batch 20 loss: 0.007086545113543253\n",
      "  batch 30 loss: 0.0038737890536140185\n",
      "  batch 40 loss: 0.006474343420268269\n",
      "  batch 50 loss: 0.007002249249671877\n",
      "  batch 60 loss: 0.009088388295185722\n",
      "  batch 70 loss: 0.004144193634238036\n",
      "  batch 80 loss: 0.004103422336720541\n",
      "LOSS train 0.004103422336720541 valid 0.00640294228302082\n",
      "EPOCH 130:\n",
      "  batch 10 loss: 0.00530037783764783\n",
      "  batch 20 loss: 0.004500647126587864\n",
      "  batch 30 loss: 0.007128797649420449\n",
      "  batch 40 loss: 0.006149397231001785\n",
      "  batch 50 loss: 0.005395246556963685\n",
      "  batch 60 loss: 0.005768730125237198\n",
      "  batch 70 loss: 0.005004688296685345\n",
      "  batch 80 loss: 0.006258121805785777\n",
      "LOSS train 0.006258121805785777 valid 0.004796422502477071\n",
      "EPOCH 131:\n",
      "  batch 10 loss: 0.006304773257215857\n",
      "  batch 20 loss: 0.007076864539794769\n",
      "  batch 30 loss: 0.00449881452250338\n",
      "  batch 40 loss: 0.005142246572722798\n",
      "  batch 50 loss: 0.004059263914496114\n",
      "  batch 60 loss: 0.007839736266305408\n",
      "  batch 70 loss: 0.005677287242997408\n",
      "  batch 80 loss: 0.006100186139337893\n",
      "LOSS train 0.006100186139337893 valid 0.004945215254483628\n",
      "EPOCH 132:\n",
      "  batch 10 loss: 0.006289081642262317\n",
      "  batch 20 loss: 0.0049106812994068605\n",
      "  batch 30 loss: 0.00458809219126124\n",
      "  batch 40 loss: 0.003564841750994674\n",
      "  batch 50 loss: 0.006311135528994783\n",
      "  batch 60 loss: 0.007447566940209071\n",
      "  batch 70 loss: 0.004254222961935739\n",
      "  batch 80 loss: 0.009111240346283012\n",
      "LOSS train 0.009111240346283012 valid 0.0061997565846832\n",
      "EPOCH 133:\n",
      "  batch 10 loss: 0.006767664537073869\n",
      "  batch 20 loss: 0.005265616156611941\n",
      "  batch 30 loss: 0.006544001181191561\n",
      "  batch 40 loss: 0.005904808324430633\n",
      "  batch 50 loss: 0.0049187615919436215\n",
      "  batch 60 loss: 0.004877727716029767\n",
      "  batch 70 loss: 0.005429103741334984\n",
      "  batch 80 loss: 0.006311877802272647\n",
      "LOSS train 0.006311877802272647 valid 0.005085200743342284\n",
      "EPOCH 134:\n",
      "  batch 10 loss: 0.0024740532940995762\n",
      "  batch 20 loss: 0.0033872653324579006\n",
      "  batch 30 loss: 0.004461429330149258\n",
      "  batch 40 loss: 0.006009677510337497\n",
      "  batch 50 loss: 0.004863853131791984\n",
      "  batch 60 loss: 0.010139583277305065\n",
      "  batch 70 loss: 0.004960660793221905\n",
      "  batch 80 loss: 0.006768487882527552\n",
      "LOSS train 0.006768487882527552 valid 0.004357650044621551\n",
      "EPOCH 135:\n",
      "  batch 10 loss: 0.0039042675778546256\n",
      "  batch 20 loss: 0.006385993007461366\n",
      "  batch 30 loss: 0.005950763634336909\n",
      "  batch 40 loss: 0.006128909801645932\n",
      "  batch 50 loss: 0.004836965517461067\n",
      "  batch 60 loss: 0.005264796285700868\n",
      "  batch 70 loss: 0.006461089130698383\n",
      "  batch 80 loss: 0.006536618040445319\n",
      "LOSS train 0.006536618040445319 valid 0.00463906749333546\n",
      "EPOCH 136:\n",
      "  batch 10 loss: 0.007505744585159846\n",
      "  batch 20 loss: 0.004847787442122354\n",
      "  batch 30 loss: 0.004667966490478648\n",
      "  batch 40 loss: 0.0026605997120896065\n",
      "  batch 50 loss: 0.006808599338819476\n",
      "  batch 60 loss: 0.005204917040646251\n",
      "  batch 70 loss: 0.005776446709569427\n",
      "  batch 80 loss: 0.006134724211824505\n",
      "LOSS train 0.006134724211824505 valid 0.005995832307817182\n",
      "EPOCH 137:\n",
      "  batch 10 loss: 0.0048781332661747\n",
      "  batch 20 loss: 0.006703190612279286\n",
      "  batch 30 loss: 0.006939190433604381\n",
      "  batch 40 loss: 0.00537508201214223\n",
      "  batch 50 loss: 0.006324528617460601\n",
      "  batch 60 loss: 0.006487072236268432\n",
      "  batch 70 loss: 0.004558119415651163\n",
      "  batch 80 loss: 0.002458802362525603\n",
      "LOSS train 0.002458802362525603 valid 0.003682705360188265\n",
      "EPOCH 138:\n",
      "  batch 10 loss: 0.004628193348662535\n",
      "  batch 20 loss: 0.006617699812932187\n",
      "  batch 30 loss: 0.007182096443193587\n",
      "  batch 40 loss: 0.004635142706410989\n",
      "  batch 50 loss: 0.003982470960727369\n",
      "  batch 60 loss: 0.0042739177340990865\n",
      "  batch 70 loss: 0.006540785945799143\n",
      "  batch 80 loss: 0.004696514107035909\n",
      "LOSS train 0.004696514107035909 valid 0.003481313883130497\n",
      "EPOCH 139:\n",
      "  batch 10 loss: 0.006393255952571053\n",
      "  batch 20 loss: 0.0036486057888396317\n",
      "  batch 30 loss: 0.004528606068197405\n",
      "  batch 40 loss: 0.0047372846627695255\n",
      "  batch 50 loss: 0.005119292488598148\n",
      "  batch 60 loss: 0.005811271060338186\n",
      "  batch 70 loss: 0.004022503583837533\n",
      "  batch 80 loss: 0.006080733473845612\n",
      "LOSS train 0.006080733473845612 valid 0.006320941785998002\n",
      "EPOCH 140:\n",
      "  batch 10 loss: 0.007352909691326204\n",
      "  batch 20 loss: 0.00400141851614535\n",
      "  batch 30 loss: 0.0040794187264964425\n",
      "  batch 40 loss: 0.005530949861713453\n",
      "  batch 50 loss: 0.00612346720986352\n",
      "  batch 60 loss: 0.004745116214780865\n",
      "  batch 70 loss: 0.004510526259491598\n",
      "  batch 80 loss: 0.006260830824339791\n",
      "LOSS train 0.006260830824339791 valid 0.004672371305059641\n",
      "EPOCH 141:\n",
      "  batch 10 loss: 0.00478025977499783\n",
      "  batch 20 loss: 0.004991185388280428\n",
      "  batch 30 loss: 0.003269502899820509\n",
      "  batch 40 loss: 0.0052736538874341935\n",
      "  batch 50 loss: 0.006317811455392075\n",
      "  batch 60 loss: 0.0051037727711445765\n",
      "  batch 70 loss: 0.005546229607170971\n",
      "  batch 80 loss: 0.005202493446267909\n",
      "LOSS train 0.005202493446267909 valid 0.004780222046319977\n",
      "EPOCH 142:\n",
      "  batch 10 loss: 0.003759731375885167\n",
      "  batch 20 loss: 0.0041448528429100405\n",
      "  batch 30 loss: 0.004144378087357836\n",
      "  batch 40 loss: 0.0033359056339122618\n",
      "  batch 50 loss: 0.006325902912340098\n",
      "  batch 60 loss: 0.006196483043095213\n",
      "  batch 70 loss: 0.005683019820844492\n",
      "  batch 80 loss: 0.005870962721110118\n",
      "LOSS train 0.005870962721110118 valid 0.005373874519718811\n",
      "EPOCH 143:\n",
      "  batch 10 loss: 0.00630053975392002\n",
      "  batch 20 loss: 0.005525796228721447\n",
      "  batch 30 loss: 0.004805157017926831\n",
      "  batch 40 loss: 0.00705423262925251\n",
      "  batch 50 loss: 0.0042676857390688385\n",
      "  batch 60 loss: 0.0030809055260760942\n",
      "  batch 70 loss: 0.003109608928889429\n",
      "  batch 80 loss: 0.0035075379211775724\n",
      "LOSS train 0.0035075379211775724 valid 0.004474857047207479\n",
      "EPOCH 144:\n",
      "  batch 10 loss: 0.004582712935098243\n",
      "  batch 20 loss: 0.005445111602330144\n",
      "  batch 30 loss: 0.0055837479842011815\n",
      "  batch 40 loss: 0.00495423136262616\n",
      "  batch 50 loss: 0.0026912474169876075\n",
      "  batch 60 loss: 0.0025258919257339584\n",
      "  batch 70 loss: 0.005756883110734634\n",
      "  batch 80 loss: 0.00587422854396209\n",
      "LOSS train 0.00587422854396209 valid 0.004803308336413466\n",
      "EPOCH 145:\n",
      "  batch 10 loss: 0.006767654844315985\n",
      "  batch 20 loss: 0.005364437126627308\n",
      "  batch 30 loss: 0.004092065737950179\n",
      "  batch 40 loss: 0.004873738939113537\n",
      "  batch 50 loss: 0.004295075452000674\n",
      "  batch 60 loss: 0.005077803139738535\n",
      "  batch 70 loss: 0.003517756679957529\n",
      "  batch 80 loss: 0.0035967212083960476\n",
      "LOSS train 0.0035967212083960476 valid 0.003359691615551128\n",
      "EPOCH 146:\n",
      "  batch 10 loss: 0.00511301283040666\n",
      "  batch 20 loss: 0.005269513287703376\n",
      "  batch 30 loss: 0.0056126756772300725\n",
      "  batch 40 loss: 0.0029314163237359027\n",
      "  batch 50 loss: 0.005950819339705049\n",
      "  batch 60 loss: 0.004129522070252278\n",
      "  batch 70 loss: 0.006354834532066889\n",
      "  batch 80 loss: 0.0039792521513845715\n",
      "LOSS train 0.0039792521513845715 valid 0.0036068201997477446\n",
      "EPOCH 147:\n",
      "  batch 10 loss: 0.0066255820886908625\n",
      "  batch 20 loss: 0.0037137702081963654\n",
      "  batch 30 loss: 0.0038469251124297445\n",
      "  batch 40 loss: 0.004263962877303129\n",
      "  batch 50 loss: 0.005325760664391055\n",
      "  batch 60 loss: 0.004531748159479321\n",
      "  batch 70 loss: 0.005078929379851616\n",
      "  batch 80 loss: 0.004092771188879851\n",
      "LOSS train 0.004092771188879851 valid 0.0037022582459030674\n",
      "EPOCH 148:\n",
      "  batch 10 loss: 0.005538164967492776\n",
      "  batch 20 loss: 0.005557427662643022\n",
      "  batch 30 loss: 0.005373673965141279\n",
      "  batch 40 loss: 0.0032225891826783482\n",
      "  batch 50 loss: 0.006758130214439007\n",
      "  batch 60 loss: 0.0036498756312539626\n",
      "  batch 70 loss: 0.0034485505852899223\n",
      "  batch 80 loss: 0.004717024657566071\n",
      "LOSS train 0.004717024657566071 valid 0.004458115094530512\n",
      "EPOCH 149:\n",
      "  batch 10 loss: 0.006026904993814241\n",
      "  batch 20 loss: 0.004846161549630778\n",
      "  batch 30 loss: 0.004337959371696343\n",
      "  batch 40 loss: 0.006576820699865493\n",
      "  batch 50 loss: 0.004443258293167674\n",
      "  batch 60 loss: 0.003644638299829239\n",
      "  batch 70 loss: 0.0032756391300608812\n",
      "  batch 80 loss: 0.004931280010714545\n",
      "LOSS train 0.004931280010714545 valid 0.0035383856516637024\n",
      "EPOCH 150:\n",
      "  batch 10 loss: 0.0060736997271305885\n",
      "  batch 20 loss: 0.0046078870934252334\n",
      "  batch 30 loss: 0.005063892490306898\n",
      "  batch 40 loss: 0.00555425287466278\n",
      "  batch 50 loss: 0.005253524507224938\n",
      "  batch 60 loss: 0.003596506368194241\n",
      "  batch 70 loss: 0.003800671224416874\n",
      "  batch 80 loss: 0.0028293410071000835\n",
      "LOSS train 0.0028293410071000835 valid 0.0035009413167790627\n",
      "EPOCH 151:\n",
      "  batch 10 loss: 0.0026782287446167176\n",
      "  batch 20 loss: 0.005360046278019582\n",
      "  batch 30 loss: 0.004624457156751305\n",
      "  batch 40 loss: 0.005170150622780057\n",
      "  batch 50 loss: 0.00447779604378411\n",
      "  batch 60 loss: 0.0040450811765367686\n",
      "  batch 70 loss: 0.0035867544243956217\n",
      "  batch 80 loss: 0.003697929964710056\n",
      "LOSS train 0.003697929964710056 valid 0.003782509157244931\n",
      "EPOCH 152:\n",
      "  batch 10 loss: 0.0030877287201292345\n",
      "  batch 20 loss: 0.004768463969230652\n",
      "  batch 30 loss: 0.004512461177364457\n",
      "  batch 40 loss: 0.004682783159569226\n",
      "  batch 50 loss: 0.006182680274696395\n",
      "  batch 60 loss: 0.00515983477134796\n",
      "  batch 70 loss: 0.003672321345175078\n",
      "  batch 80 loss: 0.004219909167841251\n",
      "LOSS train 0.004219909167841251 valid 0.005013375091766648\n",
      "EPOCH 153:\n",
      "  batch 10 loss: 0.003252182172582252\n",
      "  batch 20 loss: 0.003374298609651305\n",
      "  batch 30 loss: 0.002955070696089024\n",
      "  batch 40 loss: 0.005722277269433107\n",
      "  batch 50 loss: 0.0045007952388914415\n",
      "  batch 60 loss: 0.0047535426827835184\n",
      "  batch 70 loss: 0.0038979127785751188\n",
      "  batch 80 loss: 0.00676521409113775\n",
      "LOSS train 0.00676521409113775 valid 0.003571018352631654\n",
      "EPOCH 154:\n",
      "  batch 10 loss: 0.003873579515493475\n",
      "  batch 20 loss: 0.005752971539368446\n",
      "  batch 30 loss: 0.006181042676234938\n",
      "  batch 40 loss: 0.004021164697860513\n",
      "  batch 50 loss: 0.003668228274636931\n",
      "  batch 60 loss: 0.0032740741317866195\n",
      "  batch 70 loss: 0.0036869223594294454\n",
      "  batch 80 loss: 0.004086289961287548\n",
      "LOSS train 0.004086289961287548 valid 0.0036012982098691283\n",
      "EPOCH 155:\n",
      "  batch 10 loss: 0.00462371749263184\n",
      "  batch 20 loss: 0.005172161343898552\n",
      "  batch 30 loss: 0.004599036444687954\n",
      "  batch 40 loss: 0.00343384244392837\n",
      "  batch 50 loss: 0.0029957067809846193\n",
      "  batch 60 loss: 0.0037232794063129402\n",
      "  batch 70 loss: 0.004802474443977189\n",
      "  batch 80 loss: 0.003689552236755844\n",
      "LOSS train 0.003689552236755844 valid 0.003521273119367834\n",
      "EPOCH 156:\n",
      "  batch 10 loss: 0.004593979251603741\n",
      "  batch 20 loss: 0.003108067159610073\n",
      "  batch 30 loss: 0.002395316191314123\n",
      "  batch 40 loss: 0.005509217379949405\n",
      "  batch 50 loss: 0.006032136993326276\n",
      "  batch 60 loss: 0.00583904330360383\n",
      "  batch 70 loss: 0.003810051113305235\n",
      "  batch 80 loss: 0.0033445205995121796\n",
      "LOSS train 0.0033445205995121796 valid 0.0035295398329981253\n",
      "EPOCH 157:\n",
      "  batch 10 loss: 0.0042175349393801295\n",
      "  batch 20 loss: 0.004763567615918873\n",
      "  batch 30 loss: 0.004634025757832205\n",
      "  batch 40 loss: 0.0037152323428017555\n",
      "  batch 50 loss: 0.004776867935743212\n",
      "  batch 60 loss: 0.0026477606579646817\n",
      "  batch 70 loss: 0.00496637635251318\n",
      "  batch 80 loss: 0.004338137152672061\n",
      "LOSS train 0.004338137152672061 valid 0.0043031236986280415\n",
      "EPOCH 158:\n",
      "  batch 10 loss: 0.003965131058066618\n",
      "  batch 20 loss: 0.0018631053476383386\n",
      "  batch 30 loss: 0.00342301580953972\n",
      "  batch 40 loss: 0.005271420719145681\n",
      "  batch 50 loss: 0.0044005770599596875\n",
      "  batch 60 loss: 0.0056949037941194545\n",
      "  batch 70 loss: 0.004111073848116575\n",
      "  batch 80 loss: 0.0036326547867247426\n",
      "LOSS train 0.0036326547867247426 valid 0.0030513618895201943\n",
      "EPOCH 159:\n",
      "  batch 10 loss: 0.003322928435773065\n",
      "  batch 20 loss: 0.005268673241516808\n",
      "  batch 30 loss: 0.005092015868376621\n",
      "  batch 40 loss: 0.004772982853501162\n",
      "  batch 50 loss: 0.003697285677117179\n",
      "  batch 60 loss: 0.0031140878609221545\n",
      "  batch 70 loss: 0.0030839695205941097\n",
      "  batch 80 loss: 0.005144054258198594\n",
      "LOSS train 0.005144054258198594 valid 0.003261630743581918\n",
      "EPOCH 160:\n",
      "  batch 10 loss: 0.00274765824374299\n",
      "  batch 20 loss: 0.005339671210776942\n",
      "  batch 30 loss: 0.0027487491234296614\n",
      "  batch 40 loss: 0.0027753856172694213\n",
      "  batch 50 loss: 0.004310675601936964\n",
      "  batch 60 loss: 0.00478271182178105\n",
      "  batch 70 loss: 0.0038119162163638975\n",
      "  batch 80 loss: 0.003955399826008943\n",
      "LOSS train 0.003955399826008943 valid 0.007325853451911826\n",
      "EPOCH 161:\n",
      "  batch 10 loss: 0.0036313803493612795\n",
      "  batch 20 loss: 0.003530280101358585\n",
      "  batch 30 loss: 0.004179572579232627\n",
      "  batch 40 loss: 0.0036456833643114805\n",
      "  batch 50 loss: 0.0036086520919525354\n",
      "  batch 60 loss: 0.0035670736576776108\n",
      "  batch 70 loss: 0.005151109314874702\n",
      "  batch 80 loss: 0.004757313807431274\n",
      "LOSS train 0.004757313807431274 valid 0.0045445826314971785\n",
      "EPOCH 162:\n",
      "  batch 10 loss: 0.0035214413251196675\n",
      "  batch 20 loss: 0.0033610817832595784\n",
      "  batch 30 loss: 0.005787842696690859\n",
      "  batch 40 loss: 0.004868816763109862\n",
      "  batch 50 loss: 0.003654017809640209\n",
      "  batch 60 loss: 0.0034800369704498734\n",
      "  batch 70 loss: 0.002481989642365079\n",
      "  batch 80 loss: 0.003956077762177301\n",
      "LOSS train 0.003956077762177301 valid 0.004422784468115424\n",
      "EPOCH 163:\n",
      "  batch 10 loss: 0.0028178977517654856\n",
      "  batch 20 loss: 0.004557310937843795\n",
      "  batch 30 loss: 0.00319998212412429\n",
      "  batch 40 loss: 0.004455507113061685\n",
      "  batch 50 loss: 0.0037967886570186237\n",
      "  batch 60 loss: 0.005172663475514127\n",
      "  batch 70 loss: 0.0043417556294571115\n",
      "  batch 80 loss: 0.00333450389734935\n",
      "LOSS train 0.00333450389734935 valid 0.004064310187932279\n",
      "EPOCH 164:\n",
      "  batch 10 loss: 0.003867387119134946\n",
      "  batch 20 loss: 0.003370473618269898\n",
      "  batch 30 loss: 0.002022739955373254\n",
      "  batch 40 loss: 0.004867934268895624\n",
      "  batch 50 loss: 0.0037274864271239494\n",
      "  batch 60 loss: 0.0056262851220708395\n",
      "  batch 70 loss: 0.002303913931564239\n",
      "  batch 80 loss: 0.004859998788924713\n",
      "LOSS train 0.004859998788924713 valid 0.003930740258401784\n",
      "EPOCH 165:\n",
      "  batch 10 loss: 0.002011762479878598\n",
      "  batch 20 loss: 0.002732664179711719\n",
      "  batch 30 loss: 0.0035706891491827262\n",
      "  batch 40 loss: 0.0025299382986759154\n",
      "  batch 50 loss: 0.003423583787161988\n",
      "  batch 60 loss: 0.005458223117420858\n",
      "  batch 70 loss: 0.005045039954757158\n",
      "  batch 80 loss: 0.00463017388751723\n",
      "LOSS train 0.00463017388751723 valid 0.0045060809862116\n",
      "EPOCH 166:\n",
      "  batch 10 loss: 0.0037073698411404623\n",
      "  batch 20 loss: 0.004531768891865795\n",
      "  batch 30 loss: 0.002966814649244043\n",
      "  batch 40 loss: 0.0041293863841474375\n",
      "  batch 50 loss: 0.003149885325001378\n",
      "  batch 60 loss: 0.005097125686188519\n",
      "  batch 70 loss: 0.003647492583286294\n",
      "  batch 80 loss: 0.003959725482764043\n",
      "LOSS train 0.003959725482764043 valid 0.003936147047388658\n",
      "EPOCH 167:\n",
      "  batch 10 loss: 0.004556783297084621\n",
      "  batch 20 loss: 0.005152100759050882\n",
      "  batch 30 loss: 0.0026754542771868727\n",
      "  batch 40 loss: 0.0036922846546531217\n",
      "  batch 50 loss: 0.0032014930969580745\n",
      "  batch 60 loss: 0.0032539750081014064\n",
      "  batch 70 loss: 0.004818521739116477\n",
      "  batch 80 loss: 0.0038342336280948073\n",
      "LOSS train 0.0038342336280948073 valid 0.003811854562809458\n",
      "EPOCH 168:\n",
      "  batch 10 loss: 0.003311600480310517\n",
      "  batch 20 loss: 0.003474880017347459\n",
      "  batch 30 loss: 0.00372772735554463\n",
      "  batch 40 loss: 0.004184368574533437\n",
      "  batch 50 loss: 0.0026618288790814402\n",
      "  batch 60 loss: 0.004142684818043562\n",
      "  batch 70 loss: 0.0028462726832913176\n",
      "  batch 80 loss: 0.005519462876054604\n",
      "LOSS train 0.005519462876054604 valid 0.003793633605964715\n",
      "EPOCH 169:\n",
      "  batch 10 loss: 0.0031914180227431642\n",
      "  batch 20 loss: 0.002582619959548538\n",
      "  batch 30 loss: 0.004604464982548961\n",
      "  batch 40 loss: 0.003670098183738446\n",
      "  batch 50 loss: 0.0035575397655520646\n",
      "  batch 60 loss: 0.0046352731214028605\n",
      "  batch 70 loss: 0.003095427747894064\n",
      "  batch 80 loss: 0.004410465234013827\n",
      "LOSS train 0.004410465234013827 valid 0.0045028418383662935\n",
      "EPOCH 170:\n",
      "  batch 10 loss: 0.0050140197440669\n",
      "  batch 20 loss: 0.003026107043160664\n",
      "  batch 30 loss: 0.0034033195754091137\n",
      "  batch 40 loss: 0.0037221262755338103\n",
      "  batch 50 loss: 0.003931483664837287\n",
      "  batch 60 loss: 0.003852200961091512\n",
      "  batch 70 loss: 0.0038004043498858662\n",
      "  batch 80 loss: 0.003068303328927868\n",
      "LOSS train 0.003068303328927868 valid 0.003677595300250687\n",
      "EPOCH 171:\n",
      "  batch 10 loss: 0.002883961821999037\n",
      "  batch 20 loss: 0.0038382113512398066\n",
      "  batch 30 loss: 0.0026324454764107942\n",
      "  batch 40 loss: 0.003878293136585853\n",
      "  batch 50 loss: 0.0038706649523646774\n",
      "  batch 60 loss: 0.0036768694123111346\n",
      "  batch 70 loss: 0.0038000680586264936\n",
      "  batch 80 loss: 0.004085318630177426\n",
      "LOSS train 0.004085318630177426 valid 0.0038069436187288374\n",
      "EPOCH 172:\n",
      "  batch 10 loss: 0.0043822510902373326\n",
      "  batch 20 loss: 0.002257585165261844\n",
      "  batch 30 loss: 0.002688330522596516\n",
      "  batch 40 loss: 0.0025609600055304326\n",
      "  batch 50 loss: 0.00355099613375387\n",
      "  batch 60 loss: 0.005195609973543469\n",
      "  batch 70 loss: 0.004989541662143893\n",
      "  batch 80 loss: 0.0018113275300038368\n",
      "LOSS train 0.0018113275300038368 valid 0.0036930312856566162\n",
      "EPOCH 173:\n",
      "  batch 10 loss: 0.003152186532952328\n",
      "  batch 20 loss: 0.002766149963508724\n",
      "  batch 30 loss: 0.003782052319184004\n",
      "  batch 40 loss: 0.0037928916738110274\n",
      "  batch 50 loss: 0.002812482042827469\n",
      "  batch 60 loss: 0.005270840056073212\n",
      "  batch 70 loss: 0.0025377167761234886\n",
      "  batch 80 loss: 0.003490078426011678\n",
      "LOSS train 0.003490078426011678 valid 0.0033150489779291093\n",
      "EPOCH 174:\n",
      "  batch 10 loss: 0.002816398061486325\n",
      "  batch 20 loss: 0.004232644729199819\n",
      "  batch 30 loss: 0.003427937139031201\n",
      "  batch 40 loss: 0.0027339562233464677\n",
      "  batch 50 loss: 0.0037689576360207867\n",
      "  batch 60 loss: 0.004083076004098985\n",
      "  batch 70 loss: 0.0030328588943575596\n",
      "  batch 80 loss: 0.0046919879981942355\n",
      "LOSS train 0.0046919879981942355 valid 0.002637388895600452\n",
      "EPOCH 175:\n",
      "  batch 10 loss: 0.0026907772416507212\n",
      "  batch 20 loss: 0.0032750916735949432\n",
      "  batch 30 loss: 0.003097137804570593\n",
      "  batch 40 loss: 0.004284711699528998\n",
      "  batch 50 loss: 0.0021730844812736906\n",
      "  batch 60 loss: 0.005820050968122814\n",
      "  batch 70 loss: 0.0018466761900072016\n",
      "  batch 80 loss: 0.005008341941993422\n",
      "LOSS train 0.005008341941993422 valid 0.0034260903466201854\n",
      "EPOCH 176:\n",
      "  batch 10 loss: 0.0044773470775908205\n",
      "  batch 20 loss: 0.0035313685972141683\n",
      "  batch 30 loss: 0.002576444432770586\n",
      "  batch 40 loss: 0.00346458371072913\n",
      "  batch 50 loss: 0.0036334296822133185\n",
      "  batch 60 loss: 0.0038325165267906413\n",
      "  batch 70 loss: 0.003615788334263925\n",
      "  batch 80 loss: 0.0024822034979024464\n",
      "LOSS train 0.0024822034979024464 valid 0.002965746922272956\n",
      "EPOCH 177:\n",
      "  batch 10 loss: 0.002672147619136922\n",
      "  batch 20 loss: 0.0032952060984825947\n",
      "  batch 30 loss: 0.005304644993339025\n",
      "  batch 40 loss: 0.0031804076076696218\n",
      "  batch 50 loss: 0.0022635743877344795\n",
      "  batch 60 loss: 0.0035516439254934086\n",
      "  batch 70 loss: 0.0026840124270620437\n",
      "  batch 80 loss: 0.003626665539377427\n",
      "LOSS train 0.003626665539377427 valid 0.0034159175645618236\n",
      "EPOCH 178:\n",
      "  batch 10 loss: 0.004428839099182369\n",
      "  batch 20 loss: 0.0029304402901288995\n",
      "  batch 30 loss: 0.00345652054720631\n",
      "  batch 40 loss: 0.0027144127727751766\n",
      "  batch 50 loss: 0.004031018366640637\n",
      "  batch 60 loss: 0.003859971966858211\n",
      "  batch 70 loss: 0.0025625730298543203\n",
      "  batch 80 loss: 0.0036316014456133417\n",
      "LOSS train 0.0036316014456133417 valid 0.0036638570190189058\n",
      "EPOCH 179:\n",
      "  batch 10 loss: 0.0036788021279789973\n",
      "  batch 20 loss: 0.0026398630311632587\n",
      "  batch 30 loss: 0.004044703092404234\n",
      "  batch 40 loss: 0.0023778748887480104\n",
      "  batch 50 loss: 0.0030460891687653204\n",
      "  batch 60 loss: 0.0025841652339295253\n",
      "  batch 70 loss: 0.0036652682572366757\n",
      "  batch 80 loss: 0.003920100830691808\n",
      "LOSS train 0.003920100830691808 valid 0.0034295361064869212\n",
      "EPOCH 180:\n",
      "  batch 10 loss: 0.0033042223399206705\n",
      "  batch 20 loss: 0.0035829382507017726\n",
      "  batch 30 loss: 0.0035210240087053535\n",
      "  batch 40 loss: 0.002904282927784152\n",
      "  batch 50 loss: 0.005302495613022984\n",
      "  batch 60 loss: 0.003257384439530142\n",
      "  batch 70 loss: 0.002038778029736932\n",
      "  batch 80 loss: 0.003258136766271491\n",
      "LOSS train 0.003258136766271491 valid 0.003148825455973565\n",
      "EPOCH 181:\n",
      "  batch 10 loss: 0.0035737170573611364\n",
      "  batch 20 loss: 0.003715281693439465\n",
      "  batch 30 loss: 0.0039735874991038145\n",
      "  batch 40 loss: 0.0021848083904615124\n",
      "  batch 50 loss: 0.003906910639216221\n",
      "  batch 60 loss: 0.0030184676883436624\n",
      "  batch 70 loss: 0.002599919305112053\n",
      "  batch 80 loss: 0.0033022078792328104\n",
      "LOSS train 0.0033022078792328104 valid 0.0029147391646620236\n",
      "EPOCH 182:\n",
      "  batch 10 loss: 0.0039250276261100225\n",
      "  batch 20 loss: 0.003158133310625999\n",
      "  batch 30 loss: 0.004678181367125944\n",
      "  batch 40 loss: 0.003257326264724725\n",
      "  batch 50 loss: 0.0016615987962950384\n",
      "  batch 60 loss: 0.0038866865087584302\n",
      "  batch 70 loss: 0.0029317394279132713\n",
      "  batch 80 loss: 0.00366698610732783\n",
      "LOSS train 0.00366698610732783 valid 0.0034913276366569336\n",
      "EPOCH 183:\n",
      "  batch 10 loss: 0.0027927822030051177\n",
      "  batch 20 loss: 0.0035066525202182676\n",
      "  batch 30 loss: 0.0038398280156798138\n",
      "  batch 40 loss: 0.002909961981777087\n",
      "  batch 50 loss: 0.002901464105116247\n",
      "  batch 60 loss: 0.003137788660933438\n",
      "  batch 70 loss: 0.0035925639496781512\n",
      "  batch 80 loss: 0.0037339596979563796\n",
      "LOSS train 0.0037339596979563796 valid 0.003476099529689236\n",
      "EPOCH 184:\n",
      "  batch 10 loss: 0.0034037102719139513\n",
      "  batch 20 loss: 0.002834512287313373\n",
      "  batch 30 loss: 0.00201062540932071\n",
      "  batch 40 loss: 0.00475592998054708\n",
      "  batch 50 loss: 0.002269537346501238\n",
      "  batch 60 loss: 0.0028155025578143976\n",
      "  batch 70 loss: 0.002606059788649873\n",
      "  batch 80 loss: 0.003091689358552685\n",
      "LOSS train 0.003091689358552685 valid 0.004985003825058811\n",
      "EPOCH 185:\n",
      "  batch 10 loss: 0.0038822376779080515\n",
      "  batch 20 loss: 0.002796515909403752\n",
      "  batch 30 loss: 0.002990674368902546\n",
      "  batch 40 loss: 0.0035050407173002895\n",
      "  batch 50 loss: 0.0031664577680203367\n",
      "  batch 60 loss: 0.003017037051290572\n",
      "  batch 70 loss: 0.004053645737849365\n",
      "  batch 80 loss: 0.002664361592997011\n",
      "LOSS train 0.002664361592997011 valid 0.0032323044255463174\n",
      "EPOCH 186:\n",
      "  batch 10 loss: 0.002685175150691066\n",
      "  batch 20 loss: 0.0033598036655803297\n",
      "  batch 30 loss: 0.0028135944212408504\n",
      "  batch 40 loss: 0.004222826750537934\n",
      "  batch 50 loss: 0.003921283512590889\n",
      "  batch 60 loss: 0.00268675267711842\n",
      "  batch 70 loss: 0.0022557376448730793\n",
      "  batch 80 loss: 0.0029567064471848425\n",
      "LOSS train 0.0029567064471848425 valid 0.002948200434730097\n",
      "EPOCH 187:\n",
      "  batch 10 loss: 0.002660837398775584\n",
      "  batch 20 loss: 0.003661546028342855\n",
      "  batch 30 loss: 0.0030952470550346333\n",
      "  batch 40 loss: 0.002556819734854798\n",
      "  batch 50 loss: 0.004436124364292482\n",
      "  batch 60 loss: 0.002879692151100244\n",
      "  batch 70 loss: 0.0033275211486397892\n",
      "  batch 80 loss: 0.002149368519167183\n",
      "LOSS train 0.002149368519167183 valid 0.003081094275894429\n",
      "EPOCH 188:\n",
      "  batch 10 loss: 0.0037105544652831666\n",
      "  batch 20 loss: 0.0026743008455014207\n",
      "  batch 30 loss: 0.00420374353157058\n",
      "  batch 40 loss: 0.0019326742281009501\n",
      "  batch 50 loss: 0.0032527897795716853\n",
      "  batch 60 loss: 0.003653006783588353\n",
      "  batch 70 loss: 0.0028877784055566734\n",
      "  batch 80 loss: 0.0018230285331810592\n",
      "LOSS train 0.0018230285331810592 valid 0.002793566370728513\n",
      "EPOCH 189:\n",
      "  batch 10 loss: 0.002223043249773582\n",
      "  batch 20 loss: 0.002777867670033629\n",
      "  batch 30 loss: 0.0033816063763879357\n",
      "  batch 40 loss: 0.0037144092736525634\n",
      "  batch 50 loss: 0.0025774453274607367\n",
      "  batch 60 loss: 0.0029176315952554434\n",
      "  batch 70 loss: 0.0034931135405258827\n",
      "  batch 80 loss: 0.00237956748965189\n",
      "LOSS train 0.00237956748965189 valid 0.0030921239265080656\n",
      "EPOCH 190:\n",
      "  batch 10 loss: 0.0030150383455293194\n",
      "  batch 20 loss: 0.003222833518157131\n",
      "  batch 30 loss: 0.0038707333704678603\n",
      "  batch 40 loss: 0.0019851453954288444\n",
      "  batch 50 loss: 0.0024563072942555665\n",
      "  batch 60 loss: 0.0029785382246245717\n",
      "  batch 70 loss: 0.0037244975060730214\n",
      "  batch 80 loss: 0.0036906042184455144\n",
      "LOSS train 0.0036906042184455144 valid 0.0030598512561482495\n",
      "EPOCH 191:\n",
      "  batch 10 loss: 0.004269238660594965\n",
      "  batch 20 loss: 0.0028068897838693374\n",
      "  batch 30 loss: 0.003193779381808781\n",
      "  batch 40 loss: 0.0029345542023293094\n",
      "  batch 50 loss: 0.002346515958629425\n",
      "  batch 60 loss: 0.002681241493792186\n",
      "  batch 70 loss: 0.003186249218106241\n",
      "  batch 80 loss: 0.0027531512815357926\n",
      "LOSS train 0.0027531512815357926 valid 0.003422656366528827\n",
      "EPOCH 192:\n",
      "  batch 10 loss: 0.002081799680718177\n",
      "  batch 20 loss: 0.0035936749152824634\n",
      "  batch 30 loss: 0.0032158475613186964\n",
      "  batch 40 loss: 0.0033313483254687527\n",
      "  batch 50 loss: 0.003010283377579981\n",
      "  batch 60 loss: 0.002511771913896155\n",
      "  batch 70 loss: 0.0034930374234136254\n",
      "  batch 80 loss: 0.0031177908281733837\n",
      "LOSS train 0.0031177908281733837 valid 0.0035910625997530587\n",
      "EPOCH 193:\n",
      "  batch 10 loss: 0.0020384419992751646\n",
      "  batch 20 loss: 0.0037330019312321384\n",
      "  batch 30 loss: 0.0031066507518289653\n",
      "  batch 40 loss: 0.002382541571523689\n",
      "  batch 50 loss: 0.0032007666980689463\n",
      "  batch 60 loss: 0.003448662627806698\n",
      "  batch 70 loss: 0.0027364718225044273\n",
      "  batch 80 loss: 0.0032132441108842615\n",
      "LOSS train 0.0032132441108842615 valid 0.002894068932819209\n",
      "EPOCH 194:\n",
      "  batch 10 loss: 0.0023659811552988685\n",
      "  batch 20 loss: 0.0033326514392229\n",
      "  batch 30 loss: 0.0036593864173255497\n",
      "  batch 40 loss: 0.0033361701020567123\n",
      "  batch 50 loss: 0.0018640042658944367\n",
      "  batch 60 loss: 0.003231459039670881\n",
      "  batch 70 loss: 0.0022875051814480684\n",
      "  batch 80 loss: 0.003301371541556364\n",
      "LOSS train 0.003301371541556364 valid 0.003162428462310345\n",
      "EPOCH 195:\n",
      "  batch 10 loss: 0.004022583973437577\n",
      "  batch 20 loss: 0.0018623403555238839\n",
      "  batch 30 loss: 0.002984218393385163\n",
      "  batch 40 loss: 0.002800102164042073\n",
      "  batch 50 loss: 0.0024235855708866437\n",
      "  batch 60 loss: 0.0035702668666544924\n",
      "  batch 70 loss: 0.003074372359787958\n",
      "  batch 80 loss: 0.002104950946613826\n",
      "LOSS train 0.002104950946613826 valid 0.0035436059977291736\n",
      "EPOCH 196:\n",
      "  batch 10 loss: 0.002490667206029684\n",
      "  batch 20 loss: 0.003724687299722973\n",
      "  batch 30 loss: 0.001945169256441659\n",
      "  batch 40 loss: 0.0028978700841889803\n",
      "  batch 50 loss: 0.0038005223953177848\n",
      "  batch 60 loss: 0.0030908982164874034\n",
      "  batch 70 loss: 0.0035141274510124276\n",
      "  batch 80 loss: 0.0027931772237707263\n",
      "LOSS train 0.0027931772237707263 valid 0.002964494065490726\n",
      "EPOCH 197:\n",
      "  batch 10 loss: 0.0028873574697172444\n",
      "  batch 20 loss: 0.0024339307627997187\n",
      "  batch 30 loss: 0.002185607154342506\n",
      "  batch 40 loss: 0.0025945517473928703\n",
      "  batch 50 loss: 0.004058262630906029\n",
      "  batch 60 loss: 0.00363221613015412\n",
      "  batch 70 loss: 0.003291232923993448\n",
      "  batch 80 loss: 0.0019010962585525703\n",
      "LOSS train 0.0019010962585525703 valid 0.0032226884682677336\n",
      "EPOCH 198:\n",
      "  batch 10 loss: 0.0023962078060776547\n",
      "  batch 20 loss: 0.003951331336361363\n",
      "  batch 30 loss: 0.0017034476344861104\n",
      "  batch 40 loss: 0.002700587233880469\n",
      "  batch 50 loss: 0.002763660956520653\n",
      "  batch 60 loss: 0.001681102450470462\n",
      "  batch 70 loss: 0.00419537801280967\n",
      "  batch 80 loss: 0.003323968289396362\n",
      "LOSS train 0.003323968289396362 valid 0.003013195469666243\n",
      "EPOCH 199:\n",
      "  batch 10 loss: 0.0019578854879000575\n",
      "  batch 20 loss: 0.0031914673490973655\n",
      "  batch 30 loss: 0.0027345012178557225\n",
      "  batch 40 loss: 0.002932451138667602\n",
      "  batch 50 loss: 0.0026299139638013003\n",
      "  batch 60 loss: 0.0029561902775185446\n",
      "  batch 70 loss: 0.0029332579157653528\n",
      "  batch 80 loss: 0.0032436396441880787\n",
      "LOSS train 0.0032436396441880787 valid 0.0038664462866836403\n",
      "EPOCH 200:\n",
      "  batch 10 loss: 0.0030663828368687972\n",
      "  batch 20 loss: 0.0015963526831455966\n",
      "  batch 30 loss: 0.0034088415592123055\n",
      "  batch 40 loss: 0.002939483981685953\n",
      "  batch 50 loss: 0.0022447213928217026\n",
      "  batch 60 loss: 0.002316562773194164\n",
      "  batch 70 loss: 0.003428131073633267\n",
      "  batch 80 loss: 0.003786226137481208\n",
      "LOSS train 0.003786226137481208 valid 0.0031583751564357955\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch(curr_model=model)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total = len(validation_set)\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        inputs, labels = vdata\n",
    "        outputs = torch.argmax(model(inputs), dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct_count += (outputs==labels).sum().item()\n",
    "        # correct_count += (outputs==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../models/model6.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278350515463918"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_data.view(-1,21*2), train_labels)\n",
    "y_pred = knn.predict(val_data.view(-1,21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 21, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RidgeClassifierCV()\n",
    "clf.fit(train_data.view(-1, 21*2), train_labels)\n",
    "y_pred = clf.predict(val_data.view(-1, 21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_6527/1044316036.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6817, 0.0807, 0.0736, 0.0839, 0.0802],\n",
       "        [0.5953, 0.0871, 0.1020, 0.1140, 0.1016],\n",
       "        [0.6809, 0.0781, 0.0679, 0.0831, 0.0901],\n",
       "        [0.6449, 0.0783, 0.0792, 0.0871, 0.1105],\n",
       "        [0.6436, 0.0777, 0.0825, 0.0876, 0.1086],\n",
       "        [0.5774, 0.0947, 0.0956, 0.1239, 0.1083],\n",
       "        [0.5115, 0.0937, 0.0987, 0.1656, 0.1306],\n",
       "        [0.4998, 0.1001, 0.0840, 0.1717, 0.1444],\n",
       "        [0.6768, 0.0651, 0.0778, 0.0808, 0.0994],\n",
       "        [0.7222, 0.0787, 0.0629, 0.0701, 0.0661],\n",
       "        [0.7228, 0.0590, 0.0769, 0.0705, 0.0708],\n",
       "        [0.6058, 0.0801, 0.0881, 0.1046, 0.1215],\n",
       "        [0.5946, 0.0750, 0.0937, 0.1189, 0.1178],\n",
       "        [0.4962, 0.1252, 0.0958, 0.1549, 0.1279],\n",
       "        [0.5400, 0.1057, 0.0932, 0.1275, 0.1336],\n",
       "        [0.6375, 0.1178, 0.0775, 0.0829, 0.0844],\n",
       "        [0.6289, 0.0775, 0.0811, 0.1140, 0.0985],\n",
       "        [0.6558, 0.0717, 0.0818, 0.0909, 0.0998],\n",
       "        [0.6551, 0.0666, 0.0837, 0.0982, 0.0964],\n",
       "        [0.0692, 0.6421, 0.0877, 0.1231, 0.0780],\n",
       "        [0.0709, 0.6230, 0.0911, 0.1339, 0.0812],\n",
       "        [0.0713, 0.6259, 0.0887, 0.1374, 0.0766],\n",
       "        [0.0731, 0.6298, 0.0872, 0.1398, 0.0701],\n",
       "        [0.0836, 0.6127, 0.0964, 0.1132, 0.0941],\n",
       "        [0.0836, 0.5978, 0.0984, 0.1219, 0.0983],\n",
       "        [0.0737, 0.5998, 0.0957, 0.1305, 0.1002],\n",
       "        [0.0759, 0.6073, 0.0940, 0.1117, 0.1111],\n",
       "        [0.0637, 0.6749, 0.0802, 0.0768, 0.1044],\n",
       "        [0.0840, 0.6363, 0.0915, 0.0746, 0.1136],\n",
       "        [0.0836, 0.6447, 0.0907, 0.0694, 0.1116],\n",
       "        [0.0888, 0.6488, 0.0910, 0.0679, 0.1035],\n",
       "        [0.0871, 0.6840, 0.0805, 0.0661, 0.0823],\n",
       "        [0.0699, 0.6930, 0.0732, 0.0743, 0.0896],\n",
       "        [0.0963, 0.5992, 0.1002, 0.0809, 0.1235],\n",
       "        [0.0959, 0.5948, 0.1039, 0.0852, 0.1202],\n",
       "        [0.0941, 0.6012, 0.0999, 0.0767, 0.1282],\n",
       "        [0.0969, 0.5862, 0.1051, 0.0799, 0.1319],\n",
       "        [0.0812, 0.0810, 0.6525, 0.0663, 0.1190],\n",
       "        [0.0866, 0.0844, 0.6427, 0.0729, 0.1134],\n",
       "        [0.0903, 0.0872, 0.6412, 0.0718, 0.1095],\n",
       "        [0.0913, 0.0883, 0.6387, 0.0689, 0.1127],\n",
       "        [0.0952, 0.0853, 0.6478, 0.0540, 0.1177],\n",
       "        [0.0962, 0.0852, 0.6467, 0.0499, 0.1220],\n",
       "        [0.0804, 0.0695, 0.6838, 0.0544, 0.1119],\n",
       "        [0.0796, 0.0786, 0.6544, 0.0701, 0.1173],\n",
       "        [0.0836, 0.0841, 0.6444, 0.0763, 0.1116],\n",
       "        [0.0880, 0.0867, 0.6409, 0.0780, 0.1063],\n",
       "        [0.0891, 0.0842, 0.6515, 0.0734, 0.1018],\n",
       "        [0.0894, 0.0837, 0.6530, 0.0719, 0.1021],\n",
       "        [0.0874, 0.0841, 0.6332, 0.1226, 0.0727],\n",
       "        [0.0941, 0.0944, 0.6099, 0.1294, 0.0723],\n",
       "        [0.0888, 0.0941, 0.5737, 0.1660, 0.0773],\n",
       "        [0.0906, 0.0844, 0.6455, 0.1254, 0.0541],\n",
       "        [0.0858, 0.0774, 0.6550, 0.1273, 0.0544],\n",
       "        [0.0856, 0.0805, 0.6373, 0.1363, 0.0604],\n",
       "        [0.0906, 0.0884, 0.6089, 0.1498, 0.0623],\n",
       "        [0.0879, 0.0879, 0.6046, 0.1525, 0.0670],\n",
       "        [0.0923, 0.0867, 0.0797, 0.6535, 0.0878],\n",
       "        [0.0957, 0.0904, 0.0790, 0.6515, 0.0833],\n",
       "        [0.1044, 0.0945, 0.0829, 0.6379, 0.0804],\n",
       "        [0.1005, 0.0921, 0.0815, 0.6418, 0.0840],\n",
       "        [0.0983, 0.0897, 0.0809, 0.6516, 0.0796],\n",
       "        [0.1101, 0.0960, 0.0830, 0.6270, 0.0838],\n",
       "        [0.1286, 0.1032, 0.1016, 0.5771, 0.0895],\n",
       "        [0.0780, 0.0869, 0.1104, 0.6340, 0.0907],\n",
       "        [0.0775, 0.0869, 0.1030, 0.6466, 0.0860],\n",
       "        [0.1045, 0.1047, 0.0964, 0.6075, 0.0867],\n",
       "        [0.1269, 0.1253, 0.1334, 0.5058, 0.1085],\n",
       "        [0.1322, 0.1233, 0.1321, 0.5085, 0.1039],\n",
       "        [0.1046, 0.0892, 0.1067, 0.5955, 0.1040],\n",
       "        [0.0979, 0.1060, 0.1097, 0.5837, 0.1026],\n",
       "        [0.1103, 0.1136, 0.1168, 0.5579, 0.1013],\n",
       "        [0.1103, 0.1100, 0.1144, 0.5670, 0.0982],\n",
       "        [0.1073, 0.1094, 0.1066, 0.5802, 0.0965],\n",
       "        [0.1036, 0.1047, 0.1002, 0.5911, 0.1004],\n",
       "        [0.1067, 0.1124, 0.0982, 0.5870, 0.0957],\n",
       "        [0.1094, 0.1192, 0.1036, 0.5698, 0.0980],\n",
       "        [0.0880, 0.0855, 0.0918, 0.1049, 0.6298],\n",
       "        [0.0838, 0.0797, 0.0874, 0.1029, 0.6461],\n",
       "        [0.0863, 0.0841, 0.0863, 0.1071, 0.6361],\n",
       "        [0.0834, 0.0808, 0.0853, 0.1045, 0.6459],\n",
       "        [0.0873, 0.0822, 0.0982, 0.1077, 0.6245],\n",
       "        [0.1011, 0.0975, 0.1056, 0.1238, 0.5719],\n",
       "        [0.1049, 0.0998, 0.0983, 0.1199, 0.5771],\n",
       "        [0.1184, 0.1066, 0.1037, 0.1005, 0.5709],\n",
       "        [0.1120, 0.1007, 0.0971, 0.1019, 0.5883],\n",
       "        [0.1071, 0.1021, 0.1082, 0.1341, 0.5485],\n",
       "        [0.1080, 0.0999, 0.1012, 0.1313, 0.5597],\n",
       "        [0.0950, 0.0900, 0.0926, 0.1091, 0.6133],\n",
       "        [0.0928, 0.1323, 0.0920, 0.1117, 0.5713],\n",
       "        [0.0940, 0.1307, 0.0883, 0.1018, 0.5852],\n",
       "        [0.1183, 0.1211, 0.0890, 0.1068, 0.5648],\n",
       "        [0.1417, 0.1255, 0.0944, 0.1196, 0.5190],\n",
       "        [0.1463, 0.1143, 0.1095, 0.1156, 0.5142],\n",
       "        [0.1148, 0.1146, 0.0927, 0.1211, 0.5568],\n",
       "        [0.1126, 0.1125, 0.0884, 0.1162, 0.5703],\n",
       "        [0.1227, 0.1053, 0.1000, 0.1206, 0.5513]], dtype=torch.float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_data.view(-1, 21*2), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reetinav/anaconda3/envs/PIC16B/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(multi_class=\"ovr\")\n",
    "clf.fit(train_data.view(-1, 21*2), torch.argmax(train_labels, dim=1))\n",
    "clf.score(val_data.view(-1, 21*2), torch.argmax(val_labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7528, 0.0526, 0.0551, 0.0519, 0.0876],\n",
       "        [0.6623, 0.0536, 0.0913, 0.0680, 0.1248],\n",
       "        [0.7421, 0.0511, 0.0541, 0.0523, 0.1004],\n",
       "        [0.7057, 0.0502, 0.0582, 0.0611, 0.1248],\n",
       "        [0.6898, 0.0542, 0.0641, 0.0646, 0.1273],\n",
       "        [0.6079, 0.0719, 0.0888, 0.0880, 0.1435],\n",
       "        [0.5345, 0.0827, 0.1002, 0.1041, 0.1785],\n",
       "        [0.5266, 0.0871, 0.0854, 0.1148, 0.1861],\n",
       "        [0.7288, 0.0413, 0.0623, 0.0545, 0.1131],\n",
       "        [0.7814, 0.0521, 0.0439, 0.0446, 0.0779],\n",
       "        [0.8104, 0.0328, 0.0484, 0.0363, 0.0720],\n",
       "        [0.7139, 0.0439, 0.0575, 0.1079, 0.0768],\n",
       "        [0.6979, 0.0423, 0.0651, 0.1153, 0.0793],\n",
       "        [0.5230, 0.0924, 0.0954, 0.1829, 0.1062],\n",
       "        [0.5858, 0.0602, 0.0815, 0.1587, 0.1139],\n",
       "        [0.6972, 0.0675, 0.0633, 0.1023, 0.0697],\n",
       "        [0.7065, 0.0484, 0.0617, 0.1076, 0.0757],\n",
       "        [0.7538, 0.0348, 0.0552, 0.0913, 0.0649],\n",
       "        [0.7592, 0.0318, 0.0541, 0.0928, 0.0620],\n",
       "        [0.0342, 0.6723, 0.0858, 0.0939, 0.1137],\n",
       "        [0.0351, 0.6573, 0.0920, 0.0981, 0.1174],\n",
       "        [0.0358, 0.6592, 0.0934, 0.0995, 0.1120],\n",
       "        [0.0370, 0.6632, 0.0997, 0.0967, 0.1033],\n",
       "        [0.0466, 0.6359, 0.0972, 0.0931, 0.1272],\n",
       "        [0.0460, 0.6253, 0.1020, 0.0965, 0.1302],\n",
       "        [0.0381, 0.6393, 0.1065, 0.0934, 0.1227],\n",
       "        [0.0400, 0.6355, 0.1061, 0.0852, 0.1333],\n",
       "        [0.0322, 0.6886, 0.0855, 0.0671, 0.1267],\n",
       "        [0.0396, 0.6722, 0.0977, 0.0995, 0.0909],\n",
       "        [0.0361, 0.6855, 0.1014, 0.0901, 0.0869],\n",
       "        [0.0441, 0.6781, 0.0926, 0.0963, 0.0889],\n",
       "        [0.0419, 0.7072, 0.0820, 0.0910, 0.0779],\n",
       "        [0.0329, 0.7002, 0.0715, 0.1179, 0.0775],\n",
       "        [0.0454, 0.6440, 0.1134, 0.1016, 0.0956],\n",
       "        [0.0434, 0.6418, 0.1193, 0.1013, 0.0942],\n",
       "        [0.0430, 0.6512, 0.1111, 0.0971, 0.0976],\n",
       "        [0.0444, 0.6358, 0.1198, 0.0987, 0.1013],\n",
       "        [0.0250, 0.0642, 0.7968, 0.0224, 0.0915],\n",
       "        [0.0279, 0.0673, 0.7909, 0.0251, 0.0889],\n",
       "        [0.0288, 0.0671, 0.7939, 0.0245, 0.0857],\n",
       "        [0.0286, 0.0665, 0.7936, 0.0226, 0.0887],\n",
       "        [0.0262, 0.0605, 0.8107, 0.0167, 0.0859],\n",
       "        [0.0249, 0.0594, 0.8191, 0.0156, 0.0810],\n",
       "        [0.0219, 0.0483, 0.8297, 0.0150, 0.0850],\n",
       "        [0.0248, 0.0599, 0.7982, 0.0215, 0.0956],\n",
       "        [0.0265, 0.0662, 0.7978, 0.0247, 0.0846],\n",
       "        [0.0266, 0.0676, 0.8105, 0.0260, 0.0692],\n",
       "        [0.0249, 0.0641, 0.8264, 0.0225, 0.0621],\n",
       "        [0.0244, 0.0615, 0.8314, 0.0215, 0.0612],\n",
       "        [0.0329, 0.0799, 0.7396, 0.1204, 0.0273],\n",
       "        [0.0336, 0.0847, 0.7489, 0.1065, 0.0263],\n",
       "        [0.0388, 0.0874, 0.7319, 0.1013, 0.0406],\n",
       "        [0.0302, 0.0689, 0.7843, 0.0963, 0.0203],\n",
       "        [0.0284, 0.0639, 0.7903, 0.0974, 0.0200],\n",
       "        [0.0310, 0.0731, 0.7599, 0.1121, 0.0240],\n",
       "        [0.0343, 0.0859, 0.7280, 0.1252, 0.0266],\n",
       "        [0.0339, 0.0856, 0.7297, 0.1220, 0.0288],\n",
       "        [0.0791, 0.0974, 0.0770, 0.6618, 0.0846],\n",
       "        [0.0806, 0.1006, 0.0759, 0.6619, 0.0810],\n",
       "        [0.0839, 0.1058, 0.0811, 0.6492, 0.0799],\n",
       "        [0.0812, 0.1020, 0.0803, 0.6555, 0.0810],\n",
       "        [0.0796, 0.1017, 0.0821, 0.6574, 0.0791],\n",
       "        [0.0881, 0.1058, 0.0841, 0.6418, 0.0803],\n",
       "        [0.0951, 0.1042, 0.1162, 0.6037, 0.0808],\n",
       "        [0.0680, 0.1059, 0.1149, 0.6291, 0.0820],\n",
       "        [0.0674, 0.1058, 0.1085, 0.6380, 0.0803],\n",
       "        [0.0924, 0.1131, 0.1074, 0.6030, 0.0841],\n",
       "        [0.1143, 0.1253, 0.1531, 0.5061, 0.1011],\n",
       "        [0.1148, 0.1221, 0.1603, 0.5073, 0.0955],\n",
       "        [0.0970, 0.0956, 0.1271, 0.5944, 0.0860],\n",
       "        [0.0930, 0.1133, 0.1146, 0.5767, 0.1023],\n",
       "        [0.0989, 0.1148, 0.1210, 0.5660, 0.0993],\n",
       "        [0.1011, 0.1147, 0.1244, 0.5634, 0.0964],\n",
       "        [0.0971, 0.1211, 0.1236, 0.5648, 0.0934],\n",
       "        [0.0962, 0.1180, 0.1213, 0.5676, 0.0968],\n",
       "        [0.1021, 0.1363, 0.1113, 0.5483, 0.1020],\n",
       "        [0.1033, 0.1387, 0.1118, 0.5431, 0.1031],\n",
       "        [0.0799, 0.1005, 0.1137, 0.1013, 0.6046],\n",
       "        [0.0768, 0.0967, 0.1109, 0.0992, 0.6165],\n",
       "        [0.0777, 0.1019, 0.1098, 0.1004, 0.6102],\n",
       "        [0.0757, 0.0981, 0.1093, 0.1000, 0.6169],\n",
       "        [0.0798, 0.0891, 0.1133, 0.1027, 0.6151],\n",
       "        [0.0896, 0.0956, 0.1115, 0.1174, 0.5858],\n",
       "        [0.0928, 0.1051, 0.1102, 0.1116, 0.5802],\n",
       "        [0.1055, 0.1209, 0.1316, 0.1002, 0.5417],\n",
       "        [0.0985, 0.1152, 0.1257, 0.1003, 0.5603],\n",
       "        [0.0937, 0.0990, 0.1138, 0.1208, 0.5727],\n",
       "        [0.0949, 0.1017, 0.1099, 0.1177, 0.5759],\n",
       "        [0.0836, 0.1046, 0.1199, 0.0977, 0.5941],\n",
       "        [0.0881, 0.2182, 0.1045, 0.0892, 0.5000],\n",
       "        [0.0886, 0.2123, 0.1009, 0.0844, 0.5138],\n",
       "        [0.1178, 0.1436, 0.1019, 0.0864, 0.5503],\n",
       "        [0.1370, 0.1400, 0.1140, 0.0915, 0.5175],\n",
       "        [0.1351, 0.1244, 0.1427, 0.0891, 0.5087],\n",
       "        [0.1163, 0.1313, 0.1094, 0.0947, 0.5483],\n",
       "        [0.1146, 0.1283, 0.0982, 0.0911, 0.5677],\n",
       "        [0.1220, 0.1145, 0.1068, 0.0915, 0.5652]], dtype=torch.float64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
