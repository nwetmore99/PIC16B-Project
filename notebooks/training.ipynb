{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\", \"right\", \"left\", \"background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.3, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@194170.327] global loadsave.cpp:248 findDecoder imread_('../training/down.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.327] global loadsave.cpp:248 findDecoder imread_('../training/down.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.330] global loadsave.cpp:248 findDecoder imread_('../training/down.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.330] global loadsave.cpp:248 findDecoder imread_('../training/down.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.330] global loadsave.cpp:248 findDecoder imread_('../training/down.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.405] global loadsave.cpp:248 findDecoder imread_('../training/stop.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.405] global loadsave.cpp:248 findDecoder imread_('../training/stop.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.538] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.536] global loadsave.cpp:248 findDecoder imread_('../training/right.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.536] global loadsave.cpp:248 findDecoder imread_('../training/right.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.536] global loadsave.cpp:248 findDecoder imread_('../training/right.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194209.909] global loadsave.cpp:248 findDecoder imread_('../training/background.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194209.909] global loadsave.cpp:248 findDecoder imread_('../training/background.174.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(175):\n",
    "        try:\n",
    "            image = cv2.imread(f\"../training/{gesture_class}.{i}.jpg\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        except:\n",
    "            continue\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            train_label = np.zeros([len(classes)])\n",
    "            train_label[class_index] = 1\n",
    "            train_data.append(landmarks)\n",
    "            train_labels.append(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([785, 21, 2])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(train_data, train_labels)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_labels = []\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(40):\n",
    "        image = cv2.imread(f\"../validation/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            val_label = np.zeros([len(classes)])\n",
    "            val_label[class_index] = 1\n",
    "            val_data.append(landmarks)\n",
    "            val_labels.append(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(val_data, val_labels)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([310, 21, 2])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(curr_model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 2.0878557950258254\n",
      "  batch 20 loss: 2.089493823051453\n",
      "  batch 30 loss: 2.0923880636692047\n",
      "  batch 40 loss: 2.069521629810333\n",
      "  batch 50 loss: 2.0760524809360503\n",
      "  batch 60 loss: 2.0797445118427276\n",
      "  batch 70 loss: 2.079218715429306\n",
      "  batch 80 loss: 2.0730791211128237\n",
      "  batch 90 loss: 2.074336850643158\n",
      "  batch 100 loss: 2.0800801157951354\n",
      "  batch 110 loss: 2.0845657348632813\n",
      "  batch 120 loss: 2.0685907304286957\n",
      "  batch 130 loss: 2.092177817225456\n",
      "  batch 140 loss: 2.0694621056318283\n",
      "  batch 150 loss: 2.0737272053956985\n",
      "  batch 160 loss: 2.0867038160562514\n",
      "  batch 170 loss: 2.071632382273674\n",
      "  batch 180 loss: 2.0726309686899187\n",
      "  batch 190 loss: 2.058249536156654\n",
      "LOSS train 2.058249536156654 valid 2.0792298187048006\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 2.0570902675390244\n",
      "  batch 20 loss: 2.0671949476003646\n",
      "  batch 30 loss: 2.072288540005684\n",
      "  batch 40 loss: 2.079743266105652\n",
      "  batch 50 loss: 2.068570080399513\n",
      "  batch 60 loss: 2.064772054553032\n",
      "  batch 70 loss: 2.0603632986545564\n",
      "  batch 80 loss: 2.06798070371151\n",
      "  batch 90 loss: 2.0624730199575425\n",
      "  batch 100 loss: 2.0503203928470612\n",
      "  batch 110 loss: 2.0634419202804564\n",
      "  batch 120 loss: 2.0412196308374404\n",
      "  batch 130 loss: 2.0701397508382797\n",
      "  batch 140 loss: 2.0654239028692247\n",
      "  batch 150 loss: 2.087630444765091\n",
      "  batch 160 loss: 2.0471047669649125\n",
      "  batch 170 loss: 2.040412238240242\n",
      "  batch 180 loss: 2.0801672995090486\n",
      "  batch 190 loss: 2.0637885332107544\n",
      "LOSS train 2.0637885332107544 valid 2.0806283889672694\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 2.0689648926258086\n",
      "  batch 20 loss: 2.0722949266433717\n",
      "  batch 30 loss: 2.0371462225914003\n",
      "  batch 40 loss: 2.090448373556137\n",
      "  batch 50 loss: 2.0661796241998673\n",
      "  batch 60 loss: 2.070172148942947\n",
      "  batch 70 loss: 2.0672743380069734\n",
      "  batch 80 loss: 2.068069776892662\n",
      "  batch 90 loss: 2.065163168311119\n",
      "  batch 100 loss: 2.095017373561859\n",
      "  batch 110 loss: 2.089181351661682\n",
      "  batch 120 loss: 2.0389725148677824\n",
      "  batch 130 loss: 2.0583737552165986\n",
      "  batch 140 loss: 2.0580679565668105\n",
      "  batch 150 loss: 2.0283205032348635\n",
      "  batch 160 loss: 2.046481418609619\n",
      "  batch 170 loss: 2.0272442251443863\n",
      "  batch 180 loss: 1.9844309210777282\n",
      "  batch 190 loss: 2.0535339772701264\n",
      "LOSS train 2.0535339772701264 valid 2.084057793021202\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 2.0560593217611314\n",
      "  batch 20 loss: 2.0853463053703307\n",
      "  batch 30 loss: 2.0472177773714066\n",
      "  batch 40 loss: 2.0485582888126372\n",
      "  batch 50 loss: 2.0616987884044646\n",
      "  batch 60 loss: 2.042001724243164\n",
      "  batch 70 loss: 2.045454514026642\n",
      "  batch 80 loss: 2.08026562333107\n",
      "  batch 90 loss: 2.0655956089496614\n",
      "  batch 100 loss: 2.1202648490667344\n",
      "  batch 110 loss: 2.007922700047493\n",
      "  batch 120 loss: 2.0409527689218523\n",
      "  batch 130 loss: 2.0486905574798584\n",
      "  batch 140 loss: 1.9954997688531875\n",
      "  batch 150 loss: 2.0641927152872084\n",
      "  batch 160 loss: 2.0390039443969727\n",
      "  batch 170 loss: 2.028454747796059\n",
      "  batch 180 loss: 2.0592450141906737\n",
      "  batch 190 loss: 2.0248197585344316\n",
      "LOSS train 2.0248197585344316 valid 2.0875454438038363\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 2.099221956729889\n",
      "  batch 20 loss: 2.0345384240150453\n",
      "  batch 30 loss: 2.0601699471473696\n",
      "  batch 40 loss: 2.0367043048143385\n",
      "  batch 50 loss: 1.9697566628456116\n",
      "  batch 60 loss: 2.016777926683426\n",
      "  batch 70 loss: 2.061741200089455\n",
      "  batch 80 loss: 2.0569535911083223\n",
      "  batch 90 loss: 2.096097004413605\n",
      "  batch 100 loss: 2.0794590055942535\n",
      "  batch 110 loss: 2.0592783987522125\n",
      "  batch 120 loss: 2.0517808943986893\n",
      "  batch 130 loss: 2.0639210015535356\n",
      "  batch 140 loss: 2.0009645611047744\n",
      "  batch 150 loss: 2.054777094721794\n",
      "  batch 160 loss: 2.014817976951599\n",
      "  batch 170 loss: 2.0845919072628023\n",
      "  batch 180 loss: 2.0372526437044143\n",
      "  batch 190 loss: 2.020296248793602\n",
      "LOSS train 2.020296248793602 valid 2.090654289875275\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 2.014112910628319\n",
      "  batch 20 loss: 2.0578218042850493\n",
      "  batch 30 loss: 2.0660229444503786\n",
      "  batch 40 loss: 2.069650861620903\n",
      "  batch 50 loss: 2.071213021874428\n",
      "  batch 60 loss: 2.0645314037799833\n",
      "  batch 70 loss: 2.1177812844514845\n",
      "  batch 80 loss: 2.045789065957069\n",
      "  batch 90 loss: 2.0650517284870147\n",
      "  batch 100 loss: 1.9726159065961837\n",
      "  batch 110 loss: 2.022501230239868\n",
      "  batch 120 loss: 2.092787617444992\n",
      "  batch 130 loss: 2.081164672970772\n",
      "  batch 140 loss: 2.044906437397003\n",
      "  batch 150 loss: 2.0052335441112517\n",
      "  batch 160 loss: 1.9594524949789047\n",
      "  batch 170 loss: 2.02107734978199\n",
      "  batch 180 loss: 2.1073496729135512\n",
      "  batch 190 loss: 2.031420087814331\n",
      "LOSS train 2.031420087814331 valid 2.0940135618050895\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 2.0285634100437164\n",
      "  batch 20 loss: 2.0881962031126022\n",
      "  batch 30 loss: 2.0719246447086332\n",
      "  batch 40 loss: 2.041188272833824\n",
      "  batch 50 loss: 2.0352750718593597\n",
      "  batch 60 loss: 2.062901535630226\n",
      "  batch 70 loss: 2.0731334179639815\n",
      "  batch 80 loss: 2.024387076497078\n",
      "  batch 90 loss: 2.0925106048583983\n",
      "  batch 100 loss: 2.0075086832046507\n",
      "  batch 110 loss: 2.0866422027349474\n",
      "  batch 120 loss: 1.970718339085579\n",
      "  batch 130 loss: 2.0293505012989046\n",
      "  batch 140 loss: 2.017116779088974\n",
      "  batch 150 loss: 2.020699492096901\n",
      "  batch 160 loss: 2.0429228216409685\n",
      "  batch 170 loss: 2.016511008143425\n",
      "  batch 180 loss: 2.0789912581443786\n",
      "  batch 190 loss: 2.0763931035995484\n",
      "LOSS train 2.0763931035995484 valid 2.0960486091864414\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 2.078653860092163\n",
      "  batch 20 loss: 2.0219008147716524\n",
      "  batch 30 loss: 2.0812249004840853\n",
      "  batch 40 loss: 2.0858431577682497\n",
      "  batch 50 loss: 2.005154091119766\n",
      "  batch 60 loss: 2.0013275027275084\n",
      "  batch 70 loss: 2.0300100475549696\n",
      "  batch 80 loss: 2.0251250594854353\n",
      "  batch 90 loss: 2.121081519126892\n",
      "  batch 100 loss: 2.064827251434326\n",
      "  batch 110 loss: 2.018874168395996\n",
      "  batch 120 loss: 2.093106669187546\n",
      "  batch 130 loss: 2.0032146841287615\n",
      "  batch 140 loss: 2.067505267262459\n",
      "  batch 150 loss: 2.0374000042676927\n",
      "  batch 160 loss: 2.0301918357610704\n",
      "  batch 170 loss: 2.04880054295063\n",
      "  batch 180 loss: 2.00517797768116\n",
      "  batch 190 loss: 2.0164912551641465\n",
      "LOSS train 2.0164912551641465 valid 2.0976967364549637\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 2.0968416184186935\n",
      "  batch 20 loss: 2.0072796255350114\n",
      "  batch 30 loss: 2.0296754986047745\n",
      "  batch 40 loss: 2.019930139183998\n",
      "  batch 50 loss: 2.0723933547735216\n",
      "  batch 60 loss: 2.085010677576065\n",
      "  batch 70 loss: 2.0159109771251678\n",
      "  batch 80 loss: 2.0947265625\n",
      "  batch 90 loss: 2.0147203326225283\n",
      "  batch 100 loss: 2.053756135702133\n",
      "  batch 110 loss: 1.9351011097431183\n",
      "  batch 120 loss: 2.026409813761711\n",
      "  batch 130 loss: 2.0560038805007936\n",
      "  batch 140 loss: 2.0879234284162522\n",
      "  batch 150 loss: 2.053373026847839\n",
      "  batch 160 loss: 2.0706821143627168\n",
      "  batch 170 loss: 2.0274301677942277\n",
      "  batch 180 loss: 2.0504220724105835\n",
      "  batch 190 loss: 2.0532319873571394\n",
      "LOSS train 2.0532319873571394 valid 2.0988993667639217\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 1.9533864289522171\n",
      "  batch 20 loss: 2.089700976014137\n",
      "  batch 30 loss: 2.0234012812376023\n",
      "  batch 40 loss: 2.0053787529468536\n",
      "  batch 50 loss: 2.0306227684020994\n",
      "  batch 60 loss: 2.0735517770051954\n",
      "  batch 70 loss: 2.0361183911561964\n",
      "  batch 80 loss: 2.012920680642128\n",
      "  batch 90 loss: 2.0831086367368696\n",
      "  batch 100 loss: 2.08878855407238\n",
      "  batch 110 loss: 2.012604206800461\n",
      "  batch 120 loss: 2.027317187190056\n",
      "  batch 130 loss: 2.0902603298425673\n",
      "  batch 140 loss: 2.096367675065994\n",
      "  batch 150 loss: 2.02662692964077\n",
      "  batch 160 loss: 2.049232169985771\n",
      "  batch 170 loss: 2.0452072978019715\n",
      "  batch 180 loss: 2.0263719290494917\n",
      "  batch 190 loss: 2.04757699072361\n",
      "LOSS train 2.04757699072361 valid 2.099511064780064\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 2.0558534622192384\n",
      "  batch 20 loss: 2.0173261404037475\n",
      "  batch 30 loss: 1.9767569094896316\n",
      "  batch 40 loss: 2.029432716965675\n",
      "  batch 50 loss: 2.054653200507164\n",
      "  batch 60 loss: 2.024769848585129\n",
      "  batch 70 loss: 2.0610922515392303\n",
      "  batch 80 loss: 2.016798111796379\n",
      "  batch 90 loss: 2.101037985086441\n",
      "  batch 100 loss: 2.041212812066078\n",
      "  batch 110 loss: 2.0977262645959853\n",
      "  batch 120 loss: 2.065757116675377\n",
      "  batch 130 loss: 2.016877907514572\n",
      "  batch 140 loss: 2.013383573293686\n",
      "  batch 150 loss: 2.0941981464624404\n",
      "  batch 160 loss: 2.0387564092874526\n",
      "  batch 170 loss: 2.046665245294571\n",
      "  batch 180 loss: 2.0285532504320143\n",
      "  batch 190 loss: 2.041899433732033\n",
      "LOSS train 2.041899433732033 valid 2.100082585826898\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 2.125316345691681\n",
      "  batch 20 loss: 2.002662581205368\n",
      "  batch 30 loss: 2.0690160274505613\n",
      "  batch 40 loss: 2.0118926048278807\n",
      "  batch 50 loss: 2.02370747923851\n",
      "  batch 60 loss: 2.0700943917036057\n",
      "  batch 70 loss: 2.037820649147034\n",
      "  batch 80 loss: 2.0738807320594788\n",
      "  batch 90 loss: 2.0321578979492188\n",
      "  batch 100 loss: 2.032180663943291\n",
      "  batch 110 loss: 2.012137472629547\n",
      "  batch 120 loss: 2.0655772417783735\n",
      "  batch 130 loss: 2.0168118685483933\n",
      "  batch 140 loss: 2.043418902158737\n",
      "  batch 150 loss: 2.02430374622345\n",
      "  batch 160 loss: 2.0433435410261156\n",
      "  batch 170 loss: 2.0211085468530654\n",
      "  batch 180 loss: 2.102443131804466\n",
      "  batch 190 loss: 2.0599761307239532\n",
      "LOSS train 2.0599761307239532 valid 2.100742261379193\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 2.0054493874311445\n",
      "  batch 20 loss: 2.0975609540939333\n",
      "  batch 30 loss: 2.0036471396684647\n",
      "  batch 40 loss: 2.0229303359985353\n",
      "  batch 50 loss: 2.033455121517181\n",
      "  batch 60 loss: 2.0729894578456878\n",
      "  batch 70 loss: 2.062000569701195\n",
      "  batch 80 loss: 2.0597862154245377\n",
      "  batch 90 loss: 2.0744290918111803\n",
      "  batch 100 loss: 2.046686515212059\n",
      "  batch 110 loss: 2.031000238656998\n",
      "  batch 120 loss: 2.02141487300396\n",
      "  batch 130 loss: 2.0345250487327577\n",
      "  batch 140 loss: 2.0484293341636657\n",
      "  batch 150 loss: 1.9974824249744416\n",
      "  batch 160 loss: 2.019505834579468\n",
      "  batch 170 loss: 2.0109143584966658\n",
      "  batch 180 loss: 2.051631599664688\n",
      "  batch 190 loss: 2.091757541894913\n",
      "LOSS train 2.091757541894913 valid 2.101614999847534\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 2.092600578069687\n",
      "  batch 20 loss: 2.0418041229248045\n",
      "  batch 30 loss: 1.9900014877319336\n",
      "  batch 40 loss: 2.037557044625282\n",
      "  batch 50 loss: 2.0429727345705033\n",
      "  batch 60 loss: 1.9907527208328246\n",
      "  batch 70 loss: 2.0718445479869843\n",
      "  batch 80 loss: 2.1401302963495255\n",
      "  batch 90 loss: 2.0115446597337723\n",
      "  batch 100 loss: 2.0820594310760496\n",
      "  batch 110 loss: 2.057778459787369\n",
      "  batch 120 loss: 2.0950036704540254\n",
      "  batch 130 loss: 1.989420971274376\n",
      "  batch 140 loss: 1.9902437925338745\n",
      "  batch 150 loss: 1.9751418024301528\n",
      "  batch 160 loss: 2.07102589905262\n",
      "  batch 170 loss: 1.9947455376386642\n",
      "  batch 180 loss: 2.0663888573646547\n",
      "  batch 190 loss: 2.0728610694408416\n",
      "LOSS train 2.0728610694408416 valid 2.1024246456531377\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 2.039347085356712\n",
      "  batch 20 loss: 2.0181130856275558\n",
      "  batch 30 loss: 2.03192378282547\n",
      "  batch 40 loss: 2.05064971446991\n",
      "  batch 50 loss: 2.0535724490880964\n",
      "  batch 60 loss: 2.0927991688251497\n",
      "  batch 70 loss: 2.070133638381958\n",
      "  batch 80 loss: 2.0567007035017015\n",
      "  batch 90 loss: 2.0336337268352507\n",
      "  batch 100 loss: 1.9840658903121948\n",
      "  batch 110 loss: 2.080325099825859\n",
      "  batch 120 loss: 2.0212301880121233\n",
      "  batch 130 loss: 2.047950139641762\n",
      "  batch 140 loss: 2.1162349045276643\n",
      "  batch 150 loss: 2.059938353300095\n",
      "  batch 160 loss: 2.0139882981777193\n",
      "  batch 170 loss: 2.0663921028375625\n",
      "  batch 180 loss: 2.011569356918335\n",
      "  batch 190 loss: 2.012775132060051\n",
      "LOSS train 2.012775132060051 valid 2.1030966440836587\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 2.089347389340401\n",
      "  batch 20 loss: 2.0552234023809435\n",
      "  batch 30 loss: 2.06101670563221\n",
      "  batch 40 loss: 2.1011981785297396\n",
      "  batch 50 loss: 1.9505896508693694\n",
      "  batch 60 loss: 2.014752686023712\n",
      "  batch 70 loss: 2.002996963262558\n",
      "  batch 80 loss: 2.0344135373830796\n",
      "  batch 90 loss: 2.0685064107179643\n",
      "  batch 100 loss: 2.0695890367031096\n",
      "  batch 110 loss: 2.0431777268648146\n",
      "  batch 120 loss: 2.050271597504616\n",
      "  batch 130 loss: 2.081197926402092\n",
      "  batch 140 loss: 2.0738768845796587\n",
      "  batch 150 loss: 1.979148057103157\n",
      "  batch 160 loss: 2.0423298090696336\n",
      "  batch 170 loss: 2.0901577293872835\n",
      "  batch 180 loss: 2.010380482673645\n",
      "  batch 190 loss: 1.9744905948638916\n",
      "LOSS train 1.9744905948638916 valid 2.1029393669122305\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 2.0488582104444504\n",
      "  batch 20 loss: 2.085146564245224\n",
      "  batch 30 loss: 2.0212507843971252\n",
      "  batch 40 loss: 2.0317833840847017\n",
      "  batch 50 loss: 2.075362581014633\n",
      "  batch 60 loss: 2.0454261869192125\n",
      "  batch 70 loss: 2.013205569982529\n",
      "  batch 80 loss: 2.035137286782265\n",
      "  batch 90 loss: 2.0955176144838332\n",
      "  batch 100 loss: 1.9810109555721283\n",
      "  batch 110 loss: 1.9868272066116333\n",
      "  batch 120 loss: 2.0572501748800276\n",
      "  batch 130 loss: 2.0902335941791534\n",
      "  batch 140 loss: 2.0264686733484267\n",
      "  batch 150 loss: 2.129984700679779\n",
      "  batch 160 loss: 2.0411124765872954\n",
      "  batch 170 loss: 2.0268343478441238\n",
      "  batch 180 loss: 1.9870529621839523\n",
      "  batch 190 loss: 2.0145170629024505\n",
      "LOSS train 2.0145170629024505 valid 2.1026222163285966\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 1.9599991768598557\n",
      "  batch 20 loss: 2.0141366004943846\n",
      "  batch 30 loss: 2.068896454572678\n",
      "  batch 40 loss: 2.0289543986320497\n",
      "  batch 50 loss: 2.1309399932622908\n",
      "  batch 60 loss: 2.0415832072496416\n",
      "  batch 70 loss: 2.0403494000434876\n",
      "  batch 80 loss: 2.028477543592453\n",
      "  batch 90 loss: 1.9997417211532593\n",
      "  batch 100 loss: 2.114454311132431\n",
      "  batch 110 loss: 2.097868397831917\n",
      "  batch 120 loss: 2.0490092158317568\n",
      "  batch 130 loss: 2.023439872264862\n",
      "  batch 140 loss: 2.042424890398979\n",
      "  batch 150 loss: 2.016088590025902\n",
      "  batch 160 loss: 2.023095440864563\n",
      "  batch 170 loss: 1.9911877006292342\n",
      "  batch 180 loss: 2.0217685252428055\n",
      "  batch 190 loss: 2.0514036446809767\n",
      "LOSS train 2.0514036446809767 valid 2.102120225246136\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 2.0151034355163575\n",
      "  batch 20 loss: 2.05390365421772\n",
      "  batch 30 loss: 2.104024109244347\n",
      "  batch 40 loss: 2.078125622868538\n",
      "  batch 50 loss: 1.9950666427612305\n",
      "  batch 60 loss: 2.0449327290058137\n",
      "  batch 70 loss: 2.0171604454517365\n",
      "  batch 80 loss: 2.044405457377434\n",
      "  batch 90 loss: 2.069659486413002\n",
      "  batch 100 loss: 2.0211278229951857\n",
      "  batch 110 loss: 2.0299094676971436\n",
      "  batch 120 loss: 1.9800508499145508\n",
      "  batch 130 loss: 2.036020517349243\n",
      "  batch 140 loss: 2.0650780320167543\n",
      "  batch 150 loss: 1.9970744788646697\n",
      "  batch 160 loss: 2.04856458902359\n",
      "  batch 170 loss: 2.074945291876793\n",
      "  batch 180 loss: 2.059934985637665\n",
      "  batch 190 loss: 2.045208755135536\n",
      "LOSS train 2.045208755135536 valid 2.1018029057826753\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 2.0649964094161986\n",
      "  batch 20 loss: 2.0459264874458314\n",
      "  batch 30 loss: 1.9556438326835632\n",
      "  batch 40 loss: 2.075751268863678\n",
      "  batch 50 loss: 2.105743858218193\n",
      "  batch 60 loss: 2.0995984584093095\n",
      "  batch 70 loss: 1.9283475130796432\n",
      "  batch 80 loss: 2.030095961689949\n",
      "  batch 90 loss: 2.085511493682861\n",
      "  batch 100 loss: 2.061122640967369\n",
      "  batch 110 loss: 2.1071573972702025\n",
      "  batch 120 loss: 2.1286214381456374\n",
      "  batch 130 loss: 2.0227357387542724\n",
      "  batch 140 loss: 2.048482283949852\n",
      "  batch 150 loss: 2.043402537703514\n",
      "  batch 160 loss: 2.064275246858597\n",
      "  batch 170 loss: 1.9719888597726822\n",
      "  batch 180 loss: 1.9585932523012162\n",
      "  batch 190 loss: 2.084500652551651\n",
      "LOSS train 2.084500652551651 valid 2.10217579244039\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 2.016638511419296\n",
      "  batch 20 loss: 2.077737659215927\n",
      "  batch 30 loss: 2.0661296367645265\n",
      "  batch 40 loss: 2.015341717004776\n",
      "  batch 50 loss: 2.0325425535440447\n",
      "  batch 60 loss: 2.0697463721036913\n",
      "  batch 70 loss: 2.0662469416856766\n",
      "  batch 80 loss: 1.9887414008378983\n",
      "  batch 90 loss: 2.043648636341095\n",
      "  batch 100 loss: 2.0306244790554047\n",
      "  batch 110 loss: 1.965620443224907\n",
      "  batch 120 loss: 2.0941468507051466\n",
      "  batch 130 loss: 2.039498472213745\n",
      "  batch 140 loss: 2.0592346459627153\n",
      "  batch 150 loss: 2.067776235938072\n",
      "  batch 160 loss: 2.0519925117492677\n",
      "  batch 170 loss: 2.0619593501091003\n",
      "  batch 180 loss: 1.9645017623901366\n",
      "  batch 190 loss: 2.047801685333252\n",
      "LOSS train 2.047801685333252 valid 2.1019108574359846\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 2.1150408029556274\n",
      "  batch 20 loss: 2.0877711176872253\n",
      "  batch 30 loss: 2.0290512919425963\n",
      "  batch 40 loss: 2.0200183004140855\n",
      "  batch 50 loss: 2.0876249819993973\n",
      "  batch 60 loss: 2.0876071482896803\n",
      "  batch 70 loss: 1.9440378546714783\n",
      "  batch 80 loss: 2.0122065365314485\n",
      "  batch 90 loss: 1.9423178136348724\n",
      "  batch 100 loss: 2.101091432571411\n",
      "  batch 110 loss: 2.0429320871829986\n",
      "  batch 120 loss: 2.070893356204033\n",
      "  batch 130 loss: 2.060962423682213\n",
      "  batch 140 loss: 2.087850585579872\n",
      "  batch 150 loss: 2.0359472423791884\n",
      "  batch 160 loss: 2.012429729104042\n",
      "  batch 170 loss: 2.0615789890289307\n",
      "  batch 180 loss: 2.0087122827768327\n",
      "  batch 190 loss: 1.9957556933164597\n",
      "LOSS train 1.9957556933164597 valid 2.1018899499605865\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 2.009679561853409\n",
      "  batch 20 loss: 2.0603433519601824\n",
      "  batch 30 loss: 1.9831140398979188\n",
      "  batch 40 loss: 2.054598960280418\n",
      "  batch 50 loss: 2.013531768321991\n",
      "  batch 60 loss: 2.0716274946928026\n",
      "  batch 70 loss: 2.0662033289670942\n",
      "  batch 80 loss: 2.0189053416252136\n",
      "  batch 90 loss: 2.0489035695791245\n",
      "  batch 100 loss: 2.0125638782978057\n",
      "  batch 110 loss: 2.078495553135872\n",
      "  batch 120 loss: 2.0221355468034745\n",
      "  batch 130 loss: 2.096217602491379\n",
      "  batch 140 loss: 2.027508392930031\n",
      "  batch 150 loss: 2.043616759777069\n",
      "  batch 160 loss: 2.1007225155830382\n",
      "  batch 170 loss: 1.9867439925670625\n",
      "  batch 180 loss: 2.010625186562538\n",
      "  batch 190 loss: 2.043559229373932\n",
      "LOSS train 2.043559229373932 valid 2.101314759789369\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 2.0896621376276014\n",
      "  batch 20 loss: 2.0976947993040085\n",
      "  batch 30 loss: 2.090600770711899\n",
      "  batch 40 loss: 2.0853305727243425\n",
      "  batch 50 loss: 1.9278979510068894\n",
      "  batch 60 loss: 2.011109393835068\n",
      "  batch 70 loss: 2.051082345843315\n",
      "  batch 80 loss: 2.0203433573246\n",
      "  batch 90 loss: 2.059640309214592\n",
      "  batch 100 loss: 2.021393910050392\n",
      "  batch 110 loss: 2.050915855169296\n",
      "  batch 120 loss: 2.010408189892769\n",
      "  batch 130 loss: 2.1370013475418093\n",
      "  batch 140 loss: 1.962005615234375\n",
      "  batch 150 loss: 1.9980774641036987\n",
      "  batch 160 loss: 2.078921157121658\n",
      "  batch 170 loss: 2.0428914934396745\n",
      "  batch 180 loss: 2.0397427827119827\n",
      "  batch 190 loss: 1.9788686513900757\n",
      "LOSS train 1.9788686513900757 valid 2.101185306142538\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 2.0386999905109406\n",
      "  batch 20 loss: 1.9929357081651689\n",
      "  batch 30 loss: 2.1000992923974993\n",
      "  batch 40 loss: 2.0205453664064406\n",
      "  batch 50 loss: 2.098928025364876\n",
      "  batch 60 loss: 2.052921336889267\n",
      "  batch 70 loss: 2.0131209194660187\n",
      "  batch 80 loss: 1.9676325142383575\n",
      "  batch 90 loss: 1.9975383400917053\n",
      "  batch 100 loss: 2.0505032032728194\n",
      "  batch 110 loss: 2.0547250479459764\n",
      "  batch 120 loss: 2.0561895191669466\n",
      "  batch 130 loss: 2.0299978882074354\n",
      "  batch 140 loss: 2.052108332514763\n",
      "  batch 150 loss: 1.974780160188675\n",
      "  batch 160 loss: 2.1012042820453645\n",
      "  batch 170 loss: 2.1003220468759536\n",
      "  batch 180 loss: 2.0932834923267363\n",
      "  batch 190 loss: 2.017878979444504\n",
      "LOSS train 2.017878979444504 valid 2.1005057314267526\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 2.0588983595371246\n",
      "  batch 20 loss: 2.054557591676712\n",
      "  batch 30 loss: 2.0210548490285873\n",
      "  batch 40 loss: 1.9947296559810639\n",
      "  batch 50 loss: 2.0248651295900344\n",
      "  batch 60 loss: 2.02468563914299\n",
      "  batch 70 loss: 1.9862245857715606\n",
      "  batch 80 loss: 1.996485647559166\n",
      "  batch 90 loss: 2.0172776222229003\n",
      "  batch 100 loss: 2.0595397025346758\n",
      "  batch 110 loss: 2.0814502388238907\n",
      "  batch 120 loss: 2.0214085280895233\n",
      "  batch 130 loss: 2.017307832837105\n",
      "  batch 140 loss: 2.083373036980629\n",
      "  batch 150 loss: 2.047102504968643\n",
      "  batch 160 loss: 2.076296493411064\n",
      "  batch 170 loss: 2.1224040389060974\n",
      "  batch 180 loss: 2.017795664072037\n",
      "  batch 190 loss: 2.073008570075035\n",
      "LOSS train 2.073008570075035 valid 2.0997872287646318\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 2.0748683273792268\n",
      "  batch 20 loss: 2.043510639667511\n",
      "  batch 30 loss: 2.0467917889356615\n",
      "  batch 40 loss: 1.9339212983846665\n",
      "  batch 50 loss: 2.0122017592191694\n",
      "  batch 60 loss: 2.0393567740917207\n",
      "  batch 70 loss: 2.0582175493240356\n",
      "  batch 80 loss: 2.084165471792221\n",
      "  batch 90 loss: 1.9960950434207916\n",
      "  batch 100 loss: 1.9583726674318314\n",
      "  batch 110 loss: 2.024262809753418\n",
      "  batch 120 loss: 2.0192782282829285\n",
      "  batch 130 loss: 2.1129921197891237\n",
      "  batch 140 loss: 2.097939571738243\n",
      "  batch 150 loss: 2.002068263292313\n",
      "  batch 160 loss: 2.061189016699791\n",
      "  batch 170 loss: 2.080371302366257\n",
      "  batch 180 loss: 2.0051973193883894\n",
      "  batch 190 loss: 2.072867697477341\n",
      "LOSS train 2.072867697477341 valid 2.0993668268888426\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 1.9751620560884475\n",
      "  batch 20 loss: 2.034350407123566\n",
      "  batch 30 loss: 2.0052911162376406\n",
      "  batch 40 loss: 1.9787785232067108\n",
      "  batch 50 loss: 2.0969480216503142\n",
      "  batch 60 loss: 2.011883044242859\n",
      "  batch 70 loss: 2.073873883485794\n",
      "  batch 80 loss: 2.071540293097496\n",
      "  batch 90 loss: 2.075268104672432\n",
      "  batch 100 loss: 2.0519905775785445\n",
      "  batch 110 loss: 1.981198987364769\n",
      "  batch 120 loss: 2.1366937071084977\n",
      "  batch 130 loss: 2.0075583159923553\n",
      "  batch 140 loss: 2.0809349715709686\n",
      "  batch 150 loss: 2.0193000078201293\n",
      "  batch 160 loss: 2.054097628593445\n",
      "  batch 170 loss: 2.062690246105194\n",
      "  batch 180 loss: 2.026395547389984\n",
      "  batch 190 loss: 2.017397564649582\n",
      "LOSS train 2.017397564649582 valid 2.099014407549149\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 2.032829949259758\n",
      "  batch 20 loss: 2.063436934351921\n",
      "  batch 30 loss: 2.061184909939766\n",
      "  batch 40 loss: 2.08838053047657\n",
      "  batch 50 loss: 2.0086850583553315\n",
      "  batch 60 loss: 1.9989247143268585\n",
      "  batch 70 loss: 2.0390708655118943\n",
      "  batch 80 loss: 2.090115436911583\n",
      "  batch 90 loss: 2.0349484980106354\n",
      "  batch 100 loss: 2.067365664243698\n",
      "  batch 110 loss: 2.0585384160280227\n",
      "  batch 120 loss: 2.0867491513490677\n",
      "  batch 130 loss: 2.064271977543831\n",
      "  batch 140 loss: 2.0135689109563826\n",
      "  batch 150 loss: 1.964675185084343\n",
      "  batch 160 loss: 1.939240425825119\n",
      "  batch 170 loss: 2.0363409399986265\n",
      "  batch 180 loss: 2.066290113329887\n",
      "  batch 190 loss: 2.020691305398941\n",
      "LOSS train 2.020691305398941 valid 2.0997867366442313\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 2.047932118177414\n",
      "  batch 20 loss: 2.032720848917961\n",
      "  batch 30 loss: 2.065371897816658\n",
      "  batch 40 loss: 2.0975555419921874\n",
      "  batch 50 loss: 2.0661249458789825\n",
      "  batch 60 loss: 2.0895170867443085\n",
      "  batch 70 loss: 2.0279791742563247\n",
      "  batch 80 loss: 1.9982508897781373\n",
      "  batch 90 loss: 2.017461973428726\n",
      "  batch 100 loss: 2.0274062126874925\n",
      "  batch 110 loss: 2.050095263123512\n",
      "  batch 120 loss: 2.0577735781669615\n",
      "  batch 130 loss: 2.0309702187776564\n",
      "  batch 140 loss: 1.9761114567518234\n",
      "  batch 150 loss: 2.0493117749691008\n",
      "  batch 160 loss: 2.0264307796955108\n",
      "  batch 170 loss: 2.0553593575954436\n",
      "  batch 180 loss: 2.004759058356285\n",
      "  batch 190 loss: 1.9830229312181473\n",
      "LOSS train 1.9830229312181473 valid 2.09966788880336\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 2.061486530303955\n",
      "  batch 20 loss: 1.9981945395469665\n",
      "  batch 30 loss: 2.030584400892258\n",
      "  batch 40 loss: 2.0975450932979585\n",
      "  batch 50 loss: 2.038057380914688\n",
      "  batch 60 loss: 2.0730004727840425\n",
      "  batch 70 loss: 2.0277080953121187\n",
      "  batch 80 loss: 2.0234239608049394\n",
      "  batch 90 loss: 2.0982217639684677\n",
      "  batch 100 loss: 2.0312686592340468\n",
      "  batch 110 loss: 2.0471953809261323\n",
      "  batch 120 loss: 2.1283649474382402\n",
      "  batch 130 loss: 2.0605861842632294\n",
      "  batch 140 loss: 2.0532257527112963\n",
      "  batch 150 loss: 1.9310911506414414\n",
      "  batch 160 loss: 1.9910498589277268\n",
      "  batch 170 loss: 2.0170692980289457\n",
      "  batch 180 loss: 2.0556870341300963\n",
      "  batch 190 loss: 2.02564155459404\n",
      "LOSS train 2.02564155459404 valid 2.0991407522024255\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 2.0808514446020125\n",
      "  batch 20 loss: 2.0738740146160124\n",
      "  batch 30 loss: 1.9928504586219788\n",
      "  batch 40 loss: 2.03182834982872\n",
      "  batch 50 loss: 2.0489635705947875\n",
      "  batch 60 loss: 2.039218559861183\n",
      "  batch 70 loss: 2.090747219324112\n",
      "  batch 80 loss: 2.0763240665197373\n",
      "  batch 90 loss: 1.9921952933073044\n",
      "  batch 100 loss: 1.9968788266181945\n",
      "  batch 110 loss: 2.047199475765228\n",
      "  batch 120 loss: 2.014866057038307\n",
      "  batch 130 loss: 2.050517252087593\n",
      "  batch 140 loss: 2.0119143664836883\n",
      "  batch 150 loss: 2.04010793864727\n",
      "  batch 160 loss: 2.0523332089185713\n",
      "  batch 170 loss: 2.0384760767221453\n",
      "  batch 180 loss: 2.0893993407487867\n",
      "  batch 190 loss: 1.9383246719837188\n",
      "LOSS train 1.9383246719837188 valid 2.098084522363467\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 2.1216781407594683\n",
      "  batch 20 loss: 1.913135901093483\n",
      "  batch 30 loss: 2.0129390090703962\n",
      "  batch 40 loss: 2.1112349003553392\n",
      "  batch 50 loss: 2.0355413138866423\n",
      "  batch 60 loss: 2.0651579022407534\n",
      "  batch 70 loss: 2.031331542134285\n",
      "  batch 80 loss: 1.9765317410230636\n",
      "  batch 90 loss: 2.0976751327514647\n",
      "  batch 100 loss: 2.0770996689796446\n",
      "  batch 110 loss: 2.0478121727705\n",
      "  batch 120 loss: 2.0314141362905502\n",
      "  batch 130 loss: 2.056047248840332\n",
      "  batch 140 loss: 2.0367582947015763\n",
      "  batch 150 loss: 1.9776852399110794\n",
      "  batch 160 loss: 2.0477154940366744\n",
      "  batch 170 loss: 1.9790924936532974\n",
      "  batch 180 loss: 1.982270395755768\n",
      "  batch 190 loss: 2.0332014948129653\n",
      "LOSS train 2.0332014948129653 valid 2.0973964983072038\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 2.0586163312196732\n",
      "  batch 20 loss: 2.0400896310806274\n",
      "  batch 30 loss: 1.9683521270751954\n",
      "  batch 40 loss: 2.004179283976555\n",
      "  batch 50 loss: 2.0466594129800795\n",
      "  batch 60 loss: 2.0829600542783737\n",
      "  batch 70 loss: 2.1000134617090227\n",
      "  batch 80 loss: 2.00889133810997\n",
      "  batch 90 loss: 2.074626791477203\n",
      "  batch 100 loss: 2.0470887362957\n",
      "  batch 110 loss: 2.035397672653198\n",
      "  batch 120 loss: 2.0545566350221636\n",
      "  batch 130 loss: 1.9949758857488633\n",
      "  batch 140 loss: 2.0653454542160032\n",
      "  batch 150 loss: 2.013852372765541\n",
      "  batch 160 loss: 2.0414138346910478\n",
      "  batch 170 loss: 2.0105294436216354\n",
      "  batch 180 loss: 2.0222127974033355\n",
      "  batch 190 loss: 2.0439787596464156\n",
      "LOSS train 2.0439787596464156 valid 2.0966919599435268\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 2.135638377070427\n",
      "  batch 20 loss: 2.0049209356307984\n",
      "  batch 30 loss: 2.0346993803977966\n",
      "  batch 40 loss: 2.0617672353982925\n",
      "  batch 50 loss: 2.0096137553453444\n",
      "  batch 60 loss: 2.0030745446681975\n",
      "  batch 70 loss: 2.026312971115112\n",
      "  batch 80 loss: 2.027253559231758\n",
      "  batch 90 loss: 2.013877737522125\n",
      "  batch 100 loss: 1.9733952820301055\n",
      "  batch 110 loss: 2.0502857476472856\n",
      "  batch 120 loss: 2.0474203675985336\n",
      "  batch 130 loss: 2.0087571680545806\n",
      "  batch 140 loss: 2.0439732283353806\n",
      "  batch 150 loss: 2.091392880678177\n",
      "  batch 160 loss: 1.9424555152654648\n",
      "  batch 170 loss: 2.0329815268516542\n",
      "  batch 180 loss: 2.081219157576561\n",
      "  batch 190 loss: 2.0242252379655836\n",
      "LOSS train 2.0242252379655836 valid 2.094755211319679\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 1.9774111330509185\n",
      "  batch 20 loss: 2.073783430457115\n",
      "  batch 30 loss: 1.9905053108930588\n",
      "  batch 40 loss: 1.996554958820343\n",
      "  batch 50 loss: 2.0567715167999268\n",
      "  batch 60 loss: 2.0084444493055345\n",
      "  batch 70 loss: 2.0598091959953306\n",
      "  batch 80 loss: 2.029127812385559\n",
      "  batch 90 loss: 2.0123257875442504\n",
      "  batch 100 loss: 1.945602574944496\n",
      "  batch 110 loss: 2.0618342071771623\n",
      "  batch 120 loss: 2.1527769565582275\n",
      "  batch 130 loss: 2.097287580370903\n",
      "  batch 140 loss: 1.9373175203800201\n",
      "  batch 150 loss: 1.9850812077522277\n",
      "  batch 160 loss: 1.967402622103691\n",
      "  batch 170 loss: 2.0933532148599623\n",
      "  batch 180 loss: 2.059778118133545\n",
      "  batch 190 loss: 2.1022687166929246\n",
      "LOSS train 2.1022687166929246 valid 2.092513687717609\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 2.0042614877223968\n",
      "  batch 20 loss: 2.0165157556533813\n",
      "  batch 30 loss: 2.0223834335803987\n",
      "  batch 40 loss: 2.075169765949249\n",
      "  batch 50 loss: 2.000554656982422\n",
      "  batch 60 loss: 2.1055360794067384\n",
      "  batch 70 loss: 2.0460221737623216\n",
      "  batch 80 loss: 1.9980054140090941\n",
      "  batch 90 loss: 1.9816592305898666\n",
      "  batch 100 loss: 2.0637487560510634\n",
      "  batch 110 loss: 1.9799601316452027\n",
      "  batch 120 loss: 2.026648086309433\n",
      "  batch 130 loss: 2.058938664197922\n",
      "  batch 140 loss: 2.057194709777832\n",
      "  batch 150 loss: 2.008857089281082\n",
      "  batch 160 loss: 2.081657674908638\n",
      "  batch 170 loss: 2.0564450711011886\n",
      "  batch 180 loss: 1.953857672214508\n",
      "  batch 190 loss: 2.040934145450592\n",
      "LOSS train 2.040934145450592 valid 2.0912431306563892\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 2.041571092605591\n",
      "  batch 20 loss: 2.0495011448860168\n",
      "  batch 30 loss: 1.9925438702106475\n",
      "  batch 40 loss: 2.018900528550148\n",
      "  batch 50 loss: 2.0349676311016083\n",
      "  batch 60 loss: 1.9941426217556\n",
      "  batch 70 loss: 2.06082583963871\n",
      "  batch 80 loss: 2.0690243363380434\n",
      "  batch 90 loss: 1.9495710134506226\n",
      "  batch 100 loss: 2.01906997859478\n",
      "  batch 110 loss: 2.0126653730869295\n",
      "  batch 120 loss: 2.028474062681198\n",
      "  batch 130 loss: 2.0688764274120333\n",
      "  batch 140 loss: 2.0780829221010206\n",
      "  batch 150 loss: 2.0518169820308687\n",
      "  batch 160 loss: 2.0183976858854296\n",
      "  batch 170 loss: 2.0055168896913527\n",
      "  batch 180 loss: 2.054067146778107\n",
      "  batch 190 loss: 2.0285405606031417\n",
      "LOSS train 2.0285405606031417 valid 2.089117949207624\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 2.0262366950511934\n",
      "  batch 20 loss: 2.00965995490551\n",
      "  batch 30 loss: 2.006430613994598\n",
      "  batch 40 loss: 2.0449505656957627\n",
      "  batch 50 loss: 1.937274956703186\n",
      "  batch 60 loss: 2.056920424103737\n",
      "  batch 70 loss: 1.9470727801322938\n",
      "  batch 80 loss: 2.0141997963190077\n",
      "  batch 90 loss: 2.0138545513153074\n",
      "  batch 100 loss: 2.0381346493959427\n",
      "  batch 110 loss: 2.0518424838781355\n",
      "  batch 120 loss: 2.1015345215797425\n",
      "  batch 130 loss: 1.9351067036390304\n",
      "  batch 140 loss: 2.0171674221754072\n",
      "  batch 150 loss: 1.9680339634418487\n",
      "  batch 160 loss: 2.049095693230629\n",
      "  batch 170 loss: 2.0787743270397185\n",
      "  batch 180 loss: 2.0609787553548813\n",
      "  batch 190 loss: 2.127828320860863\n",
      "LOSS train 2.127828320860863 valid 2.0860537722324715\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 2.0532568275928496\n",
      "  batch 20 loss: 2.1053357481956483\n",
      "  batch 30 loss: 1.998859977722168\n",
      "  batch 40 loss: 2.092683604359627\n",
      "  batch 50 loss: 2.007138568162918\n",
      "  batch 60 loss: 2.0736839354038237\n",
      "  batch 70 loss: 1.964781242609024\n",
      "  batch 80 loss: 2.0417898058891297\n",
      "  batch 90 loss: 2.004331183433533\n",
      "  batch 100 loss: 2.041986697912216\n",
      "  batch 110 loss: 2.0576942592859266\n",
      "  batch 120 loss: 1.9966511249542236\n",
      "  batch 130 loss: 1.9262282609939576\n",
      "  batch 140 loss: 2.0419096112251283\n",
      "  batch 150 loss: 2.038042959570885\n",
      "  batch 160 loss: 2.0115930825471877\n",
      "  batch 170 loss: 2.0199080109596252\n",
      "  batch 180 loss: 1.9424878418445588\n",
      "  batch 190 loss: 2.0619265377521514\n",
      "LOSS train 2.0619265377521514 valid 2.08481065202982\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 1.9278039276599883\n",
      "  batch 20 loss: 2.045647931098938\n",
      "  batch 30 loss: 1.987880527973175\n",
      "  batch 40 loss: 2.021162050962448\n",
      "  batch 50 loss: 2.0871065229177477\n",
      "  batch 60 loss: 1.983113592863083\n",
      "  batch 70 loss: 2.049241903424263\n",
      "  batch 80 loss: 1.9412558197975158\n",
      "  batch 90 loss: 2.048154520988464\n",
      "  batch 100 loss: 2.028320789337158\n",
      "  batch 110 loss: 2.021159437298775\n",
      "  batch 120 loss: 2.012648567557335\n",
      "  batch 130 loss: 2.0919778615236284\n",
      "  batch 140 loss: 2.0350490391254423\n",
      "  batch 150 loss: 1.983712837100029\n",
      "  batch 160 loss: 2.033775693178177\n",
      "  batch 170 loss: 2.0241999596357347\n",
      "  batch 180 loss: 2.048648974299431\n",
      "  batch 190 loss: 2.033636358380318\n",
      "LOSS train 2.033636358380318 valid 2.080597866804172\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 1.9713637948036193\n",
      "  batch 20 loss: 2.0430639773607253\n",
      "  batch 30 loss: 1.9928229570388794\n",
      "  batch 40 loss: 2.0476798862218857\n",
      "  batch 50 loss: 2.029318779706955\n",
      "  batch 60 loss: 2.0143875062465666\n",
      "  batch 70 loss: 2.0149673491716387\n",
      "  batch 80 loss: 2.034505221247673\n",
      "  batch 90 loss: 2.0379954397678377\n",
      "  batch 100 loss: 2.0910430908203126\n",
      "  batch 110 loss: 2.024698421359062\n",
      "  batch 120 loss: 2.032615229487419\n",
      "  batch 130 loss: 2.0000282734632493\n",
      "  batch 140 loss: 1.98566372692585\n",
      "  batch 150 loss: 2.0048240184783936\n",
      "  batch 160 loss: 1.996062409877777\n",
      "  batch 170 loss: 2.0369515240192415\n",
      "  batch 180 loss: 1.9639075934886931\n",
      "  batch 190 loss: 2.0352141469717027\n",
      "LOSS train 2.0352141469717027 valid 2.0768732806810966\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 2.0208191722631454\n",
      "  batch 20 loss: 1.9930233150720595\n",
      "  batch 30 loss: 2.0131731897592546\n",
      "  batch 40 loss: 2.0508974105119706\n",
      "  batch 50 loss: 1.9507549226284027\n",
      "  batch 60 loss: 2.022543618083\n",
      "  batch 70 loss: 2.088999927043915\n",
      "  batch 80 loss: 1.998655194044113\n",
      "  batch 90 loss: 2.0226412922143937\n",
      "  batch 100 loss: 2.000488370656967\n",
      "  batch 110 loss: 1.9849629163742066\n",
      "  batch 120 loss: 1.960820746421814\n",
      "  batch 130 loss: 1.9751757115125657\n",
      "  batch 140 loss: 2.0448869824409486\n",
      "  batch 150 loss: 1.9615551114082337\n",
      "  batch 160 loss: 2.0334647208452226\n",
      "  batch 170 loss: 2.031082698702812\n",
      "  batch 180 loss: 2.0750113427639008\n",
      "  batch 190 loss: 2.0040023654699324\n",
      "LOSS train 2.0040023654699324 valid 2.0693398098915052\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 2.0208757877349854\n",
      "  batch 20 loss: 1.9258638262748717\n",
      "  batch 30 loss: 2.0090806782245636\n",
      "  batch 40 loss: 2.0239793211221695\n",
      "  batch 50 loss: 2.0047216176986695\n",
      "  batch 60 loss: 2.0092144936323164\n",
      "  batch 70 loss: 2.0021876454353333\n",
      "  batch 80 loss: 2.014202407002449\n",
      "  batch 90 loss: 1.9427535206079483\n",
      "  batch 100 loss: 2.0753945767879487\n",
      "  batch 110 loss: 2.0356255054473875\n",
      "  batch 120 loss: 1.9890255600214004\n",
      "  batch 130 loss: 1.9232120633125305\n",
      "  batch 140 loss: 2.0236633867025375\n",
      "  batch 150 loss: 2.0395205318927765\n",
      "  batch 160 loss: 2.0479950428009035\n",
      "  batch 170 loss: 1.9457336485385894\n",
      "  batch 180 loss: 2.0629693418741226\n",
      "  batch 190 loss: 1.9826877564191818\n",
      "LOSS train 1.9826877564191818 valid 2.062019896048766\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 1.9350727081298829\n",
      "  batch 20 loss: 2.0159272730350493\n",
      "  batch 30 loss: 2.026674193143845\n",
      "  batch 40 loss: 2.0037274956703186\n",
      "  batch 50 loss: 2.0283320397138596\n",
      "  batch 60 loss: 2.0190699249505997\n",
      "  batch 70 loss: 1.9373681664466857\n",
      "  batch 80 loss: 2.001747289299965\n",
      "  batch 90 loss: 1.979071283340454\n",
      "  batch 100 loss: 1.9465815484523774\n",
      "  batch 110 loss: 2.0302171885967253\n",
      "  batch 120 loss: 2.0760140657424926\n",
      "  batch 130 loss: 1.941147431731224\n",
      "  batch 140 loss: 1.9834013819694518\n",
      "  batch 150 loss: 1.9979146599769593\n",
      "  batch 160 loss: 1.9835744559764863\n",
      "  batch 170 loss: 2.0043296694755552\n",
      "  batch 180 loss: 2.05789637863636\n",
      "  batch 190 loss: 1.9771379321813582\n",
      "LOSS train 1.9771379321813582 valid 2.053121191950945\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 1.9406768411397934\n",
      "  batch 20 loss: 2.0204565197229387\n",
      "  batch 30 loss: 1.9933033347129823\n",
      "  batch 40 loss: 1.8682956278324128\n",
      "  batch 50 loss: 1.9691268473863601\n",
      "  batch 60 loss: 2.025147059559822\n",
      "  batch 70 loss: 1.9369615316390991\n",
      "  batch 80 loss: 2.044671756029129\n",
      "  batch 90 loss: 2.0278012454509735\n",
      "  batch 100 loss: 1.9630091518163681\n",
      "  batch 110 loss: 2.0030582517385485\n",
      "  batch 120 loss: 1.9758403182029725\n",
      "  batch 130 loss: 1.9441824585199357\n",
      "  batch 140 loss: 1.949103382229805\n",
      "  batch 150 loss: 2.012084889411926\n",
      "  batch 160 loss: 2.031344249844551\n",
      "  batch 170 loss: 2.0071711212396623\n",
      "  batch 180 loss: 1.9972950845956803\n",
      "  batch 190 loss: 1.9974319636821747\n",
      "LOSS train 1.9974319636821747 valid 2.037997094102395\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 1.9261279940605163\n",
      "  batch 20 loss: 1.9936804682016374\n",
      "  batch 30 loss: 1.9431559443473816\n",
      "  batch 40 loss: 1.9375460594892502\n",
      "  batch 50 loss: 1.8917300134897232\n",
      "  batch 60 loss: 1.967099842429161\n",
      "  batch 70 loss: 2.0388045817613603\n",
      "  batch 80 loss: 1.981333601474762\n",
      "  batch 90 loss: 1.9048140615224838\n",
      "  batch 100 loss: 1.9875541597604751\n",
      "  batch 110 loss: 1.9900305241346359\n",
      "  batch 120 loss: 1.9573905259370803\n",
      "  batch 130 loss: 1.9834757775068284\n",
      "  batch 140 loss: 1.9473812103271484\n",
      "  batch 150 loss: 2.0363642066717147\n",
      "  batch 160 loss: 1.9277352750301362\n",
      "  batch 170 loss: 1.951791700720787\n",
      "  batch 180 loss: 2.0385471105575563\n",
      "  batch 190 loss: 1.9727883458137512\n",
      "LOSS train 1.9727883458137512 valid 2.020371547112098\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 1.945356845855713\n",
      "  batch 20 loss: 1.996815812587738\n",
      "  batch 30 loss: 2.012508529424667\n",
      "  batch 40 loss: 1.9070767879486084\n",
      "  batch 50 loss: 1.9459569245576858\n",
      "  batch 60 loss: 1.9983013331890107\n",
      "  batch 70 loss: 1.9597393810749053\n",
      "  batch 80 loss: 1.9003209322690964\n",
      "  batch 90 loss: 1.9659110814332963\n",
      "  batch 100 loss: 1.883943349123001\n",
      "  batch 110 loss: 1.9777261763811111\n",
      "  batch 120 loss: 1.9148861765861511\n",
      "  batch 130 loss: 1.935705941915512\n",
      "  batch 140 loss: 1.9261208355426789\n",
      "  batch 150 loss: 1.904543823003769\n",
      "  batch 160 loss: 1.9783378154039384\n",
      "  batch 170 loss: 1.9906128913164138\n",
      "  batch 180 loss: 2.009563961625099\n",
      "  batch 190 loss: 1.909098419547081\n",
      "LOSS train 1.909098419547081 valid 2.0029608660783524\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 1.8774467766284944\n",
      "  batch 20 loss: 2.022146090865135\n",
      "  batch 30 loss: 1.968787932395935\n",
      "  batch 40 loss: 1.9711885035037995\n",
      "  batch 50 loss: 2.0322808921337128\n",
      "  batch 60 loss: 1.87685084939003\n",
      "  batch 70 loss: 1.9535970777273177\n",
      "  batch 80 loss: 1.9805084109306335\n",
      "  batch 90 loss: 1.8652935355901719\n",
      "  batch 100 loss: 1.8994072735309602\n",
      "  batch 110 loss: 1.9292545527219773\n",
      "  batch 120 loss: 1.9530578523874282\n",
      "  batch 130 loss: 1.943972471356392\n",
      "  batch 140 loss: 1.8860362380743028\n",
      "  batch 150 loss: 1.8409469902515412\n",
      "  batch 160 loss: 1.9213538676500321\n",
      "  batch 170 loss: 1.824442058801651\n",
      "  batch 180 loss: 1.9155973225831986\n",
      "  batch 190 loss: 1.8230597078800201\n",
      "LOSS train 1.8230597078800201 valid 1.973331976013306\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 1.8087880730628967\n",
      "  batch 20 loss: 1.8934228450059891\n",
      "  batch 30 loss: 1.989491018652916\n",
      "  batch 40 loss: 1.9099782824516296\n",
      "  batch 50 loss: 1.803228986263275\n",
      "  batch 60 loss: 1.8757257401943206\n",
      "  batch 70 loss: 1.8835402965545653\n",
      "  batch 80 loss: 1.9571364313364028\n",
      "  batch 90 loss: 1.885296368598938\n",
      "  batch 100 loss: 1.944584196805954\n",
      "  batch 110 loss: 1.929994711279869\n",
      "  batch 120 loss: 1.9730230927467347\n",
      "  batch 130 loss: 1.9025014877319335\n",
      "  batch 140 loss: 1.8059561491012572\n",
      "  batch 150 loss: 1.9499372452497483\n",
      "  batch 160 loss: 1.8762095153331757\n",
      "  batch 170 loss: 1.7984858334064484\n",
      "  batch 180 loss: 1.860721802711487\n",
      "  batch 190 loss: 1.8403722047805786\n",
      "LOSS train 1.8403722047805786 valid 1.916825390396974\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 1.9586407959461212\n",
      "  batch 20 loss: 1.7836985617876053\n",
      "  batch 30 loss: 1.9804377377033233\n",
      "  batch 40 loss: 1.8851295411586761\n",
      "  batch 50 loss: 1.8537091165781021\n",
      "  batch 60 loss: 1.8429843187332153\n",
      "  batch 70 loss: 1.8216732740402222\n",
      "  batch 80 loss: 1.8372660875320435\n",
      "  batch 90 loss: 1.8615563213825226\n",
      "  batch 100 loss: 1.862776330113411\n",
      "  batch 110 loss: 1.9784476965665818\n",
      "  batch 120 loss: 1.846012794971466\n",
      "  batch 130 loss: 1.7697318851947785\n",
      "  batch 140 loss: 1.821922704577446\n",
      "  batch 150 loss: 1.8000708624720574\n",
      "  batch 160 loss: 1.7461927980184555\n",
      "  batch 170 loss: 1.7602873221039772\n",
      "  batch 180 loss: 1.799333307147026\n",
      "  batch 190 loss: 1.8206356644630433\n",
      "LOSS train 1.8206356644630433 valid 1.9804300035421665\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 1.9282989114522935\n",
      "  batch 20 loss: 1.8495916917920112\n",
      "  batch 30 loss: 1.9037630140781403\n",
      "  batch 40 loss: 1.7567609891295433\n",
      "  batch 50 loss: 1.6652645573019982\n",
      "  batch 60 loss: 1.930297464132309\n",
      "  batch 70 loss: 1.8564768239855767\n",
      "  batch 80 loss: 1.8289161995053291\n",
      "  batch 90 loss: 1.8298731431365014\n",
      "  batch 100 loss: 1.8585346624255181\n",
      "  batch 110 loss: 1.761582973599434\n",
      "  batch 120 loss: 1.6608562484383582\n",
      "  batch 130 loss: 1.842645674943924\n",
      "  batch 140 loss: 1.825074352324009\n",
      "  batch 150 loss: 1.7835193321108818\n",
      "  batch 160 loss: 1.781889696419239\n",
      "  batch 170 loss: 1.7540074929594993\n",
      "  batch 180 loss: 1.7848513036966325\n",
      "  batch 190 loss: 1.6442562147974968\n",
      "LOSS train 1.6442562147974968 valid 1.8939856248788345\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 1.8162205427885056\n",
      "  batch 20 loss: 1.8445037811994554\n",
      "  batch 30 loss: 1.705961635708809\n",
      "  batch 40 loss: 1.6827946141362191\n",
      "  batch 50 loss: 1.6535164922475816\n",
      "  batch 60 loss: 1.6323810897767543\n",
      "  batch 70 loss: 1.6891921631991864\n",
      "  batch 80 loss: 1.7470869608223438\n",
      "  batch 90 loss: 1.7437647141516208\n",
      "  batch 100 loss: 1.709260905534029\n",
      "  batch 110 loss: 1.843177655339241\n",
      "  batch 120 loss: 1.8061778038740157\n",
      "  batch 130 loss: 1.7291055232286454\n",
      "  batch 140 loss: 1.8083528257906436\n",
      "  batch 150 loss: 1.7600293822586537\n",
      "  batch 160 loss: 1.658112034201622\n",
      "  batch 170 loss: 1.8223018005490303\n",
      "  batch 180 loss: 1.8898965790867805\n",
      "  batch 190 loss: 1.6898631677031517\n",
      "LOSS train 1.6898631677031517 valid 1.8602774113607712\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 1.790634572505951\n",
      "  batch 20 loss: 1.640161007642746\n",
      "  batch 30 loss: 1.6331035226583481\n",
      "  batch 40 loss: 1.77406537681818\n",
      "  batch 50 loss: 1.5798834934830666\n",
      "  batch 60 loss: 1.7230897210538387\n",
      "  batch 70 loss: 1.5557649850845336\n",
      "  batch 80 loss: 1.6754324063658714\n",
      "  batch 90 loss: 1.8209776349365712\n",
      "  batch 100 loss: 1.6154685452580453\n",
      "  batch 110 loss: 1.6731901459395886\n",
      "  batch 120 loss: 1.6819731451570987\n",
      "  batch 130 loss: 1.7324843756854533\n",
      "  batch 140 loss: 1.615714753419161\n",
      "  batch 150 loss: 1.8226209700107574\n",
      "  batch 160 loss: 1.927232477068901\n",
      "  batch 170 loss: 1.6437331944704057\n",
      "  batch 180 loss: 1.8596358865499496\n",
      "  batch 190 loss: 1.8080819189548492\n",
      "LOSS train 1.8080819189548492 valid 1.7555851246684024\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 1.6815497063100338\n",
      "  batch 20 loss: 1.5764120746403933\n",
      "  batch 30 loss: 1.7142199583351612\n",
      "  batch 40 loss: 1.4959273040294647\n",
      "  batch 50 loss: 1.7159412123262883\n",
      "  batch 60 loss: 1.5653861798346043\n",
      "  batch 70 loss: 1.7625669181346892\n",
      "  batch 80 loss: 1.7457564070820808\n",
      "  batch 90 loss: 1.7290026806294918\n",
      "  batch 100 loss: 1.6285091117024422\n",
      "  batch 110 loss: 1.5623390987515449\n",
      "  batch 120 loss: 1.7134300351142884\n",
      "  batch 130 loss: 1.7038726329803466\n",
      "  batch 140 loss: 1.7560447797179222\n",
      "  batch 150 loss: 1.8897313863039016\n",
      "  batch 160 loss: 1.6996768146753312\n",
      "  batch 170 loss: 1.7554006211459636\n",
      "  batch 180 loss: 1.6724501952528954\n",
      "  batch 190 loss: 1.528321161493659\n",
      "LOSS train 1.528321161493659 valid 1.7078408108880887\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 1.734255611896515\n",
      "  batch 20 loss: 1.6426901519298553\n",
      "  batch 30 loss: 1.7933092530816794\n",
      "  batch 40 loss: 1.7170650273561479\n",
      "  batch 50 loss: 1.746184526383877\n",
      "  batch 60 loss: 1.5836015928536653\n",
      "  batch 70 loss: 1.7093274772167206\n",
      "  batch 80 loss: 1.7944500356912614\n",
      "  batch 90 loss: 1.6326360419392585\n",
      "  batch 100 loss: 1.6399028323590756\n",
      "  batch 110 loss: 1.5418684855103493\n",
      "  batch 120 loss: 1.561174639314413\n",
      "  batch 130 loss: 1.5936618581414224\n",
      "  batch 140 loss: 1.5303648106753827\n",
      "  batch 150 loss: 1.6412700224667787\n",
      "  batch 160 loss: 1.7341253086924553\n",
      "  batch 170 loss: 1.4707492567598819\n",
      "  batch 180 loss: 1.6459507502615451\n",
      "  batch 190 loss: 1.6333454176783562\n",
      "LOSS train 1.6333454176783562 valid 1.6258812533834806\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 1.7517957050353288\n",
      "  batch 20 loss: 1.5698605477809906\n",
      "  batch 30 loss: 1.6381903804838658\n",
      "  batch 40 loss: 1.4942024145275354\n",
      "  batch 50 loss: 1.5337119661271572\n",
      "  batch 60 loss: 1.444774277880788\n",
      "  batch 70 loss: 1.504741270467639\n",
      "  batch 80 loss: 1.830110566318035\n",
      "  batch 90 loss: 1.782146892696619\n",
      "  batch 100 loss: 1.5816823616623878\n",
      "  batch 110 loss: 1.6699653036892415\n",
      "  batch 120 loss: 1.6244997747242451\n",
      "  batch 130 loss: 1.756367814540863\n",
      "  batch 140 loss: 1.5632385089993477\n",
      "  batch 150 loss: 1.862752490490675\n",
      "  batch 160 loss: 1.6567392885684966\n",
      "  batch 170 loss: 1.491506739333272\n",
      "  batch 180 loss: 1.8170203581452369\n",
      "  batch 190 loss: 1.509208308532834\n",
      "LOSS train 1.509208308532834 valid 1.5826628333769548\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 1.5766120094805955\n",
      "  batch 20 loss: 1.7778306733816862\n",
      "  batch 30 loss: 1.5692332677543164\n",
      "  batch 40 loss: 1.6797637458890677\n",
      "  batch 50 loss: 1.8652756303548812\n",
      "  batch 60 loss: 1.5552990633994341\n",
      "  batch 70 loss: 1.4379792228341102\n",
      "  batch 80 loss: 1.7365289997309445\n",
      "  batch 90 loss: 1.5821401726454496\n",
      "  batch 100 loss: 1.6614332038909196\n",
      "  batch 110 loss: 1.6441557310521602\n",
      "  batch 120 loss: 1.6307850640267134\n",
      "  batch 130 loss: 1.5828190874308348\n",
      "  batch 140 loss: 1.7332402482628821\n",
      "  batch 150 loss: 1.5531449217349291\n",
      "  batch 160 loss: 1.432417892664671\n",
      "  batch 170 loss: 1.8037386290729045\n",
      "  batch 180 loss: 1.643304399214685\n",
      "  batch 190 loss: 1.4031232887879015\n",
      "LOSS train 1.4031232887879015 valid 1.7441465731901236\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 1.6149497777223587\n",
      "  batch 20 loss: 1.6475974831730127\n",
      "  batch 30 loss: 1.6614401116967201\n",
      "  batch 40 loss: 1.6017408020794393\n",
      "  batch 50 loss: 1.5718888755887748\n",
      "  batch 60 loss: 1.755926315486431\n",
      "  batch 70 loss: 1.487549871020019\n",
      "  batch 80 loss: 1.5524184964597225\n",
      "  batch 90 loss: 1.770316531509161\n",
      "  batch 100 loss: 1.5919207569211722\n",
      "  batch 110 loss: 1.8195867486298085\n",
      "  batch 120 loss: 1.6868154138326645\n",
      "  batch 130 loss: 1.4081576896831394\n",
      "  batch 140 loss: 1.5623219821602106\n",
      "  batch 150 loss: 1.504238760843873\n",
      "  batch 160 loss: 1.5829746065661312\n",
      "  batch 170 loss: 1.698268798366189\n",
      "  batch 180 loss: 1.6539014641195535\n",
      "  batch 190 loss: 1.7101776894181966\n",
      "LOSS train 1.7101776894181966 valid 1.5924923514995055\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 1.4406286573037506\n",
      "  batch 20 loss: 1.6147754291072487\n",
      "  batch 30 loss: 1.459267583861947\n",
      "  batch 40 loss: 1.7938550911843776\n",
      "  batch 50 loss: 1.6327925760298967\n",
      "  batch 60 loss: 1.6449600744992494\n",
      "  batch 70 loss: 1.7198409281671048\n",
      "  batch 80 loss: 1.5856178725138306\n",
      "  batch 90 loss: 1.7571721624583005\n",
      "  batch 100 loss: 1.5685364313423633\n",
      "  batch 110 loss: 1.491155810840428\n",
      "  batch 120 loss: 1.5869270939379931\n",
      "  batch 130 loss: 1.8625149831175805\n",
      "  batch 140 loss: 1.6844047484919429\n",
      "  batch 150 loss: 1.6758323457092046\n",
      "  batch 160 loss: 1.5711760066449643\n",
      "  batch 170 loss: 1.6490979680791498\n",
      "  batch 180 loss: 1.6116530641913414\n",
      "  batch 190 loss: 1.3191620472818613\n",
      "LOSS train 1.3191620472818613 valid 1.5838278526058183\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 1.6269317178055644\n",
      "  batch 20 loss: 1.3360907135531306\n",
      "  batch 30 loss: 1.565070629119873\n",
      "  batch 40 loss: 1.4940454103052616\n",
      "  batch 50 loss: 1.7994746923446656\n",
      "  batch 60 loss: 1.6190698672086001\n",
      "  batch 70 loss: 1.6672114960849285\n",
      "  batch 80 loss: 1.6628310535103084\n",
      "  batch 90 loss: 1.821553422510624\n",
      "  batch 100 loss: 1.4000120108947158\n",
      "  batch 110 loss: 1.5621127974241973\n",
      "  batch 120 loss: 1.4984608966857196\n",
      "  batch 130 loss: 1.3324141010642052\n",
      "  batch 140 loss: 1.4952686050906778\n",
      "  batch 150 loss: 1.6601486578583717\n",
      "  batch 160 loss: 1.811935101635754\n",
      "  batch 170 loss: 1.5735036123543977\n",
      "  batch 180 loss: 1.4832290437072515\n",
      "  batch 190 loss: 1.6954075884073974\n",
      "LOSS train 1.6954075884073974 valid 2.7397288945622935\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 1.4875588620081543\n",
      "  batch 20 loss: 1.472984717786312\n",
      "  batch 30 loss: 1.5868662383407355\n",
      "  batch 40 loss: 1.5584883652627468\n",
      "  batch 50 loss: 1.3528954612091184\n",
      "  batch 60 loss: 1.6351047335192561\n",
      "  batch 70 loss: 1.6319426352158188\n",
      "  batch 80 loss: 1.7491180565208198\n",
      "  batch 90 loss: 1.4854890590533614\n",
      "  batch 100 loss: 1.604961183294654\n",
      "  batch 110 loss: 1.600398125499487\n",
      "  batch 120 loss: 1.4376278284937143\n",
      "  batch 130 loss: 1.726124919950962\n",
      "  batch 140 loss: 1.5412641081959009\n",
      "  batch 150 loss: 1.5049874242395163\n",
      "  batch 160 loss: 1.6320644427090882\n",
      "  batch 170 loss: 1.7324832858517767\n",
      "  batch 180 loss: 1.7094812929630279\n",
      "  batch 190 loss: 1.5390031097456813\n",
      "LOSS train 1.5390031097456813 valid 1.5235557251203902\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 1.671896481886506\n",
      "  batch 20 loss: 1.5848457338288426\n",
      "  batch 30 loss: 1.646216407418251\n",
      "  batch 40 loss: 1.3636401392519475\n",
      "  batch 50 loss: 1.5752284562215209\n",
      "  batch 60 loss: 1.9013274114578962\n",
      "  batch 70 loss: 1.6588451396673918\n",
      "  batch 80 loss: 1.6811461992561818\n",
      "  batch 90 loss: 1.3852104235440492\n",
      "  batch 100 loss: 1.4268079483881593\n",
      "  batch 110 loss: 1.5373162902891635\n",
      "  batch 120 loss: 1.7043633956462145\n",
      "  batch 130 loss: 1.5598658557981253\n",
      "  batch 140 loss: 1.478295081295073\n",
      "  batch 150 loss: 1.4427424803376199\n",
      "  batch 160 loss: 1.4845861423760653\n",
      "  batch 170 loss: 1.489872974716127\n",
      "  batch 180 loss: 1.6142636915668844\n",
      "  batch 190 loss: 1.7616230603307486\n",
      "LOSS train 1.7616230603307486 valid 1.5286097272943993\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 1.5491803113371134\n",
      "  batch 20 loss: 1.4168198816478252\n",
      "  batch 30 loss: 1.6684026207774878\n",
      "  batch 40 loss: 1.684923733957112\n",
      "  batch 50 loss: 1.401322165504098\n",
      "  batch 60 loss: 1.5708047334104775\n",
      "  batch 70 loss: 1.3492946311831475\n",
      "  batch 80 loss: 1.8466799266636371\n",
      "  batch 90 loss: 1.577606333978474\n",
      "  batch 100 loss: 1.3413121536374093\n",
      "  batch 110 loss: 1.7505208399146794\n",
      "  batch 120 loss: 1.484650394320488\n",
      "  batch 130 loss: 1.3808819212019443\n",
      "  batch 140 loss: 1.5763285428285598\n",
      "  batch 150 loss: 1.6598600352182984\n",
      "  batch 160 loss: 1.6892293062061072\n",
      "  batch 170 loss: 1.46253324393183\n",
      "  batch 180 loss: 1.5707970069721342\n",
      "  batch 190 loss: 1.7756542201153933\n",
      "LOSS train 1.7756542201153933 valid 1.5748693044416797\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 1.3222824715077877\n",
      "  batch 20 loss: 1.7134754115715622\n",
      "  batch 30 loss: 1.8202670695260168\n",
      "  batch 40 loss: 1.67824739664793\n",
      "  batch 50 loss: 1.5032771054655314\n",
      "  batch 60 loss: 1.545783467963338\n",
      "  batch 70 loss: 1.4358270702883602\n",
      "  batch 80 loss: 1.7331572413444518\n",
      "  batch 90 loss: 1.5248060738667846\n",
      "  batch 100 loss: 1.591634969599545\n",
      "  batch 110 loss: 1.5512743297964335\n",
      "  batch 120 loss: 1.669386926945299\n",
      "  batch 130 loss: 1.4461192049086093\n",
      "  batch 140 loss: 1.543025529012084\n",
      "  batch 150 loss: 1.4446288738399744\n",
      "  batch 160 loss: 1.1969200696796178\n",
      "  batch 170 loss: 1.546105859428644\n",
      "  batch 180 loss: 1.4052124049514532\n",
      "  batch 190 loss: 1.5655665386468172\n",
      "LOSS train 1.5655665386468172 valid 1.6551751193518822\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 1.6768262416124344\n",
      "  batch 20 loss: 1.4315929852426053\n",
      "  batch 30 loss: 1.300727099739015\n",
      "  batch 40 loss: 1.574739534035325\n",
      "  batch 50 loss: 1.6810168681666255\n",
      "  batch 60 loss: 1.4560989543795586\n",
      "  batch 70 loss: 1.5644810240715743\n",
      "  batch 80 loss: 1.5638400435447692\n",
      "  batch 90 loss: 1.7552542600780725\n",
      "  batch 100 loss: 1.9148673847317697\n",
      "  batch 110 loss: 1.3927065469324589\n",
      "  batch 120 loss: 1.3158116033300757\n",
      "  batch 130 loss: 1.4327401455491782\n",
      "  batch 140 loss: 1.7007142540067435\n",
      "  batch 150 loss: 1.4309053326956929\n",
      "  batch 160 loss: 1.5800293611362577\n",
      "  batch 170 loss: 1.5635604148730635\n",
      "  batch 180 loss: 1.5612120294943452\n",
      "  batch 190 loss: 1.4964556612074376\n",
      "LOSS train 1.4964556612074376 valid 1.530399971283399\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 1.446661769412458\n",
      "  batch 20 loss: 1.4257929854094982\n",
      "  batch 30 loss: 1.3070080557838082\n",
      "  batch 40 loss: 1.3065240934491158\n",
      "  batch 50 loss: 1.3699228143319488\n",
      "  batch 60 loss: 1.781219115294516\n",
      "  batch 70 loss: 1.2060030994936823\n",
      "  batch 80 loss: 1.5960658136755228\n",
      "  batch 90 loss: 1.5844141369685532\n",
      "  batch 100 loss: 1.6681289145722986\n",
      "  batch 110 loss: 1.4798159012570977\n",
      "  batch 120 loss: 1.6825994610786439\n",
      "  batch 130 loss: 1.5122744377702475\n",
      "  batch 140 loss: 1.5822912940755487\n",
      "  batch 150 loss: 1.6722890883684158\n",
      "  batch 160 loss: 1.677256963402033\n",
      "  batch 170 loss: 1.7273724041879177\n",
      "  batch 180 loss: 1.4403090020641685\n",
      "  batch 190 loss: 1.3862993221729993\n",
      "LOSS train 1.3862993221729993 valid 1.6268436115903733\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 1.566415842808783\n",
      "  batch 20 loss: 1.4351985171437263\n",
      "  batch 30 loss: 1.6944227825850249\n",
      "  batch 40 loss: 1.6316254746168852\n",
      "  batch 50 loss: 1.4095664709806441\n",
      "  batch 60 loss: 1.6022649995982647\n",
      "  batch 70 loss: 1.6393867081031204\n",
      "  batch 80 loss: 1.5042423244565726\n",
      "  batch 90 loss: 1.4925131192430854\n",
      "  batch 100 loss: 1.5864684091880918\n",
      "  batch 110 loss: 1.525928182527423\n",
      "  batch 120 loss: 1.478878490626812\n",
      "  batch 130 loss: 1.5390469731763006\n",
      "  batch 140 loss: 1.4766029249876738\n",
      "  batch 150 loss: 1.3256481731310488\n",
      "  batch 160 loss: 1.5585296934470534\n",
      "  batch 170 loss: 1.3918913204222918\n",
      "  batch 180 loss: 1.71166484169662\n",
      "  batch 190 loss: 1.276264508999884\n",
      "LOSS train 1.276264508999884 valid 1.5268344623394883\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 1.5160831409506499\n",
      "  batch 20 loss: 1.6765786580741406\n",
      "  batch 30 loss: 1.6127588618546724\n",
      "  batch 40 loss: 1.2552306571975351\n",
      "  batch 50 loss: 1.4541386794298887\n",
      "  batch 60 loss: 1.5347660511732102\n",
      "  batch 70 loss: 1.6587872564792634\n",
      "  batch 80 loss: 1.6367603324353694\n",
      "  batch 90 loss: 1.4557456394657493\n",
      "  batch 100 loss: 1.6043967789039015\n",
      "  batch 110 loss: 1.3871780835092067\n",
      "  batch 120 loss: 1.608920030388981\n",
      "  batch 130 loss: 1.3898887161165476\n",
      "  batch 140 loss: 1.45128145320341\n",
      "  batch 150 loss: 1.520718575734645\n",
      "  batch 160 loss: 1.6003230655565859\n",
      "  batch 170 loss: 1.496777083352208\n",
      "  batch 180 loss: 1.484189480356872\n",
      "  batch 190 loss: 1.7135360725224018\n",
      "LOSS train 1.7135360725224018 valid 1.811065760279934\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 1.3851598013192414\n",
      "  batch 20 loss: 1.6731950853019952\n",
      "  batch 30 loss: 1.6793882098048925\n",
      "  batch 40 loss: 1.609831805061549\n",
      "  batch 50 loss: 1.5046731293201447\n",
      "  batch 60 loss: 1.459941455721855\n",
      "  batch 70 loss: 1.6746793555095791\n",
      "  batch 80 loss: 1.5250295747071505\n",
      "  batch 90 loss: 1.5067879244685174\n",
      "  batch 100 loss: 1.531149611622095\n",
      "  batch 110 loss: 1.4811329562216997\n",
      "  batch 120 loss: 1.5600019374862313\n",
      "  batch 130 loss: 1.3606465484946966\n",
      "  batch 140 loss: 1.4434426574036479\n",
      "  batch 150 loss: 1.5676898512989283\n",
      "  batch 160 loss: 1.4914783837273717\n",
      "  batch 170 loss: 1.4773207126185297\n",
      "  batch 180 loss: 1.433601006306708\n",
      "  batch 190 loss: 1.4330708649009467\n",
      "LOSS train 1.4330708649009467 valid 1.4878003602632537\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 1.498043963406235\n",
      "  batch 20 loss: 1.467972253356129\n",
      "  batch 30 loss: 1.5439984071999788\n",
      "  batch 40 loss: 1.5717953814193606\n",
      "  batch 50 loss: 1.5181175161153078\n",
      "  batch 60 loss: 1.4824531853199006\n",
      "  batch 70 loss: 1.5327736482024192\n",
      "  batch 80 loss: 1.550924956984818\n",
      "  batch 90 loss: 1.6443994690664112\n",
      "  batch 100 loss: 1.32638445738703\n",
      "  batch 110 loss: 1.6225748375058173\n",
      "  batch 120 loss: 1.3194457424804569\n",
      "  batch 130 loss: 1.4878184255212545\n",
      "  batch 140 loss: 1.2624704313464463\n",
      "  batch 150 loss: 1.5057876190170645\n",
      "  batch 160 loss: 1.2933965842239559\n",
      "  batch 170 loss: 1.6916377676650882\n",
      "  batch 180 loss: 1.4603256035596133\n",
      "  batch 190 loss: 1.4815822418779134\n",
      "LOSS train 1.4815822418779134 valid 1.4228740789664862\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 1.3523678148165346\n",
      "  batch 20 loss: 1.484889882337302\n",
      "  batch 30 loss: 1.4275187699124217\n",
      "  batch 40 loss: 1.5219308521598578\n",
      "  batch 50 loss: 1.4752060927450656\n",
      "  batch 60 loss: 1.0752866927534341\n",
      "  batch 70 loss: 1.539338632300496\n",
      "  batch 80 loss: 1.4963275267742575\n",
      "  batch 90 loss: 1.4867713974788785\n",
      "  batch 100 loss: 1.4021359534934164\n",
      "  batch 110 loss: 1.6183411210775376\n",
      "  batch 120 loss: 1.7463962373323738\n",
      "  batch 130 loss: 1.6648351583629846\n",
      "  batch 140 loss: 1.5757503101602197\n",
      "  batch 150 loss: 1.5076354943215846\n",
      "  batch 160 loss: 1.5484012996777892\n",
      "  batch 170 loss: 1.4394930573180318\n",
      "  batch 180 loss: 1.6426045209169389\n",
      "  batch 190 loss: 1.5805116876959802\n",
      "LOSS train 1.5805116876959802 valid 1.4205304905294607\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 1.507915596663952\n",
      "  batch 20 loss: 1.5960842322558164\n",
      "  batch 30 loss: 1.368580454401672\n",
      "  batch 40 loss: 1.5887841977179051\n",
      "  batch 50 loss: 1.5560287931933998\n",
      "  batch 60 loss: 1.48450836725533\n",
      "  batch 70 loss: 1.5396871376782655\n",
      "  batch 80 loss: 1.3745943043380975\n",
      "  batch 90 loss: 1.5122766744345426\n",
      "  batch 100 loss: 1.7817481961101294\n",
      "  batch 110 loss: 1.3077303119003774\n",
      "  batch 120 loss: 1.462031148187816\n",
      "  batch 130 loss: 1.4575710706412792\n",
      "  batch 140 loss: 1.4699734641239046\n",
      "  batch 150 loss: 1.4996981400996447\n",
      "  batch 160 loss: 1.5266104942187666\n",
      "  batch 170 loss: 1.4258030936121942\n",
      "  batch 180 loss: 1.5878080859780312\n",
      "  batch 190 loss: 1.4825705802068114\n",
      "LOSS train 1.4825705802068114 valid 1.4516189261459005\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 1.4618233164772392\n",
      "  batch 20 loss: 1.6068255580961703\n",
      "  batch 30 loss: 1.390457521751523\n",
      "  batch 40 loss: 1.3716183457523585\n",
      "  batch 50 loss: 1.3611068928614258\n",
      "  batch 60 loss: 1.5641208891756833\n",
      "  batch 70 loss: 1.5501161631196738\n",
      "  batch 80 loss: 1.740027168393135\n",
      "  batch 90 loss: 1.3672799544408918\n",
      "  batch 100 loss: 1.5987350650131702\n",
      "  batch 110 loss: 1.3809498630464077\n",
      "  batch 120 loss: 1.4800807185471059\n",
      "  batch 130 loss: 1.5829605607315897\n",
      "  batch 140 loss: 1.496083864197135\n",
      "  batch 150 loss: 1.4272006630897522\n",
      "  batch 160 loss: 1.4075031824409963\n",
      "  batch 170 loss: 1.4505152322351933\n",
      "  batch 180 loss: 1.412687476351857\n",
      "  batch 190 loss: 1.553478090465069\n",
      "LOSS train 1.553478090465069 valid 1.4088867048804576\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 1.50498191639781\n",
      "  batch 20 loss: 1.2691886138170958\n",
      "  batch 30 loss: 1.6808788578957319\n",
      "  batch 40 loss: 1.5729042841121554\n",
      "  batch 50 loss: 1.6434741767123342\n",
      "  batch 60 loss: 1.526665537431836\n",
      "  batch 70 loss: 1.4839996203780175\n",
      "  batch 80 loss: 1.3120442539453507\n",
      "  batch 90 loss: 1.4298885339871048\n",
      "  batch 100 loss: 1.4969138659536838\n",
      "  batch 110 loss: 1.3656402306631208\n",
      "  batch 120 loss: 1.3355115368962287\n",
      "  batch 130 loss: 1.525118362251669\n",
      "  batch 140 loss: 1.4208249285817147\n",
      "  batch 150 loss: 1.6481159700080752\n",
      "  batch 160 loss: 1.3495415242388844\n",
      "  batch 170 loss: 1.4082805268466472\n",
      "  batch 180 loss: 1.4380388887599111\n",
      "  batch 190 loss: 1.5902882503345608\n",
      "LOSS train 1.5902882503345608 valid 1.505650031260955\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 1.5308730494230987\n",
      "  batch 20 loss: 1.5242853114381432\n",
      "  batch 30 loss: 1.5239822715520859\n",
      "  batch 40 loss: 1.613731324672699\n",
      "  batch 50 loss: 1.3896538704633712\n",
      "  batch 60 loss: 1.4087876131758095\n",
      "  batch 70 loss: 1.440393315628171\n",
      "  batch 80 loss: 1.6139071945101022\n",
      "  batch 90 loss: 1.4818204527720809\n",
      "  batch 100 loss: 1.5251219004392624\n",
      "  batch 110 loss: 1.3739083614200354\n",
      "  batch 120 loss: 1.4416935618966817\n",
      "  batch 130 loss: 1.2985943246632814\n",
      "  batch 140 loss: 1.3841127038002015\n",
      "  batch 150 loss: 1.3234054997563363\n",
      "  batch 160 loss: 1.521493798494339\n",
      "  batch 170 loss: 1.5817465711385013\n",
      "  batch 180 loss: 1.300595360994339\n",
      "  batch 190 loss: 1.3741923348978162\n",
      "LOSS train 1.3741923348978162 valid 1.7125769190203686\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 1.3063870884478093\n",
      "  batch 20 loss: 1.4134111061692238\n",
      "  batch 30 loss: 1.650565517321229\n",
      "  batch 40 loss: 1.3485578838735819\n",
      "  batch 50 loss: 1.590408580377698\n",
      "  batch 60 loss: 1.4456420090049504\n",
      "  batch 70 loss: 1.5388704793527723\n",
      "  batch 80 loss: 1.5049334404524415\n",
      "  batch 90 loss: 1.5650236193090676\n",
      "  batch 100 loss: 1.467744757886976\n",
      "  batch 110 loss: 1.354968431405723\n",
      "  batch 120 loss: 1.3662743780761957\n",
      "  batch 130 loss: 1.5688603861257433\n",
      "  batch 140 loss: 1.5529347199946641\n",
      "  batch 150 loss: 1.6625313196331262\n",
      "  batch 160 loss: 1.3375846365466715\n",
      "  batch 170 loss: 1.377072223648429\n",
      "  batch 180 loss: 1.2541626032441855\n",
      "  batch 190 loss: 1.354867907613516\n",
      "LOSS train 1.354867907613516 valid 1.388251547056895\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 1.3917401660233737\n",
      "  batch 20 loss: 1.3356611905619502\n",
      "  batch 30 loss: 1.5050762064754963\n",
      "  batch 40 loss: 1.2865339225158094\n",
      "  batch 50 loss: 1.345898761972785\n",
      "  batch 60 loss: 1.5217631978914141\n",
      "  batch 70 loss: 1.623873732239008\n",
      "  batch 80 loss: 1.3774386374279857\n",
      "  batch 90 loss: 1.4667740412056447\n",
      "  batch 100 loss: 1.4239943630993366\n",
      "  batch 110 loss: 1.45574432015419\n",
      "  batch 120 loss: 1.3757045514881612\n",
      "  batch 130 loss: 1.5353328265249728\n",
      "  batch 140 loss: 1.5219269149005412\n",
      "  batch 150 loss: 1.5767344065010547\n",
      "  batch 160 loss: 1.4787210579961538\n",
      "  batch 170 loss: 1.66374687589705\n",
      "  batch 180 loss: 1.487895642966032\n",
      "  batch 190 loss: 1.3815758980810642\n",
      "LOSS train 1.3815758980810642 valid 1.551473932603422\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 1.2219156718812882\n",
      "  batch 20 loss: 1.5497440825216473\n",
      "  batch 30 loss: 1.519288720190525\n",
      "  batch 40 loss: 1.467778892070055\n",
      "  batch 50 loss: 1.4020538534969091\n",
      "  batch 60 loss: 1.5756208915263414\n",
      "  batch 70 loss: 1.434241621568799\n",
      "  batch 80 loss: 1.5287324810400604\n",
      "  batch 90 loss: 1.5034987784922123\n",
      "  batch 100 loss: 1.4058101523667574\n",
      "  batch 110 loss: 1.4818155087530613\n",
      "  batch 120 loss: 1.343681326881051\n",
      "  batch 130 loss: 1.3561536215245724\n",
      "  batch 140 loss: 1.4515210218727588\n",
      "  batch 150 loss: 1.3360461922362448\n",
      "  batch 160 loss: 1.549999973922968\n",
      "  batch 170 loss: 1.4390705993399024\n",
      "  batch 180 loss: 1.3496820382773875\n",
      "  batch 190 loss: 1.3801279133185744\n",
      "LOSS train 1.3801279133185744 valid 1.872377164208163\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 1.5481940885074437\n",
      "  batch 20 loss: 1.5349283089861274\n",
      "  batch 30 loss: 1.2945835383608937\n",
      "  batch 40 loss: 1.8349616043269634\n",
      "  batch 50 loss: 1.5794530795887112\n",
      "  batch 60 loss: 1.4138560988008977\n",
      "  batch 70 loss: 1.4514985103160143\n",
      "  batch 80 loss: 1.409595713764429\n",
      "  batch 90 loss: 1.2627685714513064\n",
      "  batch 100 loss: 1.451722027733922\n",
      "  batch 110 loss: 1.3184121325612068\n",
      "  batch 120 loss: 1.4805168451741337\n",
      "  batch 130 loss: 1.3910875312052666\n",
      "  batch 140 loss: 1.449662577174604\n",
      "  batch 150 loss: 1.3177542202174664\n",
      "  batch 160 loss: 1.5368692770600318\n",
      "  batch 170 loss: 1.4381503604352475\n",
      "  batch 180 loss: 1.289650215022266\n",
      "  batch 190 loss: 1.387833141721785\n",
      "LOSS train 1.387833141721785 valid 1.437209703386403\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 1.383484805561602\n",
      "  batch 20 loss: 1.3325520001351834\n",
      "  batch 30 loss: 1.4527538057416678\n",
      "  batch 40 loss: 1.4650646355003119\n",
      "  batch 50 loss: 1.4217989904806019\n",
      "  batch 60 loss: 1.4695121973752976\n",
      "  batch 70 loss: 1.3983821986243128\n",
      "  batch 80 loss: 1.3189710648730397\n",
      "  batch 90 loss: 1.3881515546701848\n",
      "  batch 100 loss: 1.4718390895053743\n",
      "  batch 110 loss: 1.5020446360111237\n",
      "  batch 120 loss: 1.5236285146325828\n",
      "  batch 130 loss: 1.4625496095046402\n",
      "  batch 140 loss: 1.3821138857863844\n",
      "  batch 150 loss: 1.3294059747830034\n",
      "  batch 160 loss: 1.6035928699187934\n",
      "  batch 170 loss: 1.4999033328145743\n",
      "  batch 180 loss: 1.3305358720943332\n",
      "  batch 190 loss: 1.4601849637925626\n",
      "LOSS train 1.4601849637925626 valid 1.4806533735723069\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 1.6631381025537848\n",
      "  batch 20 loss: 1.398558445647359\n",
      "  batch 30 loss: 1.335089250281453\n",
      "  batch 40 loss: 1.4925010591745376\n",
      "  batch 50 loss: 1.2840742740780116\n",
      "  batch 60 loss: 1.3735068196430802\n",
      "  batch 70 loss: 1.4358924120664596\n",
      "  batch 80 loss: 1.1437962193973363\n",
      "  batch 90 loss: 1.461736287921667\n",
      "  batch 100 loss: 1.4875733755528926\n",
      "  batch 110 loss: 1.5731276515871286\n",
      "  batch 120 loss: 1.4909395814873279\n",
      "  batch 130 loss: 1.5421579034999013\n",
      "  batch 140 loss: 1.5543032892979682\n",
      "  batch 150 loss: 1.5611535809934138\n",
      "  batch 160 loss: 1.1541651705279947\n",
      "  batch 170 loss: 1.4048981280997395\n",
      "  batch 180 loss: 1.4123524533584715\n",
      "  batch 190 loss: 1.3354287380352616\n",
      "LOSS train 1.3354287380352616 valid 1.865088771503323\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 1.197903211787343\n",
      "  batch 20 loss: 1.5209406558424234\n",
      "  batch 30 loss: 1.423110943287611\n",
      "  batch 40 loss: 1.3586447345092894\n",
      "  batch 50 loss: 1.2990846436470747\n",
      "  batch 60 loss: 1.4882767394185066\n",
      "  batch 70 loss: 1.6420635234564542\n",
      "  batch 80 loss: 1.468246066570282\n",
      "  batch 90 loss: 1.3972305057570338\n",
      "  batch 100 loss: 1.8010689647868277\n",
      "  batch 110 loss: 1.8453631944954396\n",
      "  batch 120 loss: 1.5330356732010841\n",
      "  batch 130 loss: 1.4087830469012261\n",
      "  batch 140 loss: 1.5138124041259289\n",
      "  batch 150 loss: 1.3256924394518137\n",
      "  batch 160 loss: 1.3651379719376564\n",
      "  batch 170 loss: 1.3332161413505674\n",
      "  batch 180 loss: 1.2821362728253007\n",
      "  batch 190 loss: 1.337903369963169\n",
      "LOSS train 1.337903369963169 valid 2.2351812939517774\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 1.3199367105960846\n",
      "  batch 20 loss: 1.1941234638914466\n",
      "  batch 30 loss: 1.3874548844993115\n",
      "  batch 40 loss: 1.4462263081222773\n",
      "  batch 50 loss: 1.3468144885264337\n",
      "  batch 60 loss: 1.1255125675350428\n",
      "  batch 70 loss: 1.3363046031445265\n",
      "  batch 80 loss: 1.2735375981777906\n",
      "  batch 90 loss: 1.405247564893216\n",
      "  batch 100 loss: 1.6011510286480188\n",
      "  batch 110 loss: 1.4347011182457208\n",
      "  batch 120 loss: 1.4943906255066395\n",
      "  batch 130 loss: 1.628809006512165\n",
      "  batch 140 loss: 1.4547413644380867\n",
      "  batch 150 loss: 1.5604658567346632\n",
      "  batch 160 loss: 1.4566436923108994\n",
      "  batch 170 loss: 1.5706562338396908\n",
      "  batch 180 loss: 1.506977160461247\n",
      "  batch 190 loss: 1.2878930711187422\n",
      "LOSS train 1.2878930711187422 valid 2.7211457565426826\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 1.6688985880464315\n",
      "  batch 20 loss: 1.4117270939052105\n",
      "  batch 30 loss: 1.4388962727040053\n",
      "  batch 40 loss: 1.281744484230876\n",
      "  batch 50 loss: 1.162530755624175\n",
      "  batch 60 loss: 1.4849045278504491\n",
      "  batch 70 loss: 1.460131373628974\n",
      "  batch 80 loss: 1.4665768982842564\n",
      "  batch 90 loss: 1.373719966597855\n",
      "  batch 100 loss: 1.3579052019864322\n",
      "  batch 110 loss: 1.1384867100045084\n",
      "  batch 120 loss: 1.7710096849128605\n",
      "  batch 130 loss: 1.397690854407847\n",
      "  batch 140 loss: 1.4810359165072442\n",
      "  batch 150 loss: 1.385320930182934\n",
      "  batch 160 loss: 1.3529047375544905\n",
      "  batch 170 loss: 1.3561239486560226\n",
      "  batch 180 loss: 1.4877624824643134\n",
      "  batch 190 loss: 1.5907386815175415\n",
      "LOSS train 1.5907386815175415 valid 1.3778385646545734\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 1.3708461109548806\n",
      "  batch 20 loss: 1.4702075988054275\n",
      "  batch 30 loss: 1.5240461591631174\n",
      "  batch 40 loss: 1.245084885135293\n",
      "  batch 50 loss: 1.3493846192955972\n",
      "  batch 60 loss: 1.5259844202548265\n",
      "  batch 70 loss: 1.6095475498586893\n",
      "  batch 80 loss: 1.2978694323450326\n",
      "  batch 90 loss: 1.3789533793926239\n",
      "  batch 100 loss: 1.613383188098669\n",
      "  batch 110 loss: 1.4242356155067681\n",
      "  batch 120 loss: 1.462173968553543\n",
      "  batch 130 loss: 1.483934124931693\n",
      "  batch 140 loss: 1.4246134862303734\n",
      "  batch 150 loss: 1.4000845327973366\n",
      "  batch 160 loss: 1.3194471763446927\n",
      "  batch 170 loss: 1.3405042810365557\n",
      "  batch 180 loss: 1.4601446058601142\n",
      "  batch 190 loss: 1.3908484507352115\n",
      "LOSS train 1.3908484507352115 valid 1.353048362984107\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 1.405128226801753\n",
      "  batch 20 loss: 1.2739991508424282\n",
      "  batch 30 loss: 1.3575266478583217\n",
      "  batch 40 loss: 1.5587390465661883\n",
      "  batch 50 loss: 1.366922004148364\n",
      "  batch 60 loss: 1.3366078598424793\n",
      "  batch 70 loss: 1.4329405386932195\n",
      "  batch 80 loss: 1.3083519451320171\n",
      "  batch 90 loss: 1.663829387910664\n",
      "  batch 100 loss: 1.4228981375694274\n",
      "  batch 110 loss: 1.3901176530867816\n",
      "  batch 120 loss: 1.3676713116466999\n",
      "  batch 130 loss: 1.4234655741602182\n",
      "  batch 140 loss: 1.27749443911016\n",
      "  batch 150 loss: 1.6034379415214062\n",
      "  batch 160 loss: 1.1891179296188057\n",
      "  batch 170 loss: 1.5352647349238395\n",
      "  batch 180 loss: 1.4560756323859096\n",
      "  batch 190 loss: 1.4014611434191466\n",
      "LOSS train 1.4014611434191466 valid 1.5014265913229723\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 1.7153964688070118\n",
      "  batch 20 loss: 1.5049438569694757\n",
      "  batch 30 loss: 1.4246758334338665\n",
      "  batch 40 loss: 1.7020882135257125\n",
      "  batch 50 loss: 1.524591689184308\n",
      "  batch 60 loss: 1.2712036546319723\n",
      "  batch 70 loss: 1.4397509932518004\n",
      "  batch 80 loss: 1.4496661502867938\n",
      "  batch 90 loss: 1.3469475204125048\n",
      "  batch 100 loss: 1.3440120605751873\n",
      "  batch 110 loss: 1.409469597414136\n",
      "  batch 120 loss: 1.3554951895028353\n",
      "  batch 130 loss: 1.4870124497450887\n",
      "  batch 140 loss: 1.3758420014753938\n",
      "  batch 150 loss: 1.5595732811838388\n",
      "  batch 160 loss: 1.1802054954692722\n",
      "  batch 170 loss: 1.4730133928358555\n",
      "  batch 180 loss: 1.7187533287331462\n",
      "  batch 190 loss: 1.3813265342265368\n",
      "LOSS train 1.3813265342265368 valid 1.3390303052102144\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 1.3887155771255493\n",
      "  batch 20 loss: 1.3200618596747518\n",
      "  batch 30 loss: 1.5924919934943318\n",
      "  batch 40 loss: 1.6562382837757468\n",
      "  batch 50 loss: 1.334425689280033\n",
      "  batch 60 loss: 1.272227588109672\n",
      "  batch 70 loss: 1.5088711351156234\n",
      "  batch 80 loss: 1.668094915151596\n",
      "  batch 90 loss: 1.3463806800544262\n",
      "  batch 100 loss: 1.2820369940251113\n",
      "  batch 110 loss: 1.4390878153964877\n",
      "  batch 120 loss: 1.3524136824533344\n",
      "  batch 130 loss: 1.4355684902518988\n",
      "  batch 140 loss: 1.3170698890462518\n",
      "  batch 150 loss: 1.5683744290843606\n",
      "  batch 160 loss: 1.3209397930651903\n",
      "  batch 170 loss: 1.1620821923017501\n",
      "  batch 180 loss: 1.396726356446743\n",
      "  batch 190 loss: 1.4921793146058917\n",
      "LOSS train 1.4921793146058917 valid 1.8383579045199814\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 1.3471111323684455\n",
      "  batch 20 loss: 1.5359066110104322\n",
      "  batch 30 loss: 1.3191422965377568\n",
      "  batch 40 loss: 1.4081678960472346\n",
      "  batch 50 loss: 1.265474848076701\n",
      "  batch 60 loss: 1.2805798759683966\n",
      "  batch 70 loss: 1.3537603665143252\n",
      "  batch 80 loss: 1.2234491974115371\n",
      "  batch 90 loss: 1.2853479824028908\n",
      "  batch 100 loss: 1.349527097120881\n",
      "  batch 110 loss: 1.5427758753299714\n",
      "  batch 120 loss: 1.441762925684452\n",
      "  batch 130 loss: 1.522348978370428\n",
      "  batch 140 loss: 1.433165902737528\n",
      "  batch 150 loss: 1.246343438513577\n",
      "  batch 160 loss: 1.439412106387317\n",
      "  batch 170 loss: 1.590315227024257\n",
      "  batch 180 loss: 1.6314066339284181\n",
      "  batch 190 loss: 1.4681357381865383\n",
      "LOSS train 1.4681357381865383 valid 1.466503575635262\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 1.5104852233082056\n",
      "  batch 20 loss: 1.394459454715252\n",
      "  batch 30 loss: 1.1502640690654515\n",
      "  batch 40 loss: 1.1882450569421052\n",
      "  batch 50 loss: 1.9596631083637477\n",
      "  batch 60 loss: 1.3445806367322803\n",
      "  batch 70 loss: 1.6664381388574838\n",
      "  batch 80 loss: 1.6257055796682836\n",
      "  batch 90 loss: 1.1761496784165502\n",
      "  batch 100 loss: 1.4217467980459333\n",
      "  batch 110 loss: 1.0308387452736496\n",
      "  batch 120 loss: 1.282641550619155\n",
      "  batch 130 loss: 1.3936685792170465\n",
      "  batch 140 loss: 1.5242667277343571\n",
      "  batch 150 loss: 1.4233877268619834\n",
      "  batch 160 loss: 1.2977096851915122\n",
      "  batch 170 loss: 1.3393325545825063\n",
      "  batch 180 loss: 1.4351184125989676\n",
      "  batch 190 loss: 1.4987682182341815\n",
      "LOSS train 1.4987682182341815 valid 1.29573891338152\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 1.4540207916870713\n",
      "  batch 20 loss: 1.3777775444090365\n",
      "  batch 30 loss: 1.1950871830806136\n",
      "  batch 40 loss: 1.3538135943934322\n",
      "  batch 50 loss: 1.193649777583778\n",
      "  batch 60 loss: 1.4784831568598746\n",
      "  batch 70 loss: 1.3478474728763103\n",
      "  batch 80 loss: 1.4737066916190087\n",
      "  batch 90 loss: 1.254312565503642\n",
      "  batch 100 loss: 1.3886998230591416\n",
      "  batch 110 loss: 1.489805362559855\n",
      "  batch 120 loss: 1.3004243986681103\n",
      "  batch 130 loss: 1.2715193200856447\n",
      "  batch 140 loss: 1.5812245864421128\n",
      "  batch 150 loss: 1.2478143226355314\n",
      "  batch 160 loss: 1.3605693558230996\n",
      "  batch 170 loss: 1.7319520361721517\n",
      "  batch 180 loss: 1.4904579345136881\n",
      "  batch 190 loss: 1.3299258871003985\n",
      "LOSS train 1.3299258871003985 valid 1.3597645041270134\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 1.416329226270318\n",
      "  batch 20 loss: 1.4752088386565447\n",
      "  batch 30 loss: 1.495821764320135\n",
      "  batch 40 loss: 1.4055243488401175\n",
      "  batch 50 loss: 1.3244147986173629\n",
      "  batch 60 loss: 1.3177148970775305\n",
      "  batch 70 loss: 1.6529351895675064\n",
      "  batch 80 loss: 1.206114600226283\n",
      "  batch 90 loss: 1.3828952871263027\n",
      "  batch 100 loss: 1.4649715136736632\n",
      "  batch 110 loss: 1.482425620779395\n",
      "  batch 120 loss: 1.3507388029247522\n",
      "  batch 130 loss: 1.3763436457142233\n",
      "  batch 140 loss: 1.313444091565907\n",
      "  batch 150 loss: 1.5007265411317348\n",
      "  batch 160 loss: 1.5001713341102003\n",
      "  batch 170 loss: 1.332642375677824\n",
      "  batch 180 loss: 1.5031603515148162\n",
      "  batch 190 loss: 1.3949658280238508\n",
      "LOSS train 1.3949658280238508 valid 1.3573331533190913\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 1.3325859853997826\n",
      "  batch 20 loss: 1.255711027048528\n",
      "  batch 30 loss: 1.4155507393181324\n",
      "  batch 40 loss: 1.5233764037489892\n",
      "  batch 50 loss: 1.3500764166936279\n",
      "  batch 60 loss: 1.3252845520153642\n",
      "  batch 70 loss: 1.3626499484293162\n",
      "  batch 80 loss: 1.1367980912327766\n",
      "  batch 90 loss: 1.2825893370434642\n",
      "  batch 100 loss: 1.4655167436227203\n",
      "  batch 110 loss: 1.369967733323574\n",
      "  batch 120 loss: 1.3696605682373046\n",
      "  batch 130 loss: 1.4476034432649612\n",
      "  batch 140 loss: 1.3804579854011536\n",
      "  batch 150 loss: 1.2853222409263254\n",
      "  batch 160 loss: 1.3031267266720534\n",
      "  batch 170 loss: 1.4343432571738959\n",
      "  batch 180 loss: 1.2170965163037182\n",
      "  batch 190 loss: 1.5066075989976526\n",
      "LOSS train 1.5066075989976526 valid 1.3199197238263412\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 1.3820582311600447\n",
      "  batch 20 loss: 1.4834948152303695\n",
      "  batch 30 loss: 1.2893022248521446\n",
      "  batch 40 loss: 1.2689697459340095\n",
      "  batch 50 loss: 1.2492512099444866\n",
      "  batch 60 loss: 1.412398699298501\n",
      "  batch 70 loss: 1.2600926216691732\n",
      "  batch 80 loss: 1.3249052833765744\n",
      "  batch 90 loss: 1.3545216877013444\n",
      "  batch 100 loss: 1.4350639451295137\n",
      "  batch 110 loss: 1.4357264768332243\n",
      "  batch 120 loss: 1.2288992872461677\n",
      "  batch 130 loss: 1.2350419225171207\n",
      "  batch 140 loss: 1.4677538544870914\n",
      "  batch 150 loss: 1.4864580368623137\n",
      "  batch 160 loss: 1.6300139978528023\n",
      "  batch 170 loss: 1.341715014539659\n",
      "  batch 180 loss: 1.4876933054998518\n",
      "  batch 190 loss: 1.455124431103468\n",
      "LOSS train 1.455124431103468 valid 1.3200439238347685\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 1.5384084962308406\n",
      "  batch 20 loss: 1.319615576416254\n",
      "  batch 30 loss: 1.28386270981282\n",
      "  batch 40 loss: 1.4102959960699082\n",
      "  batch 50 loss: 1.278061839286238\n",
      "  batch 60 loss: 1.3645591411739588\n",
      "  batch 70 loss: 1.221778044104576\n",
      "  batch 80 loss: 1.1023474213667215\n",
      "  batch 90 loss: 1.3156583661213517\n",
      "  batch 100 loss: 1.5372486725449561\n",
      "  batch 110 loss: 1.5641978230327367\n",
      "  batch 120 loss: 1.4145869206637145\n",
      "  batch 130 loss: 1.422763984836638\n",
      "  batch 140 loss: 1.5052574201487006\n",
      "  batch 150 loss: 1.4242235405370594\n",
      "  batch 160 loss: 1.3136151056736707\n",
      "  batch 170 loss: 1.3927722476422786\n",
      "  batch 180 loss: 1.2981182042509318\n",
      "  batch 190 loss: 1.0907286815345287\n",
      "LOSS train 1.0907286815345287 valid 1.3449370822128959\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 1.2900457434356212\n",
      "  batch 20 loss: 1.399317828938365\n",
      "  batch 30 loss: 1.2271810058504342\n",
      "  batch 40 loss: 1.4034061927348376\n",
      "  batch 50 loss: 1.5039444100111723\n",
      "  batch 60 loss: 1.481852159462869\n",
      "  batch 70 loss: 1.4212244097143412\n",
      "  batch 80 loss: 1.2171608397737146\n",
      "  batch 90 loss: 1.4693698707967997\n",
      "  batch 100 loss: 1.393699536845088\n",
      "  batch 110 loss: 1.312964917719364\n",
      "  batch 120 loss: 1.3283262424170972\n",
      "  batch 130 loss: 1.1411652928218246\n",
      "  batch 140 loss: 1.4171381768770517\n",
      "  batch 150 loss: 1.3296624260023235\n",
      "  batch 160 loss: 1.309327197819948\n",
      "  batch 170 loss: 1.530622964911163\n",
      "  batch 180 loss: 1.158964783884585\n",
      "  batch 190 loss: 1.2282224794849754\n",
      "LOSS train 1.2282224794849754 valid 1.3208001319987652\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 1.4926066033542156\n",
      "  batch 20 loss: 1.3415501238778234\n",
      "  batch 30 loss: 1.2651375398039817\n",
      "  batch 40 loss: 1.187280436977744\n",
      "  batch 50 loss: 1.4719852341338993\n",
      "  batch 60 loss: 1.3979201428592205\n",
      "  batch 70 loss: 1.2292632708325981\n",
      "  batch 80 loss: 1.3859477539546787\n",
      "  batch 90 loss: 1.157563326600939\n",
      "  batch 100 loss: 1.2991273051127792\n",
      "  batch 110 loss: 1.3372557766735553\n",
      "  batch 120 loss: 1.2887687200680376\n",
      "  batch 130 loss: 1.4251918110996484\n",
      "  batch 140 loss: 1.2848409842699766\n",
      "  batch 150 loss: 1.4192423766478897\n",
      "  batch 160 loss: 1.477822683006525\n",
      "  batch 170 loss: 1.2719411421567202\n",
      "  batch 180 loss: 1.3058638777583838\n",
      "  batch 190 loss: 1.3838711442425846\n",
      "LOSS train 1.3838711442425846 valid 1.2641009001825483\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 1.4186217879876495\n",
      "  batch 20 loss: 1.389056583493948\n",
      "  batch 30 loss: 1.4537523198872804\n",
      "  batch 40 loss: 1.4711846873164176\n",
      "  batch 50 loss: 1.288574918732047\n",
      "  batch 60 loss: 1.3904385397210717\n",
      "  batch 70 loss: 1.3031961552798748\n",
      "  batch 80 loss: 1.4101504059508443\n",
      "  batch 90 loss: 1.4183843333274126\n",
      "  batch 100 loss: 1.24364609234035\n",
      "  batch 110 loss: 1.2630393657833339\n",
      "  batch 120 loss: 0.9538259717635811\n",
      "  batch 130 loss: 1.4343936420977115\n",
      "  batch 140 loss: 1.4300384171307088\n",
      "  batch 150 loss: 1.4313277289271356\n",
      "  batch 160 loss: 1.0780727045610548\n",
      "  batch 170 loss: 1.2816714357584715\n",
      "  batch 180 loss: 1.3728032549843192\n",
      "  batch 190 loss: 1.3999790944159032\n",
      "LOSS train 1.3999790944159032 valid 1.4653195063225353\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 1.427070135064423\n",
      "  batch 20 loss: 1.3843525655567646\n",
      "  batch 30 loss: 1.3975620072335004\n",
      "  batch 40 loss: 1.2386264868080616\n",
      "  batch 50 loss: 1.4345819087699057\n",
      "  batch 60 loss: 1.4975275691598653\n",
      "  batch 70 loss: 1.3773354664444923\n",
      "  batch 80 loss: 1.1631630696356297\n",
      "  batch 90 loss: 1.1736049400642514\n",
      "  batch 100 loss: 1.2946522877551616\n",
      "  batch 110 loss: 1.4461150374263525\n",
      "  batch 120 loss: 1.4399353464134037\n",
      "  batch 130 loss: 1.6908140467479824\n",
      "  batch 140 loss: 1.3222147446125745\n",
      "  batch 150 loss: 1.3384046718478202\n",
      "  batch 160 loss: 1.3374324813485146\n",
      "  batch 170 loss: 1.2500996917486191\n",
      "  batch 180 loss: 1.2849299982190132\n",
      "  batch 190 loss: 1.3721682099625467\n",
      "LOSS train 1.3721682099625467 valid 1.3202068212991342\n",
      "EPOCH 101:\n",
      "  batch 10 loss: 1.5229161255061627\n",
      "  batch 20 loss: 1.2028925631195306\n",
      "  batch 30 loss: 1.0569091254845262\n",
      "  batch 40 loss: 1.390809664875269\n",
      "  batch 50 loss: 1.3991507863625885\n",
      "  batch 60 loss: 1.3989337569102644\n",
      "  batch 70 loss: 1.192381877359003\n",
      "  batch 80 loss: 1.397063466720283\n",
      "  batch 90 loss: 1.4045945344492794\n",
      "  batch 100 loss: 1.4827190827578307\n",
      "  batch 110 loss: 1.3470037659630179\n",
      "  batch 120 loss: 1.342720067035407\n",
      "  batch 130 loss: 1.1537204516120254\n",
      "  batch 140 loss: 1.0935075145214797\n",
      "  batch 150 loss: 1.3447422036901115\n",
      "  batch 160 loss: 1.705703306570649\n",
      "  batch 170 loss: 1.320118340756744\n",
      "  batch 180 loss: 1.2701676914468407\n",
      "  batch 190 loss: 1.472878391481936\n",
      "LOSS train 1.472878391481936 valid 1.411069083481263\n",
      "EPOCH 102:\n",
      "  batch 10 loss: 1.3534715880639852\n",
      "  batch 20 loss: 1.2510370187461377\n",
      "  batch 30 loss: 1.308507827669382\n",
      "  batch 40 loss: 1.4492719227448105\n",
      "  batch 50 loss: 1.3896971672773362\n",
      "  batch 60 loss: 1.32715211994946\n",
      "  batch 70 loss: 1.3449704453349114\n",
      "  batch 80 loss: 1.5506378766149282\n",
      "  batch 90 loss: 1.234551190584898\n",
      "  batch 100 loss: 1.4150032110512256\n",
      "  batch 110 loss: 1.2173892153427004\n",
      "  batch 120 loss: 1.3696196228265762\n",
      "  batch 130 loss: 1.3087947860360145\n",
      "  batch 140 loss: 1.238357551023364\n",
      "  batch 150 loss: 1.3270234942436219\n",
      "  batch 160 loss: 1.2152283104136585\n",
      "  batch 170 loss: 1.3547450603917242\n",
      "  batch 180 loss: 1.2983362542465329\n",
      "  batch 190 loss: 1.3303431881591679\n",
      "LOSS train 1.3303431881591679 valid 3.2518657480294886\n",
      "EPOCH 103:\n",
      "  batch 10 loss: 1.387043846026063\n",
      "  batch 20 loss: 1.6685829751193524\n",
      "  batch 30 loss: 1.413273623585701\n",
      "  batch 40 loss: 1.3061910601332785\n",
      "  batch 50 loss: 1.3986146207898855\n",
      "  batch 60 loss: 1.2724031491205097\n",
      "  batch 70 loss: 1.4911448750644922\n",
      "  batch 80 loss: 1.3198840878903866\n",
      "  batch 90 loss: 1.2720095133408904\n",
      "  batch 100 loss: 1.4495310951024294\n",
      "  batch 110 loss: 1.4202515250071883\n",
      "  batch 120 loss: 1.4135902130976319\n",
      "  batch 130 loss: 1.216477026976645\n",
      "  batch 140 loss: 1.0859684191644192\n",
      "  batch 150 loss: 1.338135380949825\n",
      "  batch 160 loss: 1.2199980326462536\n",
      "  batch 170 loss: 1.126997137069702\n",
      "  batch 180 loss: 1.2710268387570978\n",
      "  batch 190 loss: 1.2759772883728147\n",
      "LOSS train 1.2759772883728147 valid 3.1376701168811474\n",
      "EPOCH 104:\n",
      "  batch 10 loss: 1.6974219230934977\n",
      "  batch 20 loss: 1.1439227504655718\n",
      "  batch 30 loss: 1.2239298053085803\n",
      "  batch 40 loss: 1.2624064669013024\n",
      "  batch 50 loss: 1.330315475538373\n",
      "  batch 60 loss: 1.2055842344649137\n",
      "  batch 70 loss: 1.4187030464410781\n",
      "  batch 80 loss: 1.2961170099675656\n",
      "  batch 90 loss: 1.30976436547935\n",
      "  batch 100 loss: 1.1907185251824557\n",
      "  batch 110 loss: 1.5917041912674903\n",
      "  batch 120 loss: 1.3861361976712943\n",
      "  batch 130 loss: 1.2598271845839917\n",
      "  batch 140 loss: 1.1544458098709582\n",
      "  batch 150 loss: 1.2816646803170442\n",
      "  batch 160 loss: 1.3774425599724054\n",
      "  batch 170 loss: 1.176095461845398\n",
      "  batch 180 loss: 1.3188418343663215\n",
      "  batch 190 loss: 1.211242703255266\n",
      "LOSS train 1.211242703255266 valid 1.2304586689465513\n",
      "EPOCH 105:\n",
      "  batch 10 loss: 1.1500570034608244\n",
      "  batch 20 loss: 1.398991501983255\n",
      "  batch 30 loss: 1.323419144563377\n",
      "  batch 40 loss: 1.4283842554315924\n",
      "  batch 50 loss: 1.2723857454955578\n",
      "  batch 60 loss: 1.1642129948362707\n",
      "  batch 70 loss: 1.3221099007874728\n",
      "  batch 80 loss: 1.2889355825260282\n",
      "  batch 90 loss: 1.3732412384822965\n",
      "  batch 100 loss: 1.4101749306544662\n",
      "  batch 110 loss: 1.08284550961107\n",
      "  batch 120 loss: 1.3353823766112327\n",
      "  batch 130 loss: 1.3810291312634946\n",
      "  batch 140 loss: 1.316861111111939\n",
      "  batch 150 loss: 1.4803093679249286\n",
      "  batch 160 loss: 1.2655677726492285\n",
      "  batch 170 loss: 1.208864992763847\n",
      "  batch 180 loss: 1.1368012961000205\n",
      "  batch 190 loss: 1.2084355244413019\n",
      "LOSS train 1.2084355244413019 valid 1.2694705837430098\n",
      "EPOCH 106:\n",
      "  batch 10 loss: 1.2313320839777588\n",
      "  batch 20 loss: 1.090050814114511\n",
      "  batch 30 loss: 1.3460597770288587\n",
      "  batch 40 loss: 1.2104150866158307\n",
      "  batch 50 loss: 1.4530370272696018\n",
      "  batch 60 loss: 1.3588583339005709\n",
      "  batch 70 loss: 1.2751335291191936\n",
      "  batch 80 loss: 1.0728277264162898\n",
      "  batch 90 loss: 1.3477422149851919\n",
      "  batch 100 loss: 1.3464953744783998\n",
      "  batch 110 loss: 1.5085361689329146\n",
      "  batch 120 loss: 1.3730283550918103\n",
      "  batch 130 loss: 1.4076301760971546\n",
      "  batch 140 loss: 1.4046854743734003\n",
      "  batch 150 loss: 1.2556746302172541\n",
      "  batch 160 loss: 1.2712126433849336\n",
      "  batch 170 loss: 1.3167562644928694\n",
      "  batch 180 loss: 1.292340132314712\n",
      "  batch 190 loss: 0.9766699603758753\n",
      "LOSS train 0.9766699603758753 valid 2.3793910558407125\n",
      "EPOCH 107:\n",
      "  batch 10 loss: 1.3771068442612886\n",
      "  batch 20 loss: 1.606034622900188\n",
      "  batch 30 loss: 1.2300108812749386\n",
      "  batch 40 loss: 1.3842283464968204\n",
      "  batch 50 loss: 1.1169572126120328\n",
      "  batch 60 loss: 1.4549212884157896\n",
      "  batch 70 loss: 1.462386067211628\n",
      "  batch 80 loss: 1.2251041892915964\n",
      "  batch 90 loss: 1.3648849559016525\n",
      "  batch 100 loss: 1.2438969070091843\n",
      "  batch 110 loss: 1.254231534525752\n",
      "  batch 120 loss: 1.1305720487609505\n",
      "  batch 130 loss: 1.169314910005778\n",
      "  batch 140 loss: 1.2207175055518746\n",
      "  batch 150 loss: 1.3324245857074857\n",
      "  batch 160 loss: 1.4707217488437891\n",
      "  batch 170 loss: 1.169636151753366\n",
      "  batch 180 loss: 1.1573559590615332\n",
      "  batch 190 loss: 1.1494226987473666\n",
      "LOSS train 1.1494226987473666 valid 1.4266555434904802\n",
      "EPOCH 108:\n",
      "  batch 10 loss: 1.314428768865764\n",
      "  batch 20 loss: 1.5174342764541506\n",
      "  batch 30 loss: 0.9913266175426543\n",
      "  batch 40 loss: 1.2441945758648216\n",
      "  batch 50 loss: 1.2005144137889148\n",
      "  batch 60 loss: 1.3758187916129827\n",
      "  batch 70 loss: 1.249374397099018\n",
      "  batch 80 loss: 1.6054990094155073\n",
      "  batch 90 loss: 1.348483431339264\n",
      "  batch 100 loss: 1.3665933125652372\n",
      "  batch 110 loss: 1.2961102941073477\n",
      "  batch 120 loss: 1.4530165050178767\n",
      "  batch 130 loss: 1.3402186416089534\n",
      "  batch 140 loss: 1.413601880520582\n",
      "  batch 150 loss: 1.3257090363651514\n",
      "  batch 160 loss: 1.2330558732151986\n",
      "  batch 170 loss: 1.6354733059182762\n",
      "  batch 180 loss: 1.5031064901500941\n",
      "  batch 190 loss: 1.2221410021185874\n",
      "LOSS train 1.2221410021185874 valid 1.3871332633619506\n",
      "EPOCH 109:\n",
      "  batch 10 loss: 1.304104994982481\n",
      "  batch 20 loss: 1.3343296349048615\n",
      "  batch 30 loss: 1.4069652169942857\n",
      "  batch 40 loss: 1.2096665969118476\n",
      "  batch 50 loss: 1.302730866894126\n",
      "  batch 60 loss: 1.3837597891688347\n",
      "  batch 70 loss: 1.311992091871798\n",
      "  batch 80 loss: 1.2176465405151249\n",
      "  batch 90 loss: 1.245505791902542\n",
      "  batch 100 loss: 1.195899249985814\n",
      "  batch 110 loss: 1.367230237647891\n",
      "  batch 120 loss: 1.352701329998672\n",
      "  batch 130 loss: 1.2377699746750295\n",
      "  batch 140 loss: 1.4335268681868911\n",
      "  batch 150 loss: 1.304671723023057\n",
      "  batch 160 loss: 1.1894525425508617\n",
      "  batch 170 loss: 1.3339620012789966\n",
      "  batch 180 loss: 1.130085473973304\n",
      "  batch 190 loss: 1.16869300827384\n",
      "LOSS train 1.16869300827384 valid 1.2490687612444162\n",
      "EPOCH 110:\n",
      "  batch 10 loss: 1.1750549035146833\n",
      "  batch 20 loss: 1.4520160507410764\n",
      "  batch 30 loss: 1.4071253281086684\n",
      "  batch 40 loss: 1.105027287080884\n",
      "  batch 50 loss: 0.9944634983316064\n",
      "  batch 60 loss: 1.2568615842610598\n",
      "  batch 70 loss: 1.203592363372445\n",
      "  batch 80 loss: 1.0730944282375277\n",
      "  batch 90 loss: 1.218102633021772\n",
      "  batch 100 loss: 1.5063862316310406\n",
      "  batch 110 loss: 1.1830212988890707\n",
      "  batch 120 loss: 1.572179241012782\n",
      "  batch 130 loss: 1.4393455922603606\n",
      "  batch 140 loss: 1.186771399900317\n",
      "  batch 150 loss: 1.21597660779953\n",
      "  batch 160 loss: 1.2445473150350153\n",
      "  batch 170 loss: 1.2975492814555765\n",
      "  batch 180 loss: 1.451534851640463\n",
      "  batch 190 loss: 1.2993825681507587\n",
      "LOSS train 1.2993825681507587 valid 1.2404607176207578\n",
      "EPOCH 111:\n",
      "  batch 10 loss: 1.2826164949685335\n",
      "  batch 20 loss: 1.3184666373766958\n",
      "  batch 30 loss: 1.5754271071404218\n",
      "  batch 40 loss: 1.3301587840542197\n",
      "  batch 50 loss: 1.1336394483223557\n",
      "  batch 60 loss: 1.1958174780942499\n",
      "  batch 70 loss: 1.3774584067985416\n",
      "  batch 80 loss: 1.1219185266643763\n",
      "  batch 90 loss: 1.1227023240178824\n",
      "  batch 100 loss: 1.3093804217875005\n",
      "  batch 110 loss: 1.3361535673961042\n",
      "  batch 120 loss: 1.3067641858011485\n",
      "  batch 130 loss: 1.2227547752670944\n",
      "  batch 140 loss: 1.3494595553725959\n",
      "  batch 150 loss: 1.510303625278175\n",
      "  batch 160 loss: 1.499427579715848\n",
      "  batch 170 loss: 1.293645766377449\n",
      "  batch 180 loss: 1.2115893686190247\n",
      "  batch 190 loss: 1.3879060696810483\n",
      "LOSS train 1.3879060696810483 valid 1.3600506763308284\n",
      "EPOCH 112:\n",
      "  batch 10 loss: 1.186667850986123\n",
      "  batch 20 loss: 1.0979924077168106\n",
      "  batch 30 loss: 1.1121547505259515\n",
      "  batch 40 loss: 1.1782058393582702\n",
      "  batch 50 loss: 1.2108696889132262\n",
      "  batch 60 loss: 1.357416622247547\n",
      "  batch 70 loss: 1.3132758459076286\n",
      "  batch 80 loss: 1.4061701344326138\n",
      "  batch 90 loss: 1.2022616987116634\n",
      "  batch 100 loss: 1.332164642214775\n",
      "  batch 110 loss: 1.2575696701183916\n",
      "  batch 120 loss: 1.1875889614224433\n",
      "  batch 130 loss: 1.4445388451218606\n",
      "  batch 140 loss: 1.3710057569667697\n",
      "  batch 150 loss: 1.1399701369926334\n",
      "  batch 160 loss: 1.259046277962625\n",
      "  batch 170 loss: 1.1940988053567707\n",
      "  batch 180 loss: 1.1715614615939558\n",
      "  batch 190 loss: 1.1425428248941898\n",
      "LOSS train 1.1425428248941898 valid 1.5809066833402865\n",
      "EPOCH 113:\n",
      "  batch 10 loss: 1.2663084730505942\n",
      "  batch 20 loss: 1.318014433607459\n",
      "  batch 30 loss: 1.1144597884267569\n",
      "  batch 40 loss: 1.457522962614894\n",
      "  batch 50 loss: 1.293603832833469\n",
      "  batch 60 loss: 1.6896484840661288\n",
      "  batch 70 loss: 1.1588649285957218\n",
      "  batch 80 loss: 1.2830000245943665\n",
      "  batch 90 loss: 1.250018034502864\n",
      "  batch 100 loss: 1.063169723097235\n",
      "  batch 110 loss: 1.1115270580165089\n",
      "  batch 120 loss: 1.429409665800631\n",
      "  batch 130 loss: 1.3932952378876506\n",
      "  batch 140 loss: 1.1179966282099485\n",
      "  batch 150 loss: 1.265530912205577\n",
      "  batch 160 loss: 1.1540088638663293\n",
      "  batch 170 loss: 1.4374207286164165\n",
      "  batch 180 loss: 1.1924115924164653\n",
      "  batch 190 loss: 1.2375835022889077\n",
      "LOSS train 1.2375835022889077 valid 1.520653339425245\n",
      "EPOCH 114:\n",
      "  batch 10 loss: 1.069283671490848\n",
      "  batch 20 loss: 1.3023992909118534\n",
      "  batch 30 loss: 1.1837424421682954\n",
      "  batch 40 loss: 1.0521084692329168\n",
      "  batch 50 loss: 1.0674724282696844\n",
      "  batch 60 loss: 1.246295944135636\n",
      "  batch 70 loss: 1.1915388396941125\n",
      "  batch 80 loss: 1.1957674875855446\n",
      "  batch 90 loss: 1.2206859815865756\n",
      "  batch 100 loss: 1.2407364217564463\n",
      "  batch 110 loss: 1.1596033703535795\n",
      "  batch 120 loss: 1.2150313857011497\n",
      "  batch 130 loss: 1.2261323487386107\n",
      "  batch 140 loss: 1.126364602148533\n",
      "  batch 150 loss: 1.2224647673778235\n",
      "  batch 160 loss: 1.2587993912398816\n",
      "  batch 170 loss: 1.2450513975694775\n",
      "  batch 180 loss: 1.5883582185953855\n",
      "  batch 190 loss: 1.3047650245018303\n",
      "LOSS train 1.3047650245018303 valid 1.4004801392602997\n",
      "EPOCH 115:\n",
      "  batch 10 loss: 1.0847351076081395\n",
      "  batch 20 loss: 1.2610492818057537\n",
      "  batch 30 loss: 1.5342733979225158\n",
      "  batch 40 loss: 1.2539143407717348\n",
      "  batch 50 loss: 1.2762937031686306\n",
      "  batch 60 loss: 1.25335732717067\n",
      "  batch 70 loss: 1.245019163750112\n",
      "  batch 80 loss: 1.1359851533547043\n",
      "  batch 90 loss: 1.1593208707869054\n",
      "  batch 100 loss: 1.2493703553453088\n",
      "  batch 110 loss: 1.1319913046434522\n",
      "  batch 120 loss: 1.2536161820404232\n",
      "  batch 130 loss: 1.0854435592889786\n",
      "  batch 140 loss: 1.1505759852007031\n",
      "  batch 150 loss: 1.2833433212712406\n",
      "  batch 160 loss: 1.306413884460926\n",
      "  batch 170 loss: 1.0412052972242236\n",
      "  batch 180 loss: 1.205815374944359\n",
      "  batch 190 loss: 1.162525835633278\n",
      "LOSS train 1.162525835633278 valid 1.1256125163382444\n",
      "EPOCH 116:\n",
      "  batch 10 loss: 1.288415002450347\n",
      "  batch 20 loss: 1.0947380542755127\n",
      "  batch 30 loss: 1.2762017827481031\n",
      "  batch 40 loss: 1.2881285205483437\n",
      "  batch 50 loss: 1.2099523648619652\n",
      "  batch 60 loss: 1.073396868724376\n",
      "  batch 70 loss: 1.2450853580608965\n",
      "  batch 80 loss: 1.1560488631948829\n",
      "  batch 90 loss: 1.4941618606448173\n",
      "  batch 100 loss: 1.0887985924258827\n",
      "  batch 110 loss: 1.0272591887973248\n",
      "  batch 120 loss: 1.1219938287511468\n",
      "  batch 130 loss: 1.385470391623676\n",
      "  batch 140 loss: 1.4887402247637511\n",
      "  batch 150 loss: 1.168240812793374\n",
      "  batch 160 loss: 1.1718708356842398\n",
      "  batch 170 loss: 1.418192160129547\n",
      "  batch 180 loss: 1.1132974052801727\n",
      "  batch 190 loss: 1.184608118981123\n",
      "LOSS train 1.184608118981123 valid 1.2353592770747268\n",
      "EPOCH 117:\n",
      "  batch 10 loss: 1.1694009106606245\n",
      "  batch 20 loss: 1.1228547133505344\n",
      "  batch 30 loss: 1.1911034677177668\n",
      "  batch 40 loss: 1.1373730704188347\n",
      "  batch 50 loss: 1.2304951472207903\n",
      "  batch 60 loss: 1.1676096769049764\n",
      "  batch 70 loss: 1.0355665126815439\n",
      "  batch 80 loss: 1.043960637692362\n",
      "  batch 90 loss: 1.375681708753109\n",
      "  batch 100 loss: 1.0529693733900785\n",
      "  batch 110 loss: 1.253805288579315\n",
      "  batch 120 loss: 1.159028783813119\n",
      "  batch 130 loss: 1.2699003178626298\n",
      "  batch 140 loss: 1.1934220293536781\n",
      "  batch 150 loss: 1.1121030032634736\n",
      "  batch 160 loss: 1.0350474333390594\n",
      "  batch 170 loss: 1.290969555452466\n",
      "  batch 180 loss: 1.306811890937388\n",
      "  batch 190 loss: 1.1251458171755075\n",
      "LOSS train 1.1251458171755075 valid 1.447225402503346\n",
      "EPOCH 118:\n",
      "  batch 10 loss: 1.0690468488261105\n",
      "  batch 20 loss: 0.989793860912323\n",
      "  batch 30 loss: 1.0806727008894086\n",
      "  batch 40 loss: 1.0900782248936594\n",
      "  batch 50 loss: 1.0070484967436641\n",
      "  batch 60 loss: 1.238352945074439\n",
      "  batch 70 loss: 1.07366449162364\n",
      "  batch 80 loss: 1.2011647684499622\n",
      "  batch 90 loss: 1.3507982298731804\n",
      "  batch 100 loss: 1.2410572592169047\n",
      "  batch 110 loss: 1.1485882557928562\n",
      "  batch 120 loss: 1.2492372326552867\n",
      "  batch 130 loss: 1.179564019665122\n",
      "  batch 140 loss: 1.2385615846142173\n",
      "  batch 150 loss: 1.1667240804061294\n",
      "  batch 160 loss: 1.292226842418313\n",
      "  batch 170 loss: 1.0097685182467102\n",
      "  batch 180 loss: 1.1655456257984043\n",
      "  batch 190 loss: 1.2542876180261373\n",
      "LOSS train 1.2542876180261373 valid 1.2415584140958695\n",
      "EPOCH 119:\n",
      "  batch 10 loss: 1.1351502627134322\n",
      "  batch 20 loss: 1.094958157464862\n",
      "  batch 30 loss: 1.1383598988875747\n",
      "  batch 40 loss: 1.4403489843010902\n",
      "  batch 50 loss: 1.1470407459884882\n",
      "  batch 60 loss: 1.2596324390731752\n",
      "  batch 70 loss: 1.1412623629905283\n",
      "  batch 80 loss: 1.1604888033121825\n",
      "  batch 90 loss: 1.129078803397715\n",
      "  batch 100 loss: 1.1447173388674856\n",
      "  batch 110 loss: 1.0103485757485031\n",
      "  batch 120 loss: 1.5078731188550591\n",
      "  batch 130 loss: 1.0195506356656552\n",
      "  batch 140 loss: 0.9366684714332223\n",
      "  batch 150 loss: 1.0437666112557054\n",
      "  batch 160 loss: 1.0673476372845472\n",
      "  batch 170 loss: 1.138153795711696\n",
      "  batch 180 loss: 1.1568557504098862\n",
      "  batch 190 loss: 1.1387486157938838\n",
      "LOSS train 1.1387486157938838 valid 1.0675195335912018\n",
      "EPOCH 120:\n",
      "  batch 10 loss: 1.0601769354194404\n",
      "  batch 20 loss: 1.157450813986361\n",
      "  batch 30 loss: 1.3188885748386383\n",
      "  batch 40 loss: 1.267012593522668\n",
      "  batch 50 loss: 1.0750970106571913\n",
      "  batch 60 loss: 1.2815980654209853\n",
      "  batch 70 loss: 1.3453276947140693\n",
      "  batch 80 loss: 1.0850941356271506\n",
      "  batch 90 loss: 0.9755592830479145\n",
      "  batch 100 loss: 1.2458285804837943\n",
      "  batch 110 loss: 0.7569460980594158\n",
      "  batch 120 loss: 1.1212943887338043\n",
      "  batch 130 loss: 1.0175988788716495\n",
      "  batch 140 loss: 1.3500256553292274\n",
      "  batch 150 loss: 1.0065270006656646\n",
      "  batch 160 loss: 1.0327885203063487\n",
      "  batch 170 loss: 1.022929910570383\n",
      "  batch 180 loss: 0.9791746692731976\n",
      "  batch 190 loss: 1.5190136371180416\n",
      "LOSS train 1.5190136371180416 valid 1.3349159715147927\n",
      "EPOCH 121:\n",
      "  batch 10 loss: 1.1133371400646865\n",
      "  batch 20 loss: 1.0638903193175793\n",
      "  batch 30 loss: 1.11824851334095\n",
      "  batch 40 loss: 1.213569180481136\n",
      "  batch 50 loss: 1.0257431337609888\n",
      "  batch 60 loss: 1.0224121127277612\n",
      "  batch 70 loss: 1.1005970876663924\n",
      "  batch 80 loss: 1.1832467524334789\n",
      "  batch 90 loss: 1.207680115289986\n",
      "  batch 100 loss: 1.02589023783803\n",
      "  batch 110 loss: 1.0645232474431396\n",
      "  batch 120 loss: 1.0786885572597384\n",
      "  batch 130 loss: 0.954838324803859\n",
      "  batch 140 loss: 1.3097720142453908\n",
      "  batch 150 loss: 1.1449869433417916\n",
      "  batch 160 loss: 1.097197121568024\n",
      "  batch 170 loss: 1.0362876119092106\n",
      "  batch 180 loss: 1.1429830614477396\n",
      "  batch 190 loss: 1.1683589735999704\n",
      "LOSS train 1.1683589735999704 valid 1.299661746009802\n",
      "EPOCH 122:\n",
      "  batch 10 loss: 0.990165868960321\n",
      "  batch 20 loss: 1.1623046906664967\n",
      "  batch 30 loss: 1.3298632126301526\n",
      "  batch 40 loss: 1.3709712918847798\n",
      "  batch 50 loss: 1.0398277461528778\n",
      "  batch 60 loss: 1.2088103046640755\n",
      "  batch 70 loss: 1.1186995370313526\n",
      "  batch 80 loss: 0.950645687803626\n",
      "  batch 90 loss: 1.2747046256437897\n",
      "  batch 100 loss: 0.9514217505231499\n",
      "  batch 110 loss: 1.2064073733985423\n",
      "  batch 120 loss: 1.3142092847265303\n",
      "  batch 130 loss: 1.2060269821435212\n",
      "  batch 140 loss: 0.8068572202697396\n",
      "  batch 150 loss: 1.1273304862901568\n",
      "  batch 160 loss: 0.9978861097246409\n",
      "  batch 170 loss: 1.1238072358071804\n",
      "  batch 180 loss: 0.9547128432430327\n",
      "  batch 190 loss: 0.9009939309209585\n",
      "LOSS train 0.9009939309209585 valid 1.2668648230819366\n",
      "EPOCH 123:\n",
      "  batch 10 loss: 0.9097767194733024\n",
      "  batch 20 loss: 0.8852608429268003\n",
      "  batch 30 loss: 1.3349284625612199\n",
      "  batch 40 loss: 1.2778490526601671\n",
      "  batch 50 loss: 1.0664485808461905\n",
      "  batch 60 loss: 1.120572966709733\n",
      "  batch 70 loss: 1.123991315253079\n",
      "  batch 80 loss: 0.9551937690936029\n",
      "  batch 90 loss: 0.9459631765261293\n",
      "  batch 100 loss: 1.068521170504391\n",
      "  batch 110 loss: 0.9811144208535552\n",
      "  batch 120 loss: 1.0134709692560135\n",
      "  batch 130 loss: 1.0560742645524441\n",
      "  batch 140 loss: 1.0408923264592886\n",
      "  batch 150 loss: 0.9838877392932772\n",
      "  batch 160 loss: 1.0693256733939052\n",
      "  batch 170 loss: 1.1664470855146647\n",
      "  batch 180 loss: 1.1568831535987556\n",
      "  batch 190 loss: 0.7468589376658201\n",
      "LOSS train 0.7468589376658201 valid 1.7402086461034532\n",
      "EPOCH 124:\n",
      "  batch 10 loss: 1.1118527510203422\n",
      "  batch 20 loss: 1.0684160942211747\n",
      "  batch 30 loss: 1.0883873734623193\n",
      "  batch 40 loss: 1.1465634581632913\n",
      "  batch 50 loss: 1.1977782724425197\n",
      "  batch 60 loss: 1.0508875468745829\n",
      "  batch 70 loss: 0.9703547468408942\n",
      "  batch 80 loss: 1.0418690033257008\n",
      "  batch 90 loss: 1.1051319232210517\n",
      "  batch 100 loss: 1.0807188849896192\n",
      "  batch 110 loss: 0.9711759267374873\n",
      "  batch 120 loss: 1.0813176633790136\n",
      "  batch 130 loss: 0.9382246633060276\n",
      "  batch 140 loss: 0.8184195769950747\n",
      "  batch 150 loss: 1.0222193289548158\n",
      "  batch 160 loss: 0.8569598102942109\n",
      "  batch 170 loss: 0.9396919323131442\n",
      "  batch 180 loss: 1.150936765410006\n",
      "  batch 190 loss: 1.0998424929566681\n",
      "LOSS train 1.0998424929566681 valid 1.6543969484762504\n",
      "EPOCH 125:\n",
      "  batch 10 loss: 0.9899763106368482\n",
      "  batch 20 loss: 1.128893437795341\n",
      "  batch 30 loss: 1.2505169782787562\n",
      "  batch 40 loss: 0.8819381831213832\n",
      "  batch 50 loss: 1.0835732170380652\n",
      "  batch 60 loss: 0.9748445124365389\n",
      "  batch 70 loss: 1.0424308398738504\n",
      "  batch 80 loss: 0.9963724644854665\n",
      "  batch 90 loss: 0.9039816156029701\n",
      "  batch 100 loss: 1.1340724822133779\n",
      "  batch 110 loss: 1.3143133012577892\n",
      "  batch 120 loss: 1.0542218370363117\n",
      "  batch 130 loss: 0.9803122645244002\n",
      "  batch 140 loss: 1.0821273527108133\n",
      "  batch 150 loss: 1.0096544719301164\n",
      "  batch 160 loss: 0.873413464333862\n",
      "  batch 170 loss: 0.8423546270467341\n",
      "  batch 180 loss: 0.9915867371484637\n",
      "  batch 190 loss: 0.9684964020270854\n",
      "LOSS train 0.9684964020270854 valid 1.2432369605088844\n",
      "EPOCH 126:\n",
      "  batch 10 loss: 1.1097919144667685\n",
      "  batch 20 loss: 1.0932598413899541\n",
      "  batch 30 loss: 1.0502374274656177\n",
      "  batch 40 loss: 1.161154860816896\n",
      "  batch 50 loss: 0.9193573165684938\n",
      "  batch 60 loss: 0.8144027105532586\n",
      "  batch 70 loss: 0.9863072467036545\n",
      "  batch 80 loss: 0.8703596004284918\n",
      "  batch 90 loss: 0.9694956315681338\n",
      "  batch 100 loss: 0.9688005402684212\n",
      "  batch 110 loss: 0.9699496027082205\n",
      "  batch 120 loss: 0.7906255439389497\n",
      "  batch 130 loss: 0.7583476808853448\n",
      "  batch 140 loss: 1.3981855867430568\n",
      "  batch 150 loss: 1.0952730720862747\n",
      "  batch 160 loss: 1.0003418106585742\n",
      "  batch 170 loss: 0.9632347103208303\n",
      "  batch 180 loss: 0.8984300445765256\n",
      "  batch 190 loss: 0.6654793586581945\n",
      "LOSS train 0.6654793586581945 valid 1.414600601252646\n",
      "EPOCH 127:\n",
      "  batch 10 loss: 1.2958801110275089\n",
      "  batch 20 loss: 0.8251564943231642\n",
      "  batch 30 loss: 0.9473876284435392\n",
      "  batch 40 loss: 0.847579319216311\n",
      "  batch 50 loss: 0.9940793508663773\n",
      "  batch 60 loss: 1.0010314516723156\n",
      "  batch 70 loss: 0.9502091631293297\n",
      "  batch 80 loss: 1.186479250341654\n",
      "  batch 90 loss: 0.9132115921005607\n",
      "  batch 100 loss: 0.9706439499743282\n",
      "  batch 110 loss: 0.6568234965205193\n",
      "  batch 120 loss: 1.068668141355738\n",
      "  batch 130 loss: 0.7309887913055718\n",
      "  batch 140 loss: 0.93674848661758\n",
      "  batch 150 loss: 1.1333036512136458\n",
      "  batch 160 loss: 1.022176931425929\n",
      "  batch 170 loss: 1.095898861810565\n",
      "  batch 180 loss: 0.9965977305546403\n",
      "  batch 190 loss: 0.8969747038558126\n",
      "LOSS train 0.8969747038558126 valid 0.8325236446391313\n",
      "EPOCH 128:\n",
      "  batch 10 loss: 0.7584095673635602\n",
      "  batch 20 loss: 0.8401708955876529\n",
      "  batch 30 loss: 1.1501792013645171\n",
      "  batch 40 loss: 0.8708600628189742\n",
      "  batch 50 loss: 1.0653558072634042\n",
      "  batch 60 loss: 0.9124846402555704\n",
      "  batch 70 loss: 0.7117766324430704\n",
      "  batch 80 loss: 0.9684769364073873\n",
      "  batch 90 loss: 0.9296734249219298\n",
      "  batch 100 loss: 0.9799080039840191\n",
      "  batch 110 loss: 1.1212984557263552\n",
      "  batch 120 loss: 1.0111071503721178\n",
      "  batch 130 loss: 0.9640644338913262\n",
      "  batch 140 loss: 0.9137003288138658\n",
      "  batch 150 loss: 0.8421812424436211\n",
      "  batch 160 loss: 0.9488111308775842\n",
      "  batch 170 loss: 0.9209252048283816\n",
      "  batch 180 loss: 1.0032822150737046\n",
      "  batch 190 loss: 0.7956566037610173\n",
      "LOSS train 0.7956566037610173 valid 2.292371995532169\n",
      "EPOCH 129:\n",
      "  batch 10 loss: 1.2283073717728257\n",
      "  batch 20 loss: 0.8173427427187562\n",
      "  batch 30 loss: 1.1654988434165716\n",
      "  batch 40 loss: 0.96738869138062\n",
      "  batch 50 loss: 0.807528221886605\n",
      "  batch 60 loss: 0.8251307936385274\n",
      "  batch 70 loss: 1.0206798169761897\n",
      "  batch 80 loss: 0.8128214857541025\n",
      "  batch 90 loss: 0.7090840097516775\n",
      "  batch 100 loss: 0.9701236205175519\n",
      "  batch 110 loss: 0.821189471706748\n",
      "  batch 120 loss: 0.8739248768426477\n",
      "  batch 130 loss: 1.1077369321137667\n",
      "  batch 140 loss: 0.7996543198823929\n",
      "  batch 150 loss: 0.7649049343541264\n",
      "  batch 160 loss: 0.8663160077761859\n",
      "  batch 170 loss: 0.943754094094038\n",
      "  batch 180 loss: 1.0514795671217143\n",
      "  batch 190 loss: 0.7994177794549614\n",
      "LOSS train 0.7994177794549614 valid 1.11168191044663\n",
      "EPOCH 130:\n",
      "  batch 10 loss: 0.9408458985388279\n",
      "  batch 20 loss: 0.8463885447010397\n",
      "  batch 30 loss: 0.8392412283457815\n",
      "  batch 40 loss: 0.7627554008737206\n",
      "  batch 50 loss: 1.3952668215613813\n",
      "  batch 60 loss: 0.8243608112446964\n",
      "  batch 70 loss: 1.0153599458746612\n",
      "  batch 80 loss: 0.9273268707096577\n",
      "  batch 90 loss: 0.8009053646586836\n",
      "  batch 100 loss: 0.8719583932310343\n",
      "  batch 110 loss: 0.8205150903202594\n",
      "  batch 120 loss: 0.8335361013188958\n",
      "  batch 130 loss: 0.7851893814280629\n",
      "  batch 140 loss: 0.9192089010030031\n",
      "  batch 150 loss: 0.8344667355995625\n",
      "  batch 160 loss: 1.0171651488170028\n",
      "  batch 170 loss: 0.9151680518873035\n",
      "  batch 180 loss: 0.9295360086951405\n",
      "  batch 190 loss: 0.5810003086924553\n",
      "LOSS train 0.5810003086924553 valid 0.7223973436436305\n",
      "EPOCH 131:\n",
      "  batch 10 loss: 0.9663475023582577\n",
      "  batch 20 loss: 0.8976946528069675\n",
      "  batch 30 loss: 0.9540206328034401\n",
      "  batch 40 loss: 0.7230848984327167\n",
      "  batch 50 loss: 1.4217182882595807\n",
      "  batch 60 loss: 0.7740561534184962\n",
      "  batch 70 loss: 0.8653435672167689\n",
      "  batch 80 loss: 0.6523312978446484\n",
      "  batch 90 loss: 0.9963671875419096\n",
      "  batch 100 loss: 0.8216468566097319\n",
      "  batch 110 loss: 1.0161593389697372\n",
      "  batch 120 loss: 0.8111278956755996\n",
      "  batch 130 loss: 1.0341279339045286\n",
      "  batch 140 loss: 1.0622924994677305\n",
      "  batch 150 loss: 0.7579463947564363\n",
      "  batch 160 loss: 0.6956298703327775\n",
      "  batch 170 loss: 0.8816842934116721\n",
      "  batch 180 loss: 0.9940558291040361\n",
      "  batch 190 loss: 0.8080558648332954\n",
      "LOSS train 0.8080558648332954 valid 1.010611550288442\n",
      "EPOCH 132:\n",
      "  batch 10 loss: 0.6222088166512549\n",
      "  batch 20 loss: 0.9088681405410171\n",
      "  batch 30 loss: 0.9936219930648804\n",
      "  batch 40 loss: 0.8737916143611073\n",
      "  batch 50 loss: 0.7872239461168646\n",
      "  batch 60 loss: 0.6401028085965663\n",
      "  batch 70 loss: 0.9889654982835054\n",
      "  batch 80 loss: 0.6553931602509693\n",
      "  batch 90 loss: 0.6487123689148575\n",
      "  batch 100 loss: 0.842737877368927\n",
      "  batch 110 loss: 0.9942400280851871\n",
      "  batch 120 loss: 0.9427357541397214\n",
      "  batch 130 loss: 0.9195969463326037\n",
      "  batch 140 loss: 0.8110361495986581\n",
      "  batch 150 loss: 0.929320703074336\n",
      "  batch 160 loss: 0.9295289427042007\n",
      "  batch 170 loss: 0.9556221231818199\n",
      "  batch 180 loss: 0.9344287018291653\n",
      "  batch 190 loss: 0.7956487280316651\n",
      "LOSS train 0.7956487280316651 valid 1.2340428781432984\n",
      "EPOCH 133:\n",
      "  batch 10 loss: 0.6474524319171906\n",
      "  batch 20 loss: 0.8820338574238121\n",
      "  batch 30 loss: 1.0646485603414475\n",
      "  batch 40 loss: 1.3147764556109904\n",
      "  batch 50 loss: 0.7652299694716931\n",
      "  batch 60 loss: 0.8268436740618199\n",
      "  batch 70 loss: 0.9635267017409206\n",
      "  batch 80 loss: 0.6924345112405718\n",
      "  batch 90 loss: 0.8436835237545892\n",
      "  batch 100 loss: 0.7778710721293465\n",
      "  batch 110 loss: 0.7544899922329933\n",
      "  batch 120 loss: 0.6221607157029212\n",
      "  batch 130 loss: 0.7410005520563573\n",
      "  batch 140 loss: 0.76109857885167\n",
      "  batch 150 loss: 0.6251086774282157\n",
      "  batch 160 loss: 0.877616116963327\n",
      "  batch 170 loss: 0.9656769967637956\n",
      "  batch 180 loss: 0.912010625586845\n",
      "  batch 190 loss: 0.7280764807015657\n",
      "LOSS train 0.7280764807015657 valid 1.7346666673950564\n",
      "EPOCH 134:\n",
      "  batch 10 loss: 1.1645918211899697\n",
      "  batch 20 loss: 0.8920473629143089\n",
      "  batch 30 loss: 0.8878540374338627\n",
      "  batch 40 loss: 0.9131223216652871\n",
      "  batch 50 loss: 0.8065872943028808\n",
      "  batch 60 loss: 0.7239625757094472\n",
      "  batch 70 loss: 0.9914435497950762\n",
      "  batch 80 loss: 0.8783127067610621\n",
      "  batch 90 loss: 0.8377255470026285\n",
      "  batch 100 loss: 0.9358225964009762\n",
      "  batch 110 loss: 0.8638166116084903\n",
      "  batch 120 loss: 0.7722789336228744\n",
      "  batch 130 loss: 0.901021256390959\n",
      "  batch 140 loss: 0.6349950614385307\n",
      "  batch 150 loss: 0.681616470310837\n",
      "  batch 160 loss: 0.8953850088641048\n",
      "  batch 170 loss: 0.4992107031866908\n",
      "  batch 180 loss: 1.1322006023023277\n",
      "  batch 190 loss: 0.5216835087398067\n",
      "LOSS train 0.5216835087398067 valid 1.9155703622680635\n",
      "EPOCH 135:\n",
      "  batch 10 loss: 1.0600202527828515\n",
      "  batch 20 loss: 0.8330407968722284\n",
      "  batch 30 loss: 0.9167026462033391\n",
      "  batch 40 loss: 0.7075641714967787\n",
      "  batch 50 loss: 0.8422790407203138\n",
      "  batch 60 loss: 0.6315155128482729\n",
      "  batch 70 loss: 0.6778896168339997\n",
      "  batch 80 loss: 0.6379555389285088\n",
      "  batch 90 loss: 0.7285046982113272\n",
      "  batch 100 loss: 0.649392609950155\n",
      "  batch 110 loss: 0.877526922780089\n",
      "  batch 120 loss: 0.6846752678509802\n",
      "  batch 130 loss: 0.8633523768745363\n",
      "  batch 140 loss: 0.9749701288761571\n",
      "  batch 150 loss: 0.706512208841741\n",
      "  batch 160 loss: 0.9791160709224641\n",
      "  batch 170 loss: 0.7038243879098445\n",
      "  batch 180 loss: 0.7050703658722342\n",
      "  batch 190 loss: 0.8595974538475275\n",
      "LOSS train 0.8595974538475275 valid 1.5209240715962669\n",
      "EPOCH 136:\n",
      "  batch 10 loss: 0.8721602794248611\n",
      "  batch 20 loss: 0.7311379697872326\n",
      "  batch 30 loss: 0.7873345727333799\n",
      "  batch 40 loss: 0.7697489125654101\n",
      "  batch 50 loss: 0.8510890694335103\n",
      "  batch 60 loss: 0.8289324457757175\n",
      "  batch 70 loss: 0.807802351238206\n",
      "  batch 80 loss: 0.7346691121812909\n",
      "  batch 90 loss: 0.6122879139147699\n",
      "  batch 100 loss: 1.0198099190369248\n",
      "  batch 110 loss: 0.8281742217950523\n",
      "  batch 120 loss: 0.7904840203002095\n",
      "  batch 130 loss: 0.6423050556331873\n",
      "  batch 140 loss: 0.7103649831842631\n",
      "  batch 150 loss: 0.7852091628592461\n",
      "  batch 160 loss: 0.9998496566899121\n",
      "  batch 170 loss: 0.671119666704908\n",
      "  batch 180 loss: 0.6771276242565364\n",
      "  batch 190 loss: 0.8187820602674037\n",
      "LOSS train 0.8187820602674037 valid 1.6847441419376992\n",
      "EPOCH 137:\n",
      "  batch 10 loss: 0.7478553984430618\n",
      "  batch 20 loss: 0.6990387803409248\n",
      "  batch 30 loss: 0.6432283553062007\n",
      "  batch 40 loss: 0.6249205218395218\n",
      "  batch 50 loss: 0.6521296768449247\n",
      "  batch 60 loss: 0.7360074046067894\n",
      "  batch 70 loss: 0.9645165166817605\n",
      "  batch 80 loss: 0.8361183741828426\n",
      "  batch 90 loss: 0.40953634371981024\n",
      "  batch 100 loss: 0.7978348326403648\n",
      "  batch 110 loss: 0.9835225043003447\n",
      "  batch 120 loss: 0.6816306021763012\n",
      "  batch 130 loss: 0.9553208790253848\n",
      "  batch 140 loss: 0.7330448299180716\n",
      "  batch 150 loss: 0.7405685229226947\n",
      "  batch 160 loss: 1.3867108598351479\n",
      "  batch 170 loss: 0.7986264468170703\n",
      "  batch 180 loss: 0.7068042090162635\n",
      "  batch 190 loss: 0.6904014543630183\n",
      "LOSS train 0.6904014543630183 valid 0.7007300481976321\n",
      "EPOCH 138:\n",
      "  batch 10 loss: 0.6471142885740846\n",
      "  batch 20 loss: 0.635066837258637\n",
      "  batch 30 loss: 0.6862922137603163\n",
      "  batch 40 loss: 0.8440085750073194\n",
      "  batch 50 loss: 0.8390769631136209\n",
      "  batch 60 loss: 0.7181351478211582\n",
      "  batch 70 loss: 0.7391185707878322\n",
      "  batch 80 loss: 0.7580761478282512\n",
      "  batch 90 loss: 0.9240443029440939\n",
      "  batch 100 loss: 0.8012596446555108\n",
      "  batch 110 loss: 0.8711662586778403\n",
      "  batch 120 loss: 0.6516744936350733\n",
      "  batch 130 loss: 0.6967519353609533\n",
      "  batch 140 loss: 0.8452500562183559\n",
      "  batch 150 loss: 0.7423824816476554\n",
      "  batch 160 loss: 0.5979955439921468\n",
      "  batch 170 loss: 0.6566547606140375\n",
      "  batch 180 loss: 0.5656797880074009\n",
      "  batch 190 loss: 0.7499390118289739\n",
      "LOSS train 0.7499390118289739 valid 1.7573093354033353\n",
      "EPOCH 139:\n",
      "  batch 10 loss: 0.7921993242809549\n",
      "  batch 20 loss: 0.5474811398424209\n",
      "  batch 30 loss: 0.9482994745485485\n",
      "  batch 40 loss: 0.8140671307221055\n",
      "  batch 50 loss: 0.6403398173861206\n",
      "  batch 60 loss: 0.6776933680288494\n",
      "  batch 70 loss: 0.7115933492779731\n",
      "  batch 80 loss: 0.722276920848526\n",
      "  batch 90 loss: 0.8469050447456539\n",
      "  batch 100 loss: 0.7635002404917032\n",
      "  batch 110 loss: 0.7292043505236506\n",
      "  batch 120 loss: 0.765834036655724\n",
      "  batch 130 loss: 0.8482675137463958\n",
      "  batch 140 loss: 0.7344084553653374\n",
      "  batch 150 loss: 0.702657341491431\n",
      "  batch 160 loss: 0.8584673069417477\n",
      "  batch 170 loss: 0.6345567789394408\n",
      "  batch 180 loss: 0.6387102937325835\n",
      "  batch 190 loss: 0.7078946307301521\n",
      "LOSS train 0.7078946307301521 valid 1.655326779897456\n",
      "EPOCH 140:\n",
      "  batch 10 loss: 0.9453155554714613\n",
      "  batch 20 loss: 0.5934581745415926\n",
      "  batch 30 loss: 0.9489657310768962\n",
      "  batch 40 loss: 0.7463267083512619\n",
      "  batch 50 loss: 0.7730373740196228\n",
      "  batch 60 loss: 0.5381548529025167\n",
      "  batch 70 loss: 0.7210985334590078\n",
      "  batch 80 loss: 0.6770261931698769\n",
      "  batch 90 loss: 0.7064398715738207\n",
      "  batch 100 loss: 0.6758842634269968\n",
      "  batch 110 loss: 0.725568697717972\n",
      "  batch 120 loss: 0.8953971268609167\n",
      "  batch 130 loss: 0.6750848343130201\n",
      "  batch 140 loss: 0.5751061737537384\n",
      "  batch 150 loss: 0.794502244819887\n",
      "  batch 160 loss: 0.7090135926380754\n",
      "  batch 170 loss: 0.8580427763517946\n",
      "  batch 180 loss: 0.5730481683043763\n",
      "  batch 190 loss: 0.7436258398927749\n",
      "LOSS train 0.7436258398927749 valid 0.8827069261863542\n",
      "EPOCH 141:\n",
      "  batch 10 loss: 0.780827785958536\n",
      "  batch 20 loss: 0.7430748035083525\n",
      "  batch 30 loss: 0.5448841860983521\n",
      "  batch 40 loss: 0.5743020435096696\n",
      "  batch 50 loss: 0.7796649745490868\n",
      "  batch 60 loss: 0.7531889407895506\n",
      "  batch 70 loss: 0.6058010164648294\n",
      "  batch 80 loss: 0.7952117016073317\n",
      "  batch 90 loss: 0.6319468214642256\n",
      "  batch 100 loss: 0.5085223001660779\n",
      "  batch 110 loss: 0.800024674530141\n",
      "  batch 120 loss: 0.8715346210869029\n",
      "  batch 130 loss: 0.7433611535001546\n",
      "  batch 140 loss: 0.5302970079705119\n",
      "  batch 150 loss: 0.6310020172968507\n",
      "  batch 160 loss: 0.7109167668968439\n",
      "  batch 170 loss: 1.0229448514757677\n",
      "  batch 180 loss: 1.0347759319585748\n",
      "  batch 190 loss: 0.8852355623617768\n",
      "LOSS train 0.8852355623617768 valid 2.3504849573394333\n",
      "EPOCH 142:\n",
      "  batch 10 loss: 1.5808201819192618\n",
      "  batch 20 loss: 0.8243036409956404\n",
      "  batch 30 loss: 0.7004994778893888\n",
      "  batch 40 loss: 0.6950966262258589\n",
      "  batch 50 loss: 0.6058367399964482\n",
      "  batch 60 loss: 0.7264660010114312\n",
      "  batch 70 loss: 0.6680341427680105\n",
      "  batch 80 loss: 0.6447415302973241\n",
      "  batch 90 loss: 0.6520260617136955\n",
      "  batch 100 loss: 0.7197725842241198\n",
      "  batch 110 loss: 0.42682161264820023\n",
      "  batch 120 loss: 0.6245243266690522\n",
      "  batch 130 loss: 0.9310081798466854\n",
      "  batch 140 loss: 0.7414449969597626\n",
      "  batch 150 loss: 0.6725773361045867\n",
      "  batch 160 loss: 0.6094025654019788\n",
      "  batch 170 loss: 0.7834789292886853\n",
      "  batch 180 loss: 0.6229228465585038\n",
      "  batch 190 loss: 0.718980860360898\n",
      "LOSS train 0.718980860360898 valid 1.1809269935400106\n",
      "EPOCH 143:\n",
      "  batch 10 loss: 0.9096443662187085\n",
      "  batch 20 loss: 0.8459295974695124\n",
      "  batch 30 loss: 0.7392206376418471\n",
      "  batch 40 loss: 0.46562505681067706\n",
      "  batch 50 loss: 0.6415636065183208\n",
      "  batch 60 loss: 0.6061592395650223\n",
      "  batch 70 loss: 0.6888818139326759\n",
      "  batch 80 loss: 0.9307316004764289\n",
      "  batch 90 loss: 0.6650340711232274\n",
      "  batch 100 loss: 0.7829535740893334\n",
      "  batch 110 loss: 0.6212498292792589\n",
      "  batch 120 loss: 0.8514828850515187\n",
      "  batch 130 loss: 0.6642133371206\n",
      "  batch 140 loss: 0.4732089603319764\n",
      "  batch 150 loss: 0.6544376184698194\n",
      "  batch 160 loss: 0.6140065344981849\n",
      "  batch 170 loss: 0.8006603724788874\n",
      "  batch 180 loss: 0.7564723713556305\n",
      "  batch 190 loss: 0.4785476516233757\n",
      "LOSS train 0.4785476516233757 valid 0.6185270475652009\n",
      "EPOCH 144:\n",
      "  batch 10 loss: 0.7251567282248288\n",
      "  batch 20 loss: 0.9161069989204407\n",
      "  batch 30 loss: 0.9216056908597239\n",
      "  batch 40 loss: 0.6252266706025693\n",
      "  batch 50 loss: 0.7332475605187938\n",
      "  batch 60 loss: 0.6338328565936535\n",
      "  batch 70 loss: 0.738724562257994\n",
      "  batch 80 loss: 0.7570038615725935\n",
      "  batch 90 loss: 0.5226790472399443\n",
      "  batch 100 loss: 0.6910170293180272\n",
      "  batch 110 loss: 0.9390480273170396\n",
      "  batch 120 loss: 0.6054300604620948\n",
      "  batch 130 loss: 0.4849194270092994\n",
      "  batch 140 loss: 0.7357405779883266\n",
      "  batch 150 loss: 0.5477574782678858\n",
      "  batch 160 loss: 0.6241652512690052\n",
      "  batch 170 loss: 0.67131068687886\n",
      "  batch 180 loss: 1.113987429207191\n",
      "  batch 190 loss: 0.734693827969022\n",
      "LOSS train 0.734693827969022 valid 0.7429748935511526\n",
      "EPOCH 145:\n",
      "  batch 10 loss: 0.7628249077126383\n",
      "  batch 20 loss: 0.7699659266043455\n",
      "  batch 30 loss: 0.971790233918\n",
      "  batch 40 loss: 0.7318147615762427\n",
      "  batch 50 loss: 0.7227710248436778\n",
      "  batch 60 loss: 1.1932238979963585\n",
      "  batch 70 loss: 0.6887894527520985\n",
      "  batch 80 loss: 0.6424015645403415\n",
      "  batch 90 loss: 0.7064716369146481\n",
      "  batch 100 loss: 0.7115385473938659\n",
      "  batch 110 loss: 0.4687466307543218\n",
      "  batch 120 loss: 0.7808097629342228\n",
      "  batch 130 loss: 0.6266380979679524\n",
      "  batch 140 loss: 0.548839592654258\n",
      "  batch 150 loss: 0.5291727855335921\n",
      "  batch 160 loss: 0.6464723659679293\n",
      "  batch 170 loss: 0.9176064155995846\n",
      "  batch 180 loss: 0.6230814637150616\n",
      "  batch 190 loss: 0.5055894447374157\n",
      "LOSS train 0.5055894447374157 valid 0.950429634958243\n",
      "EPOCH 146:\n",
      "  batch 10 loss: 0.6987053022021428\n",
      "  batch 20 loss: 0.6797604942694306\n",
      "  batch 30 loss: 0.6826557947555557\n",
      "  batch 40 loss: 0.6595747786574065\n",
      "  batch 50 loss: 0.5863432859070599\n",
      "  batch 60 loss: 0.5533207232248969\n",
      "  batch 70 loss: 0.6941864752094261\n",
      "  batch 80 loss: 0.7129733334295452\n",
      "  batch 90 loss: 0.8098314296454191\n",
      "  batch 100 loss: 0.7268584356177599\n",
      "  batch 110 loss: 0.8156402059132233\n",
      "  batch 120 loss: 0.7033283850178123\n",
      "  batch 130 loss: 0.6070118823321536\n",
      "  batch 140 loss: 0.7381805798504502\n",
      "  batch 150 loss: 0.6627791104838252\n",
      "  batch 160 loss: 0.46179878516122697\n",
      "  batch 170 loss: 0.6634633107110858\n",
      "  batch 180 loss: 0.7463105364702642\n",
      "  batch 190 loss: 0.8016180590726435\n",
      "LOSS train 0.8016180590726435 valid 1.0531881429286352\n",
      "EPOCH 147:\n",
      "  batch 10 loss: 0.7848993991501629\n",
      "  batch 20 loss: 0.5528991899453104\n",
      "  batch 30 loss: 0.4917503658682108\n",
      "  batch 40 loss: 0.6389159372076392\n",
      "  batch 50 loss: 0.5820208575343713\n",
      "  batch 60 loss: 0.46775945329573004\n",
      "  batch 70 loss: 0.9708097957773134\n",
      "  batch 80 loss: 0.639199276920408\n",
      "  batch 90 loss: 0.7562570257345215\n",
      "  batch 100 loss: 0.7491002134745941\n",
      "  batch 110 loss: 0.7845548597164452\n",
      "  batch 120 loss: 0.6282574514159933\n",
      "  batch 130 loss: 0.8564988804981113\n",
      "  batch 140 loss: 0.658149047603365\n",
      "  batch 150 loss: 0.663463422562927\n",
      "  batch 160 loss: 0.8266570317093282\n",
      "  batch 170 loss: 0.5600153707200661\n",
      "  batch 180 loss: 0.8203011209145188\n",
      "  batch 190 loss: 0.6771322218293789\n",
      "LOSS train 0.6771322218293789 valid 0.5524416881708953\n",
      "EPOCH 148:\n",
      "  batch 10 loss: 0.6432399115525186\n",
      "  batch 20 loss: 0.6825267215259373\n",
      "  batch 30 loss: 0.5094946775585413\n",
      "  batch 40 loss: 0.498528464557603\n",
      "  batch 50 loss: 0.6540964523795992\n",
      "  batch 60 loss: 0.616578578390181\n",
      "  batch 70 loss: 0.8894014678429812\n",
      "  batch 80 loss: 0.6902738984208554\n",
      "  batch 90 loss: 0.7629529335768893\n",
      "  batch 100 loss: 0.6344344421755522\n",
      "  batch 110 loss: 0.40484473143005745\n",
      "  batch 120 loss: 0.6134297557058744\n",
      "  batch 130 loss: 0.8433142565190792\n",
      "  batch 140 loss: 0.7058364680036903\n",
      "  batch 150 loss: 0.9228678919840604\n",
      "  batch 160 loss: 0.5738140572793782\n",
      "  batch 170 loss: 0.5794728040753399\n",
      "  batch 180 loss: 0.8315587307326495\n",
      "  batch 190 loss: 0.8155668440274895\n",
      "LOSS train 0.8155668440274895 valid 1.1329921012362227\n",
      "EPOCH 149:\n",
      "  batch 10 loss: 0.5306253253249451\n",
      "  batch 20 loss: 0.759091396140866\n",
      "  batch 30 loss: 0.6100572576047852\n",
      "  batch 40 loss: 0.7969237391254864\n",
      "  batch 50 loss: 0.49136620075441895\n",
      "  batch 60 loss: 0.7369436141103506\n",
      "  batch 70 loss: 0.46309106117114424\n",
      "  batch 80 loss: 0.523973366105929\n",
      "  batch 90 loss: 0.8190821966156363\n",
      "  batch 100 loss: 0.8743757694028318\n",
      "  batch 110 loss: 0.37006545342737807\n",
      "  batch 120 loss: 0.6120336396154016\n",
      "  batch 130 loss: 0.7827619317453355\n",
      "  batch 140 loss: 0.648215217795223\n",
      "  batch 150 loss: 0.5196255959919653\n",
      "  batch 160 loss: 0.5483223674120381\n",
      "  batch 170 loss: 0.9446244338992983\n",
      "  batch 180 loss: 0.9674163583898917\n",
      "  batch 190 loss: 0.9119879069505259\n",
      "LOSS train 0.9119879069505259 valid 0.5148401484296371\n",
      "EPOCH 150:\n",
      "  batch 10 loss: 0.5192836542613805\n",
      "  batch 20 loss: 0.626815830450505\n",
      "  batch 30 loss: 0.7904113724129275\n",
      "  batch 40 loss: 0.5330710332142189\n",
      "  batch 50 loss: 0.4004823827184737\n",
      "  batch 60 loss: 0.5514319408219308\n",
      "  batch 70 loss: 0.8049705772660672\n",
      "  batch 80 loss: 0.7710974873509258\n",
      "  batch 90 loss: 0.5004968194290995\n",
      "  batch 100 loss: 0.556814486021176\n",
      "  batch 110 loss: 0.900206673471257\n",
      "  batch 120 loss: 0.60969640747644\n",
      "  batch 130 loss: 0.5643292281078175\n",
      "  batch 140 loss: 0.5687585227424279\n",
      "  batch 150 loss: 1.1589154762914404\n",
      "  batch 160 loss: 0.619589859538246\n",
      "  batch 170 loss: 0.7315065063303337\n",
      "  batch 180 loss: 0.49259405651828275\n",
      "  batch 190 loss: 0.6891512248374057\n",
      "LOSS train 0.6891512248374057 valid 0.61661542159541\n",
      "EPOCH 151:\n",
      "  batch 10 loss: 0.6152896541403606\n",
      "  batch 20 loss: 0.4763127972371876\n",
      "  batch 30 loss: 0.6948783823288978\n",
      "  batch 40 loss: 0.9375840160995722\n",
      "  batch 50 loss: 0.43110780654242264\n",
      "  batch 60 loss: 0.5765444035991095\n",
      "  batch 70 loss: 0.9596510589006357\n",
      "  batch 80 loss: 0.7187944770790636\n",
      "  batch 90 loss: 0.6228676809114404\n",
      "  batch 100 loss: 0.5516944391187281\n",
      "  batch 110 loss: 0.6822476550238207\n",
      "  batch 120 loss: 0.47202241854975\n",
      "  batch 130 loss: 0.4482865299039986\n",
      "  batch 140 loss: 0.6556454184697941\n",
      "  batch 150 loss: 0.7197329185553827\n",
      "  batch 160 loss: 0.9574403365608305\n",
      "  batch 170 loss: 0.5679435558849946\n",
      "  batch 180 loss: 0.998166631301865\n",
      "  batch 190 loss: 0.6496546195121482\n",
      "LOSS train 0.6496546195121482 valid 1.0922782973637088\n",
      "EPOCH 152:\n",
      "  batch 10 loss: 1.0633347084745766\n",
      "  batch 20 loss: 0.623470689158421\n",
      "  batch 30 loss: 0.6197104250546545\n",
      "  batch 40 loss: 0.6305608100723475\n",
      "  batch 50 loss: 0.7356547291623429\n",
      "  batch 60 loss: 0.6581937342183665\n",
      "  batch 70 loss: 0.5249022913631052\n",
      "  batch 80 loss: 0.5536015832098201\n",
      "  batch 90 loss: 0.9193197439424694\n",
      "  batch 100 loss: 0.6242978242225945\n",
      "  batch 110 loss: 0.42541525423293935\n",
      "  batch 120 loss: 0.8352265195455402\n",
      "  batch 130 loss: 0.5624509602086618\n",
      "  batch 140 loss: 0.5265841964632273\n",
      "  batch 150 loss: 0.9258628168725409\n",
      "  batch 160 loss: 0.5965611289837398\n",
      "  batch 170 loss: 0.845117734442465\n",
      "  batch 180 loss: 0.6858714711852372\n",
      "  batch 190 loss: 0.5957289765356109\n",
      "LOSS train 0.5957289765356109 valid 1.4754996594065466\n",
      "EPOCH 153:\n",
      "  batch 10 loss: 0.8549907875945791\n",
      "  batch 20 loss: 0.4750215016189031\n",
      "  batch 30 loss: 0.46949561790097505\n",
      "  batch 40 loss: 0.39987890811171384\n",
      "  batch 50 loss: 0.5552964807313401\n",
      "  batch 60 loss: 0.677109335095156\n",
      "  batch 70 loss: 0.7042289021657779\n",
      "  batch 80 loss: 0.6332968394970522\n",
      "  batch 90 loss: 0.7168466207454912\n",
      "  batch 100 loss: 0.5934152446046937\n",
      "  batch 110 loss: 0.6745503344689496\n",
      "  batch 120 loss: 0.6270038137678057\n",
      "  batch 130 loss: 0.5413148830877617\n",
      "  batch 140 loss: 0.4760447283508256\n",
      "  batch 150 loss: 0.7220337582984939\n",
      "  batch 160 loss: 0.7030995190667454\n",
      "  batch 170 loss: 0.7621034900425002\n",
      "  batch 180 loss: 0.6868697723606602\n",
      "  batch 190 loss: 0.6557115632109344\n",
      "LOSS train 0.6557115632109344 valid 0.6559372093925515\n",
      "EPOCH 154:\n",
      "  batch 10 loss: 0.86949774089735\n",
      "  batch 20 loss: 0.48029931942000986\n",
      "  batch 30 loss: 0.4811500249430537\n",
      "  batch 40 loss: 0.29867038477677854\n",
      "  batch 50 loss: 0.46037851576693356\n",
      "  batch 60 loss: 0.8348721858579665\n",
      "  batch 70 loss: 0.4374766283493955\n",
      "  batch 80 loss: 0.5647551424801349\n",
      "  batch 90 loss: 0.6919457794167101\n",
      "  batch 100 loss: 0.38495299956994133\n",
      "  batch 110 loss: 0.41763821737840773\n",
      "  batch 120 loss: 0.3949918766040355\n",
      "  batch 130 loss: 0.7541675634216517\n",
      "  batch 140 loss: 0.684815248142695\n",
      "  batch 150 loss: 0.8020274649141357\n",
      "  batch 160 loss: 0.6113074097316712\n",
      "  batch 170 loss: 0.5360213705804199\n",
      "  batch 180 loss: 0.51196996034123\n",
      "  batch 190 loss: 0.6984196458011865\n",
      "LOSS train 0.6984196458011865 valid 2.545607630593272\n",
      "EPOCH 155:\n",
      "  batch 10 loss: 1.3183427346637473\n",
      "  batch 20 loss: 0.7451279907487333\n",
      "  batch 30 loss: 0.5873311542905867\n",
      "  batch 40 loss: 0.7356858121813274\n",
      "  batch 50 loss: 0.5019455996807665\n",
      "  batch 60 loss: 0.4511248759459704\n",
      "  batch 70 loss: 0.7619020890444517\n",
      "  batch 80 loss: 0.5061451833578758\n",
      "  batch 90 loss: 0.42167382879415527\n",
      "  batch 100 loss: 0.8276379152433947\n",
      "  batch 110 loss: 0.523679576232098\n",
      "  batch 120 loss: 0.4085116126923822\n",
      "  batch 130 loss: 0.6074444076512009\n",
      "  batch 140 loss: 0.6168390322010964\n",
      "  batch 150 loss: 0.7363778975326568\n",
      "  batch 160 loss: 0.5581998175242916\n",
      "  batch 170 loss: 0.46315885183867067\n",
      "  batch 180 loss: 0.45504244677722455\n",
      "  batch 190 loss: 0.37757142232730984\n",
      "LOSS train 0.37757142232730984 valid 2.3611329386297326\n",
      "EPOCH 156:\n",
      "  batch 10 loss: 0.9239034789497964\n",
      "  batch 20 loss: 0.7110398972057738\n",
      "  batch 30 loss: 0.5012296258704737\n",
      "  batch 40 loss: 0.4679344512056559\n",
      "  batch 50 loss: 0.5022619995987043\n",
      "  batch 60 loss: 0.48819463152904063\n",
      "  batch 70 loss: 0.9964119992451742\n",
      "  batch 80 loss: 0.7389930150762666\n",
      "  batch 90 loss: 0.43938360302709045\n",
      "  batch 100 loss: 0.42998076751246117\n",
      "  batch 110 loss: 0.47867456657113505\n",
      "  batch 120 loss: 0.7360053093638271\n",
      "  batch 130 loss: 0.8270985435869079\n",
      "  batch 140 loss: 0.7834884755429812\n",
      "  batch 150 loss: 0.5582303396367934\n",
      "  batch 160 loss: 0.5353799159405753\n",
      "  batch 170 loss: 0.5906698286766187\n",
      "  batch 180 loss: 0.5156137804500759\n",
      "  batch 190 loss: 0.5829450785648078\n",
      "LOSS train 0.5829450785648078 valid 1.0538790172998769\n",
      "EPOCH 157:\n",
      "  batch 10 loss: 0.500443042156985\n",
      "  batch 20 loss: 0.32342249718494714\n",
      "  batch 30 loss: 0.5741706129396335\n",
      "  batch 40 loss: 0.5068202083930373\n",
      "  batch 50 loss: 0.5107067877426743\n",
      "  batch 60 loss: 0.4485258347762283\n",
      "  batch 70 loss: 0.713970772526227\n",
      "  batch 80 loss: 0.4004961806116626\n",
      "  batch 90 loss: 0.8347027268726379\n",
      "  batch 100 loss: 0.9487276053521783\n",
      "  batch 110 loss: 0.5502409460954368\n",
      "  batch 120 loss: 0.48883366766385733\n",
      "  batch 130 loss: 0.554686841450166\n",
      "  batch 140 loss: 0.45498214206891135\n",
      "  batch 150 loss: 0.46237195779103785\n",
      "  batch 160 loss: 0.6250128025654703\n",
      "  batch 170 loss: 0.7295694111846387\n",
      "  batch 180 loss: 0.6897871986031532\n",
      "  batch 190 loss: 0.6392132102977485\n",
      "LOSS train 0.6392132102977485 valid 1.639654321331107\n",
      "EPOCH 158:\n",
      "  batch 10 loss: 0.5802034112799447\n",
      "  batch 20 loss: 0.6357736891950481\n",
      "  batch 30 loss: 0.431862439471297\n",
      "  batch 40 loss: 0.5699557495419867\n",
      "  batch 50 loss: 0.7201200877840165\n",
      "  batch 60 loss: 0.6815691380063071\n",
      "  batch 70 loss: 0.5141159611521289\n",
      "  batch 80 loss: 0.453861212573247\n",
      "  batch 90 loss: 0.6432328172028065\n",
      "  batch 100 loss: 0.7397394066734705\n",
      "  batch 110 loss: 0.8482953322352842\n",
      "  batch 120 loss: 0.4918597684125416\n",
      "  batch 130 loss: 0.7221635116729885\n",
      "  batch 140 loss: 0.3552847036276944\n",
      "  batch 150 loss: 0.5091851143864915\n",
      "  batch 160 loss: 0.838878793339245\n",
      "  batch 170 loss: 0.39876379244960847\n",
      "  batch 180 loss: 0.8378798785211984\n",
      "  batch 190 loss: 1.3159413313725963\n",
      "LOSS train 1.3159413313725963 valid 2.856841385289161\n",
      "EPOCH 159:\n",
      "  batch 10 loss: 1.1872745419852435\n",
      "  batch 20 loss: 0.7344308412168175\n",
      "  batch 30 loss: 0.8999337601475418\n",
      "  batch 40 loss: 0.5997915029060096\n",
      "  batch 50 loss: 0.6325939472066239\n",
      "  batch 60 loss: 0.3190916781546548\n",
      "  batch 70 loss: 0.35186882589478047\n",
      "  batch 80 loss: 0.5162537817377597\n",
      "  batch 90 loss: 0.5287194614298641\n",
      "  batch 100 loss: 0.6826124103274196\n",
      "  batch 110 loss: 0.838004897418432\n",
      "  batch 120 loss: 0.5302237617783249\n",
      "  batch 130 loss: 0.6025298179942183\n",
      "  batch 140 loss: 0.575111968559213\n",
      "  batch 150 loss: 0.4700602991972119\n",
      "  batch 160 loss: 0.511789545416832\n",
      "  batch 170 loss: 0.6239069193601608\n",
      "  batch 180 loss: 0.5293833507341332\n",
      "  batch 190 loss: 0.43412351809674876\n",
      "LOSS train 0.43412351809674876 valid 0.5331874593184927\n",
      "EPOCH 160:\n",
      "  batch 10 loss: 0.5400755382433999\n",
      "  batch 20 loss: 0.7744858031161129\n",
      "  batch 30 loss: 0.5179024128825404\n",
      "  batch 40 loss: 0.5156003529205918\n",
      "  batch 50 loss: 0.4980128552997485\n",
      "  batch 60 loss: 0.5085549464274663\n",
      "  batch 70 loss: 0.528898369311355\n",
      "  batch 80 loss: 0.6304802282014862\n",
      "  batch 90 loss: 0.5026225922512821\n",
      "  batch 100 loss: 0.5436870522098616\n",
      "  batch 110 loss: 0.8013780895096716\n",
      "  batch 120 loss: 0.733778717298992\n",
      "  batch 130 loss: 0.6791316421004012\n",
      "  batch 140 loss: 0.6003614621702582\n",
      "  batch 150 loss: 0.6150887186173349\n",
      "  batch 160 loss: 0.4972874344792217\n",
      "  batch 170 loss: 0.917611315153772\n",
      "  batch 180 loss: 0.688552045437973\n",
      "  batch 190 loss: 0.7678431157371961\n",
      "LOSS train 0.7678431157371961 valid 0.43316314592569444\n",
      "EPOCH 161:\n",
      "  batch 10 loss: 0.5065883097006008\n",
      "  batch 20 loss: 0.5355166875349824\n",
      "  batch 30 loss: 0.47181548171211035\n",
      "  batch 40 loss: 0.7765231575700454\n",
      "  batch 50 loss: 0.4581118270754814\n",
      "  batch 60 loss: 0.6348411917482736\n",
      "  batch 70 loss: 0.5006225898454432\n",
      "  batch 80 loss: 0.6008911498473026\n",
      "  batch 90 loss: 0.8550652745645493\n",
      "  batch 100 loss: 0.4737637647544034\n",
      "  batch 110 loss: 0.46823224439285693\n",
      "  batch 120 loss: 0.39262807260965926\n",
      "  batch 130 loss: 0.39098133439547383\n",
      "  batch 140 loss: 0.37326472718268633\n",
      "  batch 150 loss: 0.3559876571875066\n",
      "  batch 160 loss: 0.5902213345631026\n",
      "  batch 170 loss: 0.7928528194432147\n",
      "  batch 180 loss: 0.5492040410405025\n",
      "  batch 190 loss: 0.8248350109206513\n",
      "LOSS train 0.8248350109206513 valid 0.5600643606277373\n",
      "EPOCH 162:\n",
      "  batch 10 loss: 0.4331165022565983\n",
      "  batch 20 loss: 0.6658829157473519\n",
      "  batch 30 loss: 0.7493888701079413\n",
      "  batch 40 loss: 0.42716945290449077\n",
      "  batch 50 loss: 0.3749922627815977\n",
      "  batch 60 loss: 0.7120926553965546\n",
      "  batch 70 loss: 0.4372402084816713\n",
      "  batch 80 loss: 0.3396022167697083\n",
      "  batch 90 loss: 0.4383980166632682\n",
      "  batch 100 loss: 0.5411579726263881\n",
      "  batch 110 loss: 0.58132707295008\n",
      "  batch 120 loss: 0.6908776022261008\n",
      "  batch 130 loss: 0.7725918277865276\n",
      "  batch 140 loss: 0.6774331557564437\n",
      "  batch 150 loss: 0.5697095099138096\n",
      "  batch 160 loss: 0.7219890423410107\n",
      "  batch 170 loss: 0.5671467467676848\n",
      "  batch 180 loss: 0.371141847060062\n",
      "  batch 190 loss: 0.7457894594990648\n",
      "LOSS train 0.7457894594990648 valid 1.0032448789813013\n",
      "EPOCH 163:\n",
      "  batch 10 loss: 0.5203337303129956\n",
      "  batch 20 loss: 0.5250625295680947\n",
      "  batch 30 loss: 0.5633246741257608\n",
      "  batch 40 loss: 0.5721028371423017\n",
      "  batch 50 loss: 0.5947552009951323\n",
      "  batch 60 loss: 0.7062948064412922\n",
      "  batch 70 loss: 0.5374102838919498\n",
      "  batch 80 loss: 0.4987418971257284\n",
      "  batch 90 loss: 0.44680466637946664\n",
      "  batch 100 loss: 0.6841436366958078\n",
      "  batch 110 loss: 0.4877220803522505\n",
      "  batch 120 loss: 0.418696200591512\n",
      "  batch 130 loss: 0.4975557499506976\n",
      "  batch 140 loss: 0.6233936708886176\n",
      "  batch 150 loss: 0.5659855950856582\n",
      "  batch 160 loss: 0.39640885600820186\n",
      "  batch 170 loss: 0.5083730105048744\n",
      "  batch 180 loss: 0.6149232550407759\n",
      "  batch 190 loss: 0.4800277646048926\n",
      "LOSS train 0.4800277646048926 valid 0.8110580681867969\n",
      "EPOCH 164:\n",
      "  batch 10 loss: 0.8222532618208789\n",
      "  batch 20 loss: 0.5741210817825049\n",
      "  batch 30 loss: 0.5580009747296572\n",
      "  batch 40 loss: 0.6789871147018858\n",
      "  batch 50 loss: 0.8157865442452021\n",
      "  batch 60 loss: 0.5779025503667071\n",
      "  batch 70 loss: 0.5223513550590724\n",
      "  batch 80 loss: 0.5042197118862533\n",
      "  batch 90 loss: 0.46807447403552943\n",
      "  batch 100 loss: 0.5115499835985247\n",
      "  batch 110 loss: 0.8348444034345448\n",
      "  batch 120 loss: 0.5845143818296492\n",
      "  batch 130 loss: 0.5171507028746418\n",
      "  batch 140 loss: 0.6835453930078075\n",
      "  batch 150 loss: 0.5737201917450875\n",
      "  batch 160 loss: 0.30944574982859196\n",
      "  batch 170 loss: 0.5550740549282637\n",
      "  batch 180 loss: 0.5227845819143113\n",
      "  batch 190 loss: 0.3539641973446123\n",
      "LOSS train 0.3539641973446123 valid 0.42094467319951107\n",
      "EPOCH 165:\n",
      "  batch 10 loss: 0.3213922625349369\n",
      "  batch 20 loss: 0.6442344819195569\n",
      "  batch 30 loss: 0.7336119797662832\n",
      "  batch 40 loss: 0.8130051053012721\n",
      "  batch 50 loss: 0.7421333229372976\n",
      "  batch 60 loss: 0.6558060706127435\n",
      "  batch 70 loss: 0.5037009605206549\n",
      "  batch 80 loss: 0.46275090873241426\n",
      "  batch 90 loss: 0.7552593196043744\n",
      "  batch 100 loss: 0.47473840436432513\n",
      "  batch 110 loss: 0.7600477290776325\n",
      "  batch 120 loss: 0.8368595578125678\n",
      "  batch 130 loss: 0.3112634653225541\n",
      "  batch 140 loss: 0.4897555636416655\n",
      "  batch 150 loss: 0.6071071895130444\n",
      "  batch 160 loss: 0.5411568196024745\n",
      "  batch 170 loss: 0.44796319406596014\n",
      "  batch 180 loss: 0.40395587456878274\n",
      "  batch 190 loss: 0.5847631604177878\n",
      "LOSS train 0.5847631604177878 valid 0.6546599927301614\n",
      "EPOCH 166:\n",
      "  batch 10 loss: 1.1319340022513642\n",
      "  batch 20 loss: 0.3308942818373907\n",
      "  batch 30 loss: 0.4885967160575092\n",
      "  batch 40 loss: 0.9444214154034853\n",
      "  batch 50 loss: 0.4107728728558868\n",
      "  batch 60 loss: 0.43935675946995617\n",
      "  batch 70 loss: 0.540952491061762\n",
      "  batch 80 loss: 0.4071461951942183\n",
      "  batch 90 loss: 0.5823807193315588\n",
      "  batch 100 loss: 0.37903365313541143\n",
      "  batch 110 loss: 0.5667411157977768\n",
      "  batch 120 loss: 0.5623072337592021\n",
      "  batch 130 loss: 0.4335116848233156\n",
      "  batch 140 loss: 0.7290142544195988\n",
      "  batch 150 loss: 0.7062775767641142\n",
      "  batch 160 loss: 0.34125771434046326\n",
      "  batch 170 loss: 0.5038924263208173\n",
      "  batch 180 loss: 0.5632593584479764\n",
      "  batch 190 loss: 0.5861447108443827\n",
      "LOSS train 0.5861447108443827 valid 0.47804733411444783\n",
      "EPOCH 167:\n",
      "  batch 10 loss: 0.41030646332073956\n",
      "  batch 20 loss: 0.7386027637752705\n",
      "  batch 30 loss: 0.8678774860221893\n",
      "  batch 40 loss: 0.5449978644144722\n",
      "  batch 50 loss: 0.5068563723063562\n",
      "  batch 60 loss: 0.4741490382235497\n",
      "  batch 70 loss: 0.6925199445773614\n",
      "  batch 80 loss: 0.750806276377989\n",
      "  batch 90 loss: 0.5503831931739114\n",
      "  batch 100 loss: 0.42525750085769687\n",
      "  batch 110 loss: 0.35353891749982724\n",
      "  batch 120 loss: 0.5958890012494521\n",
      "  batch 130 loss: 0.4052402633242309\n",
      "  batch 140 loss: 0.5167325368791353\n",
      "  batch 150 loss: 0.46354593197465876\n",
      "  batch 160 loss: 0.45872337242471983\n",
      "  batch 170 loss: 0.29634486975846813\n",
      "  batch 180 loss: 0.659677418647334\n",
      "  batch 190 loss: 0.4381511013954878\n",
      "LOSS train 0.4381511013954878 valid 0.43954109513418727\n",
      "EPOCH 168:\n",
      "  batch 10 loss: 0.5135536967893131\n",
      "  batch 20 loss: 0.4766632695798762\n",
      "  batch 30 loss: 0.6273015841608867\n",
      "  batch 40 loss: 0.35003969621029685\n",
      "  batch 50 loss: 0.4814400526927784\n",
      "  batch 60 loss: 0.819076472800225\n",
      "  batch 70 loss: 0.5820758027490228\n",
      "  batch 80 loss: 0.5514069920056499\n",
      "  batch 90 loss: 0.32083674246823646\n",
      "  batch 100 loss: 0.34145648973062637\n",
      "  batch 110 loss: 0.5018746867659502\n",
      "  batch 120 loss: 0.5225041901605436\n",
      "  batch 130 loss: 0.6665323846740648\n",
      "  batch 140 loss: 0.542908634187188\n",
      "  batch 150 loss: 0.5854887664201669\n",
      "  batch 160 loss: 0.4552533074049279\n",
      "  batch 170 loss: 0.5012989995419048\n",
      "  batch 180 loss: 0.5000265274662524\n",
      "  batch 190 loss: 0.5002044390712399\n",
      "LOSS train 0.5002044390712399 valid 0.40037487183685583\n",
      "EPOCH 169:\n",
      "  batch 10 loss: 0.5058653648244217\n",
      "  batch 20 loss: 0.28723344111349436\n",
      "  batch 30 loss: 0.38195355407951864\n",
      "  batch 40 loss: 0.5110015033300442\n",
      "  batch 50 loss: 0.5137684125918895\n",
      "  batch 60 loss: 0.528478261193959\n",
      "  batch 70 loss: 0.8054951635131147\n",
      "  batch 80 loss: 0.5750091369089205\n",
      "  batch 90 loss: 0.7599984965752811\n",
      "  batch 100 loss: 0.48901715452084316\n",
      "  batch 110 loss: 0.4388554149190895\n",
      "  batch 120 loss: 0.4411799507157411\n",
      "  batch 130 loss: 0.432916909463529\n",
      "  batch 140 loss: 0.5560172471916303\n",
      "  batch 150 loss: 0.6166745888069272\n",
      "  batch 160 loss: 0.7214241553505417\n",
      "  batch 170 loss: 0.3571750874922145\n",
      "  batch 180 loss: 0.6912299607414752\n",
      "  batch 190 loss: 0.8505048883613199\n",
      "LOSS train 0.8505048883613199 valid 0.8446151806220326\n",
      "EPOCH 170:\n",
      "  batch 10 loss: 0.6274838626413839\n",
      "  batch 20 loss: 0.8236959339585155\n",
      "  batch 30 loss: 0.31356305406661705\n",
      "  batch 40 loss: 0.531056186844944\n",
      "  batch 50 loss: 0.5294918581246748\n",
      "  batch 60 loss: 0.4985203247226309\n",
      "  batch 70 loss: 0.4024672247236595\n",
      "  batch 80 loss: 0.4682339844875969\n",
      "  batch 90 loss: 0.5730894896667451\n",
      "  batch 100 loss: 0.5852286808425561\n",
      "  batch 110 loss: 0.49157591434195635\n",
      "  batch 120 loss: 0.23184675781521946\n",
      "  batch 130 loss: 0.3996562165673822\n",
      "  batch 140 loss: 0.6467641736409859\n",
      "  batch 150 loss: 0.5569704182707937\n",
      "  batch 160 loss: 0.46395002625649795\n",
      "  batch 170 loss: 0.5326582031557336\n",
      "  batch 180 loss: 0.5413148754683789\n",
      "  batch 190 loss: 0.47731682572630235\n",
      "LOSS train 0.47731682572630235 valid 2.340105660938813\n",
      "EPOCH 171:\n",
      "  batch 10 loss: 0.9214619851321914\n",
      "  batch 20 loss: 0.40737568177864886\n",
      "  batch 30 loss: 0.5357615287066437\n",
      "  batch 40 loss: 0.5530189021141269\n",
      "  batch 50 loss: 0.3863089168560691\n",
      "  batch 60 loss: 0.4108308854396455\n",
      "  batch 70 loss: 0.4798329203389585\n",
      "  batch 80 loss: 0.7240557583805639\n",
      "  batch 90 loss: 0.555557570559904\n",
      "  batch 100 loss: 0.7367242245090893\n",
      "  batch 110 loss: 0.4532176622422412\n",
      "  batch 120 loss: 0.43704593267757447\n",
      "  batch 130 loss: 0.538388884847518\n",
      "  batch 140 loss: 0.8495986430323683\n",
      "  batch 150 loss: 0.6580799974035472\n",
      "  batch 160 loss: 0.6380760577740148\n",
      "  batch 170 loss: 0.3050621263682842\n",
      "  batch 180 loss: 0.458270602664561\n",
      "  batch 190 loss: 0.260957162885461\n",
      "LOSS train 0.260957162885461 valid 0.42546949545831947\n",
      "EPOCH 172:\n",
      "  batch 10 loss: 0.3752403680904536\n",
      "  batch 20 loss: 0.8016028880607337\n",
      "  batch 30 loss: 0.6056426908704452\n",
      "  batch 40 loss: 0.5650902229826897\n",
      "  batch 50 loss: 0.4518134945537895\n",
      "  batch 60 loss: 0.4322729772655293\n",
      "  batch 70 loss: 0.7393249642394949\n",
      "  batch 80 loss: 0.4076901187072508\n",
      "  batch 90 loss: 0.3944305431912653\n",
      "  batch 100 loss: 0.612713248381624\n",
      "  batch 110 loss: 0.5028126011486165\n",
      "  batch 120 loss: 0.8004863218287938\n",
      "  batch 130 loss: 0.6862348252499941\n",
      "  batch 140 loss: 0.47941950146923773\n",
      "  batch 150 loss: 0.7604911908332724\n",
      "  batch 160 loss: 0.348056184858433\n",
      "  batch 170 loss: 0.4599137558601797\n",
      "  batch 180 loss: 0.5443512787343934\n",
      "  batch 190 loss: 0.5439284987864085\n",
      "LOSS train 0.5439284987864085 valid 0.38719731738764007\n",
      "EPOCH 173:\n",
      "  batch 10 loss: 0.4092476060963236\n",
      "  batch 20 loss: 0.6470612731878645\n",
      "  batch 30 loss: 0.591390584805049\n",
      "  batch 40 loss: 0.681296932924306\n",
      "  batch 50 loss: 0.5617173221777193\n",
      "  batch 60 loss: 0.4603513179928996\n",
      "  batch 70 loss: 0.7209176954231225\n",
      "  batch 80 loss: 0.5250687812222168\n",
      "  batch 90 loss: 0.3780156238237396\n",
      "  batch 100 loss: 0.4244150048587471\n",
      "  batch 110 loss: 0.6532832998549566\n",
      "  batch 120 loss: 0.37922901518177243\n",
      "  batch 130 loss: 0.35344246723107065\n",
      "  batch 140 loss: 0.5074904753942974\n",
      "  batch 150 loss: 0.44480305520701224\n",
      "  batch 160 loss: 0.31370480806799605\n",
      "  batch 170 loss: 0.5691419849812519\n",
      "  batch 180 loss: 0.3498032472562045\n",
      "  batch 190 loss: 0.7102970147505403\n",
      "LOSS train 0.7102970147505403 valid 0.472992025817243\n",
      "EPOCH 174:\n",
      "  batch 10 loss: 0.35655627865344286\n",
      "  batch 20 loss: 0.4991404718020931\n",
      "  batch 30 loss: 0.27709405209170657\n",
      "  batch 40 loss: 0.7273230439372128\n",
      "  batch 50 loss: 0.3225965074030682\n",
      "  batch 60 loss: 0.6587658354721497\n",
      "  batch 70 loss: 0.7404739302815869\n",
      "  batch 80 loss: 0.5841924469918013\n",
      "  batch 90 loss: 0.46514108137926086\n",
      "  batch 100 loss: 0.45502309625735504\n",
      "  batch 110 loss: 0.43803485174430534\n",
      "  batch 120 loss: 0.38374162995023653\n",
      "  batch 130 loss: 0.5809880423825234\n",
      "  batch 140 loss: 0.4020978522486985\n",
      "  batch 150 loss: 0.4308055878689629\n",
      "  batch 160 loss: 0.468842906242935\n",
      "  batch 170 loss: 0.3981123309029499\n",
      "  batch 180 loss: 0.6114316091348883\n",
      "  batch 190 loss: 0.5777040879940614\n",
      "LOSS train 0.5777040879940614 valid 0.41682269833817814\n",
      "EPOCH 175:\n",
      "  batch 10 loss: 0.391146272077458\n",
      "  batch 20 loss: 0.7480645179166459\n",
      "  batch 30 loss: 0.8534184096555691\n",
      "  batch 40 loss: 0.4907330890797311\n",
      "  batch 50 loss: 0.23521764904726297\n",
      "  batch 60 loss: 0.39248982672870625\n",
      "  batch 70 loss: 0.43778456100844776\n",
      "  batch 80 loss: 0.5433982670132537\n",
      "  batch 90 loss: 0.6801018999423831\n",
      "  batch 100 loss: 0.4761376831738744\n",
      "  batch 110 loss: 0.45996130813728087\n",
      "  batch 120 loss: 0.35821745833090973\n",
      "  batch 130 loss: 0.3610421593490173\n",
      "  batch 140 loss: 0.4283954740240006\n",
      "  batch 150 loss: 0.38419441555452066\n",
      "  batch 160 loss: 0.6735385182197206\n",
      "  batch 170 loss: 1.077230544341728\n",
      "  batch 180 loss: 0.5313531974039506\n",
      "  batch 190 loss: 0.7867594392620958\n",
      "LOSS train 0.7867594392620958 valid 2.615917627157894\n",
      "EPOCH 176:\n",
      "  batch 10 loss: 1.1943933477043174\n",
      "  batch 20 loss: 0.668406199439778\n",
      "  batch 30 loss: 0.29814936760813\n",
      "  batch 40 loss: 0.4836134458659217\n",
      "  batch 50 loss: 0.4638020807877183\n",
      "  batch 60 loss: 0.6482068331446499\n",
      "  batch 70 loss: 0.6177174029871821\n",
      "  batch 80 loss: 0.5946849950822071\n",
      "  batch 90 loss: 0.4283702166634612\n",
      "  batch 100 loss: 1.1753020445816218\n",
      "  batch 110 loss: 0.5048404117580503\n",
      "  batch 120 loss: 0.4329080488620093\n",
      "  batch 130 loss: 0.4269290335243568\n",
      "  batch 140 loss: 0.3906665784015786\n",
      "  batch 150 loss: 0.5189286441192962\n",
      "  batch 160 loss: 0.41316046883584934\n",
      "  batch 170 loss: 0.4899242796818726\n",
      "  batch 180 loss: 0.4611072597326711\n",
      "  batch 190 loss: 0.30085306939436124\n",
      "LOSS train 0.30085306939436124 valid 0.4996489116717548\n",
      "EPOCH 177:\n",
      "  batch 10 loss: 0.38141254868387475\n",
      "  batch 20 loss: 0.4105241378885694\n",
      "  batch 30 loss: 0.3852764977840707\n",
      "  batch 40 loss: 0.4065124373853905\n",
      "  batch 50 loss: 0.495212763408199\n",
      "  batch 60 loss: 0.5402831556915771\n",
      "  batch 70 loss: 0.3464016297890339\n",
      "  batch 80 loss: 0.818890592281241\n",
      "  batch 90 loss: 0.9226301773189334\n",
      "  batch 100 loss: 0.38244694098830223\n",
      "  batch 110 loss: 0.6008205360674765\n",
      "  batch 120 loss: 0.47725239183055235\n",
      "  batch 130 loss: 0.401440169790294\n",
      "  batch 140 loss: 0.42206623880774713\n",
      "  batch 150 loss: 0.27287437614868393\n",
      "  batch 160 loss: 0.45096067264676093\n",
      "  batch 170 loss: 0.5911141496879282\n",
      "  batch 180 loss: 0.460277925664559\n",
      "  batch 190 loss: 0.4269492924911901\n",
      "LOSS train 0.4269492924911901 valid 0.5060102917949478\n",
      "EPOCH 178:\n",
      "  batch 10 loss: 0.3290995652205311\n",
      "  batch 20 loss: 0.3799256396247074\n",
      "  batch 30 loss: 0.30160102421650664\n",
      "  batch 40 loss: 0.6068431857507676\n",
      "  batch 50 loss: 0.46448596760747024\n",
      "  batch 60 loss: 0.746872304295539\n",
      "  batch 70 loss: 0.5433002977631987\n",
      "  batch 80 loss: 0.49836711938260125\n",
      "  batch 90 loss: 0.5031559914234094\n",
      "  batch 100 loss: 0.45503388343495316\n",
      "  batch 110 loss: 0.8079287482658402\n",
      "  batch 120 loss: 0.8053972247056663\n",
      "  batch 130 loss: 0.5130465698079206\n",
      "  batch 140 loss: 0.39416869888664224\n",
      "  batch 150 loss: 0.4383887846110156\n",
      "  batch 160 loss: 0.5794027599389665\n",
      "  batch 170 loss: 0.4997973159421235\n",
      "  batch 180 loss: 0.37809649159607944\n",
      "  batch 190 loss: 0.44302794225513936\n",
      "LOSS train 0.44302794225513936 valid 0.55833073211901\n",
      "EPOCH 179:\n",
      "  batch 10 loss: 0.4871034851530567\n",
      "  batch 20 loss: 0.37426561523461715\n",
      "  batch 30 loss: 0.5941586780594662\n",
      "  batch 40 loss: 0.5794169949891511\n",
      "  batch 50 loss: 0.6394776834407822\n",
      "  batch 60 loss: 0.3992304938554298\n",
      "  batch 70 loss: 0.49191313146147875\n",
      "  batch 80 loss: 0.34969341045361946\n",
      "  batch 90 loss: 0.4582206595339812\n",
      "  batch 100 loss: 0.5129287491668947\n",
      "  batch 110 loss: 0.6703478661074769\n",
      "  batch 120 loss: 0.4648713360540569\n",
      "  batch 130 loss: 0.39978199224715355\n",
      "  batch 140 loss: 0.4031272346212063\n",
      "  batch 150 loss: 0.4054177555372007\n",
      "  batch 160 loss: 0.47442431043018585\n",
      "  batch 170 loss: 0.40609338093781844\n",
      "  batch 180 loss: 0.6862377656856552\n",
      "  batch 190 loss: 0.4225879535719287\n",
      "LOSS train 0.4225879535719287 valid 0.48161518910731105\n",
      "EPOCH 180:\n",
      "  batch 10 loss: 0.6899435740313493\n",
      "  batch 20 loss: 0.41508934684097765\n",
      "  batch 30 loss: 1.0797352316789328\n",
      "  batch 40 loss: 0.4270387825090438\n",
      "  batch 50 loss: 0.6294455001247116\n",
      "  batch 60 loss: 0.411270428262651\n",
      "  batch 70 loss: 0.37033420441439374\n",
      "  batch 80 loss: 0.4258425814070506\n",
      "  batch 90 loss: 0.5907708548475057\n",
      "  batch 100 loss: 0.43690494557959025\n",
      "  batch 110 loss: 0.4761539498955244\n",
      "  batch 120 loss: 0.7306451232972904\n",
      "  batch 130 loss: 0.4643479338774341\n",
      "  batch 140 loss: 0.25118290113750846\n",
      "  batch 150 loss: 0.4698817016964313\n",
      "  batch 160 loss: 0.43071789036039265\n",
      "  batch 170 loss: 0.463599202860496\n",
      "  batch 180 loss: 0.7066031140973792\n",
      "  batch 190 loss: 0.5074135209491942\n",
      "LOSS train 0.5074135209491942 valid 0.6884366863314659\n",
      "EPOCH 181:\n",
      "  batch 10 loss: 0.3377493965730537\n",
      "  batch 20 loss: 0.35585355397779495\n",
      "  batch 30 loss: 0.4355237734678667\n",
      "  batch 40 loss: 0.3892916334210895\n",
      "  batch 50 loss: 0.3498528600553982\n",
      "  batch 60 loss: 0.747211883100681\n",
      "  batch 70 loss: 0.4747994892881252\n",
      "  batch 80 loss: 0.6846672567073255\n",
      "  batch 90 loss: 0.6996363162994385\n",
      "  batch 100 loss: 0.4669325810682494\n",
      "  batch 110 loss: 0.5890211842604913\n",
      "  batch 120 loss: 0.8821449799928814\n",
      "  batch 130 loss: 0.4903220553183928\n",
      "  batch 140 loss: 0.36205187094165014\n",
      "  batch 150 loss: 0.5862654657219537\n",
      "  batch 160 loss: 0.4272037047194317\n",
      "  batch 170 loss: 0.32986177267739547\n",
      "  batch 180 loss: 0.8298440932296216\n",
      "  batch 190 loss: 0.3505398172419518\n",
      "LOSS train 0.3505398172419518 valid 0.42493023860674256\n",
      "EPOCH 182:\n",
      "  batch 10 loss: 0.4148958834353834\n",
      "  batch 20 loss: 0.3421532587846741\n",
      "  batch 30 loss: 0.7949112688191236\n",
      "  batch 40 loss: 0.4163526586125954\n",
      "  batch 50 loss: 0.48746083745500074\n",
      "  batch 60 loss: 0.6374622822506353\n",
      "  batch 70 loss: 0.3701667283894494\n",
      "  batch 80 loss: 0.4914250989910215\n",
      "  batch 90 loss: 0.32061282208305786\n",
      "  batch 100 loss: 0.6054323303163983\n",
      "  batch 110 loss: 0.26148390021407975\n",
      "  batch 120 loss: 0.29715712055913174\n",
      "  batch 130 loss: 0.5106006505026016\n",
      "  batch 140 loss: 0.6245559824776137\n",
      "  batch 150 loss: 0.4612003089307109\n",
      "  batch 160 loss: 0.5393917589070043\n",
      "  batch 170 loss: 0.4002883363704314\n",
      "  batch 180 loss: 0.44944112860248425\n",
      "  batch 190 loss: 0.37187748998403547\n",
      "LOSS train 0.37187748998403547 valid 0.5304728176152611\n",
      "EPOCH 183:\n",
      "  batch 10 loss: 0.570308170557837\n",
      "  batch 20 loss: 0.49082995365024545\n",
      "  batch 30 loss: 0.4570389740198152\n",
      "  batch 40 loss: 0.6627208572055678\n",
      "  batch 50 loss: 0.6941192359576235\n",
      "  batch 60 loss: 0.9034209344885312\n",
      "  batch 70 loss: 0.7968586604023585\n",
      "  batch 80 loss: 0.4219032756605884\n",
      "  batch 90 loss: 0.22966419326839968\n",
      "  batch 100 loss: 0.5600810438685585\n",
      "  batch 110 loss: 0.4348166367795784\n",
      "  batch 120 loss: 0.3873429930245038\n",
      "  batch 130 loss: 0.4198498263082001\n",
      "  batch 140 loss: 0.7239147856016643\n",
      "  batch 150 loss: 0.6002127361120074\n",
      "  batch 160 loss: 0.6418761686480139\n",
      "  batch 170 loss: 0.33433326026424764\n",
      "  batch 180 loss: 0.4467276329756714\n",
      "  batch 190 loss: 0.4577897543902509\n",
      "LOSS train 0.4577897543902509 valid 0.44575874123331277\n",
      "EPOCH 184:\n",
      "  batch 10 loss: 0.5182059942861088\n",
      "  batch 20 loss: 0.5622834995680023\n",
      "  batch 30 loss: 0.3323043559110374\n",
      "  batch 40 loss: 0.5736028938263189\n",
      "  batch 50 loss: 0.36409347812004855\n",
      "  batch 60 loss: 0.4941358984884573\n",
      "  batch 70 loss: 0.5217246198211797\n",
      "  batch 80 loss: 0.5776512932498008\n",
      "  batch 90 loss: 0.4087729159349692\n",
      "  batch 100 loss: 0.2741861778136808\n",
      "  batch 110 loss: 0.5185327418032102\n",
      "  batch 120 loss: 0.38394284625537695\n",
      "  batch 130 loss: 0.39132417739601805\n",
      "  batch 140 loss: 0.45748534936865326\n",
      "  batch 150 loss: 0.35670712730207016\n",
      "  batch 160 loss: 0.5235091720242053\n",
      "  batch 170 loss: 0.33239713148796\n",
      "  batch 180 loss: 0.3518756788631435\n",
      "  batch 190 loss: 0.741996471316088\n",
      "LOSS train 0.741996471316088 valid 0.8182820027025465\n",
      "EPOCH 185:\n",
      "  batch 10 loss: 0.3343635449768044\n",
      "  batch 20 loss: 0.38218591850600203\n",
      "  batch 30 loss: 0.41493610676843673\n",
      "  batch 40 loss: 0.4864016893552616\n",
      "  batch 50 loss: 0.5707422234117985\n",
      "  batch 60 loss: 0.375506543222582\n",
      "  batch 70 loss: 0.5785394204198383\n",
      "  batch 80 loss: 0.23020814730552958\n",
      "  batch 90 loss: 0.33638707841164434\n",
      "  batch 100 loss: 0.3876809643348679\n",
      "  batch 110 loss: 0.7857812463247683\n",
      "  batch 120 loss: 0.5045973106476594\n",
      "  batch 130 loss: 0.45056685095187277\n",
      "  batch 140 loss: 0.6271058854123112\n",
      "  batch 150 loss: 0.39063450188259596\n",
      "  batch 160 loss: 0.6260026531759649\n",
      "  batch 170 loss: 0.4248722227406688\n",
      "  batch 180 loss: 0.40324100719881245\n",
      "  batch 190 loss: 0.2636641176068224\n",
      "LOSS train 0.2636641176068224 valid 1.0897584196175627\n",
      "EPOCH 186:\n",
      "  batch 10 loss: 0.521195003570756\n",
      "  batch 20 loss: 0.614952548808651\n",
      "  batch 30 loss: 0.2519729281833861\n",
      "  batch 40 loss: 0.4078529197082389\n",
      "  batch 50 loss: 0.4656904923860566\n",
      "  batch 60 loss: 0.31001978962449356\n",
      "  batch 70 loss: 0.36417163132573477\n",
      "  batch 80 loss: 0.5826110695721581\n",
      "  batch 90 loss: 0.7159796426290995\n",
      "  batch 100 loss: 0.6891696775215678\n",
      "  batch 110 loss: 0.24771385323692813\n",
      "  batch 120 loss: 0.6122223201818997\n",
      "  batch 130 loss: 0.5716393194452394\n",
      "  batch 140 loss: 0.36989732129732145\n",
      "  batch 150 loss: 0.41028090614126994\n",
      "  batch 160 loss: 0.36088528839172795\n",
      "  batch 170 loss: 0.4830610922479536\n",
      "  batch 180 loss: 0.7034367308719084\n",
      "  batch 190 loss: 0.9576019080530387\n",
      "LOSS train 0.9576019080530387 valid 0.7292151281519461\n",
      "EPOCH 187:\n",
      "  batch 10 loss: 0.5222112974792253\n",
      "  batch 20 loss: 0.4501402207883075\n",
      "  batch 30 loss: 0.33699860964843537\n",
      "  batch 40 loss: 0.570627489083563\n",
      "  batch 50 loss: 0.4021758759918157\n",
      "  batch 60 loss: 0.4027156256081071\n",
      "  batch 70 loss: 0.48136909292370544\n",
      "  batch 80 loss: 0.2973137850814965\n",
      "  batch 90 loss: 0.3932224385062\n",
      "  batch 100 loss: 0.7075862332712859\n",
      "  batch 110 loss: 0.5450599406380207\n",
      "  batch 120 loss: 0.5169562316819792\n",
      "  batch 130 loss: 1.067682382510975\n",
      "  batch 140 loss: 0.5910394997976255\n",
      "  batch 150 loss: 0.4379939307342283\n",
      "  batch 160 loss: 0.3599037543113809\n",
      "  batch 170 loss: 0.4904641549102962\n",
      "  batch 180 loss: 0.4111966632655822\n",
      "  batch 190 loss: 0.6082821781310486\n",
      "LOSS train 0.6082821781310486 valid 0.34757531196802843\n",
      "EPOCH 188:\n",
      "  batch 10 loss: 0.3306067124765832\n",
      "  batch 20 loss: 0.6533238714357139\n",
      "  batch 30 loss: 0.32929371917853134\n",
      "  batch 40 loss: 0.672149573895149\n",
      "  batch 50 loss: 0.3833819002233213\n",
      "  batch 60 loss: 0.49952268025372176\n",
      "  batch 70 loss: 0.37019740701362025\n",
      "  batch 80 loss: 0.2680084969848394\n",
      "  batch 90 loss: 0.17216550617595203\n",
      "  batch 100 loss: 0.3545570674497867\n",
      "  batch 110 loss: 0.4030874623102136\n",
      "  batch 120 loss: 0.44213624399271795\n",
      "  batch 130 loss: 0.39673242682474663\n",
      "  batch 140 loss: 0.4314756441861391\n",
      "  batch 150 loss: 0.6193235303740948\n",
      "  batch 160 loss: 0.573746103036683\n",
      "  batch 170 loss: 0.5245718204881996\n",
      "  batch 180 loss: 1.124772710929392\n",
      "  batch 190 loss: 0.4119202279136516\n",
      "LOSS train 0.4119202279136516 valid 1.5154932597757487\n",
      "EPOCH 189:\n",
      "  batch 10 loss: 0.5402085497611552\n",
      "  batch 20 loss: 0.7434451706067193\n",
      "  batch 30 loss: 0.5541735062608495\n",
      "  batch 40 loss: 0.2179742428706959\n",
      "  batch 50 loss: 0.42591626700013874\n",
      "  batch 60 loss: 0.3251827167230658\n",
      "  batch 70 loss: 0.4758951702620834\n",
      "  batch 80 loss: 0.7826012502191588\n",
      "  batch 90 loss: 0.36067036583554\n",
      "  batch 100 loss: 0.32074315678328275\n",
      "  batch 110 loss: 0.1722971937910188\n",
      "  batch 120 loss: 0.8829104791861028\n",
      "  batch 130 loss: 0.4354481656599091\n",
      "  batch 140 loss: 0.4018859799252823\n",
      "  batch 150 loss: 0.4075881621916778\n",
      "  batch 160 loss: 0.4464187514677178\n",
      "  batch 170 loss: 0.2711907005810644\n",
      "  batch 180 loss: 0.3229774960607756\n",
      "  batch 190 loss: 0.8339444036711938\n",
      "LOSS train 0.8339444036711938 valid 0.4238888114460935\n",
      "EPOCH 190:\n",
      "  batch 10 loss: 0.4000980570097454\n",
      "  batch 20 loss: 0.3065579818518017\n",
      "  batch 30 loss: 0.21153209501935635\n",
      "  batch 40 loss: 0.5957972704200074\n",
      "  batch 50 loss: 0.40013287251786095\n",
      "  batch 60 loss: 0.3188723672530614\n",
      "  batch 70 loss: 0.5001139130094089\n",
      "  batch 80 loss: 0.574241472943686\n",
      "  batch 90 loss: 0.3229039030964486\n",
      "  batch 100 loss: 0.35721586049185133\n",
      "  batch 110 loss: 0.6950663211755455\n",
      "  batch 120 loss: 0.587525415350683\n",
      "  batch 130 loss: 0.3807367946341401\n",
      "  batch 140 loss: 0.44149611578322945\n",
      "  batch 150 loss: 0.5412454756675288\n",
      "  batch 160 loss: 0.42574595361948014\n",
      "  batch 170 loss: 0.41503265596693384\n",
      "  batch 180 loss: 0.4691014064475894\n",
      "  batch 190 loss: 0.6516313952743076\n",
      "LOSS train 0.6516313952743076 valid 0.43246143243950197\n",
      "EPOCH 191:\n",
      "  batch 10 loss: 0.5711304717347957\n",
      "  batch 20 loss: 0.9396956045005936\n",
      "  batch 30 loss: 0.4021562670939602\n",
      "  batch 40 loss: 0.5032819629937876\n",
      "  batch 50 loss: 0.3277607797179371\n",
      "  batch 60 loss: 0.4166145076858811\n",
      "  batch 70 loss: 0.2899071804189589\n",
      "  batch 80 loss: 0.3786703310906887\n",
      "  batch 90 loss: 0.44213256424409336\n",
      "  batch 100 loss: 0.761204308224842\n",
      "  batch 110 loss: 0.43292964036227205\n",
      "  batch 120 loss: 0.41726307466742585\n",
      "  batch 130 loss: 0.3933679199602921\n",
      "  batch 140 loss: 0.30598103067022747\n",
      "  batch 150 loss: 0.318793552799616\n",
      "  batch 160 loss: 0.38831470932636875\n",
      "  batch 170 loss: 0.532605554885231\n",
      "  batch 180 loss: 0.1454535527213011\n",
      "  batch 190 loss: 0.33010115881916136\n",
      "LOSS train 0.33010115881916136 valid 3.4745493370588894\n",
      "EPOCH 192:\n",
      "  batch 10 loss: 1.35527211233275\n",
      "  batch 20 loss: 0.8023642422864213\n",
      "  batch 30 loss: 0.4615559634868987\n",
      "  batch 40 loss: 0.34746970264823174\n",
      "  batch 50 loss: 0.692971185548231\n",
      "  batch 60 loss: 0.3779182931408286\n",
      "  batch 70 loss: 0.1622153581236489\n",
      "  batch 80 loss: 0.30837276428355836\n",
      "  batch 90 loss: 0.5044893047888763\n",
      "  batch 100 loss: 0.5651596231618896\n",
      "  batch 110 loss: 0.44767061949241904\n",
      "  batch 120 loss: 0.3994315365096554\n",
      "  batch 130 loss: 0.5131948201684281\n",
      "  batch 140 loss: 0.22145778135454747\n",
      "  batch 150 loss: 0.49771597011131236\n",
      "  batch 160 loss: 0.6780328197870403\n",
      "  batch 170 loss: 0.4393054340820527\n",
      "  batch 180 loss: 0.47704838661593385\n",
      "  batch 190 loss: 0.3313947536982596\n",
      "LOSS train 0.3313947536982596 valid 0.3551569535099985\n",
      "EPOCH 193:\n",
      "  batch 10 loss: 0.5229736502602463\n",
      "  batch 20 loss: 0.2716215909575112\n",
      "  batch 30 loss: 0.43642547106719576\n",
      "  batch 40 loss: 0.45512594952015206\n",
      "  batch 50 loss: 0.43565141876460983\n",
      "  batch 60 loss: 0.5206363239674829\n",
      "  batch 70 loss: 0.6884296722942963\n",
      "  batch 80 loss: 0.5203651098127011\n",
      "  batch 90 loss: 0.46547523498302323\n",
      "  batch 100 loss: 0.33592366493976444\n",
      "  batch 110 loss: 0.32083641943172553\n",
      "  batch 120 loss: 0.28985279162297956\n",
      "  batch 130 loss: 0.3447528453922132\n",
      "  batch 140 loss: 0.46747828652733003\n",
      "  batch 150 loss: 0.4358962021069601\n",
      "  batch 160 loss: 0.24989207903272473\n",
      "  batch 170 loss: 0.5324530345707899\n",
      "  batch 180 loss: 0.49585255984566173\n",
      "  batch 190 loss: 0.3904456956486683\n",
      "LOSS train 0.3904456956486683 valid 0.7917525039878316\n",
      "EPOCH 194:\n",
      "  batch 10 loss: 0.48251128506381064\n",
      "  batch 20 loss: 0.29837347538705217\n",
      "  batch 30 loss: 0.44579112872015686\n",
      "  batch 40 loss: 0.3267223695263965\n",
      "  batch 50 loss: 0.3451495635505125\n",
      "  batch 60 loss: 0.4943945549544878\n",
      "  batch 70 loss: 0.252742261660751\n",
      "  batch 80 loss: 0.608310672652442\n",
      "  batch 90 loss: 0.3853699009865522\n",
      "  batch 100 loss: 0.4368159176665358\n",
      "  batch 110 loss: 0.2915252179576783\n",
      "  batch 120 loss: 0.6081585975742201\n",
      "  batch 130 loss: 0.6221731783996802\n",
      "  batch 140 loss: 0.45247101156564895\n",
      "  batch 150 loss: 0.7819509508321062\n",
      "  batch 160 loss: 0.3314587926957756\n",
      "  batch 170 loss: 0.3350024488347117\n",
      "  batch 180 loss: 0.2791267339605838\n",
      "  batch 190 loss: 0.40015216144383886\n",
      "LOSS train 0.40015216144383886 valid 0.3948617047512343\n",
      "EPOCH 195:\n",
      "  batch 10 loss: 0.3177050317171961\n",
      "  batch 20 loss: 0.35945917550416195\n",
      "  batch 30 loss: 0.25036696944152936\n",
      "  batch 40 loss: 0.39216513230931016\n",
      "  batch 50 loss: 0.3665656404045876\n",
      "  batch 60 loss: 0.6251801283360692\n",
      "  batch 70 loss: 0.49473732968326656\n",
      "  batch 80 loss: 0.24162855438917177\n",
      "  batch 90 loss: 0.6316836018435424\n",
      "  batch 100 loss: 0.34665106089087205\n",
      "  batch 110 loss: 0.2343059126913431\n",
      "  batch 120 loss: 0.424985932855634\n",
      "  batch 130 loss: 0.6131155097595183\n",
      "  batch 140 loss: 0.2615771357988706\n",
      "  batch 150 loss: 0.44546056903200226\n",
      "  batch 160 loss: 0.5587958271033131\n",
      "  batch 170 loss: 0.2995583099720534\n",
      "  batch 180 loss: 0.8537862853496335\n",
      "  batch 190 loss: 0.4050583453150466\n",
      "LOSS train 0.4050583453150466 valid 1.2422311265418569\n",
      "EPOCH 196:\n",
      "  batch 10 loss: 0.5494488122159964\n",
      "  batch 20 loss: 0.4930170891835587\n",
      "  batch 30 loss: 0.6319287226535379\n",
      "  batch 40 loss: 0.3507609189968207\n",
      "  batch 50 loss: 0.5519152838824084\n",
      "  batch 60 loss: 0.4651630793581717\n",
      "  batch 70 loss: 1.0336056969710625\n",
      "  batch 80 loss: 0.7531539486721158\n",
      "  batch 90 loss: 0.44428739114664495\n",
      "  batch 100 loss: 0.310258242895361\n",
      "  batch 110 loss: 0.42776774659287187\n",
      "  batch 120 loss: 0.3216930917289574\n",
      "  batch 130 loss: 0.24330459375632926\n",
      "  batch 140 loss: 0.32969095587031916\n",
      "  batch 150 loss: 0.3055166365316836\n",
      "  batch 160 loss: 0.18440324172552208\n",
      "  batch 170 loss: 0.7234342087409459\n",
      "  batch 180 loss: 0.4038639813894406\n",
      "  batch 190 loss: 0.43469949158024973\n",
      "LOSS train 0.43469949158024973 valid 0.33055992427025327\n",
      "EPOCH 197:\n",
      "  batch 10 loss: 0.4519217857829062\n",
      "  batch 20 loss: 0.3143609458973515\n",
      "  batch 30 loss: 0.5909523914800957\n",
      "  batch 40 loss: 0.3815026322437916\n",
      "  batch 50 loss: 0.3832291759754298\n",
      "  batch 60 loss: 0.3161356279932079\n",
      "  batch 70 loss: 0.5107085095252841\n",
      "  batch 80 loss: 0.38974424122861817\n",
      "  batch 90 loss: 0.46571613257401623\n",
      "  batch 100 loss: 0.38791349350940435\n",
      "  batch 110 loss: 0.34728142927015143\n",
      "  batch 120 loss: 0.3076237491623033\n",
      "  batch 130 loss: 0.39354369269422024\n",
      "  batch 140 loss: 0.46541568386019205\n",
      "  batch 150 loss: 0.320532216981519\n",
      "  batch 160 loss: 0.643226420308929\n",
      "  batch 170 loss: 0.29371734013548123\n",
      "  batch 180 loss: 0.39967858758755026\n",
      "  batch 190 loss: 0.3569886004202999\n",
      "LOSS train 0.3569886004202999 valid 0.3321481175834122\n",
      "EPOCH 198:\n",
      "  batch 10 loss: 0.3754304130736273\n",
      "  batch 20 loss: 0.35792698950972407\n",
      "  batch 30 loss: 0.3588394377089571\n",
      "  batch 40 loss: 0.21230318850721233\n",
      "  batch 50 loss: 0.5820692310488085\n",
      "  batch 60 loss: 0.3753755808633287\n",
      "  batch 70 loss: 0.67300397423096\n",
      "  batch 80 loss: 0.43249352360144255\n",
      "  batch 90 loss: 0.4044022244750522\n",
      "  batch 100 loss: 0.25038918687496337\n",
      "  batch 110 loss: 0.7330992561241146\n",
      "  batch 120 loss: 0.39129247032105924\n",
      "  batch 130 loss: 0.43924332699971275\n",
      "  batch 140 loss: 1.0526238002465107\n",
      "  batch 150 loss: 0.24673252118518577\n",
      "  batch 160 loss: 0.33775571270380167\n",
      "  batch 170 loss: 0.2679809881024994\n",
      "  batch 180 loss: 0.3855690228519961\n",
      "  batch 190 loss: 0.5236554234288633\n",
      "LOSS train 0.5236554234288633 valid 0.4560953537935021\n",
      "EPOCH 199:\n",
      "  batch 10 loss: 0.5675870133796707\n",
      "  batch 20 loss: 0.416212468163576\n",
      "  batch 30 loss: 0.8606672022346175\n",
      "  batch 40 loss: 0.3364602156507317\n",
      "  batch 50 loss: 0.32575969349127265\n",
      "  batch 60 loss: 0.6258755379880313\n",
      "  batch 70 loss: 0.39809434658382087\n",
      "  batch 80 loss: 0.29762757536955176\n",
      "  batch 90 loss: 0.25499379568500447\n",
      "  batch 100 loss: 0.32107993905665355\n",
      "  batch 110 loss: 0.30108149399748074\n",
      "  batch 120 loss: 0.4193558322498575\n",
      "  batch 130 loss: 0.5055714903399349\n",
      "  batch 140 loss: 0.4160644785995828\n",
      "  batch 150 loss: 0.28872107400675306\n",
      "  batch 160 loss: 0.35190828176564537\n",
      "  batch 170 loss: 0.23426745398319326\n",
      "  batch 180 loss: 0.5440822086704429\n",
      "  batch 190 loss: 0.23065678378916346\n",
      "LOSS train 0.23065678378916346 valid 0.7043307768166042\n",
      "EPOCH 200:\n",
      "  batch 10 loss: 0.6230681943008676\n",
      "  batch 20 loss: 0.5381645389832557\n",
      "  batch 30 loss: 0.34013162620831283\n",
      "  batch 40 loss: 0.33405319735175\n",
      "  batch 50 loss: 0.2616473377449438\n",
      "  batch 60 loss: 0.41424621339538137\n",
      "  batch 70 loss: 0.5365449746226659\n",
      "  batch 80 loss: 0.4843427804124076\n",
      "  batch 90 loss: 0.3999150897376239\n",
      "  batch 100 loss: 0.2437995118671097\n",
      "  batch 110 loss: 0.3883621139524621\n",
      "  batch 120 loss: 0.8789701182453428\n",
      "  batch 130 loss: 0.27429998183797577\n",
      "  batch 140 loss: 0.32475743457325734\n",
      "  batch 150 loss: 0.5059479336545337\n",
      "  batch 160 loss: 0.3341174631554168\n",
      "  batch 170 loss: 0.45877847824012863\n",
      "  batch 180 loss: 0.3641691335651558\n",
      "  batch 190 loss: 0.37158792213595004\n",
      "LOSS train 0.37158792213595004 valid 0.7216681912731684\n",
      "EPOCH 201:\n",
      "  batch 10 loss: 0.45972766205959487\n",
      "  batch 20 loss: 0.4436629906180315\n",
      "  batch 30 loss: 0.5451508275407833\n",
      "  batch 40 loss: 0.3992716580425622\n",
      "  batch 50 loss: 0.31736988889460915\n",
      "  batch 60 loss: 0.5179522757011\n",
      "  batch 70 loss: 0.4275288379867561\n",
      "  batch 80 loss: 0.7842244309256785\n",
      "  batch 90 loss: 0.2826046806410886\n",
      "  batch 100 loss: 0.4558322804456111\n",
      "  batch 110 loss: 0.34097351409727705\n",
      "  batch 120 loss: 0.25757022578036415\n",
      "  batch 130 loss: 0.32500062288017945\n",
      "  batch 140 loss: 0.3186649387847865\n",
      "  batch 150 loss: 0.37272613479872235\n",
      "  batch 160 loss: 0.2519247610121965\n",
      "  batch 170 loss: 0.17761766257317504\n",
      "  batch 180 loss: 0.2646181173287914\n",
      "  batch 190 loss: 0.5200997887804988\n",
      "LOSS train 0.5200997887804988 valid 0.3119060395650031\n",
      "EPOCH 202:\n",
      "  batch 10 loss: 0.2708346725048614\n",
      "  batch 20 loss: 0.6099121766543248\n",
      "  batch 30 loss: 0.2838869930477813\n",
      "  batch 40 loss: 0.7174559645543923\n",
      "  batch 50 loss: 0.4157125916390214\n",
      "  batch 60 loss: 0.34411238195898475\n",
      "  batch 70 loss: 0.45070896452816667\n",
      "  batch 80 loss: 0.3688229326158762\n",
      "  batch 90 loss: 0.3429212284157984\n",
      "  batch 100 loss: 0.28114278587454467\n",
      "  batch 110 loss: 0.31576247882912867\n",
      "  batch 120 loss: 0.2717490934563102\n",
      "  batch 130 loss: 0.3038442758275778\n",
      "  batch 140 loss: 0.8204415800748392\n",
      "  batch 150 loss: 0.30838520219549537\n",
      "  batch 160 loss: 0.7566223480855114\n",
      "  batch 170 loss: 0.43286803662776946\n",
      "  batch 180 loss: 0.2658887272715219\n",
      "  batch 190 loss: 0.296071018379007\n",
      "LOSS train 0.296071018379007 valid 1.3952186065878498\n",
      "EPOCH 203:\n",
      "  batch 10 loss: 0.3267802663845941\n",
      "  batch 20 loss: 0.7080449644010514\n",
      "  batch 30 loss: 0.36118060715380124\n",
      "  batch 40 loss: 0.313761360058561\n",
      "  batch 50 loss: 0.5301498717744835\n",
      "  batch 60 loss: 0.4078619607724249\n",
      "  batch 70 loss: 0.34402509639039636\n",
      "  batch 80 loss: 0.5462196021748241\n",
      "  batch 90 loss: 0.37100186140742153\n",
      "  batch 100 loss: 1.112767069124675\n",
      "  batch 110 loss: 0.5683346649748273\n",
      "  batch 120 loss: 0.33039116294239645\n",
      "  batch 130 loss: 0.4576596415834501\n",
      "  batch 140 loss: 0.2151527942129178\n",
      "  batch 150 loss: 1.083802544180071\n",
      "  batch 160 loss: 0.3226603762654122\n",
      "  batch 170 loss: 0.3760059904307127\n",
      "  batch 180 loss: 0.4869427875702968\n",
      "  batch 190 loss: 0.5156889193807729\n",
      "LOSS train 0.5156889193807729 valid 0.599078870714248\n",
      "EPOCH 204:\n",
      "  batch 10 loss: 0.38145775574521396\n",
      "  batch 20 loss: 0.468708218052052\n",
      "  batch 30 loss: 0.3211939175205771\n",
      "  batch 40 loss: 0.6058358959853649\n",
      "  batch 50 loss: 1.0092726449365728\n",
      "  batch 60 loss: 0.38531289803504476\n",
      "  batch 70 loss: 0.3975300652760779\n",
      "  batch 80 loss: 0.37420698747155257\n",
      "  batch 90 loss: 0.23527811547828606\n",
      "  batch 100 loss: 0.32466304702684284\n",
      "  batch 110 loss: 0.23641276012349408\n",
      "  batch 120 loss: 0.31550565639336126\n",
      "  batch 130 loss: 0.36668723365291955\n",
      "  batch 140 loss: 0.35669280993752184\n",
      "  batch 150 loss: 0.5987592769786716\n",
      "  batch 160 loss: 0.4985720828000922\n",
      "  batch 170 loss: 0.4649491797783412\n",
      "  batch 180 loss: 0.41352820133615753\n",
      "  batch 190 loss: 0.24136512447148561\n",
      "LOSS train 0.24136512447148561 valid 0.36617700226204947\n",
      "EPOCH 205:\n",
      "  batch 10 loss: 0.4443949213498854\n",
      "  batch 20 loss: 0.5361326949787326\n",
      "  batch 30 loss: 0.5326126808351546\n",
      "  batch 40 loss: 0.3398315877770074\n",
      "  batch 50 loss: 0.3597768804174848\n",
      "  batch 60 loss: 0.4346133523620665\n",
      "  batch 70 loss: 0.4505918338720221\n",
      "  batch 80 loss: 0.5008462739380775\n",
      "  batch 90 loss: 0.4841513386025326\n",
      "  batch 100 loss: 0.5892162242613267\n",
      "  batch 110 loss: 0.39235439943731765\n",
      "  batch 120 loss: 0.6254211877821945\n",
      "  batch 130 loss: 0.3704994810075732\n",
      "  batch 140 loss: 0.3846693565414171\n",
      "  batch 150 loss: 0.3189364928708528\n",
      "  batch 160 loss: 0.4272960316026001\n",
      "  batch 170 loss: 0.2963460519735236\n",
      "  batch 180 loss: 0.25448249220207797\n",
      "  batch 190 loss: 0.44037061017006635\n",
      "LOSS train 0.44037061017006635 valid 0.7203000861769303\n",
      "EPOCH 206:\n",
      "  batch 10 loss: 0.3816312023845967\n",
      "  batch 20 loss: 0.3559213640575763\n",
      "  batch 30 loss: 0.18678320473409257\n",
      "  batch 40 loss: 0.3537064467993332\n",
      "  batch 50 loss: 0.5122870930077624\n",
      "  batch 60 loss: 0.22539293895824813\n",
      "  batch 70 loss: 0.6207076317281462\n",
      "  batch 80 loss: 0.2825818537559826\n",
      "  batch 90 loss: 0.274780010106042\n",
      "  batch 100 loss: 0.4725349665939575\n",
      "  batch 110 loss: 0.21609103474183938\n",
      "  batch 120 loss: 0.351536109377048\n",
      "  batch 130 loss: 0.30581078434479425\n",
      "  batch 140 loss: 0.5898625708417967\n",
      "  batch 150 loss: 0.6453400044294539\n",
      "  batch 160 loss: 0.36752509549842216\n",
      "  batch 170 loss: 0.6527716933036573\n",
      "  batch 180 loss: 0.3767842660032329\n",
      "  batch 190 loss: 0.4204332472392707\n",
      "LOSS train 0.4204332472392707 valid 0.6902814067065125\n",
      "EPOCH 207:\n",
      "  batch 10 loss: 0.6451862717163749\n",
      "  batch 20 loss: 0.2426452916872222\n",
      "  batch 30 loss: 0.7267534791608341\n",
      "  batch 40 loss: 0.16616228672792205\n",
      "  batch 50 loss: 0.43602359733486085\n",
      "  batch 60 loss: 0.4898637560778297\n",
      "  batch 70 loss: 0.40739578188513403\n",
      "  batch 80 loss: 0.44805991635948883\n",
      "  batch 90 loss: 0.33880919101065954\n",
      "  batch 100 loss: 0.4218376148550306\n",
      "  batch 110 loss: 0.3222627698210999\n",
      "  batch 120 loss: 0.9512965970294317\n",
      "  batch 130 loss: 0.2208196797524579\n",
      "  batch 140 loss: 0.3132252335868543\n",
      "  batch 150 loss: 0.39999139233841563\n",
      "  batch 160 loss: 0.4261769414209994\n",
      "  batch 170 loss: 0.34058650528313594\n",
      "  batch 180 loss: 0.39024954867491035\n",
      "  batch 190 loss: 0.3295154204213759\n",
      "LOSS train 0.3295154204213759 valid 0.5098835481371796\n",
      "EPOCH 208:\n",
      "  batch 10 loss: 0.5983477808244061\n",
      "  batch 20 loss: 0.6744818941107951\n",
      "  batch 30 loss: 0.3957531528256368\n",
      "  batch 40 loss: 0.4320689104730263\n",
      "  batch 50 loss: 0.5036646743537858\n",
      "  batch 60 loss: 0.507425426505506\n",
      "  batch 70 loss: 0.37509939750889315\n",
      "  batch 80 loss: 0.5698505960637703\n",
      "  batch 90 loss: 0.3277833989879582\n",
      "  batch 100 loss: 0.4285051776445471\n",
      "  batch 110 loss: 0.42465931677725166\n",
      "  batch 120 loss: 0.35205156470183285\n",
      "  batch 130 loss: 0.4838735795463435\n",
      "  batch 140 loss: 0.3409283421613509\n",
      "  batch 150 loss: 0.32009125244803727\n",
      "  batch 160 loss: 0.22935289878369075\n",
      "  batch 170 loss: 0.43285043368232434\n",
      "  batch 180 loss: 0.37995334329316394\n",
      "  batch 190 loss: 0.24677368861739524\n",
      "LOSS train 0.24677368861739524 valid 0.35266044140548614\n",
      "EPOCH 209:\n",
      "  batch 10 loss: 0.2643801228958182\n",
      "  batch 20 loss: 0.4629298970219679\n",
      "  batch 30 loss: 0.47645977755018976\n",
      "  batch 40 loss: 0.36748792832368055\n",
      "  batch 50 loss: 0.47704143642040436\n",
      "  batch 60 loss: 0.2871038331039017\n",
      "  batch 70 loss: 0.5183284887927584\n",
      "  batch 80 loss: 0.3799999725044472\n",
      "  batch 90 loss: 0.4774628269544337\n",
      "  batch 100 loss: 0.36876145122223536\n",
      "  batch 110 loss: 0.25210726129007527\n",
      "  batch 120 loss: 0.29528446702170186\n",
      "  batch 130 loss: 0.3336986110603902\n",
      "  batch 140 loss: 0.4860845169983804\n",
      "  batch 150 loss: 0.7018293417757377\n",
      "  batch 160 loss: 0.41188481892459095\n",
      "  batch 170 loss: 0.49536793708684856\n",
      "  batch 180 loss: 0.3185493321856484\n",
      "  batch 190 loss: 0.4445837224222487\n",
      "LOSS train 0.4445837224222487 valid 0.46530516407167455\n",
      "EPOCH 210:\n",
      "  batch 10 loss: 0.5398784954100847\n",
      "  batch 20 loss: 0.22647478403087007\n",
      "  batch 30 loss: 0.3758800596027868\n",
      "  batch 40 loss: 0.30902707764762455\n",
      "  batch 50 loss: 0.4187343168479856\n",
      "  batch 60 loss: 0.31139728692360225\n",
      "  batch 70 loss: 0.5533771505375625\n",
      "  batch 80 loss: 0.7605353189152083\n",
      "  batch 90 loss: 0.4324871855031233\n",
      "  batch 100 loss: 0.4339223085960839\n",
      "  batch 110 loss: 0.31427246976527384\n",
      "  batch 120 loss: 0.31847239144262857\n",
      "  batch 130 loss: 0.27470560715009923\n",
      "  batch 140 loss: 0.35157934318413026\n",
      "  batch 150 loss: 0.5615501773834695\n",
      "  batch 160 loss: 0.2740830231225118\n",
      "  batch 170 loss: 0.19602385034086184\n",
      "  batch 180 loss: 0.2132349874576903\n",
      "  batch 190 loss: 0.4010481271339813\n",
      "LOSS train 0.4010481271339813 valid 1.9055348721576126\n",
      "EPOCH 211:\n",
      "  batch 10 loss: 0.833304510707967\n",
      "  batch 20 loss: 0.44874933317187243\n",
      "  batch 30 loss: 0.3443027500063181\n",
      "  batch 40 loss: 0.42615658744471147\n",
      "  batch 50 loss: 0.3120249711966608\n",
      "  batch 60 loss: 0.5791083857300692\n",
      "  batch 70 loss: 0.3193877109792084\n",
      "  batch 80 loss: 0.15995522347802762\n",
      "  batch 90 loss: 0.2236657798901433\n",
      "  batch 100 loss: 0.1793315948889358\n",
      "  batch 110 loss: 0.2974499128118623\n",
      "  batch 120 loss: 0.4522511741437484\n",
      "  batch 130 loss: 0.5388112642918713\n",
      "  batch 140 loss: 0.4048708636430092\n",
      "  batch 150 loss: 0.41510689568167436\n",
      "  batch 160 loss: 0.24618729005160275\n",
      "  batch 170 loss: 0.4399517278652638\n",
      "  batch 180 loss: 0.39115380509756503\n",
      "  batch 190 loss: 0.2559993758201017\n",
      "LOSS train 0.2559993758201017 valid 0.3263599085726011\n",
      "EPOCH 212:\n",
      "  batch 10 loss: 0.28508367959875613\n",
      "  batch 20 loss: 0.27887558371876364\n",
      "  batch 30 loss: 0.585695795060019\n",
      "  batch 40 loss: 0.3716644602041924\n",
      "  batch 50 loss: 0.41147274209361057\n",
      "  batch 60 loss: 0.3098990346305072\n",
      "  batch 70 loss: 0.4336613608757034\n",
      "  batch 80 loss: 0.20823000965756364\n",
      "  batch 90 loss: 0.36151278684847055\n",
      "  batch 100 loss: 0.3274445411982015\n",
      "  batch 110 loss: 0.45555294323712586\n",
      "  batch 120 loss: 0.2846107540128287\n",
      "  batch 130 loss: 0.4379426678206073\n",
      "  batch 140 loss: 0.42909753785206706\n",
      "  batch 150 loss: 0.16161151395353954\n",
      "  batch 160 loss: 0.41200740534695796\n",
      "  batch 170 loss: 0.4162992010344169\n",
      "  batch 180 loss: 0.25067785928840747\n",
      "  batch 190 loss: 0.35871166867436843\n",
      "LOSS train 0.35871166867436843 valid 0.4764744191578267\n",
      "EPOCH 213:\n",
      "  batch 10 loss: 0.4226059008971788\n",
      "  batch 20 loss: 0.11735089574067388\n",
      "  batch 30 loss: 0.38452967188786713\n",
      "  batch 40 loss: 0.18216789411962964\n",
      "  batch 50 loss: 0.46856726917321795\n",
      "  batch 60 loss: 0.3308960101567209\n",
      "  batch 70 loss: 0.34047389215556906\n",
      "  batch 80 loss: 0.561369049915811\n",
      "  batch 90 loss: 0.27066116040805355\n",
      "  batch 100 loss: 0.40462216368759985\n",
      "  batch 110 loss: 0.6218687530170428\n",
      "  batch 120 loss: 0.34399493007804266\n",
      "  batch 130 loss: 0.3595994930365123\n",
      "  batch 140 loss: 0.33352688631857746\n",
      "  batch 150 loss: 0.3568517648032866\n",
      "  batch 160 loss: 0.6404529250808991\n",
      "  batch 170 loss: 0.3360446792037692\n",
      "  batch 180 loss: 0.36519799031084405\n",
      "  batch 190 loss: 0.2540748588566203\n",
      "LOSS train 0.2540748588566203 valid 0.2896910688992088\n",
      "EPOCH 214:\n",
      "  batch 10 loss: 0.24143808034714312\n",
      "  batch 20 loss: 0.253592134360224\n",
      "  batch 30 loss: 0.44502316174912265\n",
      "  batch 40 loss: 0.7568092152680037\n",
      "  batch 50 loss: 0.5346157100313575\n",
      "  batch 60 loss: 0.7248776337364689\n",
      "  batch 70 loss: 0.5454814765194896\n",
      "  batch 80 loss: 0.23093175637768582\n",
      "  batch 90 loss: 0.38947334765689445\n",
      "  batch 100 loss: 0.3736264180974104\n",
      "  batch 110 loss: 0.2897799761616625\n",
      "  batch 120 loss: 0.7029308839140868\n",
      "  batch 130 loss: 0.2966353959869593\n",
      "  batch 140 loss: 0.3460866019478999\n",
      "  batch 150 loss: 0.3483252938138321\n",
      "  batch 160 loss: 0.28426621099642946\n",
      "  batch 170 loss: 0.20187927644146839\n",
      "  batch 180 loss: 0.28147482182830574\n",
      "  batch 190 loss: 0.24289528233493912\n",
      "LOSS train 0.24289528233493912 valid 1.7268957509621918\n",
      "EPOCH 215:\n",
      "  batch 10 loss: 0.5013348921434954\n",
      "  batch 20 loss: 0.4367096338464762\n",
      "  batch 30 loss: 0.3114243216405157\n",
      "  batch 40 loss: 0.4114217965514399\n",
      "  batch 50 loss: 0.5931500474718632\n",
      "  batch 60 loss: 0.4120505399478134\n",
      "  batch 70 loss: 0.2669385829285602\n",
      "  batch 80 loss: 0.29490494789206423\n",
      "  batch 90 loss: 0.227784667830565\n",
      "  batch 100 loss: 0.4450950404687319\n",
      "  batch 110 loss: 0.4221810766146518\n",
      "  batch 120 loss: 0.24320934284478427\n",
      "  batch 130 loss: 0.2733449203136843\n",
      "  batch 140 loss: 0.2941607756161829\n",
      "  batch 150 loss: 0.42729074039962145\n",
      "  batch 160 loss: 0.4915884209571232\n",
      "  batch 170 loss: 0.24438338993059006\n",
      "  batch 180 loss: 0.40803927306551485\n",
      "  batch 190 loss: 0.35434052615164546\n",
      "LOSS train 0.35434052615164546 valid 0.6914385757741118\n",
      "EPOCH 216:\n",
      "  batch 10 loss: 0.5969317220791709\n",
      "  batch 20 loss: 0.4462735218199668\n",
      "  batch 30 loss: 0.28892612318741157\n",
      "  batch 40 loss: 0.3850322361744475\n",
      "  batch 50 loss: 0.8005869202344911\n",
      "  batch 60 loss: 0.2554650364880217\n",
      "  batch 70 loss: 0.3705016507417895\n",
      "  batch 80 loss: 0.364686712785624\n",
      "  batch 90 loss: 0.437538301825407\n",
      "  batch 100 loss: 0.2361089996935334\n",
      "  batch 110 loss: 0.6536948014661903\n",
      "  batch 120 loss: 0.179566239065025\n",
      "  batch 130 loss: 0.19202154586382675\n",
      "  batch 140 loss: 0.3321287959290203\n",
      "  batch 150 loss: 0.40244118526461536\n",
      "  batch 160 loss: 0.2908724761015037\n",
      "  batch 170 loss: 0.5072288314957405\n",
      "  batch 180 loss: 0.3202420422283467\n",
      "  batch 190 loss: 0.3340597813192289\n",
      "LOSS train 0.3340597813192289 valid 0.3522680169281114\n",
      "EPOCH 217:\n",
      "  batch 10 loss: 0.2778614247858059\n",
      "  batch 20 loss: 0.34128844393999314\n",
      "  batch 30 loss: 0.5725398366921581\n",
      "  batch 40 loss: 0.6540725139107962\n",
      "  batch 50 loss: 0.3125748172176827\n",
      "  batch 60 loss: 0.294510912214173\n",
      "  batch 70 loss: 0.2670747210504487\n",
      "  batch 80 loss: 0.31284246467548654\n",
      "  batch 90 loss: 0.3556636448163772\n",
      "  batch 100 loss: 0.3742671071391669\n",
      "  batch 110 loss: 0.32586424956825794\n",
      "  batch 120 loss: 0.6373628628381993\n",
      "  batch 130 loss: 0.31971276108088204\n",
      "  batch 140 loss: 0.34367226719041355\n",
      "  batch 150 loss: 0.38698378383705856\n",
      "  batch 160 loss: 0.26954250638955274\n",
      "  batch 170 loss: 0.39245799407945015\n",
      "  batch 180 loss: 0.3535697695566341\n",
      "  batch 190 loss: 0.17776878679869695\n",
      "LOSS train 0.17776878679869695 valid 0.48926067805624396\n",
      "EPOCH 218:\n",
      "  batch 10 loss: 0.46843848782446\n",
      "  batch 20 loss: 0.20012254854809725\n",
      "  batch 30 loss: 0.32254826575517653\n",
      "  batch 40 loss: 0.21126944255665875\n",
      "  batch 50 loss: 0.3016010458173696\n",
      "  batch 60 loss: 0.5089696843060665\n",
      "  batch 70 loss: 0.2831216416103416\n",
      "  batch 80 loss: 0.3736085896729492\n",
      "  batch 90 loss: 0.5353599531663349\n",
      "  batch 100 loss: 0.19964664550789166\n",
      "  batch 110 loss: 0.1692848949925974\n",
      "  batch 120 loss: 0.3534021850122372\n",
      "  batch 130 loss: 0.2560178139261552\n",
      "  batch 140 loss: 0.3955863078183029\n",
      "  batch 150 loss: 0.2892403723650204\n",
      "  batch 160 loss: 0.20228737888392062\n",
      "  batch 170 loss: 0.6314177691616351\n",
      "  batch 180 loss: 0.3889358553686179\n",
      "  batch 190 loss: 0.335778130327526\n",
      "LOSS train 0.335778130327526 valid 0.49100018574077137\n",
      "EPOCH 219:\n",
      "  batch 10 loss: 0.2124169471557252\n",
      "  batch 20 loss: 0.38540024786489085\n",
      "  batch 30 loss: 0.30236260632955236\n",
      "  batch 40 loss: 0.19066492626734544\n",
      "  batch 50 loss: 0.36035181075712897\n",
      "  batch 60 loss: 0.2122079087763268\n",
      "  batch 70 loss: 0.3457401488383766\n",
      "  batch 80 loss: 0.21861143496789737\n",
      "  batch 90 loss: 0.2448744400520809\n",
      "  batch 100 loss: 0.28373159721959385\n",
      "  batch 110 loss: 0.6818718531227205\n",
      "  batch 120 loss: 0.36027352796754714\n",
      "  batch 130 loss: 0.6313192912057275\n",
      "  batch 140 loss: 0.43775004044000526\n",
      "  batch 150 loss: 0.2848265158580034\n",
      "  batch 160 loss: 0.4102164697178523\n",
      "  batch 170 loss: 0.2418752810684964\n",
      "  batch 180 loss: 0.21708994107902982\n",
      "  batch 190 loss: 0.661887207714608\n",
      "LOSS train 0.661887207714608 valid 0.5935522413945784\n",
      "EPOCH 220:\n",
      "  batch 10 loss: 0.976458556088619\n",
      "  batch 20 loss: 0.20666320488089696\n",
      "  batch 30 loss: 0.2568178285000613\n",
      "  batch 40 loss: 0.45331580547790506\n",
      "  batch 50 loss: 0.5526533766707871\n",
      "  batch 60 loss: 0.26729382626363074\n",
      "  batch 70 loss: 0.3888796652267047\n",
      "  batch 80 loss: 0.3731193095445633\n",
      "  batch 90 loss: 0.3728419748687884\n",
      "  batch 100 loss: 0.32247775134746914\n",
      "  batch 110 loss: 0.500465307480772\n",
      "  batch 120 loss: 0.4658391676755855\n",
      "  batch 130 loss: 0.3687855472730007\n",
      "  batch 140 loss: 0.14932438652031124\n",
      "  batch 150 loss: 0.16179080005094876\n",
      "  batch 160 loss: 0.2705111428324017\n",
      "  batch 170 loss: 0.35687311929068527\n",
      "  batch 180 loss: 0.35121207595802845\n",
      "  batch 190 loss: 0.30443136992980724\n",
      "LOSS train 0.30443136992980724 valid 0.5387206073965689\n",
      "EPOCH 221:\n",
      "  batch 10 loss: 0.43684634469100275\n",
      "  batch 20 loss: 0.5836709776922362\n",
      "  batch 30 loss: 0.4052185045205988\n",
      "  batch 40 loss: 0.4259221054875525\n",
      "  batch 50 loss: 0.38131445753824667\n",
      "  batch 60 loss: 0.3457296551510808\n",
      "  batch 70 loss: 0.29901630451204253\n",
      "  batch 80 loss: 0.3802807421016041\n",
      "  batch 90 loss: 0.22817111185868272\n",
      "  batch 100 loss: 0.1847274330968503\n",
      "  batch 110 loss: 0.28293224035951425\n",
      "  batch 120 loss: 0.5279802744014888\n",
      "  batch 130 loss: 0.47291232652496545\n",
      "  batch 140 loss: 0.3242432264596573\n",
      "  batch 150 loss: 0.7157352470385376\n",
      "  batch 160 loss: 0.32390022519975903\n",
      "  batch 170 loss: 0.24518891790794442\n",
      "  batch 180 loss: 0.42154668894509084\n",
      "  batch 190 loss: 0.353668570634909\n",
      "LOSS train 0.353668570634909 valid 0.7843764138274426\n",
      "EPOCH 222:\n",
      "  batch 10 loss: 0.3460378585310536\n",
      "  batch 20 loss: 0.21993378693878185\n",
      "  batch 30 loss: 0.44059303451213055\n",
      "  batch 40 loss: 0.2830458362594072\n",
      "  batch 50 loss: 0.28881860815454274\n",
      "  batch 60 loss: 0.29219171251024817\n",
      "  batch 70 loss: 0.6380919219227508\n",
      "  batch 80 loss: 0.24406553037115372\n",
      "  batch 90 loss: 0.39441662895260376\n",
      "  batch 100 loss: 0.4347339423839003\n",
      "  batch 110 loss: 0.19365886597661303\n",
      "  batch 120 loss: 0.2043832086666953\n",
      "  batch 130 loss: 0.41352588558802383\n",
      "  batch 140 loss: 0.3006646404246567\n",
      "  batch 150 loss: 0.4165930765448138\n",
      "  batch 160 loss: 0.6286911817325744\n",
      "  batch 170 loss: 0.3126440621854272\n",
      "  batch 180 loss: 0.32216657034296076\n",
      "  batch 190 loss: 0.2745832837885246\n",
      "LOSS train 0.2745832837885246 valid 0.37240572887355644\n",
      "EPOCH 223:\n",
      "  batch 10 loss: 0.28423138918587937\n",
      "  batch 20 loss: 0.2616670102943317\n",
      "  batch 30 loss: 0.19828708883942453\n",
      "  batch 40 loss: 0.2423297799163265\n",
      "  batch 50 loss: 0.30505881446879357\n",
      "  batch 60 loss: 0.28629396036703836\n",
      "  batch 70 loss: 0.39621609981986694\n",
      "  batch 80 loss: 0.3562435582600301\n",
      "  batch 90 loss: 0.33537932874751275\n",
      "  batch 100 loss: 0.5888735940767219\n",
      "  batch 110 loss: 0.3738307629711926\n",
      "  batch 120 loss: 1.2573642027098686\n",
      "  batch 130 loss: 0.3533919825858902\n",
      "  batch 140 loss: 0.20359923247888218\n",
      "  batch 150 loss: 0.3214016145997448\n",
      "  batch 160 loss: 0.2229769567376934\n",
      "  batch 170 loss: 0.22798556721536442\n",
      "  batch 180 loss: 0.2354522320383694\n",
      "  batch 190 loss: 0.5024486192618497\n",
      "LOSS train 0.5024486192618497 valid 0.27338578882709275\n",
      "EPOCH 224:\n",
      "  batch 10 loss: 0.5793816584977322\n",
      "  batch 20 loss: 0.24614455027040094\n",
      "  batch 30 loss: 0.25583587753935716\n",
      "  batch 40 loss: 0.20983321486564818\n",
      "  batch 50 loss: 0.41314796976803336\n",
      "  batch 60 loss: 0.2420346818456892\n",
      "  batch 70 loss: 0.2927547996456269\n",
      "  batch 80 loss: 0.42610552650876343\n",
      "  batch 90 loss: 0.23810905998107046\n",
      "  batch 100 loss: 0.2142802942675189\n",
      "  batch 110 loss: 0.28797827987000346\n",
      "  batch 120 loss: 0.28538687737454893\n",
      "  batch 130 loss: 0.5032753260864411\n",
      "  batch 140 loss: 0.5039435370068531\n",
      "  batch 150 loss: 0.4327687205426628\n",
      "  batch 160 loss: 0.33839598500053397\n",
      "  batch 170 loss: 0.25398542185430417\n",
      "  batch 180 loss: 0.6452614082721994\n",
      "  batch 190 loss: 0.1933313653775258\n",
      "LOSS train 0.1933313653775258 valid 2.0573802435090047\n",
      "EPOCH 225:\n",
      "  batch 10 loss: 0.6489928345421504\n",
      "  batch 20 loss: 0.39581103548116514\n",
      "  batch 30 loss: 0.18297796105252928\n",
      "  batch 40 loss: 0.3761960501680733\n",
      "  batch 50 loss: 0.2947674797629588\n",
      "  batch 60 loss: 0.27559661454288287\n",
      "  batch 70 loss: 0.22559564622933975\n",
      "  batch 80 loss: 0.3703117209428456\n",
      "  batch 90 loss: 0.35409799326444047\n",
      "  batch 100 loss: 0.4642090861976612\n",
      "  batch 110 loss: 0.28615356870286635\n",
      "  batch 120 loss: 0.23340366321790498\n",
      "  batch 130 loss: 0.29294015449122524\n",
      "  batch 140 loss: 0.40876968776865397\n",
      "  batch 150 loss: 0.40182474211906083\n",
      "  batch 160 loss: 0.41620756886550225\n",
      "  batch 170 loss: 0.5106050627844525\n",
      "  batch 180 loss: 0.22148801224684575\n",
      "  batch 190 loss: 0.47216670305933806\n",
      "LOSS train 0.47216670305933806 valid 0.35634365761833553\n",
      "EPOCH 226:\n",
      "  batch 10 loss: 0.618058455735445\n",
      "  batch 20 loss: 0.4111914230379625\n",
      "  batch 30 loss: 0.7069977762439521\n",
      "  batch 40 loss: 0.2850592721573776\n",
      "  batch 50 loss: 0.1540495273016859\n",
      "  batch 60 loss: 0.1558125377559918\n",
      "  batch 70 loss: 0.5260315697079931\n",
      "  batch 80 loss: 0.4563738652621396\n",
      "  batch 90 loss: 0.46011777591047576\n",
      "  batch 100 loss: 0.363398142970982\n",
      "  batch 110 loss: 0.29470213069726015\n",
      "  batch 120 loss: 0.25689637050381864\n",
      "  batch 130 loss: 0.26961198128847175\n",
      "  batch 140 loss: 0.31777868897479494\n",
      "  batch 150 loss: 0.6631906665061251\n",
      "  batch 160 loss: 0.1700554696493782\n",
      "  batch 170 loss: 0.28397828920715257\n",
      "  batch 180 loss: 0.4417220694915159\n",
      "  batch 190 loss: 0.4623891783907311\n",
      "LOSS train 0.4623891783907311 valid 0.2938148505868762\n",
      "EPOCH 227:\n",
      "  batch 10 loss: 0.28035013614426135\n",
      "  batch 20 loss: 0.487192904096446\n",
      "  batch 30 loss: 0.16363210744602838\n",
      "  batch 40 loss: 0.30439862722996625\n",
      "  batch 50 loss: 0.3081331721652532\n",
      "  batch 60 loss: 0.4468310833690339\n",
      "  batch 70 loss: 0.4869276550562063\n",
      "  batch 80 loss: 0.2701830173129565\n",
      "  batch 90 loss: 0.20898414733892423\n",
      "  batch 100 loss: 0.23956007828819564\n",
      "  batch 110 loss: 0.6295887382817454\n",
      "  batch 120 loss: 0.5678358957578894\n",
      "  batch 130 loss: 0.3686309879711189\n",
      "  batch 140 loss: 0.17139354090759298\n",
      "  batch 150 loss: 0.16224016651758574\n",
      "  batch 160 loss: 0.16036964455270208\n",
      "  batch 170 loss: 0.3000984596699709\n",
      "  batch 180 loss: 0.3038903975844732\n",
      "  batch 190 loss: 0.45904303136630914\n",
      "LOSS train 0.45904303136630914 valid 1.098365842927589\n",
      "EPOCH 228:\n",
      "  batch 10 loss: 0.5144752401654842\n",
      "  batch 20 loss: 0.44937625469174236\n",
      "  batch 30 loss: 0.256629739102209\n",
      "  batch 40 loss: 0.3211995408659277\n",
      "  batch 50 loss: 0.14883411390292167\n",
      "  batch 60 loss: 0.2988941779654851\n",
      "  batch 70 loss: 0.41079010794783244\n",
      "  batch 80 loss: 0.3547449864097871\n",
      "  batch 90 loss: 0.33455009520985185\n",
      "  batch 100 loss: 0.2603060840338003\n",
      "  batch 110 loss: 0.6611394792489591\n",
      "  batch 120 loss: 0.3787518021039432\n",
      "  batch 130 loss: 0.2171618637861684\n",
      "  batch 140 loss: 0.475393074767635\n",
      "  batch 150 loss: 0.5073532733513275\n",
      "  batch 160 loss: 0.3191473862854764\n",
      "  batch 170 loss: 0.27437299922312375\n",
      "  batch 180 loss: 0.37204152545309627\n",
      "  batch 190 loss: 0.22796277167508378\n",
      "LOSS train 0.22796277167508378 valid 0.6207917172121084\n",
      "EPOCH 229:\n",
      "  batch 10 loss: 0.15157419093011412\n",
      "  batch 20 loss: 0.10384997528308304\n",
      "  batch 30 loss: 0.3288022766006179\n",
      "  batch 40 loss: 0.32388406318204943\n",
      "  batch 50 loss: 0.1850102361102472\n",
      "  batch 60 loss: 0.46769727042119486\n",
      "  batch 70 loss: 0.1594853045709897\n",
      "  batch 80 loss: 0.36744157610228284\n",
      "  batch 90 loss: 0.25994145997683515\n",
      "  batch 100 loss: 0.4095886690134648\n",
      "  batch 110 loss: 0.38667336297221483\n",
      "  batch 120 loss: 0.487300782672537\n",
      "  batch 130 loss: 0.26962922171951503\n",
      "  batch 140 loss: 0.286075796912337\n",
      "  batch 150 loss: 0.7304037480149418\n",
      "  batch 160 loss: 0.9604235544444236\n",
      "  batch 170 loss: 0.24269160161784384\n",
      "  batch 180 loss: 0.2931086370474077\n",
      "  batch 190 loss: 0.2749042996176286\n",
      "LOSS train 0.2749042996176286 valid 1.5024224196399116\n",
      "EPOCH 230:\n",
      "  batch 10 loss: 0.19240050736480044\n",
      "  batch 20 loss: 0.4459573678730521\n",
      "  batch 30 loss: 0.2569944745846442\n",
      "  batch 40 loss: 0.36036897072553986\n",
      "  batch 50 loss: 0.4006368191083311\n",
      "  batch 60 loss: 0.2669793710716476\n",
      "  batch 70 loss: 0.22284400249336614\n",
      "  batch 80 loss: 0.3542303061840357\n",
      "  batch 90 loss: 0.2775872054095089\n",
      "  batch 100 loss: 0.3803674886468798\n",
      "  batch 110 loss: 0.48285449118411633\n",
      "  batch 120 loss: 0.19039605489524547\n",
      "  batch 130 loss: 0.43287634161970345\n",
      "  batch 140 loss: 0.5035138850071235\n",
      "  batch 150 loss: 0.13994060218683443\n",
      "  batch 160 loss: 0.34019815461069813\n",
      "  batch 170 loss: 0.19481936597512686\n",
      "  batch 180 loss: 0.2587856687794556\n",
      "  batch 190 loss: 0.31947875908444984\n",
      "LOSS train 0.31947875908444984 valid 0.31601285450522437\n",
      "EPOCH 231:\n",
      "  batch 10 loss: 0.6861997088839417\n",
      "  batch 20 loss: 0.47260911376470177\n",
      "  batch 30 loss: 0.3880751827251515\n",
      "  batch 40 loss: 0.21893043326563202\n",
      "  batch 50 loss: 0.19846499401173787\n",
      "  batch 60 loss: 0.3129161675715295\n",
      "  batch 70 loss: 0.3476002070994582\n",
      "  batch 80 loss: 0.37275504710851237\n",
      "  batch 90 loss: 0.3728133702083142\n",
      "  batch 100 loss: 0.2507720053879893\n",
      "  batch 110 loss: 0.3735259622822923\n",
      "  batch 120 loss: 0.3541326058213599\n",
      "  batch 130 loss: 0.41417056219652293\n",
      "  batch 140 loss: 0.1839109284512233\n",
      "  batch 150 loss: 0.22914467069058447\n",
      "  batch 160 loss: 0.3932937920151744\n",
      "  batch 170 loss: 0.1331788986630272\n",
      "  batch 180 loss: 0.4626605467812624\n",
      "  batch 190 loss: 0.2587648497588816\n",
      "LOSS train 0.2587648497588816 valid 4.924500425218439\n",
      "EPOCH 232:\n",
      "  batch 10 loss: 1.892792444870429\n",
      "  batch 20 loss: 0.3652433951268904\n",
      "  batch 30 loss: 0.2628859221120365\n",
      "  batch 40 loss: 0.27448590328276623\n",
      "  batch 50 loss: 0.4278705692500807\n",
      "  batch 60 loss: 0.3067880735703511\n",
      "  batch 70 loss: 0.2793439015193144\n",
      "  batch 80 loss: 0.26438531402818627\n",
      "  batch 90 loss: 0.24017600996303373\n",
      "  batch 100 loss: 0.39988261804683134\n",
      "  batch 110 loss: 0.1900197331313393\n",
      "  batch 120 loss: 0.4994358506897697\n",
      "  batch 130 loss: 0.33040981350204673\n",
      "  batch 140 loss: 0.18741377368132817\n",
      "  batch 150 loss: 0.26565859635156813\n",
      "  batch 160 loss: 0.4448023264922085\n",
      "  batch 170 loss: 0.6202693966712104\n",
      "  batch 180 loss: 0.1947891219460871\n",
      "  batch 190 loss: 0.12913789343292592\n",
      "LOSS train 0.12913789343292592 valid 0.3232738005987276\n",
      "EPOCH 233:\n",
      "  batch 10 loss: 0.5512870730366558\n",
      "  batch 20 loss: 0.3122648762218887\n",
      "  batch 30 loss: 0.30029065551279926\n",
      "  batch 40 loss: 0.7360265229639481\n",
      "  batch 50 loss: 0.22682164523139364\n",
      "  batch 60 loss: 0.3879155126840487\n",
      "  batch 70 loss: 0.27938370004994795\n",
      "  batch 80 loss: 0.17614244807045906\n",
      "  batch 90 loss: 0.5523528915044154\n",
      "  batch 100 loss: 0.22597847801662282\n",
      "  batch 110 loss: 0.33380499029517524\n",
      "  batch 120 loss: 0.36563108685513723\n",
      "  batch 130 loss: 0.4661291863885708\n",
      "  batch 140 loss: 0.3081011085276259\n",
      "  batch 150 loss: 0.25923268539772837\n",
      "  batch 160 loss: 0.30614007989424863\n",
      "  batch 170 loss: 0.1755836162432388\n",
      "  batch 180 loss: 0.2651536289748037\n",
      "  batch 190 loss: 0.269933044546633\n",
      "LOSS train 0.269933044546633 valid 0.30997272437973433\n",
      "EPOCH 234:\n",
      "  batch 10 loss: 0.12095327272982104\n",
      "  batch 20 loss: 0.5219760030944599\n",
      "  batch 30 loss: 0.669120047710021\n",
      "  batch 40 loss: 0.23135868755925912\n",
      "  batch 50 loss: 0.18702065956313163\n",
      "  batch 60 loss: 0.23752920740371336\n",
      "  batch 70 loss: 0.24030414917651796\n",
      "  batch 80 loss: 0.35431682975031437\n",
      "  batch 90 loss: 0.36459057694883085\n",
      "  batch 100 loss: 0.2038299583411572\n",
      "  batch 110 loss: 0.22276206210590316\n",
      "  batch 120 loss: 0.20416835470168734\n",
      "  batch 130 loss: 0.5529531430176575\n",
      "  batch 140 loss: 0.4111515098280506\n",
      "  batch 150 loss: 0.20412170388444792\n",
      "  batch 160 loss: 0.2856623514555395\n",
      "  batch 170 loss: 0.2562779412022792\n",
      "  batch 180 loss: 0.350211995880818\n",
      "  batch 190 loss: 0.40921197716379537\n",
      "LOSS train 0.40921197716379537 valid 1.4764343982814097\n",
      "EPOCH 235:\n",
      "  batch 10 loss: 0.3268791762428009\n",
      "  batch 20 loss: 0.3764946838287869\n",
      "  batch 30 loss: 0.4083967210113769\n",
      "  batch 40 loss: 0.36819403492700076\n",
      "  batch 50 loss: 0.5923210695742455\n",
      "  batch 60 loss: 0.47585218585882105\n",
      "  batch 70 loss: 0.20426405849248114\n",
      "  batch 80 loss: 0.2797341002689791\n",
      "  batch 90 loss: 0.21227511124743614\n",
      "  batch 100 loss: 0.21321157979036798\n",
      "  batch 110 loss: 0.39504116927037103\n",
      "  batch 120 loss: 0.2611844553910487\n",
      "  batch 130 loss: 0.5252292704994034\n",
      "  batch 140 loss: 0.35928044090396727\n",
      "  batch 150 loss: 0.2701007449737517\n",
      "  batch 160 loss: 0.3185146217307192\n",
      "  batch 170 loss: 0.3999263103076373\n",
      "  batch 180 loss: 0.25954085644480074\n",
      "  batch 190 loss: 0.32225470463890815\n",
      "LOSS train 0.32225470463890815 valid 0.28058554783582135\n",
      "EPOCH 236:\n",
      "  batch 10 loss: 0.3574864462694677\n",
      "  batch 20 loss: 0.20182865375973053\n",
      "  batch 30 loss: 0.3730988665309269\n",
      "  batch 40 loss: 0.1761449885307229\n",
      "  batch 50 loss: 0.49312646892212797\n",
      "  batch 60 loss: 0.3258570767560741\n",
      "  batch 70 loss: 0.29514956639613954\n",
      "  batch 80 loss: 0.21091637871577404\n",
      "  batch 90 loss: 0.35548545345664023\n",
      "  batch 100 loss: 0.327605941305228\n",
      "  batch 110 loss: 0.25670455472718456\n",
      "  batch 120 loss: 0.2343525640462758\n",
      "  batch 130 loss: 0.36834174450705176\n",
      "  batch 140 loss: 0.3604454835498473\n",
      "  batch 150 loss: 0.47986170068033973\n",
      "  batch 160 loss: 0.2965216820331989\n",
      "  batch 170 loss: 0.3651346297905548\n",
      "  batch 180 loss: 0.33541852733615085\n",
      "  batch 190 loss: 0.3283117857252364\n",
      "LOSS train 0.3283117857252364 valid 0.2687164948938283\n",
      "EPOCH 237:\n",
      "  batch 10 loss: 0.4532743516283517\n",
      "  batch 20 loss: 0.2914835314790253\n",
      "  batch 30 loss: 0.11979080336168409\n",
      "  batch 40 loss: 0.09542125091684284\n",
      "  batch 50 loss: 0.2400056804093765\n",
      "  batch 60 loss: 0.24694521291967247\n",
      "  batch 70 loss: 0.27417578004278764\n",
      "  batch 80 loss: 0.3230664776798221\n",
      "  batch 90 loss: 0.39030572008268793\n",
      "  batch 100 loss: 0.4181459280789568\n",
      "  batch 110 loss: 0.340225355529401\n",
      "  batch 120 loss: 0.2927214557748812\n",
      "  batch 130 loss: 0.6397896539307112\n",
      "  batch 140 loss: 0.24764539118332324\n",
      "  batch 150 loss: 0.6527914688907913\n",
      "  batch 160 loss: 0.2569857921844232\n",
      "  batch 170 loss: 0.39888181683054424\n",
      "  batch 180 loss: 0.48715241440513635\n",
      "  batch 190 loss: 0.29379422729834914\n",
      "LOSS train 0.29379422729834914 valid 1.0274164576675946\n",
      "EPOCH 238:\n",
      "  batch 10 loss: 0.26297511272423435\n",
      "  batch 20 loss: 0.28022455629106846\n",
      "  batch 30 loss: 0.2340855983115034\n",
      "  batch 40 loss: 0.4155156278735376\n",
      "  batch 50 loss: 0.3608800277870614\n",
      "  batch 60 loss: 0.3180769346479792\n",
      "  batch 70 loss: 0.38550952487566975\n",
      "  batch 80 loss: 0.45584803146339253\n",
      "  batch 90 loss: 0.2996783256341587\n",
      "  batch 100 loss: 0.35075606291065925\n",
      "  batch 110 loss: 0.676558751014818\n",
      "  batch 120 loss: 0.6245601837872528\n",
      "  batch 130 loss: 0.4682624698383734\n",
      "  batch 140 loss: 0.4488989522709744\n",
      "  batch 150 loss: 0.3155815539052128\n",
      "  batch 160 loss: 0.24414300270436798\n",
      "  batch 170 loss: 0.22173498080155696\n",
      "  batch 180 loss: 0.341210395462258\n",
      "  batch 190 loss: 0.20104317832192464\n",
      "LOSS train 0.20104317832192464 valid 0.33521568826108705\n",
      "EPOCH 239:\n",
      "  batch 10 loss: 0.25324654865034973\n",
      "  batch 20 loss: 0.3461976040503941\n",
      "  batch 30 loss: 0.32282040012069046\n",
      "  batch 40 loss: 0.30024044240999503\n",
      "  batch 50 loss: 0.30386498851148647\n",
      "  batch 60 loss: 0.33225306198437465\n",
      "  batch 70 loss: 0.5821706989187078\n",
      "  batch 80 loss: 0.3111059961163846\n",
      "  batch 90 loss: 0.10373104568207055\n",
      "  batch 100 loss: 0.22476919841319615\n",
      "  batch 110 loss: 0.3014917905326001\n",
      "  batch 120 loss: 0.19944259042385964\n",
      "  batch 130 loss: 0.2277083884080639\n",
      "  batch 140 loss: 0.23827849807275925\n",
      "  batch 150 loss: 0.3105030450475169\n",
      "  batch 160 loss: 0.41695455966255396\n",
      "  batch 170 loss: 0.14572342301835306\n",
      "  batch 180 loss: 0.3598643691235338\n",
      "  batch 190 loss: 0.2392397264833562\n",
      "LOSS train 0.2392397264833562 valid 1.294782710744841\n",
      "EPOCH 240:\n",
      "  batch 10 loss: 0.49129932859941616\n",
      "  batch 20 loss: 0.24729887152207083\n",
      "  batch 30 loss: 0.2709336688611074\n",
      "  batch 40 loss: 0.2067668993564439\n",
      "  batch 50 loss: 0.40183775535988386\n",
      "  batch 60 loss: 0.21298148425412364\n",
      "  batch 70 loss: 0.32065372896613553\n",
      "  batch 80 loss: 0.4675236708346347\n",
      "  batch 90 loss: 0.32674261515785474\n",
      "  batch 100 loss: 0.35814629098167644\n",
      "  batch 110 loss: 0.3239503133634571\n",
      "  batch 120 loss: 0.257337376261421\n",
      "  batch 130 loss: 0.242086938401917\n",
      "  batch 140 loss: 0.1150330698947073\n",
      "  batch 150 loss: 0.19137682151340413\n",
      "  batch 160 loss: 0.3003099690482486\n",
      "  batch 170 loss: 0.3258403248357354\n",
      "  batch 180 loss: 0.2018350516707869\n",
      "  batch 190 loss: 0.26687935541704066\n",
      "LOSS train 0.26687935541704066 valid 0.29301935776945753\n",
      "EPOCH 241:\n",
      "  batch 10 loss: 0.4934340207808418\n",
      "  batch 20 loss: 0.11704948413389502\n",
      "  batch 30 loss: 0.15640216812607832\n",
      "  batch 40 loss: 0.2126504446772742\n",
      "  batch 50 loss: 0.4338403022789862\n",
      "  batch 60 loss: 0.3340466773814114\n",
      "  batch 70 loss: 0.22456911576300626\n",
      "  batch 80 loss: 0.3010033035418019\n",
      "  batch 90 loss: 0.8759218993014656\n",
      "  batch 100 loss: 0.5473512448079418\n",
      "  batch 110 loss: 0.2083760435525619\n",
      "  batch 120 loss: 0.40720876479608703\n",
      "  batch 130 loss: 0.7162633645784808\n",
      "  batch 140 loss: 0.14663916730787604\n",
      "  batch 150 loss: 0.2858737405902502\n",
      "  batch 160 loss: 0.1960791078163311\n",
      "  batch 170 loss: 0.39774898784962714\n",
      "  batch 180 loss: 0.3699105363804847\n",
      "  batch 190 loss: 0.30811710931011477\n",
      "LOSS train 0.30811710931011477 valid 0.3384507657733943\n",
      "EPOCH 242:\n",
      "  batch 10 loss: 0.10744324105908162\n",
      "  batch 20 loss: 0.2372440624269075\n",
      "  batch 30 loss: 0.3717003793921322\n",
      "  batch 40 loss: 0.2884401376504684\n",
      "  batch 50 loss: 0.7090770538110519\n",
      "  batch 60 loss: 0.5474535822402686\n",
      "  batch 70 loss: 0.3700202172971331\n",
      "  batch 80 loss: 0.22925391807220877\n",
      "  batch 90 loss: 0.3312286682426929\n",
      "  batch 100 loss: 0.21548876901360928\n",
      "  batch 110 loss: 0.31121365905128184\n",
      "  batch 120 loss: 0.428363322885707\n",
      "  batch 130 loss: 0.5545461170637281\n",
      "  batch 140 loss: 0.24728667718882208\n",
      "  batch 150 loss: 0.38975591328635345\n",
      "  batch 160 loss: 0.17979730367369484\n",
      "  batch 170 loss: 0.6862249859448639\n",
      "  batch 180 loss: 0.30302241938770746\n",
      "  batch 190 loss: 0.47884048176929356\n",
      "LOSS train 0.47884048176929356 valid 0.2686953395918513\n",
      "EPOCH 243:\n",
      "  batch 10 loss: 0.3302066509320866\n",
      "  batch 20 loss: 0.1484714459074894\n",
      "  batch 30 loss: 0.3287038409267552\n",
      "  batch 40 loss: 0.1507156406325521\n",
      "  batch 50 loss: 0.20632392110710499\n",
      "  batch 60 loss: 0.1810006323692505\n",
      "  batch 70 loss: 0.23818568690039682\n",
      "  batch 80 loss: 0.09563730845111422\n",
      "  batch 90 loss: 0.37304763875145\n",
      "  batch 100 loss: 0.6524391808663494\n",
      "  batch 110 loss: 0.4275981215090724\n",
      "  batch 120 loss: 0.40004185424331806\n",
      "  batch 130 loss: 0.2406010038073873\n",
      "  batch 140 loss: 0.3816734414052917\n",
      "  batch 150 loss: 0.18253908670740202\n",
      "  batch 160 loss: 0.13563537230656947\n",
      "  batch 170 loss: 0.2912486858840566\n",
      "  batch 180 loss: 0.33207907904143213\n",
      "  batch 190 loss: 0.3268001283839112\n",
      "LOSS train 0.3268001283839112 valid 1.4051051540848745\n",
      "EPOCH 244:\n",
      "  batch 10 loss: 0.5147286000545137\n",
      "  batch 20 loss: 0.24299735568783945\n",
      "  batch 30 loss: 0.1652950701347436\n",
      "  batch 40 loss: 0.1412491761118872\n",
      "  batch 50 loss: 0.32546183077283786\n",
      "  batch 60 loss: 0.3414775924044079\n",
      "  batch 70 loss: 0.36166974263032897\n",
      "  batch 80 loss: 0.3247673810576089\n",
      "  batch 90 loss: 0.2552431020885706\n",
      "  batch 100 loss: 0.2122158473604941\n",
      "  batch 110 loss: 0.3527051737677539\n",
      "  batch 120 loss: 0.24791066267353018\n",
      "  batch 130 loss: 0.20834880496186087\n",
      "  batch 140 loss: 0.3229908655077452\n",
      "  batch 150 loss: 0.24139866237055685\n",
      "  batch 160 loss: 0.24014841583921226\n",
      "  batch 170 loss: 0.44531336674117483\n",
      "  batch 180 loss: 0.24010667307156836\n",
      "  batch 190 loss: 0.33604720968869517\n",
      "LOSS train 0.33604720968869517 valid 0.3299523828839786\n",
      "EPOCH 245:\n",
      "  batch 10 loss: 0.22838622353156096\n",
      "  batch 20 loss: 0.16600817768339765\n",
      "  batch 30 loss: 0.2882432569458615\n",
      "  batch 40 loss: 0.3817468184483005\n",
      "  batch 50 loss: 0.24997919342713432\n",
      "  batch 60 loss: 0.10823371157457587\n",
      "  batch 70 loss: 0.22837668572028633\n",
      "  batch 80 loss: 0.29077169983793283\n",
      "  batch 90 loss: 0.3378750858744752\n",
      "  batch 100 loss: 0.21216856677201576\n",
      "  batch 110 loss: 0.19514606799857576\n",
      "  batch 120 loss: 0.19335120309697232\n",
      "  batch 130 loss: 0.3378440468222834\n",
      "  batch 140 loss: 0.2718469371095125\n",
      "  batch 150 loss: 0.42628862434066833\n",
      "  batch 160 loss: 0.3693926978317904\n",
      "  batch 170 loss: 0.3009642373173847\n",
      "  batch 180 loss: 0.4376247383388545\n",
      "  batch 190 loss: 0.27993604751172824\n",
      "LOSS train 0.27993604751172824 valid 0.2929416057066611\n",
      "EPOCH 246:\n",
      "  batch 10 loss: 0.4058801695209695\n",
      "  batch 20 loss: 0.14661950344743674\n",
      "  batch 30 loss: 0.15938181874225849\n",
      "  batch 40 loss: 0.4386079321353463\n",
      "  batch 50 loss: 0.26426811345809254\n",
      "  batch 60 loss: 0.4084950347070844\n",
      "  batch 70 loss: 1.1605501821628423\n",
      "  batch 80 loss: 0.49619458916276926\n",
      "  batch 90 loss: 0.4188018705106515\n",
      "  batch 100 loss: 0.2302999234147137\n",
      "  batch 110 loss: 0.1593120069883298\n",
      "  batch 120 loss: 0.179881018721062\n",
      "  batch 130 loss: 0.22075428628595545\n",
      "  batch 140 loss: 0.1488855644594878\n",
      "  batch 150 loss: 0.3611375460430281\n",
      "  batch 160 loss: 0.25106008551010744\n",
      "  batch 170 loss: 0.24508421286736848\n",
      "  batch 180 loss: 0.24007949858059874\n",
      "  batch 190 loss: 0.24845570202742237\n",
      "LOSS train 0.24845570202742237 valid 0.2968108611398165\n",
      "EPOCH 247:\n",
      "  batch 10 loss: 0.20275192010449244\n",
      "  batch 20 loss: 0.37568603965482905\n",
      "  batch 30 loss: 0.36237192778498867\n",
      "  batch 40 loss: 0.3114130602451041\n",
      "  batch 50 loss: 0.21191770148943762\n",
      "  batch 60 loss: 0.24675042854287313\n",
      "  batch 70 loss: 0.0598619648393651\n",
      "  batch 80 loss: 0.2676136368798325\n",
      "  batch 90 loss: 0.45851989223010603\n",
      "  batch 100 loss: 0.35016850581669134\n",
      "  batch 110 loss: 0.4350710345170228\n",
      "  batch 120 loss: 0.14132031380286209\n",
      "  batch 130 loss: 0.23645458446408157\n",
      "  batch 140 loss: 0.593213219824247\n",
      "  batch 150 loss: 0.3866384172186372\n",
      "  batch 160 loss: 0.34706989179831\n",
      "  batch 170 loss: 0.3267213703249581\n",
      "  batch 180 loss: 0.3189786303555593\n",
      "  batch 190 loss: 0.47357325340562967\n",
      "LOSS train 0.47357325340562967 valid 0.2886573576703925\n",
      "EPOCH 248:\n",
      "  batch 10 loss: 0.6375024234461307\n",
      "  batch 20 loss: 0.44719666962191695\n",
      "  batch 30 loss: 0.5105962718596857\n",
      "  batch 40 loss: 0.25476685807079774\n",
      "  batch 50 loss: 0.4232753636613779\n",
      "  batch 60 loss: 0.2205904864880722\n",
      "  batch 70 loss: 0.3558900071773678\n",
      "  batch 80 loss: 0.18354513348749607\n",
      "  batch 90 loss: 0.3843690763562336\n",
      "  batch 100 loss: 0.3195452816376928\n",
      "  batch 110 loss: 0.3626916584860737\n",
      "  batch 120 loss: 0.2595218833826948\n",
      "  batch 130 loss: 0.3126286951024667\n",
      "  batch 140 loss: 0.21309926783578703\n",
      "  batch 150 loss: 0.4327891834051115\n",
      "  batch 160 loss: 0.27799928795575396\n",
      "  batch 170 loss: 0.1850645674217958\n",
      "  batch 180 loss: 0.2902101054991363\n",
      "  batch 190 loss: 0.4151016102550784\n",
      "LOSS train 0.4151016102550784 valid 0.292499331598941\n",
      "EPOCH 249:\n",
      "  batch 10 loss: 0.16626363020623103\n",
      "  batch 20 loss: 0.4006379065438523\n",
      "  batch 30 loss: 0.4226125869725365\n",
      "  batch 40 loss: 0.2872227149142418\n",
      "  batch 50 loss: 0.4011938912241021\n",
      "  batch 60 loss: 0.2804643378243782\n",
      "  batch 70 loss: 0.4051472904771799\n",
      "  batch 80 loss: 0.29406019254092824\n",
      "  batch 90 loss: 1.0989099881364381\n",
      "  batch 100 loss: 0.3684766052872874\n",
      "  batch 110 loss: 0.20163612840115092\n",
      "  batch 120 loss: 0.17379376635653898\n",
      "  batch 130 loss: 0.17554023724806028\n",
      "  batch 140 loss: 0.2776702565708547\n",
      "  batch 150 loss: 0.307537639894872\n",
      "  batch 160 loss: 0.4152847194309288\n",
      "  batch 170 loss: 0.16754560942717944\n",
      "  batch 180 loss: 0.2356915587937692\n",
      "  batch 190 loss: 0.21541584275255446\n",
      "LOSS train 0.21541584275255446 valid 0.3756201703206152\n",
      "EPOCH 250:\n",
      "  batch 10 loss: 0.47855702411688983\n",
      "  batch 20 loss: 0.5003564509788703\n",
      "  batch 30 loss: 0.1550453003597795\n",
      "  batch 40 loss: 0.2152772801258834\n",
      "  batch 50 loss: 0.24819446083929506\n",
      "  batch 60 loss: 0.3798812438122695\n",
      "  batch 70 loss: 0.1698713415142265\n",
      "  batch 80 loss: 0.25543081813520985\n",
      "  batch 90 loss: 0.1382333372021094\n",
      "  batch 100 loss: 0.29218437054369134\n",
      "  batch 110 loss: 0.1945634659510688\n",
      "  batch 120 loss: 0.20827816936725868\n",
      "  batch 130 loss: 0.19825499961152673\n",
      "  batch 140 loss: 0.43157077996438603\n",
      "  batch 150 loss: 0.4703131885285984\n",
      "  batch 160 loss: 0.5271560608336585\n",
      "  batch 170 loss: 0.3108922274710494\n",
      "  batch 180 loss: 0.2147745726411813\n",
      "  batch 190 loss: 0.1355898085232184\n",
      "LOSS train 0.1355898085232184 valid 0.4297987900107938\n",
      "EPOCH 251:\n",
      "  batch 10 loss: 0.2486942860443378\n",
      "  batch 20 loss: 0.4769212831102777\n",
      "  batch 30 loss: 0.2880857731390279\n",
      "  batch 40 loss: 0.11384888532775221\n",
      "  batch 50 loss: 0.11372739762300625\n",
      "  batch 60 loss: 0.17140407752012835\n",
      "  batch 70 loss: 0.2644678884287714\n",
      "  batch 80 loss: 0.1794819124846981\n",
      "  batch 90 loss: 0.27588315851971856\n",
      "  batch 100 loss: 0.5070491276681424\n",
      "  batch 110 loss: 0.19176028468064032\n",
      "  batch 120 loss: 0.18655056322313612\n",
      "  batch 130 loss: 0.2441516717983177\n",
      "  batch 140 loss: 0.31640184310672337\n",
      "  batch 150 loss: 0.1409869716007961\n",
      "  batch 160 loss: 0.3988265433048582\n",
      "  batch 170 loss: 0.1376657645974774\n",
      "  batch 180 loss: 0.34825899946590655\n",
      "  batch 190 loss: 0.35322080842161085\n",
      "LOSS train 0.35322080842161085 valid 0.35115296619831216\n",
      "EPOCH 252:\n",
      "  batch 10 loss: 0.5063493483481579\n",
      "  batch 20 loss: 0.15466969247900125\n",
      "  batch 30 loss: 0.48700654986641895\n",
      "  batch 40 loss: 0.1893695648293942\n",
      "  batch 50 loss: 0.27057414588634854\n",
      "  batch 60 loss: 0.2070080785517348\n",
      "  batch 70 loss: 0.3402192838431802\n",
      "  batch 80 loss: 0.18033757538651116\n",
      "  batch 90 loss: 0.22632004408660578\n",
      "  batch 100 loss: 0.21836684819791116\n",
      "  batch 110 loss: 0.09610337956110016\n",
      "  batch 120 loss: 0.32931224204367027\n",
      "  batch 130 loss: 0.5385614450118738\n",
      "  batch 140 loss: 0.4248292490177846\n",
      "  batch 150 loss: 0.6181919013419247\n",
      "  batch 160 loss: 0.5802232047630241\n",
      "  batch 170 loss: 0.34037617384965413\n",
      "  batch 180 loss: 0.32363168657320784\n",
      "  batch 190 loss: 0.4278871044196421\n",
      "LOSS train 0.4278871044196421 valid 0.2903855400075908\n",
      "EPOCH 253:\n",
      "  batch 10 loss: 0.3436343008419499\n",
      "  batch 20 loss: 0.24917149612592765\n",
      "  batch 30 loss: 0.34925281382747925\n",
      "  batch 40 loss: 0.15539008221385303\n",
      "  batch 50 loss: 0.2562534809258068\n",
      "  batch 60 loss: 0.37787735095480457\n",
      "  batch 70 loss: 0.2887727156186884\n",
      "  batch 80 loss: 0.1169869035598822\n",
      "  batch 90 loss: 0.33751711575023363\n",
      "  batch 100 loss: 0.3579573336697649\n",
      "  batch 110 loss: 0.2412799549911142\n",
      "  batch 120 loss: 0.19256261333357544\n",
      "  batch 130 loss: 0.18311934204102726\n",
      "  batch 140 loss: 0.23105456379271344\n",
      "  batch 150 loss: 0.5621114750400011\n",
      "  batch 160 loss: 0.377073074283544\n",
      "  batch 170 loss: 0.39982165545079623\n",
      "  batch 180 loss: 0.24382827226127118\n",
      "  batch 190 loss: 0.35260617572384945\n",
      "LOSS train 0.35260617572384945 valid 0.27659522584657775\n",
      "EPOCH 254:\n",
      "  batch 10 loss: 0.3845849288336467\n",
      "  batch 20 loss: 0.18303922853374388\n",
      "  batch 30 loss: 0.19621381675824523\n",
      "  batch 40 loss: 0.29423895641375564\n",
      "  batch 50 loss: 0.28073057298897763\n",
      "  batch 60 loss: 0.2950418320222525\n",
      "  batch 70 loss: 0.39746462527400583\n",
      "  batch 80 loss: 0.4317487451204215\n",
      "  batch 90 loss: 0.1832462606536865\n",
      "  batch 100 loss: 0.22615633433306356\n",
      "  batch 110 loss: 0.2446763128398743\n",
      "  batch 120 loss: 0.2398936670506373\n",
      "  batch 130 loss: 0.16413878323801329\n",
      "  batch 140 loss: 0.27747199227596864\n",
      "  batch 150 loss: 0.24879636349942302\n",
      "  batch 160 loss: 0.30760012582777563\n",
      "  batch 170 loss: 0.294540977505676\n",
      "  batch 180 loss: 0.1898499166507463\n",
      "  batch 190 loss: 0.27340328871596287\n",
      "LOSS train 0.27340328871596287 valid 0.48400356657622556\n",
      "EPOCH 255:\n",
      "  batch 10 loss: 0.23071731548625393\n",
      "  batch 20 loss: 0.5788650154116113\n",
      "  batch 30 loss: 0.31756763442317604\n",
      "  batch 40 loss: 0.7182002715213457\n",
      "  batch 50 loss: 0.6799117894828669\n",
      "  batch 60 loss: 0.17827718679473037\n",
      "  batch 70 loss: 0.5974043712310959\n",
      "  batch 80 loss: 0.3378992150072008\n",
      "  batch 90 loss: 0.30742123942509353\n",
      "  batch 100 loss: 0.27173681691783713\n",
      "  batch 110 loss: 0.6663641834900773\n",
      "  batch 120 loss: 0.1290815888962243\n",
      "  batch 130 loss: 0.3322062256251229\n",
      "  batch 140 loss: 0.22273055569967254\n",
      "  batch 150 loss: 0.17687935761059634\n",
      "  batch 160 loss: 0.31024766720511254\n",
      "  batch 170 loss: 0.2511042634534533\n",
      "  batch 180 loss: 0.15418645971731165\n",
      "  batch 190 loss: 0.4219108437377145\n",
      "LOSS train 0.4219108437377145 valid 0.25299122908827254\n",
      "EPOCH 256:\n",
      "  batch 10 loss: 0.26643081912770866\n",
      "  batch 20 loss: 0.4678838976440602\n",
      "  batch 30 loss: 0.40229650850451437\n",
      "  batch 40 loss: 0.27025510181847495\n",
      "  batch 50 loss: 0.08579590180888771\n",
      "  batch 60 loss: 0.2824811463629885\n",
      "  batch 70 loss: 0.2705385684799694\n",
      "  batch 80 loss: 0.41401432460988874\n",
      "  batch 90 loss: 0.2337124338184367\n",
      "  batch 100 loss: 0.2986379749832849\n",
      "  batch 110 loss: 0.4318312662260723\n",
      "  batch 120 loss: 0.31730285564408406\n",
      "  batch 130 loss: 0.2070561805536272\n",
      "  batch 140 loss: 0.24859716221835698\n",
      "  batch 150 loss: 0.18604631994094234\n",
      "  batch 160 loss: 0.264739001302587\n",
      "  batch 170 loss: 0.19919624282629228\n",
      "  batch 180 loss: 0.23852923150261632\n",
      "  batch 190 loss: 0.20505643919714203\n",
      "LOSS train 0.20505643919714203 valid 0.34731644518409993\n",
      "EPOCH 257:\n",
      "  batch 10 loss: 0.2223184641894477\n",
      "  batch 20 loss: 0.1278811148222303\n",
      "  batch 30 loss: 0.21139842657285043\n",
      "  batch 40 loss: 0.19747158784448401\n",
      "  batch 50 loss: 0.5420042751415167\n",
      "  batch 60 loss: 0.2480396850633042\n",
      "  batch 70 loss: 0.1301081906807667\n",
      "  batch 80 loss: 0.3174218739091884\n",
      "  batch 90 loss: 0.18280799803396802\n",
      "  batch 100 loss: 0.4580966369336238\n",
      "  batch 110 loss: 0.1761701674651704\n",
      "  batch 120 loss: 0.2779250063944346\n",
      "  batch 130 loss: 0.3518617268069647\n",
      "  batch 140 loss: 0.39516918884419283\n",
      "  batch 150 loss: 0.2850112176551193\n",
      "  batch 160 loss: 0.24392883021064335\n",
      "  batch 170 loss: 0.04786073210925679\n",
      "  batch 180 loss: 0.3062007593780436\n",
      "  batch 190 loss: 0.48394194325373974\n",
      "LOSS train 0.48394194325373974 valid 3.5731166360326543\n",
      "EPOCH 258:\n",
      "  batch 10 loss: 1.329950478405226\n",
      "  batch 20 loss: 0.2650396885437658\n",
      "  batch 30 loss: 0.23118516037939116\n",
      "  batch 40 loss: 0.16768839582655345\n",
      "  batch 50 loss: 0.44868517442664596\n",
      "  batch 60 loss: 0.6086063497103169\n",
      "  batch 70 loss: 0.12889334170686198\n",
      "  batch 80 loss: 0.22486200328239647\n",
      "  batch 90 loss: 0.18730002028096351\n",
      "  batch 100 loss: 0.2813079175160965\n",
      "  batch 110 loss: 0.2926550748241425\n",
      "  batch 120 loss: 0.2243333059894212\n",
      "  batch 130 loss: 0.2653266970461118\n",
      "  batch 140 loss: 0.4678461012954358\n",
      "  batch 150 loss: 0.11646854756545508\n",
      "  batch 160 loss: 0.33952597751558644\n",
      "  batch 170 loss: 0.30702455142891266\n",
      "  batch 180 loss: 0.4135124771564733\n",
      "  batch 190 loss: 0.18187520743886126\n",
      "LOSS train 0.18187520743886126 valid 1.8700019446424938\n",
      "EPOCH 259:\n",
      "  batch 10 loss: 0.9428480352471524\n",
      "  batch 20 loss: 0.16284041891049128\n",
      "  batch 30 loss: 0.17528744646915584\n",
      "  batch 40 loss: 0.33597489770618266\n",
      "  batch 50 loss: 0.2571436844649725\n",
      "  batch 60 loss: 0.19225077337177937\n",
      "  batch 70 loss: 0.19169623377238168\n",
      "  batch 80 loss: 0.2720768381710513\n",
      "  batch 90 loss: 0.2620232686618692\n",
      "  batch 100 loss: 0.2642813676910009\n",
      "  batch 110 loss: 0.29427613492516685\n",
      "  batch 120 loss: 0.5222830447659362\n",
      "  batch 130 loss: 0.3666186203699908\n",
      "  batch 140 loss: 0.1083249548159074\n",
      "  batch 150 loss: 0.124669722244289\n",
      "  batch 160 loss: 0.4121158277630457\n",
      "  batch 170 loss: 0.5560728250791727\n",
      "  batch 180 loss: 0.5938514660243527\n",
      "  batch 190 loss: 0.24030306705317345\n",
      "LOSS train 0.24030306705317345 valid 0.6912144823160279\n",
      "EPOCH 260:\n",
      "  batch 10 loss: 0.4727808009803994\n",
      "  batch 20 loss: 0.13780472528887913\n",
      "  batch 30 loss: 0.07729911445640028\n",
      "  batch 40 loss: 0.2530368693616765\n",
      "  batch 50 loss: 0.15008804286189842\n",
      "  batch 60 loss: 0.4235058088248479\n",
      "  batch 70 loss: 0.13208873183175457\n",
      "  batch 80 loss: 0.12173575211199932\n",
      "  batch 90 loss: 0.2612372530005814\n",
      "  batch 100 loss: 0.3214518925240554\n",
      "  batch 110 loss: 0.234967572925234\n",
      "  batch 120 loss: 0.2156597166453139\n",
      "  batch 130 loss: 0.19730724136315986\n",
      "  batch 140 loss: 0.3546052869925916\n",
      "  batch 150 loss: 0.21111564233433455\n",
      "  batch 160 loss: 0.2588384943537676\n",
      "  batch 170 loss: 0.698344050371088\n",
      "  batch 180 loss: 0.2946962958347285\n",
      "  batch 190 loss: 0.1935613638124778\n",
      "LOSS train 0.1935613638124778 valid 0.36562638475170856\n",
      "EPOCH 261:\n",
      "  batch 10 loss: 0.1058485923007538\n",
      "  batch 20 loss: 0.2867757699175854\n",
      "  batch 30 loss: 0.11403501683380454\n",
      "  batch 40 loss: 0.3827054460212821\n",
      "  batch 50 loss: 0.3567900378780905\n",
      "  batch 60 loss: 0.42099637996871025\n",
      "  batch 70 loss: 0.0985407245636452\n",
      "  batch 80 loss: 0.4020371612044983\n",
      "  batch 90 loss: 0.24695358165627113\n",
      "  batch 100 loss: 0.5854228270356543\n",
      "  batch 110 loss: 0.3367440449022979\n",
      "  batch 120 loss: 0.20008148739289028\n",
      "  batch 130 loss: 0.23339722124219406\n",
      "  batch 140 loss: 0.18407383959347498\n",
      "  batch 150 loss: 0.13701344572764357\n",
      "  batch 160 loss: 0.5877615179138956\n",
      "  batch 170 loss: 0.17169073544500862\n",
      "  batch 180 loss: 0.48596000458637717\n",
      "  batch 190 loss: 0.18093310566910076\n",
      "LOSS train 0.18093310566910076 valid 0.25328777423923243\n",
      "EPOCH 262:\n",
      "  batch 10 loss: 0.3140703566066804\n",
      "  batch 20 loss: 0.363645941845607\n",
      "  batch 30 loss: 0.2247203618811909\n",
      "  batch 40 loss: 0.15369711518869736\n",
      "  batch 50 loss: 0.11291501423111186\n",
      "  batch 60 loss: 0.234985369106289\n",
      "  batch 70 loss: 0.24015053238254042\n",
      "  batch 80 loss: 0.3426953387534013\n",
      "  batch 90 loss: 0.2707117404002929\n",
      "  batch 100 loss: 0.14467986842792016\n",
      "  batch 110 loss: 0.42978708441951313\n",
      "  batch 120 loss: 0.4085219751170371\n",
      "  batch 130 loss: 0.27763221150380557\n",
      "  batch 140 loss: 0.20824960808677134\n",
      "  batch 150 loss: 0.24341806163429283\n",
      "  batch 160 loss: 0.22021292665449438\n",
      "  batch 170 loss: 0.2987818841924309\n",
      "  batch 180 loss: 0.2980466046210495\n",
      "  batch 190 loss: 0.21129358913676696\n",
      "LOSS train 0.21129358913676696 valid 0.33245972128283124\n",
      "EPOCH 263:\n",
      "  batch 10 loss: 0.12019433267705608\n",
      "  batch 20 loss: 0.1672932519519236\n",
      "  batch 30 loss: 0.275731319507031\n",
      "  batch 40 loss: 0.12434457063500304\n",
      "  batch 50 loss: 0.24665951879760542\n",
      "  batch 60 loss: 0.5535254921851447\n",
      "  batch 70 loss: 0.42829437272957877\n",
      "  batch 80 loss: 0.17488537372846621\n",
      "  batch 90 loss: 0.22499895713553997\n",
      "  batch 100 loss: 0.4335631033565733\n",
      "  batch 110 loss: 0.24302599115544582\n",
      "  batch 120 loss: 0.47557664705236674\n",
      "  batch 130 loss: 0.07545424900017679\n",
      "  batch 140 loss: 0.19700425831542817\n",
      "  batch 150 loss: 0.1194255488429917\n",
      "  batch 160 loss: 0.20650195782000083\n",
      "  batch 170 loss: 0.3971745365561219\n",
      "  batch 180 loss: 0.4050104999274481\n",
      "  batch 190 loss: 0.665852820895816\n",
      "LOSS train 0.665852820895816 valid 0.43870787375831594\n",
      "EPOCH 264:\n",
      "  batch 10 loss: 0.19941582099272637\n",
      "  batch 20 loss: 0.2029208342806669\n",
      "  batch 30 loss: 0.20565781079712905\n",
      "  batch 40 loss: 0.27011705332406566\n",
      "  batch 50 loss: 0.36537910260303763\n",
      "  batch 60 loss: 0.6762801879973267\n",
      "  batch 70 loss: 0.41738710466306655\n",
      "  batch 80 loss: 0.3020825387473451\n",
      "  batch 90 loss: 0.19966275952610885\n",
      "  batch 100 loss: 0.2311798426817404\n",
      "  batch 110 loss: 0.28756588262913285\n",
      "  batch 120 loss: 0.244414212488482\n",
      "  batch 130 loss: 0.217689651701221\n",
      "  batch 140 loss: 0.21163416587005485\n",
      "  batch 150 loss: 0.39589533823454987\n",
      "  batch 160 loss: 0.4765560735733743\n",
      "  batch 170 loss: 0.40297142790004725\n",
      "  batch 180 loss: 0.4081181871035369\n",
      "  batch 190 loss: 0.11208774675033055\n",
      "LOSS train 0.11208774675033055 valid 0.30228636400398756\n",
      "EPOCH 265:\n",
      "  batch 10 loss: 0.1474087609560229\n",
      "  batch 20 loss: 0.34129237554589054\n",
      "  batch 30 loss: 0.1898803520569345\n",
      "  batch 40 loss: 0.21071480955870356\n",
      "  batch 50 loss: 0.16226601079688407\n",
      "  batch 60 loss: 0.16888095155882182\n",
      "  batch 70 loss: 0.17949445662379732\n",
      "  batch 80 loss: 0.16861005882383323\n",
      "  batch 90 loss: 0.19737039605533938\n",
      "  batch 100 loss: 0.35274678069108634\n",
      "  batch 110 loss: 0.17506433225862567\n",
      "  batch 120 loss: 0.283180180756608\n",
      "  batch 130 loss: 0.2515498869906878\n",
      "  batch 140 loss: 0.2330109621379961\n",
      "  batch 150 loss: 0.5823918006382882\n",
      "  batch 160 loss: 0.19673309843346942\n",
      "  batch 170 loss: 0.32437854948220773\n",
      "  batch 180 loss: 0.2029664597910596\n",
      "  batch 190 loss: 0.1422206248738803\n",
      "LOSS train 0.1422206248738803 valid 0.2943903997299621\n",
      "EPOCH 266:\n",
      "  batch 10 loss: 0.21564657084854844\n",
      "  batch 20 loss: 0.5999365454190411\n",
      "  batch 30 loss: 0.29213372251469993\n",
      "  batch 40 loss: 0.21990770336124116\n",
      "  batch 50 loss: 0.4226561907922587\n",
      "  batch 60 loss: 0.15377298893290572\n",
      "  batch 70 loss: 0.21553739556111395\n",
      "  batch 80 loss: 0.19355942780384794\n",
      "  batch 90 loss: 0.22020875658636213\n",
      "  batch 100 loss: 0.14118364347996248\n",
      "  batch 110 loss: 0.49075185797410087\n",
      "  batch 120 loss: 0.2274781320767943\n",
      "  batch 130 loss: 0.14702582649697432\n",
      "  batch 140 loss: 0.2567482139886124\n",
      "  batch 150 loss: 0.12245640299952357\n",
      "  batch 160 loss: 0.2520289983018301\n",
      "  batch 170 loss: 0.267119740776252\n",
      "  batch 180 loss: 0.2142369818939187\n",
      "  batch 190 loss: 0.09788077651828644\n",
      "LOSS train 0.09788077651828644 valid 0.24800475258477186\n",
      "EPOCH 267:\n",
      "  batch 10 loss: 0.30854650994078836\n",
      "  batch 20 loss: 0.16156354668055428\n",
      "  batch 30 loss: 0.2401749949902296\n",
      "  batch 40 loss: 0.16396165512487643\n",
      "  batch 50 loss: 0.10126663786068094\n",
      "  batch 60 loss: 0.37050960214837686\n",
      "  batch 70 loss: 0.29732657306449256\n",
      "  batch 80 loss: 0.11438980600942159\n",
      "  batch 90 loss: 0.38238275074691047\n",
      "  batch 100 loss: 0.20883026755909667\n",
      "  batch 110 loss: 0.4547347972082207\n",
      "  batch 120 loss: 0.17206782853754704\n",
      "  batch 130 loss: 0.19209682306100148\n",
      "  batch 140 loss: 0.17204481491389742\n",
      "  batch 150 loss: 0.4589450072584441\n",
      "  batch 160 loss: 0.4739376992365578\n",
      "  batch 170 loss: 0.11053155522167799\n",
      "  batch 180 loss: 0.42914239878882654\n",
      "  batch 190 loss: 0.43028480937937275\n",
      "LOSS train 0.43028480937937275 valid 0.2963307108120017\n",
      "EPOCH 268:\n",
      "  batch 10 loss: 0.15234530898742377\n",
      "  batch 20 loss: 0.3734059340786189\n",
      "  batch 30 loss: 0.3309121893165866\n",
      "  batch 40 loss: 0.41597283180199157\n",
      "  batch 50 loss: 0.26371665271508393\n",
      "  batch 60 loss: 0.314657019918377\n",
      "  batch 70 loss: 0.3691012476425385\n",
      "  batch 80 loss: 0.09188527216465445\n",
      "  batch 90 loss: 0.27878804399369983\n",
      "  batch 100 loss: 0.18011175285791978\n",
      "  batch 110 loss: 0.3056498737685615\n",
      "  batch 120 loss: 0.18059388419933384\n",
      "  batch 130 loss: 0.16165077612022288\n",
      "  batch 140 loss: 0.09127101118356222\n",
      "  batch 150 loss: 0.19071536178671522\n",
      "  batch 160 loss: 0.16803096246076166\n",
      "  batch 170 loss: 0.22983985210012178\n",
      "  batch 180 loss: 0.22913699543641997\n",
      "  batch 190 loss: 0.5508736434778256\n",
      "LOSS train 0.5508736434778256 valid 0.33684239913681946\n",
      "EPOCH 269:\n",
      "  batch 10 loss: 0.22573908044541896\n",
      "  batch 20 loss: 0.22171144931271555\n",
      "  batch 30 loss: 0.1252593691140646\n",
      "  batch 40 loss: 0.20130169909025425\n",
      "  batch 50 loss: 0.325537220782644\n",
      "  batch 60 loss: 0.20703692764218432\n",
      "  batch 70 loss: 0.5972095723413077\n",
      "  batch 80 loss: 0.10251415392885974\n",
      "  batch 90 loss: 0.15394191796076484\n",
      "  batch 100 loss: 0.4808690221838333\n",
      "  batch 110 loss: 0.37747261461117887\n",
      "  batch 120 loss: 0.1358043844553322\n",
      "  batch 130 loss: 0.5145983634211007\n",
      "  batch 140 loss: 0.1610179141338449\n",
      "  batch 150 loss: 0.16342225186526776\n",
      "  batch 160 loss: 0.1639866482495563\n",
      "  batch 170 loss: 0.1969782969514199\n",
      "  batch 180 loss: 0.1472582204543869\n",
      "  batch 190 loss: 0.6141537689749385\n",
      "LOSS train 0.6141537689749385 valid 0.35338555240527153\n",
      "EPOCH 270:\n",
      "  batch 10 loss: 0.2994812065135193\n",
      "  batch 20 loss: 0.3764058650340303\n",
      "  batch 30 loss: 0.09347892279038206\n",
      "  batch 40 loss: 0.1124984299800417\n",
      "  batch 50 loss: 0.08007934288179967\n",
      "  batch 60 loss: 0.37730535775699536\n",
      "  batch 70 loss: 0.11878058877337025\n",
      "  batch 80 loss: 0.2809403854480479\n",
      "  batch 90 loss: 0.34852130043727814\n",
      "  batch 100 loss: 0.4135424690888613\n",
      "  batch 110 loss: 0.191291563858249\n",
      "  batch 120 loss: 0.7110414573951858\n",
      "  batch 130 loss: 0.1896480170165887\n",
      "  batch 140 loss: 0.26906925821094774\n",
      "  batch 150 loss: 0.17845639055176435\n",
      "  batch 160 loss: 0.07513749609061052\n",
      "  batch 170 loss: 0.2565899293185794\n",
      "  batch 180 loss: 0.3277634496756946\n",
      "  batch 190 loss: 0.4451454815629404\n",
      "LOSS train 0.4451454815629404 valid 0.3335609618038572\n",
      "EPOCH 271:\n",
      "  batch 10 loss: 0.2818554616562324\n",
      "  batch 20 loss: 0.2430221715461812\n",
      "  batch 30 loss: 0.4521854675243958\n",
      "  batch 40 loss: 0.5266728633949243\n",
      "  batch 50 loss: 0.07358632373470755\n",
      "  batch 60 loss: 0.13153584183964995\n",
      "  batch 70 loss: 0.4205331043282058\n",
      "  batch 80 loss: 0.3763816544225847\n",
      "  batch 90 loss: 0.1930278289357375\n",
      "  batch 100 loss: 0.566416871236288\n",
      "  batch 110 loss: 0.08745151776602142\n",
      "  batch 120 loss: 0.32883442433230814\n",
      "  batch 130 loss: 0.1923111508898728\n",
      "  batch 140 loss: 0.2613421683468914\n",
      "  batch 150 loss: 0.2019076967575529\n",
      "  batch 160 loss: 0.31314634463560653\n",
      "  batch 170 loss: 0.3314208631360088\n",
      "  batch 180 loss: 0.12965241192723626\n",
      "  batch 190 loss: 0.20090626915480242\n",
      "LOSS train 0.20090626915480242 valid 0.4229349792026019\n",
      "EPOCH 272:\n",
      "  batch 10 loss: 0.30521931758667054\n",
      "  batch 20 loss: 0.341650143401057\n",
      "  batch 30 loss: 0.07104172621657198\n",
      "  batch 40 loss: 0.46910781392944045\n",
      "  batch 50 loss: 0.171733649707312\n",
      "  batch 60 loss: 0.4424278771824902\n",
      "  batch 70 loss: 0.35284131759253795\n",
      "  batch 80 loss: 0.5254530770311249\n",
      "  batch 90 loss: 0.1842454050805827\n",
      "  batch 100 loss: 0.08552920755755622\n",
      "  batch 110 loss: 0.18288778562709923\n",
      "  batch 120 loss: 0.20030175024585334\n",
      "  batch 130 loss: 0.21857394367652888\n",
      "  batch 140 loss: 0.1887125147652114\n",
      "  batch 150 loss: 0.18034910678688904\n",
      "  batch 160 loss: 1.2123100757715293\n",
      "  batch 170 loss: 0.2985210960265249\n",
      "  batch 180 loss: 0.44191629320412174\n",
      "  batch 190 loss: 0.33351906869211234\n",
      "LOSS train 0.33351906869211234 valid 0.271098992359326\n",
      "EPOCH 273:\n",
      "  batch 10 loss: 0.2263127062600688\n",
      "  batch 20 loss: 0.3664725391223328\n",
      "  batch 30 loss: 0.2984155797981657\n",
      "  batch 40 loss: 0.23251237548829523\n",
      "  batch 50 loss: 0.33505941209441514\n",
      "  batch 60 loss: 0.15347243117284962\n",
      "  batch 70 loss: 0.2021599801882985\n",
      "  batch 80 loss: 0.400730164561719\n",
      "  batch 90 loss: 0.11473062699078582\n",
      "  batch 100 loss: 0.27950482867017856\n",
      "  batch 110 loss: 0.19851884389499902\n",
      "  batch 120 loss: 0.19918230979237705\n",
      "  batch 130 loss: 0.1853902451097383\n",
      "  batch 140 loss: 0.2488717526633991\n",
      "  batch 150 loss: 0.2144259684777353\n",
      "  batch 160 loss: 0.2332508335384773\n",
      "  batch 170 loss: 0.15828561897942564\n",
      "  batch 180 loss: 0.17861061884177615\n",
      "  batch 190 loss: 0.4888887311455619\n",
      "LOSS train 0.4888887311455619 valid 0.7386165691908061\n",
      "EPOCH 274:\n",
      "  batch 10 loss: 0.6258900819328119\n",
      "  batch 20 loss: 0.18161370031521074\n",
      "  batch 30 loss: 0.05498791543213884\n",
      "  batch 40 loss: 0.13823767745270743\n",
      "  batch 50 loss: 0.23733360228070524\n",
      "  batch 60 loss: 0.34964207324374\n",
      "  batch 70 loss: 0.12352996437512047\n",
      "  batch 80 loss: 0.4230545824902947\n",
      "  batch 90 loss: 0.19684354324708692\n",
      "  batch 100 loss: 0.5395622354324587\n",
      "  batch 110 loss: 0.33387820195057427\n",
      "  batch 120 loss: 0.2025318339401565\n",
      "  batch 130 loss: 0.38699960319208915\n",
      "  batch 140 loss: 0.31909595681208885\n",
      "  batch 150 loss: 0.2757312729459954\n",
      "  batch 160 loss: 0.07664396092441166\n",
      "  batch 170 loss: 0.203396640587016\n",
      "  batch 180 loss: 0.3833539597559138\n",
      "  batch 190 loss: 0.23511084173223934\n",
      "LOSS train 0.23511084173223934 valid 0.341062178836019\n",
      "EPOCH 275:\n",
      "  batch 10 loss: 0.17088848216226324\n",
      "  batch 20 loss: 0.24007788603112204\n",
      "  batch 30 loss: 0.11689089089195477\n",
      "  batch 40 loss: 0.22787096183456015\n",
      "  batch 50 loss: 0.12104892058123369\n",
      "  batch 60 loss: 0.22753704018250573\n",
      "  batch 70 loss: 0.2531009929138236\n",
      "  batch 80 loss: 0.22487903859873767\n",
      "  batch 90 loss: 0.18638419393682853\n",
      "  batch 100 loss: 0.2549253511686402\n",
      "  batch 110 loss: 0.11554876092704944\n",
      "  batch 120 loss: 0.24725087027181872\n",
      "  batch 130 loss: 0.27580857207994997\n",
      "  batch 140 loss: 0.1510857587272767\n",
      "  batch 150 loss: 0.20085096293332755\n",
      "  batch 160 loss: 0.2144411702931393\n",
      "  batch 170 loss: 0.25052520279714374\n",
      "  batch 180 loss: 0.2361543230334064\n",
      "  batch 190 loss: 0.5481782817918429\n",
      "LOSS train 0.5481782817918429 valid 0.3012558187675985\n",
      "EPOCH 276:\n",
      "  batch 10 loss: 0.2636505722170114\n",
      "  batch 20 loss: 0.2659500346548157\n",
      "  batch 30 loss: 0.09990278346376727\n",
      "  batch 40 loss: 0.12560933673084945\n",
      "  batch 50 loss: 0.13671325386530953\n",
      "  batch 60 loss: 0.3578143436068785\n",
      "  batch 70 loss: 0.7893170314779127\n",
      "  batch 80 loss: 0.15256356854952174\n",
      "  batch 90 loss: 0.10678723930177511\n",
      "  batch 100 loss: 0.3371410648629535\n",
      "  batch 110 loss: 0.2952773969125701\n",
      "  batch 120 loss: 0.19912508635025006\n",
      "  batch 130 loss: 0.2423646080031176\n",
      "  batch 140 loss: 0.3522932641441002\n",
      "  batch 150 loss: 0.27650850446079855\n",
      "  batch 160 loss: 0.18458509934862377\n",
      "  batch 170 loss: 0.15936657976999413\n",
      "  batch 180 loss: 0.1686121354723582\n",
      "  batch 190 loss: 0.28846541597886244\n",
      "LOSS train 0.28846541597886244 valid 0.2863916589853127\n",
      "EPOCH 277:\n",
      "  batch 10 loss: 0.22251035867957397\n",
      "  batch 20 loss: 0.5322575403930386\n",
      "  batch 30 loss: 0.25146009160671384\n",
      "  batch 40 loss: 0.10975348334250157\n",
      "  batch 50 loss: 0.2876132143457653\n",
      "  batch 60 loss: 0.1411078108343645\n",
      "  batch 70 loss: 0.11278391412342899\n",
      "  batch 80 loss: 0.2776117750443518\n",
      "  batch 90 loss: 0.15728945801747612\n",
      "  batch 100 loss: 0.14276006610161857\n",
      "  batch 110 loss: 0.39201113145099953\n",
      "  batch 120 loss: 0.1731917650133255\n",
      "  batch 130 loss: 0.4327655586130277\n",
      "  batch 140 loss: 0.28233756431509394\n",
      "  batch 150 loss: 0.13288578373612836\n",
      "  batch 160 loss: 0.4023129684253945\n",
      "  batch 170 loss: 0.17998175116517814\n",
      "  batch 180 loss: 0.1959228872510721\n",
      "  batch 190 loss: 0.03960510645410977\n",
      "LOSS train 0.03960510645410977 valid 0.3015047078389706\n",
      "EPOCH 278:\n",
      "  batch 10 loss: 0.14708038121680148\n",
      "  batch 20 loss: 0.31141830632986967\n",
      "  batch 30 loss: 0.2656774189337739\n",
      "  batch 40 loss: 0.14032026503700762\n",
      "  batch 50 loss: 0.43489971819508355\n",
      "  batch 60 loss: 0.18497942521498772\n",
      "  batch 70 loss: 0.27194973338919226\n",
      "  batch 80 loss: 0.2431603180928505\n",
      "  batch 90 loss: 0.16572934060804984\n",
      "  batch 100 loss: 0.16971941205847543\n",
      "  batch 110 loss: 0.2459835067136737\n",
      "  batch 120 loss: 0.13870851960600702\n",
      "  batch 130 loss: 0.07441066461251467\n",
      "  batch 140 loss: 0.30682968224864454\n",
      "  batch 150 loss: 0.2569572795095155\n",
      "  batch 160 loss: 0.3065245265024714\n",
      "  batch 170 loss: 0.5088583456003107\n",
      "  batch 180 loss: 0.09218012489072862\n",
      "  batch 190 loss: 0.1389555011926859\n",
      "LOSS train 0.1389555011926859 valid 0.2884008230221531\n",
      "EPOCH 279:\n",
      "  batch 10 loss: 0.16001950886638952\n",
      "  batch 20 loss: 0.34327431423444066\n",
      "  batch 30 loss: 0.23731976656272308\n",
      "  batch 40 loss: 0.28677940739144103\n",
      "  batch 50 loss: 0.20562559080281062\n",
      "  batch 60 loss: 0.09930415238050046\n",
      "  batch 70 loss: 0.225888139817107\n",
      "  batch 80 loss: 0.35582757435040546\n",
      "  batch 90 loss: 0.25633682841726113\n",
      "  batch 100 loss: 0.15877470977175107\n",
      "  batch 110 loss: 0.07467895266017877\n",
      "  batch 120 loss: 0.12821512340378832\n",
      "  batch 130 loss: 0.37068938667143814\n",
      "  batch 140 loss: 0.766781266633916\n",
      "  batch 150 loss: 0.3120700884239341\n",
      "  batch 160 loss: 0.17411337321609607\n",
      "  batch 170 loss: 0.2262895297651994\n",
      "  batch 180 loss: 0.41221493114426266\n",
      "  batch 190 loss: 0.09856073682021815\n",
      "LOSS train 0.09856073682021815 valid 0.21312327135540096\n",
      "EPOCH 280:\n",
      "  batch 10 loss: 0.07993339280728833\n",
      "  batch 20 loss: 0.10239662055391818\n",
      "  batch 30 loss: 0.06416782169944782\n",
      "  batch 40 loss: 0.16459161710372427\n",
      "  batch 50 loss: 0.3409317117795581\n",
      "  batch 60 loss: 0.26590205838147085\n",
      "  batch 70 loss: 0.2554897622714634\n",
      "  batch 80 loss: 0.5516984031411993\n",
      "  batch 90 loss: 0.242615779524931\n",
      "  batch 100 loss: 0.3930026798494509\n",
      "  batch 110 loss: 0.33243381421780216\n",
      "  batch 120 loss: 0.1083965961872309\n",
      "  batch 130 loss: 0.19713876977039035\n",
      "  batch 140 loss: 0.3219065983197652\n",
      "  batch 150 loss: 0.12554562663208343\n",
      "  batch 160 loss: 0.1664510386843176\n",
      "  batch 170 loss: 0.1939418456677231\n",
      "  batch 180 loss: 0.19869614532599372\n",
      "  batch 190 loss: 0.2727663786521589\n",
      "LOSS train 0.2727663786521589 valid 0.24000325462349373\n",
      "EPOCH 281:\n",
      "  batch 10 loss: 0.08721171284778392\n",
      "  batch 20 loss: 0.27175085304734237\n",
      "  batch 30 loss: 0.21270898591210424\n",
      "  batch 40 loss: 0.29335552645425195\n",
      "  batch 50 loss: 0.27543350487030693\n",
      "  batch 60 loss: 0.12283588700884138\n",
      "  batch 70 loss: 0.3596342836943222\n",
      "  batch 80 loss: 0.15295368478218735\n",
      "  batch 90 loss: 0.3909544239169918\n",
      "  batch 100 loss: 0.1492693278443767\n",
      "  batch 110 loss: 0.41819747575354993\n",
      "  batch 120 loss: 0.17973372469568857\n",
      "  batch 130 loss: 0.2097183710851823\n",
      "  batch 140 loss: 0.09676187276418205\n",
      "  batch 150 loss: 0.6476866246695863\n",
      "  batch 160 loss: 0.3113361387091572\n",
      "  batch 170 loss: 0.43282423190539704\n",
      "  batch 180 loss: 0.18939258592272382\n",
      "  batch 190 loss: 0.09798041561880382\n",
      "LOSS train 0.09798041561880382 valid 0.2644743458720488\n",
      "EPOCH 282:\n",
      "  batch 10 loss: 0.20440331096178851\n",
      "  batch 20 loss: 0.19770654861058573\n",
      "  batch 30 loss: 0.37238970540493027\n",
      "  batch 40 loss: 0.13125939384481172\n",
      "  batch 50 loss: 0.33912413925863805\n",
      "  batch 60 loss: 0.18586367420866737\n",
      "  batch 70 loss: 0.29692540416945123\n",
      "  batch 80 loss: 0.14588876888155938\n",
      "  batch 90 loss: 0.24723234795383178\n",
      "  batch 100 loss: 0.3857171638184809\n",
      "  batch 110 loss: 0.10060848382418044\n",
      "  batch 120 loss: 0.19186136786447605\n",
      "  batch 130 loss: 0.07538462834854727\n",
      "  batch 140 loss: 0.29025342977402036\n",
      "  batch 150 loss: 0.3543373182859796\n",
      "  batch 160 loss: 0.0648373246731353\n",
      "  batch 170 loss: 0.40410563791083404\n",
      "  batch 180 loss: 0.1927491265014396\n",
      "  batch 190 loss: 0.17010557115863775\n",
      "LOSS train 0.17010557115863775 valid 0.23444765205894305\n",
      "EPOCH 283:\n",
      "  batch 10 loss: 0.256261914899369\n",
      "  batch 20 loss: 0.07767468648671638\n",
      "  batch 30 loss: 0.1222819936243468\n",
      "  batch 40 loss: 0.18131550535399582\n",
      "  batch 50 loss: 0.12153605295170564\n",
      "  batch 60 loss: 0.07819483061030041\n",
      "  batch 70 loss: 0.4426762437535217\n",
      "  batch 80 loss: 0.22450325505487853\n",
      "  batch 90 loss: 0.06157444512064103\n",
      "  batch 100 loss: 0.12197979034244781\n",
      "  batch 110 loss: 0.2086642459500581\n",
      "  batch 120 loss: 0.28256907973991474\n",
      "  batch 130 loss: 0.3771668121597031\n",
      "  batch 140 loss: 0.198999712117984\n",
      "  batch 150 loss: 0.06659004418470431\n",
      "  batch 160 loss: 0.15335913164599332\n",
      "  batch 170 loss: 0.25614133215349283\n",
      "  batch 180 loss: 0.46387852848056355\n",
      "  batch 190 loss: 0.44242715295222296\n",
      "LOSS train 0.44242715295222296 valid 0.46719537830843666\n",
      "EPOCH 284:\n",
      "  batch 10 loss: 0.606297756510321\n",
      "  batch 20 loss: 0.16669065878013498\n",
      "  batch 30 loss: 0.17899074989254588\n",
      "  batch 40 loss: 0.1084066750947386\n",
      "  batch 50 loss: 0.2930875357284094\n",
      "  batch 60 loss: 0.21393807650601956\n",
      "  batch 70 loss: 0.12289259012541151\n",
      "  batch 80 loss: 0.9321297882852377\n",
      "  batch 90 loss: 0.19396153206907912\n",
      "  batch 100 loss: 0.392287801196926\n",
      "  batch 110 loss: 0.44173144288106414\n",
      "  batch 120 loss: 0.561079102740041\n",
      "  batch 130 loss: 0.2937482927169185\n",
      "  batch 140 loss: 0.1770906230143737\n",
      "  batch 150 loss: 0.15972548905556322\n",
      "  batch 160 loss: 0.20480996671249158\n",
      "  batch 170 loss: 0.35043393855667093\n",
      "  batch 180 loss: 0.24983927041030257\n",
      "  batch 190 loss: 0.1442116634498234\n",
      "LOSS train 0.1442116634498234 valid 0.34136739824754886\n",
      "EPOCH 285:\n",
      "  batch 10 loss: 0.15017806301748352\n",
      "  batch 20 loss: 0.29160513416791217\n",
      "  batch 30 loss: 0.1855259786680108\n",
      "  batch 40 loss: 0.20541006654675584\n",
      "  batch 50 loss: 0.268835055836098\n",
      "  batch 60 loss: 0.23510269398630043\n",
      "  batch 70 loss: 0.19215108468124525\n",
      "  batch 80 loss: 0.10817158999998355\n",
      "  batch 90 loss: 0.1626543864083942\n",
      "  batch 100 loss: 0.21971590231842128\n",
      "  batch 110 loss: 0.3422363198398671\n",
      "  batch 120 loss: 0.19662132592820852\n",
      "  batch 130 loss: 0.1732038272850332\n",
      "  batch 140 loss: 0.441807563011389\n",
      "  batch 150 loss: 0.39555177834809\n",
      "  batch 160 loss: 0.1286193406340317\n",
      "  batch 170 loss: 0.21295502573193517\n",
      "  batch 180 loss: 0.6095958306075773\n",
      "  batch 190 loss: 0.061650123899744357\n",
      "LOSS train 0.061650123899744357 valid 0.2938374845192205\n",
      "EPOCH 286:\n",
      "  batch 10 loss: 0.13141533506350242\n",
      "  batch 20 loss: 0.2247093612098979\n",
      "  batch 30 loss: 0.11063059187727049\n",
      "  batch 40 loss: 0.09385008970930357\n",
      "  batch 50 loss: 0.09722167744475882\n",
      "  batch 60 loss: 0.5038071266318298\n",
      "  batch 70 loss: 0.13713921281596414\n",
      "  batch 80 loss: 0.23316915611430886\n",
      "  batch 90 loss: 0.17966150983447732\n",
      "  batch 100 loss: 0.26159915641073894\n",
      "  batch 110 loss: 0.20884562712917615\n",
      "  batch 120 loss: 0.258375527105818\n",
      "  batch 130 loss: 0.24149662029085447\n",
      "  batch 140 loss: 0.10776124343828997\n",
      "  batch 150 loss: 0.13695815000119183\n",
      "  batch 160 loss: 0.5788083575491327\n",
      "  batch 170 loss: 0.5207424351887312\n",
      "  batch 180 loss: 0.28570129366707986\n",
      "  batch 190 loss: 0.1385146464337595\n",
      "LOSS train 0.1385146464337595 valid 0.2296222324629735\n",
      "EPOCH 287:\n",
      "  batch 10 loss: 0.17051200516798418\n",
      "  batch 20 loss: 0.12421661776679685\n",
      "  batch 30 loss: 0.11590957822845667\n",
      "  batch 40 loss: 0.5152352429970051\n",
      "  batch 50 loss: 0.18379229507845593\n",
      "  batch 60 loss: 0.10490559639438288\n",
      "  batch 70 loss: 0.11871288512193132\n",
      "  batch 80 loss: 0.1257562558457721\n",
      "  batch 90 loss: 0.4868646130315028\n",
      "  batch 100 loss: 0.2435780495914514\n",
      "  batch 110 loss: 0.10385690676630474\n",
      "  batch 120 loss: 0.15790458569390467\n",
      "  batch 130 loss: 0.2618115849167225\n",
      "  batch 140 loss: 0.18115340798794932\n",
      "  batch 150 loss: 0.16331832773357746\n",
      "  batch 160 loss: 0.19657042261751484\n",
      "  batch 170 loss: 0.4385962938416924\n",
      "  batch 180 loss: 0.16632885690378316\n",
      "  batch 190 loss: 0.10016281054122374\n",
      "LOSS train 0.10016281054122374 valid 0.3093262360896402\n",
      "EPOCH 288:\n",
      "  batch 10 loss: 0.2402306734038575\n",
      "  batch 20 loss: 0.26409458429225197\n",
      "  batch 30 loss: 0.3168932267410128\n",
      "  batch 40 loss: 0.16342040004892625\n",
      "  batch 50 loss: 0.18036573967583536\n",
      "  batch 60 loss: 0.08047307742381235\n",
      "  batch 70 loss: 0.14363488842645894\n",
      "  batch 80 loss: 0.11045032801303023\n",
      "  batch 90 loss: 0.3245411532734579\n",
      "  batch 100 loss: 0.29074902051906976\n",
      "  batch 110 loss: 0.15822791465761837\n",
      "  batch 120 loss: 0.19614076755970017\n",
      "  batch 130 loss: 0.14062383611380938\n",
      "  batch 140 loss: 0.1714752735068032\n",
      "  batch 150 loss: 0.4187389967788476\n",
      "  batch 160 loss: 0.2645525207517494\n",
      "  batch 170 loss: 0.2958471914003894\n",
      "  batch 180 loss: 0.34937870516423575\n",
      "  batch 190 loss: 0.3110145897808252\n",
      "LOSS train 0.3110145897808252 valid 0.29813690834693357\n",
      "EPOCH 289:\n",
      "  batch 10 loss: 0.2508455480892735\n",
      "  batch 20 loss: 0.16225862684841558\n",
      "  batch 30 loss: 0.2917464139914955\n",
      "  batch 40 loss: 0.15875213847320993\n",
      "  batch 50 loss: 0.13031532371496723\n",
      "  batch 60 loss: 0.41594902581127824\n",
      "  batch 70 loss: 0.3725425240074401\n",
      "  batch 80 loss: 0.047517114734728236\n",
      "  batch 90 loss: 0.13618628431286198\n",
      "  batch 100 loss: 0.25626542894751764\n",
      "  batch 110 loss: 0.3156181512011244\n",
      "  batch 120 loss: 0.21457640715889284\n",
      "  batch 130 loss: 0.11501853940035289\n",
      "  batch 140 loss: 0.3892049213325663\n",
      "  batch 150 loss: 0.1577111328355386\n",
      "  batch 160 loss: 0.27491783018631394\n",
      "  batch 170 loss: 0.3217972505306534\n",
      "  batch 180 loss: 0.24662062238166982\n",
      "  batch 190 loss: 0.2239647557191347\n",
      "LOSS train 0.2239647557191347 valid 0.3950114720571433\n",
      "EPOCH 290:\n",
      "  batch 10 loss: 0.21664172550517832\n",
      "  batch 20 loss: 0.47239837633751447\n",
      "  batch 30 loss: 0.2112718289718032\n",
      "  batch 40 loss: 0.13361179935745895\n",
      "  batch 50 loss: 0.3621387607403449\n",
      "  batch 60 loss: 0.2721264078063541\n",
      "  batch 70 loss: 0.09935970499136601\n",
      "  batch 80 loss: 0.17562633724883198\n",
      "  batch 90 loss: 0.06458077347779181\n",
      "  batch 100 loss: 0.24385266851604684\n",
      "  batch 110 loss: 0.22045483995316317\n",
      "  batch 120 loss: 0.19083442252667737\n",
      "  batch 130 loss: 0.23156755063719175\n",
      "  batch 140 loss: 0.1725687147329154\n",
      "  batch 150 loss: 0.16324417660580365\n",
      "  batch 160 loss: 0.7647592744164285\n",
      "  batch 170 loss: 0.11849495904680225\n",
      "  batch 180 loss: 0.1310914036948816\n",
      "  batch 190 loss: 0.1108987724976032\n",
      "LOSS train 0.1108987724976032 valid 0.46691853488790425\n",
      "EPOCH 291:\n",
      "  batch 10 loss: 0.2983731995525886\n",
      "  batch 20 loss: 0.09170747809330351\n",
      "  batch 30 loss: 0.20071049991929613\n",
      "  batch 40 loss: 0.10944188872890663\n",
      "  batch 50 loss: 0.08328337996208575\n",
      "  batch 60 loss: 0.1250954487033596\n",
      "  batch 70 loss: 0.16761238972503634\n",
      "  batch 80 loss: 0.12871778406770318\n",
      "  batch 90 loss: 0.12066911750916916\n",
      "  batch 100 loss: 0.05568896907898306\n",
      "  batch 110 loss: 0.20669070040112275\n",
      "  batch 120 loss: 0.6394834330476442\n",
      "  batch 130 loss: 0.4245405258254323\n",
      "  batch 140 loss: 0.14518847927247408\n",
      "  batch 150 loss: 0.3599761573568685\n",
      "  batch 160 loss: 0.21462268121831585\n",
      "  batch 170 loss: 0.2166815486329142\n",
      "  batch 180 loss: 0.22427931696001907\n",
      "  batch 190 loss: 0.15284545414251624\n",
      "LOSS train 0.15284545414251624 valid 0.572149608658527\n",
      "EPOCH 292:\n",
      "  batch 10 loss: 0.28681846379186027\n",
      "  batch 20 loss: 0.20261478196043753\n",
      "  batch 30 loss: 0.3218561913250596\n",
      "  batch 40 loss: 0.43706808310962514\n",
      "  batch 50 loss: 0.2659110266184143\n",
      "  batch 60 loss: 0.19278002306818962\n",
      "  batch 70 loss: 0.25987927452915754\n",
      "  batch 80 loss: 0.0679872987093404\n",
      "  batch 90 loss: 0.14236520808553904\n",
      "  batch 100 loss: 0.039383233143598775\n",
      "  batch 110 loss: 0.27652420809026806\n",
      "  batch 120 loss: 0.3238358602000517\n",
      "  batch 130 loss: 0.16958715273358393\n",
      "  batch 140 loss: 0.26782664096244846\n",
      "  batch 150 loss: 0.13961519448203036\n",
      "  batch 160 loss: 0.5802089255987084\n",
      "  batch 170 loss: 0.3704098888392764\n",
      "  batch 180 loss: 0.28384183819507597\n",
      "  batch 190 loss: 0.09213615328917513\n",
      "LOSS train 0.09213615328917513 valid 0.23046856949729724\n",
      "EPOCH 293:\n",
      "  batch 10 loss: 0.08342861625278601\n",
      "  batch 20 loss: 0.32664265413332033\n",
      "  batch 30 loss: 0.12301632118851558\n",
      "  batch 40 loss: 0.15578095357850544\n",
      "  batch 50 loss: 0.3462912283386686\n",
      "  batch 60 loss: 0.19815179468278074\n",
      "  batch 70 loss: 0.1502270087832585\n",
      "  batch 80 loss: 0.38998675438997454\n",
      "  batch 90 loss: 0.09417701238817244\n",
      "  batch 100 loss: 0.2529254621735163\n",
      "  batch 110 loss: 0.10700573162612273\n",
      "  batch 120 loss: 0.24152561628325203\n",
      "  batch 130 loss: 0.16400238662317862\n",
      "  batch 140 loss: 0.6368833571308642\n",
      "  batch 150 loss: 0.15807858778807712\n",
      "  batch 160 loss: 0.7716170599860561\n",
      "  batch 170 loss: 0.24747799783945085\n",
      "  batch 180 loss: 0.1603090966193122\n",
      "  batch 190 loss: 0.18646258043299896\n",
      "LOSS train 0.18646258043299896 valid 0.24706115234701614\n",
      "EPOCH 294:\n",
      "  batch 10 loss: 0.2859706289524183\n",
      "  batch 20 loss: 0.1710941719080438\n",
      "  batch 30 loss: 0.4566633211074077\n",
      "  batch 40 loss: 0.10920811300547939\n",
      "  batch 50 loss: 0.13248147700505797\n",
      "  batch 60 loss: 0.1560783022257965\n",
      "  batch 70 loss: 0.4263854290285963\n",
      "  batch 80 loss: 0.10352478611202968\n",
      "  batch 90 loss: 0.29400816105262495\n",
      "  batch 100 loss: 0.09908729498856701\n",
      "  batch 110 loss: 0.2377120314682543\n",
      "  batch 120 loss: 0.32273594278813106\n",
      "  batch 130 loss: 0.3473625129496213\n",
      "  batch 140 loss: 0.15051196829226682\n",
      "  batch 150 loss: 0.15471393555781104\n",
      "  batch 160 loss: 0.07336700213418226\n",
      "  batch 170 loss: 0.13427398182466277\n",
      "  batch 180 loss: 0.07878630594932474\n",
      "  batch 190 loss: 0.24251950687976204\n",
      "LOSS train 0.24251950687976204 valid 0.21003803460805032\n",
      "EPOCH 295:\n",
      "  batch 10 loss: 0.09369725864889915\n",
      "  batch 20 loss: 0.255038291006349\n",
      "  batch 30 loss: 0.43751336693385384\n",
      "  batch 40 loss: 0.37843649336518864\n",
      "  batch 50 loss: 0.16362894085032167\n",
      "  batch 60 loss: 0.29691377744711644\n",
      "  batch 70 loss: 0.20167309209646192\n",
      "  batch 80 loss: 0.45849081644046236\n",
      "  batch 90 loss: 0.1790807768851664\n",
      "  batch 100 loss: 0.2861836541997036\n",
      "  batch 110 loss: 0.2305911122879479\n",
      "  batch 120 loss: 0.38760472477952135\n",
      "  batch 130 loss: 0.1858780288217531\n",
      "  batch 140 loss: 0.2737003070520586\n",
      "  batch 150 loss: 0.09998589858223568\n",
      "  batch 160 loss: 0.18314001949656813\n",
      "  batch 170 loss: 0.14999189137488428\n",
      "  batch 180 loss: 0.09558297733892687\n",
      "  batch 190 loss: 0.09335814487567404\n",
      "LOSS train 0.09335814487567404 valid 0.280429344476523\n",
      "EPOCH 296:\n",
      "  batch 10 loss: 0.08478042632850702\n",
      "  batch 20 loss: 0.06326282553491183\n",
      "  batch 30 loss: 0.1230703793669818\n",
      "  batch 40 loss: 0.20943496417239657\n",
      "  batch 50 loss: 0.14781580610324455\n",
      "  batch 60 loss: 0.22203253042389406\n",
      "  batch 70 loss: 0.30929490497583173\n",
      "  batch 80 loss: 0.21769761559917242\n",
      "  batch 90 loss: 0.322787213272386\n",
      "  batch 100 loss: 0.20619032163704104\n",
      "  batch 110 loss: 0.14292682085542766\n",
      "  batch 120 loss: 0.20213700915810479\n",
      "  batch 130 loss: 0.15926909612026066\n",
      "  batch 140 loss: 0.08900063191686058\n",
      "  batch 150 loss: 0.22858817951273522\n",
      "  batch 160 loss: 0.2598016209391062\n",
      "  batch 170 loss: 0.18635215680151304\n",
      "  batch 180 loss: 0.39621973116445586\n",
      "  batch 190 loss: 0.2880547685934289\n",
      "LOSS train 0.2880547685934289 valid 0.6458890990358598\n",
      "EPOCH 297:\n",
      "  batch 10 loss: 0.23760431522096043\n",
      "  batch 20 loss: 0.2961421070518554\n",
      "  batch 30 loss: 0.4606934203487981\n",
      "  batch 40 loss: 0.2560469312767964\n",
      "  batch 50 loss: 0.14743162769300397\n",
      "  batch 60 loss: 0.21086947907169815\n",
      "  batch 70 loss: 0.5708404507913656\n",
      "  batch 80 loss: 0.16965677618391056\n",
      "  batch 90 loss: 0.09533168434700201\n",
      "  batch 100 loss: 0.171398250607308\n",
      "  batch 110 loss: 0.45494450512342155\n",
      "  batch 120 loss: 0.3506265454765526\n",
      "  batch 130 loss: 0.43149427101629956\n",
      "  batch 140 loss: 0.13003572294837795\n",
      "  batch 150 loss: 0.10867134871368762\n",
      "  batch 160 loss: 0.1385905528157309\n",
      "  batch 170 loss: 0.3395745252542838\n",
      "  batch 180 loss: 0.19489240893744864\n",
      "  batch 190 loss: 0.06416777876293053\n",
      "LOSS train 0.06416777876293053 valid 0.28037271295733474\n",
      "EPOCH 298:\n",
      "  batch 10 loss: 0.20317662259476493\n",
      "  batch 20 loss: 0.2510889048651734\n",
      "  batch 30 loss: 0.039033564570127056\n",
      "  batch 40 loss: 0.135389424080131\n",
      "  batch 50 loss: 0.23813955054502003\n",
      "  batch 60 loss: 0.20360679867153522\n",
      "  batch 70 loss: 0.446798565379504\n",
      "  batch 80 loss: 0.25905854286284014\n",
      "  batch 90 loss: 0.13960475818894338\n",
      "  batch 100 loss: 0.1667928500824928\n",
      "  batch 110 loss: 0.3375643399285764\n",
      "  batch 120 loss: 0.1971341698041215\n",
      "  batch 130 loss: 0.16380660314644047\n",
      "  batch 140 loss: 0.17788428681378718\n",
      "  batch 150 loss: 0.16282389672196587\n",
      "  batch 160 loss: 0.26011912709582247\n",
      "  batch 170 loss: 0.20326884167297976\n",
      "  batch 180 loss: 0.22808866466693872\n",
      "  batch 190 loss: 0.16157200091329288\n",
      "LOSS train 0.16157200091329288 valid 0.2331315529446352\n",
      "EPOCH 299:\n",
      "  batch 10 loss: 0.18223488726926007\n",
      "  batch 20 loss: 0.11840017885988345\n",
      "  batch 30 loss: 0.16587279190134724\n",
      "  batch 40 loss: 0.239908716105856\n",
      "  batch 50 loss: 0.2362423093021789\n",
      "  batch 60 loss: 0.5484522217680933\n",
      "  batch 70 loss: 0.1028948350380233\n",
      "  batch 80 loss: 0.18270206189336022\n",
      "  batch 90 loss: 0.23813945122092264\n",
      "  batch 100 loss: 0.1848991888051387\n",
      "  batch 110 loss: 0.1270299358162447\n",
      "  batch 120 loss: 0.19917244818934704\n",
      "  batch 130 loss: 0.0632804894932633\n",
      "  batch 140 loss: 0.059349131214548836\n",
      "  batch 150 loss: 0.1608508693485419\n",
      "  batch 160 loss: 0.16548668394607374\n",
      "  batch 170 loss: 0.3585682705277577\n",
      "  batch 180 loss: 0.4070061981037725\n",
      "  batch 190 loss: 0.30085770342393514\n",
      "LOSS train 0.30085770342393514 valid 5.3324021349281585\n",
      "EPOCH 300:\n",
      "  batch 10 loss: 1.4320932683680438\n",
      "  batch 20 loss: 0.1079538228950696\n",
      "  batch 30 loss: 0.1388937961384727\n",
      "  batch 40 loss: 0.08397470063500805\n",
      "  batch 50 loss: 0.3190284535288811\n",
      "  batch 60 loss: 0.21256174540758366\n",
      "  batch 70 loss: 0.18282961560908007\n",
      "  batch 80 loss: 0.16542186373590084\n",
      "  batch 90 loss: 0.17876254587354196\n",
      "  batch 100 loss: 0.1363116127107787\n",
      "  batch 110 loss: 0.14299294290249237\n",
      "  batch 120 loss: 0.4907817227434862\n",
      "  batch 130 loss: 0.29005685425227057\n",
      "  batch 140 loss: 0.09943673379311804\n",
      "  batch 150 loss: 0.18912740789091914\n",
      "  batch 160 loss: 0.30490420212081515\n",
      "  batch 170 loss: 0.09366430006339214\n",
      "  batch 180 loss: 0.22880491563701072\n",
      "  batch 190 loss: 0.2294034136837581\n",
      "LOSS train 0.2294034136837581 valid 0.27028988072104254\n",
      "EPOCH 301:\n",
      "  batch 10 loss: 0.1836901758419117\n",
      "  batch 20 loss: 0.15067954428814118\n",
      "  batch 30 loss: 0.14771812333347042\n",
      "  batch 40 loss: 0.2267897374666063\n",
      "  batch 50 loss: 0.06186693483377894\n",
      "  batch 60 loss: 0.28223837195891976\n",
      "  batch 70 loss: 0.05398624004155863\n",
      "  batch 80 loss: 0.17967786211011116\n",
      "  batch 90 loss: 0.2370228170286282\n",
      "  batch 100 loss: 0.3047008490472763\n",
      "  batch 110 loss: 0.16717389354744228\n",
      "  batch 120 loss: 0.23464407757273875\n",
      "  batch 130 loss: 0.16631790742758312\n",
      "  batch 140 loss: 0.33910258146934213\n",
      "  batch 150 loss: 0.14521987829357386\n",
      "  batch 160 loss: 0.15952649379178183\n",
      "  batch 170 loss: 0.43204518192842445\n",
      "  batch 180 loss: 0.16319427053040272\n",
      "  batch 190 loss: 0.16306147325594794\n",
      "LOSS train 0.16306147325594794 valid 0.30054229284033707\n",
      "EPOCH 302:\n",
      "  batch 10 loss: 0.10500193744737771\n",
      "  batch 20 loss: 0.28480125409914764\n",
      "  batch 30 loss: 0.16158097108418587\n",
      "  batch 40 loss: 0.14706207589188125\n",
      "  batch 50 loss: 0.1603965536953183\n",
      "  batch 60 loss: 0.18538610900868663\n",
      "  batch 70 loss: 0.12967162181448658\n",
      "  batch 80 loss: 0.37893125817063267\n",
      "  batch 90 loss: 0.09464612673255032\n",
      "  batch 100 loss: 0.1850378774062847\n",
      "  batch 110 loss: 0.19756692449154797\n",
      "  batch 120 loss: 0.16790100095822708\n",
      "  batch 130 loss: 0.12462040479294956\n",
      "  batch 140 loss: 0.23304518340228242\n",
      "  batch 150 loss: 0.21035926004769862\n",
      "  batch 160 loss: 0.09973520711937453\n",
      "  batch 170 loss: 0.189611585111561\n",
      "  batch 180 loss: 0.24168211009382504\n",
      "  batch 190 loss: 0.384354047292436\n",
      "LOSS train 0.384354047292436 valid 0.2581269981077266\n",
      "EPOCH 303:\n",
      "  batch 10 loss: 0.19705947802285664\n",
      "  batch 20 loss: 0.2128176060079568\n",
      "  batch 30 loss: 0.2139789037981245\n",
      "  batch 40 loss: 0.10934530563245062\n",
      "  batch 50 loss: 0.23650728163147505\n",
      "  batch 60 loss: 0.2952579567303474\n",
      "  batch 70 loss: 0.20076659504720737\n",
      "  batch 80 loss: 0.12425854411412729\n",
      "  batch 90 loss: 0.3213577364293087\n",
      "  batch 100 loss: 0.2566771747704479\n",
      "  batch 110 loss: 0.1823860561627953\n",
      "  batch 120 loss: 0.13209284580298117\n",
      "  batch 130 loss: 0.16436488651079345\n",
      "  batch 140 loss: 0.11302384926821105\n",
      "  batch 150 loss: 0.4596823124331422\n",
      "  batch 160 loss: 0.2015868805712671\n",
      "  batch 170 loss: 0.3581650493128109\n",
      "  batch 180 loss: 0.03831341852928745\n",
      "  batch 190 loss: 0.12165112676739227\n",
      "LOSS train 0.12165112676739227 valid 1.6456110276824638\n",
      "EPOCH 304:\n",
      "  batch 10 loss: 0.5236451069990835\n",
      "  batch 20 loss: 0.23192042735245194\n",
      "  batch 30 loss: 0.20133386629659072\n",
      "  batch 40 loss: 0.4041756995386095\n",
      "  batch 50 loss: 0.11850073895548122\n",
      "  batch 60 loss: 0.10388127348996931\n",
      "  batch 70 loss: 0.23515262359978806\n",
      "  batch 80 loss: 0.20763768358156084\n",
      "  batch 90 loss: 0.0810958283564105\n",
      "  batch 100 loss: 0.41249505059822694\n",
      "  batch 110 loss: 0.1469617104950885\n",
      "  batch 120 loss: 0.11189076338428094\n",
      "  batch 130 loss: 0.11104093780886615\n",
      "  batch 140 loss: 0.4641140042018378\n",
      "  batch 150 loss: 0.23926104466590914\n",
      "  batch 160 loss: 0.14262419037768267\n",
      "  batch 170 loss: 0.3588272677385248\n",
      "  batch 180 loss: 0.3228158326146513\n",
      "  batch 190 loss: 0.12325107516044227\n",
      "LOSS train 0.12325107516044227 valid 0.2665302579658041\n",
      "EPOCH 305:\n",
      "  batch 10 loss: 0.3316846940993855\n",
      "  batch 20 loss: 0.09917406699969433\n",
      "  batch 30 loss: 0.15534764813019136\n",
      "  batch 40 loss: 0.15000393733898817\n",
      "  batch 50 loss: 0.49043750423261373\n",
      "  batch 60 loss: 0.06932867603845808\n",
      "  batch 70 loss: 0.16679440576081106\n",
      "  batch 80 loss: 0.45288124280632475\n",
      "  batch 90 loss: 0.11611014141017222\n",
      "  batch 100 loss: 0.07760426562472275\n",
      "  batch 110 loss: 0.21479858699840407\n",
      "  batch 120 loss: 0.0452007918858726\n",
      "  batch 130 loss: 0.13390112187407793\n",
      "  batch 140 loss: 0.08061223413615153\n",
      "  batch 150 loss: 0.36446032127496436\n",
      "  batch 160 loss: 0.30488621719414366\n",
      "  batch 170 loss: 0.4714866691534553\n",
      "  batch 180 loss: 0.5470014418344362\n",
      "  batch 190 loss: 0.519545817283506\n",
      "LOSS train 0.519545817283506 valid 0.24446772689147203\n",
      "EPOCH 306:\n",
      "  batch 10 loss: 0.09860362728359177\n",
      "  batch 20 loss: 0.07456080647298222\n",
      "  batch 30 loss: 0.2300669074461439\n",
      "  batch 40 loss: 0.5709699114064278\n",
      "  batch 50 loss: 0.09710692795633805\n",
      "  batch 60 loss: 0.06780050158795348\n",
      "  batch 70 loss: 0.17934925219378783\n",
      "  batch 80 loss: 0.26207383631553965\n",
      "  batch 90 loss: 0.2129052924854477\n",
      "  batch 100 loss: 0.1620519392665301\n",
      "  batch 110 loss: 0.19150615956386902\n",
      "  batch 120 loss: 0.24090428041381529\n",
      "  batch 130 loss: 0.08117292412207462\n",
      "  batch 140 loss: 0.18307136233343044\n",
      "  batch 150 loss: 0.15287390293779027\n",
      "  batch 160 loss: 0.0731034145472222\n",
      "  batch 170 loss: 0.3686394785901939\n",
      "  batch 180 loss: 0.045241679607715926\n",
      "  batch 190 loss: 0.40244365403050325\n",
      "LOSS train 0.40244365403050325 valid 0.23722675114008895\n",
      "EPOCH 307:\n",
      "  batch 10 loss: 0.2553296244019293\n",
      "  batch 20 loss: 0.15931676525942748\n",
      "  batch 30 loss: 0.17956113493473821\n",
      "  batch 40 loss: 0.2060086652440077\n",
      "  batch 50 loss: 0.10823716975719436\n",
      "  batch 60 loss: 0.29289688947719694\n",
      "  batch 70 loss: 0.16728186526524952\n",
      "  batch 80 loss: 0.3058221284794854\n",
      "  batch 90 loss: 0.0906987145266612\n",
      "  batch 100 loss: 0.07006881676788908\n",
      "  batch 110 loss: 0.043071149529532705\n",
      "  batch 120 loss: 0.3202189266201458\n",
      "  batch 130 loss: 0.1844685553209274\n",
      "  batch 140 loss: 0.42509060100637724\n",
      "  batch 150 loss: 0.11694298095535487\n",
      "  batch 160 loss: 0.07494484893977642\n",
      "  batch 170 loss: 0.2862981479062\n",
      "  batch 180 loss: 0.22306370709702605\n",
      "  batch 190 loss: 0.18394650638001622\n",
      "LOSS train 0.18394650638001622 valid 0.26960560373966774\n",
      "EPOCH 308:\n",
      "  batch 10 loss: 0.13901726124240668\n",
      "  batch 20 loss: 0.34086344231454857\n",
      "  batch 30 loss: 0.1860291693770705\n",
      "  batch 40 loss: 0.13146089859983476\n",
      "  batch 50 loss: 0.1624432560915011\n",
      "  batch 60 loss: 0.18319583427328326\n",
      "  batch 70 loss: 0.18959938992957176\n",
      "  batch 80 loss: 0.3750309250994178\n",
      "  batch 90 loss: 0.13956554605974816\n",
      "  batch 100 loss: 0.39224594395564055\n",
      "  batch 110 loss: 0.10131187042788951\n",
      "  batch 120 loss: 0.18898794908673153\n",
      "  batch 130 loss: 0.15479877179132018\n",
      "  batch 140 loss: 0.07739420223406342\n",
      "  batch 150 loss: 0.06049083120851719\n",
      "  batch 160 loss: 0.0743077743347385\n",
      "  batch 170 loss: 0.3092703594953491\n",
      "  batch 180 loss: 0.1412809881461726\n",
      "  batch 190 loss: 0.3056459282106516\n",
      "LOSS train 0.3056459282106516 valid 0.22853888596627933\n",
      "EPOCH 309:\n",
      "  batch 10 loss: 0.09850898719523685\n",
      "  batch 20 loss: 0.07510675870107661\n",
      "  batch 30 loss: 0.16255639877708744\n",
      "  batch 40 loss: 0.1361508125977707\n",
      "  batch 50 loss: 0.2877926667304564\n",
      "  batch 60 loss: 0.24674878209766576\n",
      "  batch 70 loss: 0.8663357496508979\n",
      "  batch 80 loss: 0.41959380549051273\n",
      "  batch 90 loss: 0.12021044168504887\n",
      "  batch 100 loss: 0.474848734374973\n",
      "  batch 110 loss: 0.07914030441897921\n",
      "  batch 120 loss: 0.2865096227531467\n",
      "  batch 130 loss: 0.10505868347900105\n",
      "  batch 140 loss: 0.09095918633029214\n",
      "  batch 150 loss: 0.2595945682810452\n",
      "  batch 160 loss: 0.11728672039644153\n",
      "  batch 170 loss: 0.07943916308395274\n",
      "  batch 180 loss: 0.11027648900872009\n",
      "  batch 190 loss: 0.20465221951017157\n",
      "LOSS train 0.20465221951017157 valid 0.2927847773414844\n",
      "EPOCH 310:\n",
      "  batch 10 loss: 0.1571326809204038\n",
      "  batch 20 loss: 0.10027960233564955\n",
      "  batch 30 loss: 0.23236531618895243\n",
      "  batch 40 loss: 0.18161720005082316\n",
      "  batch 50 loss: 0.10545305647101486\n",
      "  batch 60 loss: 0.27802362072834513\n",
      "  batch 70 loss: 0.14465730277479452\n",
      "  batch 80 loss: 0.06115751158213243\n",
      "  batch 90 loss: 0.11229607111781661\n",
      "  batch 100 loss: 0.03579146494375891\n",
      "  batch 110 loss: 0.13634513168717605\n",
      "  batch 120 loss: 0.06484927355813852\n",
      "  batch 130 loss: 0.24995220583004993\n",
      "  batch 140 loss: 0.4588056840709214\n",
      "  batch 150 loss: 0.18212170816914294\n",
      "  batch 160 loss: 0.09642173819156596\n",
      "  batch 170 loss: 0.1949400713430805\n",
      "  batch 180 loss: 0.14154631056080688\n",
      "  batch 190 loss: 0.32835630320366815\n",
      "LOSS train 0.32835630320366815 valid 0.30136693274632237\n",
      "EPOCH 311:\n",
      "  batch 10 loss: 0.1872580763338192\n",
      "  batch 20 loss: 0.0495171777332871\n",
      "  batch 30 loss: 0.09165286489296705\n",
      "  batch 40 loss: 0.0936305102557526\n",
      "  batch 50 loss: 0.21726101340391324\n",
      "  batch 60 loss: 0.13446000951589668\n",
      "  batch 70 loss: 0.12449622830863519\n",
      "  batch 80 loss: 0.16263519426211132\n",
      "  batch 90 loss: 0.18713977107136087\n",
      "  batch 100 loss: 0.12744970158673824\n",
      "  batch 110 loss: 0.2808738015140989\n",
      "  batch 120 loss: 0.12172974044078728\n",
      "  batch 130 loss: 0.18529633324651512\n",
      "  batch 140 loss: 0.2690883124429092\n",
      "  batch 150 loss: 0.23673994614873664\n",
      "  batch 160 loss: 0.08671019433240872\n",
      "  batch 170 loss: 0.16594102349481546\n",
      "  batch 180 loss: 0.06629572849469696\n",
      "  batch 190 loss: 0.31457879877852973\n",
      "LOSS train 0.31457879877852973 valid 0.47913225104252305\n",
      "EPOCH 312:\n",
      "  batch 10 loss: 0.3903792286673706\n",
      "  batch 20 loss: 0.12338709568066406\n",
      "  batch 30 loss: 0.3246294479199605\n",
      "  batch 40 loss: 0.4040068345459531\n",
      "  batch 50 loss: 0.16328929113333288\n",
      "  batch 60 loss: 0.28031316688484365\n",
      "  batch 70 loss: 0.24403713826213788\n",
      "  batch 80 loss: 0.15275999290024628\n",
      "  batch 90 loss: 0.1935899280366357\n",
      "  batch 100 loss: 0.16415260477515403\n",
      "  batch 110 loss: 0.05310139314569824\n",
      "  batch 120 loss: 0.23993291881124607\n",
      "  batch 130 loss: 0.2065754821509472\n",
      "  batch 140 loss: 0.1893933409570309\n",
      "  batch 150 loss: 0.1888703017746593\n",
      "  batch 160 loss: 0.1018213222938357\n",
      "  batch 170 loss: 0.14691516004677396\n",
      "  batch 180 loss: 0.1783978909803409\n",
      "  batch 190 loss: 0.1270074073876458\n",
      "LOSS train 0.1270074073876458 valid 0.24384942501628523\n",
      "EPOCH 313:\n",
      "  batch 10 loss: 0.21102654624046407\n",
      "  batch 20 loss: 0.08344232037848087\n",
      "  batch 30 loss: 0.2097190092350047\n",
      "  batch 40 loss: 0.08950407415250083\n",
      "  batch 50 loss: 0.11073120516830386\n",
      "  batch 60 loss: 0.13594621238080434\n",
      "  batch 70 loss: 0.11272919256043679\n",
      "  batch 80 loss: 0.6721212214830302\n",
      "  batch 90 loss: 0.17911396656854778\n",
      "  batch 100 loss: 0.19674288432506729\n",
      "  batch 110 loss: 0.19054545078251978\n",
      "  batch 120 loss: 0.034124300369694535\n",
      "  batch 130 loss: 0.3122219480062995\n",
      "  batch 140 loss: 0.11643780955273542\n",
      "  batch 150 loss: 0.12330745196140924\n",
      "  batch 160 loss: 0.2602596627933963\n",
      "  batch 170 loss: 0.19897443177687818\n",
      "  batch 180 loss: 0.0431677689011849\n",
      "  batch 190 loss: 0.3637275576184038\n",
      "LOSS train 0.3637275576184038 valid 0.31338815272934323\n",
      "EPOCH 314:\n",
      "  batch 10 loss: 0.12342547873267903\n",
      "  batch 20 loss: 0.15002345685879845\n",
      "  batch 30 loss: 0.07032669705222361\n",
      "  batch 40 loss: 0.06983684591104974\n",
      "  batch 50 loss: 0.241904090216849\n",
      "  batch 60 loss: 0.5470970686394139\n",
      "  batch 70 loss: 0.3532645152672558\n",
      "  batch 80 loss: 0.15465585738766094\n",
      "  batch 90 loss: 0.1125624918131507\n",
      "  batch 100 loss: 0.10891303708012856\n",
      "  batch 110 loss: 0.13070668567179383\n",
      "  batch 120 loss: 0.07775721437719767\n",
      "  batch 130 loss: 0.3089390708188148\n",
      "  batch 140 loss: 0.18200133896061743\n",
      "  batch 150 loss: 0.1558114758796819\n",
      "  batch 160 loss: 0.27553426785366353\n",
      "  batch 170 loss: 0.5160340747302599\n",
      "  batch 180 loss: 0.4578977165658216\n",
      "  batch 190 loss: 0.281523189994914\n",
      "LOSS train 0.281523189994914 valid 0.35853673794754753\n",
      "EPOCH 315:\n",
      "  batch 10 loss: 0.10972841512029845\n",
      "  batch 20 loss: 0.1226710676226503\n",
      "  batch 30 loss: 0.35773456772080864\n",
      "  batch 40 loss: 0.18337425189274653\n",
      "  batch 50 loss: 0.30484439276187913\n",
      "  batch 60 loss: 0.13579582386773836\n",
      "  batch 70 loss: 0.10027666567020788\n",
      "  batch 80 loss: 0.1296550332550396\n",
      "  batch 90 loss: 0.06906905818104861\n",
      "  batch 100 loss: 0.19991399019891104\n",
      "  batch 110 loss: 0.08866240161878522\n",
      "  batch 120 loss: 0.15575095692074684\n",
      "  batch 130 loss: 0.31055188567115694\n",
      "  batch 140 loss: 0.0825675901822251\n",
      "  batch 150 loss: 0.17996939747081342\n",
      "  batch 160 loss: 0.0610449373629308\n",
      "  batch 170 loss: 0.08193180995986041\n",
      "  batch 180 loss: 0.19108796781729326\n",
      "  batch 190 loss: 0.22900124446205156\n",
      "LOSS train 0.22900124446205156 valid 0.22200630189493098\n",
      "EPOCH 316:\n",
      "  batch 10 loss: 0.05380413404927822\n",
      "  batch 20 loss: 0.07303790555852174\n",
      "  batch 30 loss: 0.07764064557050006\n",
      "  batch 40 loss: 0.12921156881202478\n",
      "  batch 50 loss: 0.1124274746312949\n",
      "  batch 60 loss: 0.04020292513114328\n",
      "  batch 70 loss: 0.11964213244791608\n",
      "  batch 80 loss: 0.2215869807132549\n",
      "  batch 90 loss: 0.6065562503996261\n",
      "  batch 100 loss: 0.15956335245464287\n",
      "  batch 110 loss: 0.2605644857241714\n",
      "  batch 120 loss: 0.20950378402926617\n",
      "  batch 130 loss: 0.13339840886037563\n",
      "  batch 140 loss: 0.2452979637739645\n",
      "  batch 150 loss: 0.34770101385365704\n",
      "  batch 160 loss: 0.2658449200003815\n",
      "  batch 170 loss: 0.07700956328699249\n",
      "  batch 180 loss: 0.1533283629985817\n",
      "  batch 190 loss: 0.12567040467765764\n",
      "LOSS train 0.12567040467765764 valid 0.21042485933204985\n",
      "EPOCH 317:\n",
      "  batch 10 loss: 0.17577973104907868\n",
      "  batch 20 loss: 0.10307969089662947\n",
      "  batch 30 loss: 0.13124989038988133\n",
      "  batch 40 loss: 0.16657521496890695\n",
      "  batch 50 loss: 0.1243101268853934\n",
      "  batch 60 loss: 0.20933234529584296\n",
      "  batch 70 loss: 0.24526155040402955\n",
      "  batch 80 loss: 0.17119234408801276\n",
      "  batch 90 loss: 0.07245855846558698\n",
      "  batch 100 loss: 0.16595921128573538\n",
      "  batch 110 loss: 0.13349680779811024\n",
      "  batch 120 loss: 0.06828946199311758\n",
      "  batch 130 loss: 0.0724237889032338\n",
      "  batch 140 loss: 0.198741451090973\n",
      "  batch 150 loss: 0.21625403104935687\n",
      "  batch 160 loss: 0.4361908954100727\n",
      "  batch 170 loss: 0.4541722514455614\n",
      "  batch 180 loss: 0.1194162812187642\n",
      "  batch 190 loss: 0.06569731795825647\n",
      "LOSS train 0.06569731795825647 valid 0.2283791923955505\n",
      "EPOCH 318:\n",
      "  batch 10 loss: 0.09553067293254572\n",
      "  batch 20 loss: 0.10546885857238522\n",
      "  batch 30 loss: 0.06642296630816417\n",
      "  batch 40 loss: 0.5067946394410683\n",
      "  batch 50 loss: 0.2805412393495317\n",
      "  batch 60 loss: 0.14435712583508575\n",
      "  batch 70 loss: 0.04159204180418783\n",
      "  batch 80 loss: 0.19970345908686796\n",
      "  batch 90 loss: 0.14265727224883448\n",
      "  batch 100 loss: 0.09184005190327298\n",
      "  batch 110 loss: 0.0978789281481113\n",
      "  batch 120 loss: 0.3439907368134755\n",
      "  batch 130 loss: 0.0828235601821234\n",
      "  batch 140 loss: 0.12363030597152828\n",
      "  batch 150 loss: 0.1540550317895395\n",
      "  batch 160 loss: 0.3011808108067271\n",
      "  batch 170 loss: 0.07608158561997698\n",
      "  batch 180 loss: 0.10864757814852055\n",
      "  batch 190 loss: 0.038078667985246284\n",
      "LOSS train 0.038078667985246284 valid 0.4480073663740908\n",
      "EPOCH 319:\n",
      "  batch 10 loss: 0.051117481285291436\n",
      "  batch 20 loss: 0.055943695305177246\n",
      "  batch 30 loss: 0.11172606334639568\n",
      "  batch 40 loss: 0.2044000179061186\n",
      "  batch 50 loss: 0.08711992517355611\n",
      "  batch 60 loss: 0.5087643853824375\n",
      "  batch 70 loss: 0.48626951872956853\n",
      "  batch 80 loss: 0.16193085005734248\n",
      "  batch 90 loss: 0.13548010034719482\n",
      "  batch 100 loss: 0.06304533013535547\n",
      "  batch 110 loss: 0.20004095262465854\n",
      "  batch 120 loss: 0.15919637204024184\n",
      "  batch 130 loss: 0.21082850922703072\n",
      "  batch 140 loss: 0.2035101932116959\n",
      "  batch 150 loss: 0.11631655627425061\n",
      "  batch 160 loss: 0.18910730550160224\n",
      "  batch 170 loss: 0.19218014321158988\n",
      "  batch 180 loss: 0.38361753494937145\n",
      "  batch 190 loss: 0.09465237536533096\n",
      "LOSS train 0.09465237536533096 valid 0.1992547341827585\n",
      "EPOCH 320:\n",
      "  batch 10 loss: 0.3014368854390341\n",
      "  batch 20 loss: 0.16308866165454675\n",
      "  batch 30 loss: 0.4055485793214757\n",
      "  batch 40 loss: 0.2157427568530693\n",
      "  batch 50 loss: 0.2618851117993472\n",
      "  batch 60 loss: 0.21756910330150275\n",
      "  batch 70 loss: 0.2436199946951092\n",
      "  batch 80 loss: 0.18508607952753664\n",
      "  batch 90 loss: 0.2981028034068004\n",
      "  batch 100 loss: 0.17061612762190634\n",
      "  batch 110 loss: 0.05094987287884578\n",
      "  batch 120 loss: 0.23311262059105503\n",
      "  batch 130 loss: 0.17106888058660843\n",
      "  batch 140 loss: 0.23292532986442893\n",
      "  batch 150 loss: 0.2567829496103968\n",
      "  batch 160 loss: 0.20962278438200882\n",
      "  batch 170 loss: 0.20354107293724155\n",
      "  batch 180 loss: 0.06006222619125765\n",
      "  batch 190 loss: 0.1592090419104352\n",
      "LOSS train 0.1592090419104352 valid 0.875509529012561\n",
      "EPOCH 321:\n",
      "  batch 10 loss: 0.13335244123245502\n",
      "  batch 20 loss: 0.10099267445766599\n",
      "  batch 30 loss: 0.11246798650217897\n",
      "  batch 40 loss: 0.21817128039092495\n",
      "  batch 50 loss: 0.128124735471647\n",
      "  batch 60 loss: 0.10126106501702452\n",
      "  batch 70 loss: 0.057876129713076804\n",
      "  batch 80 loss: 0.12830560282091028\n",
      "  batch 90 loss: 0.4687019522157243\n",
      "  batch 100 loss: 0.23186021852288832\n",
      "  batch 110 loss: 0.28268517469696236\n",
      "  batch 120 loss: 0.20159071808402587\n",
      "  batch 130 loss: 0.2889262069023971\n",
      "  batch 140 loss: 0.14379704819002653\n",
      "  batch 150 loss: 0.1097794722612889\n",
      "  batch 160 loss: 0.06976759662538826\n",
      "  batch 170 loss: 0.10745600046175241\n",
      "  batch 180 loss: 0.5558811355178477\n",
      "  batch 190 loss: 0.36160593235108535\n",
      "LOSS train 0.36160593235108535 valid 0.2417152438481422\n",
      "EPOCH 322:\n",
      "  batch 10 loss: 0.07704942477867008\n",
      "  batch 20 loss: 0.06454576939941034\n",
      "  batch 30 loss: 0.2450763798753542\n",
      "  batch 40 loss: 0.1256663866686722\n",
      "  batch 50 loss: 0.1351591274546081\n",
      "  batch 60 loss: 0.08605475475851562\n",
      "  batch 70 loss: 0.10077648172700719\n",
      "  batch 80 loss: 0.08032202802460234\n",
      "  batch 90 loss: 0.13186232084253788\n",
      "  batch 100 loss: 0.05142299957415162\n",
      "  batch 110 loss: 0.09058271921558117\n",
      "  batch 120 loss: 0.12230065944413582\n",
      "  batch 130 loss: 0.32801608951685923\n",
      "  batch 140 loss: 0.21165841160764104\n",
      "  batch 150 loss: 0.1340523065882735\n",
      "  batch 160 loss: 0.09954958865710069\n",
      "  batch 170 loss: 0.2704902854473403\n",
      "  batch 180 loss: 0.33921573618972617\n",
      "  batch 190 loss: 0.2189923455875487\n",
      "LOSS train 0.2189923455875487 valid 1.29297019641905\n",
      "EPOCH 323:\n",
      "  batch 10 loss: 0.2964592988610093\n",
      "  batch 20 loss: 0.1773318186762481\n",
      "  batch 30 loss: 0.09146102811380388\n",
      "  batch 40 loss: 0.1472688561247196\n",
      "  batch 50 loss: 0.3182982000465472\n",
      "  batch 60 loss: 0.22186814638916985\n",
      "  batch 70 loss: 0.06979270905394515\n",
      "  batch 80 loss: 0.16649520457649486\n",
      "  batch 90 loss: 0.13949238937275368\n",
      "  batch 100 loss: 0.22780052929447264\n",
      "  batch 110 loss: 0.14822001171796728\n",
      "  batch 120 loss: 0.10366684442669793\n",
      "  batch 130 loss: 0.09598264880005444\n",
      "  batch 140 loss: 0.12498493432412942\n",
      "  batch 150 loss: 0.34128621959107475\n",
      "  batch 160 loss: 0.20831552320923946\n",
      "  batch 170 loss: 0.2961269087911205\n",
      "  batch 180 loss: 0.13503209065529517\n",
      "  batch 190 loss: 0.10248839049418165\n",
      "LOSS train 0.10248839049418165 valid 0.28824412766311397\n",
      "EPOCH 324:\n",
      "  batch 10 loss: 0.01634381628500705\n",
      "  batch 20 loss: 0.11611707934425794\n",
      "  batch 30 loss: 0.16559764413668746\n",
      "  batch 40 loss: 0.04428012068274256\n",
      "  batch 50 loss: 0.07888926401783465\n",
      "  batch 60 loss: 0.17789258341381356\n",
      "  batch 70 loss: 0.11931383860282949\n",
      "  batch 80 loss: 0.1457991581870374\n",
      "  batch 90 loss: 0.2908971416827626\n",
      "  batch 100 loss: 0.08368527137827186\n",
      "  batch 110 loss: 0.13003245895124566\n",
      "  batch 120 loss: 0.21347668242815415\n",
      "  batch 130 loss: 0.10767196223932843\n",
      "  batch 140 loss: 0.08136908652231796\n",
      "  batch 150 loss: 0.25597323199099264\n",
      "  batch 160 loss: 0.5056646117158834\n",
      "  batch 170 loss: 0.20409939466071592\n",
      "  batch 180 loss: 0.05443807130905043\n",
      "  batch 190 loss: 0.2877579325373517\n",
      "LOSS train 0.2877579325373517 valid 0.18120896469950729\n",
      "EPOCH 325:\n",
      "  batch 10 loss: 0.07048128878814168\n",
      "  batch 20 loss: 0.12639744120265278\n",
      "  batch 30 loss: 0.03546026069552681\n",
      "  batch 40 loss: 0.12991914849735622\n",
      "  batch 50 loss: 0.04711401375825517\n",
      "  batch 60 loss: 0.1831663791248502\n",
      "  batch 70 loss: 0.21989208172708458\n",
      "  batch 80 loss: 0.08958078075247614\n",
      "  batch 90 loss: 0.06281145931925494\n",
      "  batch 100 loss: 0.11594774301502184\n",
      "  batch 110 loss: 0.0684393597945018\n",
      "  batch 120 loss: 0.07120618863618802\n",
      "  batch 130 loss: 0.04365087655123716\n",
      "  batch 140 loss: 0.34304157870046764\n",
      "  batch 150 loss: 0.05056988389806065\n",
      "  batch 160 loss: 0.1509988221063395\n",
      "  batch 170 loss: 0.3538947371431277\n",
      "  batch 180 loss: 0.1972160986786548\n",
      "  batch 190 loss: 0.048059795041444885\n",
      "LOSS train 0.048059795041444885 valid 0.22438588320237146\n",
      "EPOCH 326:\n",
      "  batch 10 loss: 0.22758222153424867\n",
      "  batch 20 loss: 0.18178977971183485\n",
      "  batch 30 loss: 0.27959131283151917\n",
      "  batch 40 loss: 0.15476482284775556\n",
      "  batch 50 loss: 0.5266804864493679\n",
      "  batch 60 loss: 0.27677026658657267\n",
      "  batch 70 loss: 0.3390568047328998\n",
      "  batch 80 loss: 0.040006150059343784\n",
      "  batch 90 loss: 0.18095737013463803\n",
      "  batch 100 loss: 0.09294363013345901\n",
      "  batch 110 loss: 0.21824833257996942\n",
      "  batch 120 loss: 0.21609864478596136\n",
      "  batch 130 loss: 0.15352522730499912\n",
      "  batch 140 loss: 0.20276790021016494\n",
      "  batch 150 loss: 0.0724521662472398\n",
      "  batch 160 loss: 0.11270013672692585\n",
      "  batch 170 loss: 0.06522861335542984\n",
      "  batch 180 loss: 0.10193832234144792\n",
      "  batch 190 loss: 0.08358792062135763\n",
      "LOSS train 0.08358792062135763 valid 0.4588173840062761\n",
      "EPOCH 327:\n",
      "  batch 10 loss: 0.156982937293742\n",
      "  batch 20 loss: 0.18031242764336639\n",
      "  batch 30 loss: 0.12301794806917314\n",
      "  batch 40 loss: 0.048379254525207215\n",
      "  batch 50 loss: 0.12993912971064675\n",
      "  batch 60 loss: 0.07314622628450707\n",
      "  batch 70 loss: 0.08815254723704129\n",
      "  batch 80 loss: 0.14764876023371015\n",
      "  batch 90 loss: 0.06553074942785315\n",
      "  batch 100 loss: 0.08632533986719863\n",
      "  batch 110 loss: 0.27806578805093524\n",
      "  batch 120 loss: 0.25228446411674665\n",
      "  batch 130 loss: 0.15105587286889205\n",
      "  batch 140 loss: 0.11749792208211148\n",
      "  batch 150 loss: 0.16670628060310264\n",
      "  batch 160 loss: 0.08151615966880854\n",
      "  batch 170 loss: 0.28985435033155227\n",
      "  batch 180 loss: 0.23992851714347124\n",
      "  batch 190 loss: 0.4500657843973386\n",
      "LOSS train 0.4500657843973386 valid 0.3105403473501867\n",
      "EPOCH 328:\n",
      "  batch 10 loss: 0.0153902881031172\n",
      "  batch 20 loss: 0.2114844308525335\n",
      "  batch 30 loss: 0.2917628027948922\n",
      "  batch 40 loss: 0.10788635841709038\n",
      "  batch 50 loss: 0.04797627075995479\n",
      "  batch 60 loss: 0.18063240928895538\n",
      "  batch 70 loss: 0.11358371813685153\n",
      "  batch 80 loss: 0.15819461482545877\n",
      "  batch 90 loss: 0.1461312812862161\n",
      "  batch 100 loss: 0.5060699934299009\n",
      "  batch 110 loss: 0.4072757656660542\n",
      "  batch 120 loss: 0.06989511993742781\n",
      "  batch 130 loss: 0.2583140940825615\n",
      "  batch 140 loss: 0.15089512097183616\n",
      "  batch 150 loss: 0.23718954223950278\n",
      "  batch 160 loss: 0.17825823304119695\n",
      "  batch 170 loss: 0.06759984998207073\n",
      "  batch 180 loss: 0.10967663562332745\n",
      "  batch 190 loss: 0.17528055722505087\n",
      "LOSS train 0.17528055722505087 valid 0.27484007053863024\n",
      "EPOCH 329:\n",
      "  batch 10 loss: 0.12009279862468247\n",
      "  batch 20 loss: 0.0828262595949127\n",
      "  batch 30 loss: 0.02534916521366313\n",
      "  batch 40 loss: 0.12085432377753022\n",
      "  batch 50 loss: 0.306642326030078\n",
      "  batch 60 loss: 0.431617074404312\n",
      "  batch 70 loss: 0.08643148781065975\n",
      "  batch 80 loss: 0.1636291790611722\n",
      "  batch 90 loss: 0.057447667982160056\n",
      "  batch 100 loss: 0.06740717677967041\n",
      "  batch 110 loss: 0.3304798469522211\n",
      "  batch 120 loss: 0.5825940970286865\n",
      "  batch 130 loss: 0.06579738894688489\n",
      "  batch 140 loss: 0.07020932744235324\n",
      "  batch 150 loss: 0.2864086515884992\n",
      "  batch 160 loss: 0.12554119721007737\n",
      "  batch 170 loss: 0.15835898244658891\n",
      "  batch 180 loss: 0.1837368034048268\n",
      "  batch 190 loss: 0.08787674996237911\n",
      "LOSS train 0.08787674996237911 valid 0.2787968478938777\n",
      "EPOCH 330:\n",
      "  batch 10 loss: 0.24289543598169985\n",
      "  batch 20 loss: 0.17197877956350566\n",
      "  batch 30 loss: 0.11546553219350245\n",
      "  batch 40 loss: 0.05091616648496711\n",
      "  batch 50 loss: 0.07155359353055246\n",
      "  batch 60 loss: 0.13212535439306522\n",
      "  batch 70 loss: 0.12475935478123575\n",
      "  batch 80 loss: 0.1202484145223707\n",
      "  batch 90 loss: 0.40491110112816386\n",
      "  batch 100 loss: 0.08109039188257157\n",
      "  batch 110 loss: 0.11970787052521245\n",
      "  batch 120 loss: 0.05276761733111925\n",
      "  batch 130 loss: 0.09223508261384268\n",
      "  batch 140 loss: 0.07455205413316435\n",
      "  batch 150 loss: 0.25884989057319674\n",
      "  batch 160 loss: 0.2711005940240284\n",
      "  batch 170 loss: 0.19967306906182786\n",
      "  batch 180 loss: 0.029011770063834773\n",
      "  batch 190 loss: 0.15509180962817481\n",
      "LOSS train 0.15509180962817481 valid 0.23677029860400814\n",
      "EPOCH 331:\n",
      "  batch 10 loss: 0.05111245497428172\n",
      "  batch 20 loss: 0.13301555012640165\n",
      "  batch 30 loss: 0.15479053842685744\n",
      "  batch 40 loss: 0.3773714067343462\n",
      "  batch 50 loss: 0.11127376629424361\n",
      "  batch 60 loss: 0.11616213611287093\n",
      "  batch 70 loss: 0.23739476695172926\n",
      "  batch 80 loss: 0.1644704368443854\n",
      "  batch 90 loss: 0.21613527873432758\n",
      "  batch 100 loss: 0.11755652103429384\n",
      "  batch 110 loss: 0.28565480074285005\n",
      "  batch 120 loss: 0.1443134154758809\n",
      "  batch 130 loss: 0.20633729624378247\n",
      "  batch 140 loss: 0.15962869089526066\n",
      "  batch 150 loss: 0.1623204809780873\n",
      "  batch 160 loss: 0.07782552260960074\n",
      "  batch 170 loss: 0.23898613846713487\n",
      "  batch 180 loss: 0.12368448679881112\n",
      "  batch 190 loss: 0.34089490143724105\n",
      "LOSS train 0.34089490143724105 valid 0.22977062829985676\n",
      "EPOCH 332:\n",
      "  batch 10 loss: 0.02293718603864363\n",
      "  batch 20 loss: 0.1231284150171632\n",
      "  batch 30 loss: 0.11123903818353256\n",
      "  batch 40 loss: 0.11307535510211437\n",
      "  batch 50 loss: 0.09981534495909727\n",
      "  batch 60 loss: 0.15903562387029524\n",
      "  batch 70 loss: 0.29937402814393865\n",
      "  batch 80 loss: 0.12874144916568184\n",
      "  batch 90 loss: 0.2465540651342053\n",
      "  batch 100 loss: 0.1627185568225741\n",
      "  batch 110 loss: 0.25260112606501933\n",
      "  batch 120 loss: 0.08237893494078889\n",
      "  batch 130 loss: 0.062421720044767426\n",
      "  batch 140 loss: 0.1507831878606339\n",
      "  batch 150 loss: 0.1500009601986676\n",
      "  batch 160 loss: 0.15686932108587825\n",
      "  batch 170 loss: 0.1528616966177651\n",
      "  batch 180 loss: 0.13021827359734744\n",
      "  batch 190 loss: 0.17234146194005007\n",
      "LOSS train 0.17234146194005007 valid 0.24384674825434488\n",
      "EPOCH 333:\n",
      "  batch 10 loss: 0.10297396550370194\n",
      "  batch 20 loss: 0.1534722405193861\n",
      "  batch 30 loss: 0.09851015951207956\n",
      "  batch 40 loss: 0.039717202187694055\n",
      "  batch 50 loss: 0.02990215258214448\n",
      "  batch 60 loss: 0.20474792978766346\n",
      "  batch 70 loss: 0.05362640641906182\n",
      "  batch 80 loss: 0.056118311941281716\n",
      "  batch 90 loss: 0.1753304360383936\n",
      "  batch 100 loss: 0.24062568771423684\n",
      "  batch 110 loss: 0.06816709398144667\n",
      "  batch 120 loss: 0.2511775330198816\n",
      "  batch 130 loss: 0.11705657695010814\n",
      "  batch 140 loss: 0.308021868120386\n",
      "  batch 150 loss: 0.09325363262637439\n",
      "  batch 160 loss: 0.05745237747710234\n",
      "  batch 170 loss: 0.07917649149603675\n",
      "  batch 180 loss: 0.05930031850693922\n",
      "  batch 190 loss: 0.3421050136897065\n",
      "LOSS train 0.3421050136897065 valid 0.4704647757149266\n",
      "EPOCH 334:\n",
      "  batch 10 loss: 0.1932006697913266\n",
      "  batch 20 loss: 0.762207273724016\n",
      "  batch 30 loss: 0.05219856896346755\n",
      "  batch 40 loss: 0.11311892866924608\n",
      "  batch 50 loss: 0.05832366789547905\n",
      "  batch 60 loss: 0.15618319853165302\n",
      "  batch 70 loss: 0.14186096929061023\n",
      "  batch 80 loss: 0.07618085842232176\n",
      "  batch 90 loss: 0.3219960256363265\n",
      "  batch 100 loss: 0.37712368157881426\n",
      "  batch 110 loss: 0.22377892820513806\n",
      "  batch 120 loss: 0.42294989311476455\n",
      "  batch 130 loss: 0.21539122215399403\n",
      "  batch 140 loss: 0.12096081473373488\n",
      "  batch 150 loss: 0.10722348764356866\n",
      "  batch 160 loss: 0.04101316543028588\n",
      "  batch 170 loss: 0.11801521178349503\n",
      "  batch 180 loss: 0.09387842483120039\n",
      "  batch 190 loss: 0.11063123712656306\n",
      "LOSS train 0.11063123712656306 valid 0.249767197611653\n",
      "EPOCH 335:\n",
      "  batch 10 loss: 0.06473188614686479\n",
      "  batch 20 loss: 0.04294906946379342\n",
      "  batch 30 loss: 0.08273973255345482\n",
      "  batch 40 loss: 0.17150459173208218\n",
      "  batch 50 loss: 0.1476433932238251\n",
      "  batch 60 loss: 0.08405584477345655\n",
      "  batch 70 loss: 0.2908371609146343\n",
      "  batch 80 loss: 0.46000346615546733\n",
      "  batch 90 loss: 0.1542512729720329\n",
      "  batch 100 loss: 0.12971809524501624\n",
      "  batch 110 loss: 0.1337340469530318\n",
      "  batch 120 loss: 0.2600767418782198\n",
      "  batch 130 loss: 0.3444899360729323\n",
      "  batch 140 loss: 0.22301015393577472\n",
      "  batch 150 loss: 0.3428330369732066\n",
      "  batch 160 loss: 0.183688707590818\n",
      "  batch 170 loss: 0.1164324708215645\n",
      "  batch 180 loss: 0.09819253546720574\n",
      "  batch 190 loss: 0.11906436272720385\n",
      "LOSS train 0.11906436272720385 valid 0.29525901477709865\n",
      "EPOCH 336:\n",
      "  batch 10 loss: 0.15485714504384304\n",
      "  batch 20 loss: 0.06611821506885462\n",
      "  batch 30 loss: 0.050171101424530205\n",
      "  batch 40 loss: 0.13650214875997335\n",
      "  batch 50 loss: 0.17147469197584542\n",
      "  batch 60 loss: 0.10771106903589497\n",
      "  batch 70 loss: 0.08327438529563551\n",
      "  batch 80 loss: 0.04797430189346415\n",
      "  batch 90 loss: 0.3512375046435864\n",
      "  batch 100 loss: 0.05096850083500613\n",
      "  batch 110 loss: 0.14643278388120962\n",
      "  batch 120 loss: 0.22669843288722405\n",
      "  batch 130 loss: 0.49196890833991347\n",
      "  batch 140 loss: 0.15454310258564874\n",
      "  batch 150 loss: 0.09326928715927352\n",
      "  batch 160 loss: 0.09731479017791571\n",
      "  batch 170 loss: 0.08319153762640781\n",
      "  batch 180 loss: 0.29709335301577083\n",
      "  batch 190 loss: 0.04837886599193553\n",
      "LOSS train 0.04837886599193553 valid 0.24109547768141334\n",
      "EPOCH 337:\n",
      "  batch 10 loss: 0.1823905794292841\n",
      "  batch 20 loss: 0.055779287826771905\n",
      "  batch 30 loss: 0.08715150279494992\n",
      "  batch 40 loss: 0.11504102995604626\n",
      "  batch 50 loss: 0.05402208971499931\n",
      "  batch 60 loss: 0.12387458877747122\n",
      "  batch 70 loss: 0.25365040666820277\n",
      "  batch 80 loss: 0.19199920888797806\n",
      "  batch 90 loss: 0.057366683794498385\n",
      "  batch 100 loss: 0.17331251983896437\n",
      "  batch 110 loss: 0.16773644521636016\n",
      "  batch 120 loss: 0.1110148965611188\n",
      "  batch 130 loss: 0.11030735555887076\n",
      "  batch 140 loss: 0.2897737804305507\n",
      "  batch 150 loss: 0.17331034793351136\n",
      "  batch 160 loss: 0.21270340603405202\n",
      "  batch 170 loss: 0.14618363827867142\n",
      "  batch 180 loss: 0.3537925621209979\n",
      "  batch 190 loss: 0.22883704774480976\n",
      "LOSS train 0.22883704774480976 valid 0.22313936334264256\n",
      "EPOCH 338:\n",
      "  batch 10 loss: 0.15136594883115323\n",
      "  batch 20 loss: 0.12361564865786931\n",
      "  batch 30 loss: 0.06208699703813636\n",
      "  batch 40 loss: 0.10789481709107349\n",
      "  batch 50 loss: 0.07121528312927694\n",
      "  batch 60 loss: 0.13387086897673725\n",
      "  batch 70 loss: 0.25704429202760365\n",
      "  batch 80 loss: 0.39073566661236325\n",
      "  batch 90 loss: 0.10897750650069611\n",
      "  batch 100 loss: 0.07620596529136492\n",
      "  batch 110 loss: 0.08130002025718568\n",
      "  batch 120 loss: 0.10916733821559318\n",
      "  batch 130 loss: 0.2897062265645218\n",
      "  batch 140 loss: 0.06411831776240433\n",
      "  batch 150 loss: 0.12680531466285175\n",
      "  batch 160 loss: 0.15609038741677067\n",
      "  batch 170 loss: 0.316948110998419\n",
      "  batch 180 loss: 0.0711589727896353\n",
      "  batch 190 loss: 0.12624416587168524\n",
      "LOSS train 0.12624416587168524 valid 0.2182337378089636\n",
      "EPOCH 339:\n",
      "  batch 10 loss: 0.29943416934590916\n",
      "  batch 20 loss: 0.22333995808003237\n",
      "  batch 30 loss: 0.2181337507688113\n",
      "  batch 40 loss: 0.3690845298882778\n",
      "  batch 50 loss: 0.1330359837795072\n",
      "  batch 60 loss: 0.22568101820343145\n",
      "  batch 70 loss: 0.2697850117991038\n",
      "  batch 80 loss: 0.2292796915202416\n",
      "  batch 90 loss: 0.1351600867288653\n",
      "  batch 100 loss: 0.07589595964018372\n",
      "  batch 110 loss: 0.027301920689887994\n",
      "  batch 120 loss: 0.23266833471316203\n",
      "  batch 130 loss: 0.08891118047631608\n",
      "  batch 140 loss: 0.353101860321658\n",
      "  batch 150 loss: 0.0739103424239147\n",
      "  batch 160 loss: 0.23241336455439524\n",
      "  batch 170 loss: 0.1977094019406877\n",
      "  batch 180 loss: 0.1717043501320404\n",
      "  batch 190 loss: 0.17399773565575744\n",
      "LOSS train 0.17399773565575744 valid 0.305999638870815\n",
      "EPOCH 340:\n",
      "  batch 10 loss: 0.49449859182896033\n",
      "  batch 20 loss: 0.09091278528630937\n",
      "  batch 30 loss: 0.0373771941067389\n",
      "  batch 40 loss: 0.2167440472781891\n",
      "  batch 50 loss: 0.12346016346273245\n",
      "  batch 60 loss: 0.10631915064823261\n",
      "  batch 70 loss: 0.17554797493882007\n",
      "  batch 80 loss: 0.07884723790521093\n",
      "  batch 90 loss: 0.2828239293097795\n",
      "  batch 100 loss: 0.312941437210975\n",
      "  batch 110 loss: 0.06241452212780132\n",
      "  batch 120 loss: 0.13940498200099682\n",
      "  batch 130 loss: 0.04688909708565916\n",
      "  batch 140 loss: 0.0769223419487389\n",
      "  batch 150 loss: 0.266329626798597\n",
      "  batch 160 loss: 0.1448489205311489\n",
      "  batch 170 loss: 0.22441907738575537\n",
      "  batch 180 loss: 0.14138894436059674\n",
      "  batch 190 loss: 0.05933753540962243\n",
      "LOSS train 0.05933753540962243 valid 0.21590849660926836\n",
      "EPOCH 341:\n",
      "  batch 10 loss: 0.125132042326959\n",
      "  batch 20 loss: 0.11375426055351454\n",
      "  batch 30 loss: 0.18058388268314046\n",
      "  batch 40 loss: 0.09119024648789491\n",
      "  batch 50 loss: 0.13799209603294002\n",
      "  batch 60 loss: 0.15978921820124015\n",
      "  batch 70 loss: 0.029620431537478\n",
      "  batch 80 loss: 0.06936478853676817\n",
      "  batch 90 loss: 0.2313180952401126\n",
      "  batch 100 loss: 0.09200555739862466\n",
      "  batch 110 loss: 0.05152704422525858\n",
      "  batch 120 loss: 0.4141334815212758\n",
      "  batch 130 loss: 0.08688045255430552\n",
      "  batch 140 loss: 0.040623535789109155\n",
      "  batch 150 loss: 0.07003435140450165\n",
      "  batch 160 loss: 0.5070250206997343\n",
      "  batch 170 loss: 0.24403052973357262\n",
      "  batch 180 loss: 0.24715133798672467\n",
      "  batch 190 loss: 0.22411272827830545\n",
      "LOSS train 0.22411272827830545 valid 0.1832459874295427\n",
      "EPOCH 342:\n",
      "  batch 10 loss: 0.11508124070878693\n",
      "  batch 20 loss: 0.0911371226524352\n",
      "  batch 30 loss: 0.15281332326567282\n",
      "  batch 40 loss: 0.23366927756278527\n",
      "  batch 50 loss: 0.06378841441655822\n",
      "  batch 60 loss: 0.5708095698706075\n",
      "  batch 70 loss: 0.1770266674044251\n",
      "  batch 80 loss: 0.18148316881633947\n",
      "  batch 90 loss: 0.3789282124984311\n",
      "  batch 100 loss: 0.13848024008239007\n",
      "  batch 110 loss: 0.1828624294531437\n",
      "  batch 120 loss: 0.08423008831850894\n",
      "  batch 130 loss: 0.05162027183987448\n",
      "  batch 140 loss: 0.13751643714977035\n",
      "  batch 150 loss: 0.048522905871868714\n",
      "  batch 160 loss: 0.20678496530281337\n",
      "  batch 170 loss: 0.186167989225396\n",
      "  batch 180 loss: 0.040718016373057254\n",
      "  batch 190 loss: 0.05443492228660034\n",
      "LOSS train 0.05443492228660034 valid 0.25711140370135466\n",
      "EPOCH 343:\n",
      "  batch 10 loss: 0.12453222778567578\n",
      "  batch 20 loss: 0.20071492277120342\n",
      "  batch 30 loss: 0.1633666408504723\n",
      "  batch 40 loss: 0.0605247170329676\n",
      "  batch 50 loss: 0.047604442547753936\n",
      "  batch 60 loss: 0.15439770148796014\n",
      "  batch 70 loss: 0.14383731065754546\n",
      "  batch 80 loss: 0.2121490278209194\n",
      "  batch 90 loss: 0.03478004740345568\n",
      "  batch 100 loss: 0.1422190927014526\n",
      "  batch 110 loss: 0.0592070171200703\n",
      "  batch 120 loss: 0.26972775318672576\n",
      "  batch 130 loss: 0.21049843105743093\n",
      "  batch 140 loss: 0.09120724208187311\n",
      "  batch 150 loss: 0.058868901313690006\n",
      "  batch 160 loss: 0.08276936736119751\n",
      "  batch 170 loss: 0.40831069521773317\n",
      "  batch 180 loss: 0.12099034851803481\n",
      "  batch 190 loss: 0.16852171726641246\n",
      "LOSS train 0.16852171726641246 valid 0.1829272997325834\n",
      "EPOCH 344:\n",
      "  batch 10 loss: 0.04407880230573937\n",
      "  batch 20 loss: 0.11420208105264465\n",
      "  batch 30 loss: 0.22870639065195064\n",
      "  batch 40 loss: 0.3024493916578194\n",
      "  batch 50 loss: 0.2791881127272859\n",
      "  batch 60 loss: 0.05987841789792583\n",
      "  batch 70 loss: 0.16406018451580168\n",
      "  batch 80 loss: 0.09779879377092585\n",
      "  batch 90 loss: 0.23085681297116026\n",
      "  batch 100 loss: 0.15280423139302002\n",
      "  batch 110 loss: 0.5979489453721726\n",
      "  batch 120 loss: 0.240405616445787\n",
      "  batch 130 loss: 0.035385403979671534\n",
      "  batch 140 loss: 0.08847532360541663\n",
      "  batch 150 loss: 0.23633397071971557\n",
      "  batch 160 loss: 0.18205331665030827\n",
      "  batch 170 loss: 0.09551524316848373\n",
      "  batch 180 loss: 0.15260196285489655\n",
      "  batch 190 loss: 0.19510048672400443\n",
      "LOSS train 0.19510048672400443 valid 0.28844993883797965\n",
      "EPOCH 345:\n",
      "  batch 10 loss: 0.14811710606882117\n",
      "  batch 20 loss: 0.28961595548789776\n",
      "  batch 30 loss: 0.07604874434800876\n",
      "  batch 40 loss: 0.10270389012111991\n",
      "  batch 50 loss: 0.2611458650284476\n",
      "  batch 60 loss: 0.11997898709541914\n",
      "  batch 70 loss: 0.019243538675391393\n",
      "  batch 80 loss: 0.25521849618871784\n",
      "  batch 90 loss: 0.2841276120304883\n",
      "  batch 100 loss: 0.11653317115251412\n",
      "  batch 110 loss: 0.07998681569843029\n",
      "  batch 120 loss: 0.22823427394250756\n",
      "  batch 130 loss: 0.0709659075791933\n",
      "  batch 140 loss: 0.10046344859038073\n",
      "  batch 150 loss: 0.161884116327019\n",
      "  batch 160 loss: 0.10798685636727896\n",
      "  batch 170 loss: 0.0674220954892462\n",
      "  batch 180 loss: 0.04725327283667866\n",
      "  batch 190 loss: 0.04961127591159311\n",
      "LOSS train 0.04961127591159311 valid 0.3077398297398945\n",
      "EPOCH 346:\n",
      "  batch 10 loss: 0.09983229918407233\n",
      "  batch 20 loss: 0.16074841406161794\n",
      "  batch 30 loss: 0.1382261819240739\n",
      "  batch 40 loss: 0.02294175271426866\n",
      "  batch 50 loss: 0.5832173695038136\n",
      "  batch 60 loss: 0.10107227612934366\n",
      "  batch 70 loss: 0.43013916125514695\n",
      "  batch 80 loss: 0.16899905186510294\n",
      "  batch 90 loss: 0.11088738025864586\n",
      "  batch 100 loss: 0.21752373103881836\n",
      "  batch 110 loss: 0.38122791709174636\n",
      "  batch 120 loss: 0.12876701825880446\n",
      "  batch 130 loss: 0.12446080556037487\n",
      "  batch 140 loss: 0.16245948582145503\n",
      "  batch 150 loss: 0.23161499790012385\n",
      "  batch 160 loss: 0.08036706875391246\n",
      "  batch 170 loss: 0.133362649645278\n",
      "  batch 180 loss: 0.0658042767844563\n",
      "  batch 190 loss: 0.21373757801593457\n",
      "LOSS train 0.21373757801593457 valid 0.267097842623419\n",
      "EPOCH 347:\n",
      "  batch 10 loss: 0.2216075703043316\n",
      "  batch 20 loss: 0.08446932751348868\n",
      "  batch 30 loss: 0.09300582894361469\n",
      "  batch 40 loss: 0.19339393633526925\n",
      "  batch 50 loss: 0.0928004358970611\n",
      "  batch 60 loss: 0.16781072331896213\n",
      "  batch 70 loss: 0.07941695255140076\n",
      "  batch 80 loss: 0.06544264169995131\n",
      "  batch 90 loss: 0.03522940777747863\n",
      "  batch 100 loss: 0.25588725103359594\n",
      "  batch 110 loss: 0.24767595188941413\n",
      "  batch 120 loss: 0.27597945364541376\n",
      "  batch 130 loss: 0.3145688273345513\n",
      "  batch 140 loss: 0.4513282217356391\n",
      "  batch 150 loss: 0.24361665141505\n",
      "  batch 160 loss: 0.19228840130874686\n",
      "  batch 170 loss: 0.09131764020712581\n",
      "  batch 180 loss: 0.1416260290374339\n",
      "  batch 190 loss: 0.15926711181782593\n",
      "LOSS train 0.15926711181782593 valid 0.20321016812459464\n",
      "EPOCH 348:\n",
      "  batch 10 loss: 0.24322234655373903\n",
      "  batch 20 loss: 0.04832049419119357\n",
      "  batch 30 loss: 0.06525026835879544\n",
      "  batch 40 loss: 0.16558217773090292\n",
      "  batch 50 loss: 0.09373465369617406\n",
      "  batch 60 loss: 0.08230606554425321\n",
      "  batch 70 loss: 0.05512087760325812\n",
      "  batch 80 loss: 0.07280798439896899\n",
      "  batch 90 loss: 0.10837623202569376\n",
      "  batch 100 loss: 0.1566571950769969\n",
      "  batch 110 loss: 0.3291533256789535\n",
      "  batch 120 loss: 0.43278008685410896\n",
      "  batch 130 loss: 0.24225126271992395\n",
      "  batch 140 loss: 0.06903473874554038\n",
      "  batch 150 loss: 0.13084897034368623\n",
      "  batch 160 loss: 0.1293725565781642\n",
      "  batch 170 loss: 0.055190987402829704\n",
      "  batch 180 loss: 0.155242548059789\n",
      "  batch 190 loss: 0.06409519205790275\n",
      "LOSS train 0.06409519205790275 valid 0.27537973573672414\n",
      "EPOCH 349:\n",
      "  batch 10 loss: 0.1134347505223559\n",
      "  batch 20 loss: 0.10495051822927053\n",
      "  batch 30 loss: 0.17316064840015316\n",
      "  batch 40 loss: 0.05872284629149362\n",
      "  batch 50 loss: 0.14818531469863955\n",
      "  batch 60 loss: 0.09405564962908101\n",
      "  batch 70 loss: 0.03747239557269495\n",
      "  batch 80 loss: 0.08016217610365857\n",
      "  batch 90 loss: 0.17249415469880205\n",
      "  batch 100 loss: 0.21462087333120508\n",
      "  batch 110 loss: 0.16522829609020845\n",
      "  batch 120 loss: 0.05171344314730959\n",
      "  batch 130 loss: 0.22370758741149643\n",
      "  batch 140 loss: 0.06698744820641878\n",
      "  batch 150 loss: 0.250878543842893\n",
      "  batch 160 loss: 0.16771394489023805\n",
      "  batch 170 loss: 0.056859679255649095\n",
      "  batch 180 loss: 0.11008794694716925\n",
      "  batch 190 loss: 0.09097671116821289\n",
      "LOSS train 0.09097671116821289 valid 0.21605550039552887\n",
      "EPOCH 350:\n",
      "  batch 10 loss: 0.27827588042964635\n",
      "  batch 20 loss: 0.24732437922866665\n",
      "  batch 30 loss: 0.10322500822221628\n",
      "  batch 40 loss: 0.41070508066804906\n",
      "  batch 50 loss: 0.22360276329009138\n",
      "  batch 60 loss: 0.06099778652094301\n",
      "  batch 70 loss: 0.11502869882151572\n",
      "  batch 80 loss: 0.044076697535956555\n",
      "  batch 90 loss: 0.09681766266226077\n",
      "  batch 100 loss: 0.0745845024940536\n",
      "  batch 110 loss: 0.07811791775311576\n",
      "  batch 120 loss: 0.07597544240197748\n",
      "  batch 130 loss: 0.0829339274193103\n",
      "  batch 140 loss: 0.13138913942602812\n",
      "  batch 150 loss: 0.0570788412038894\n",
      "  batch 160 loss: 0.0430815955770413\n",
      "  batch 170 loss: 0.07898367895204501\n",
      "  batch 180 loss: 0.07863267582233675\n",
      "  batch 190 loss: 0.048362480869400315\n",
      "LOSS train 0.048362480869400315 valid 0.2835066973672768\n",
      "EPOCH 351:\n",
      "  batch 10 loss: 0.02172436099999686\n",
      "  batch 20 loss: 0.06921240631754699\n",
      "  batch 30 loss: 0.3328205935240476\n",
      "  batch 40 loss: 0.1525769670084401\n",
      "  batch 50 loss: 0.12928083908345797\n",
      "  batch 60 loss: 0.03177498954282783\n",
      "  batch 70 loss: 0.06238696900481955\n",
      "  batch 80 loss: 0.07565742137021517\n",
      "  batch 90 loss: 0.28693613028954135\n",
      "  batch 100 loss: 0.10321670082084893\n",
      "  batch 110 loss: 0.20438203184330633\n",
      "  batch 120 loss: 0.09516688856238034\n",
      "  batch 130 loss: 0.3594057447322484\n",
      "  batch 140 loss: 0.26283259554179494\n",
      "  batch 150 loss: 0.11621496014049626\n",
      "  batch 160 loss: 0.055193132922067886\n",
      "  batch 170 loss: 0.10168525245440492\n",
      "  batch 180 loss: 0.10193111619810225\n",
      "  batch 190 loss: 0.1512614006458989\n",
      "LOSS train 0.1512614006458989 valid 0.24300194199736908\n",
      "EPOCH 352:\n",
      "  batch 10 loss: 0.10648313459969358\n",
      "  batch 20 loss: 0.054041707176656927\n",
      "  batch 30 loss: 0.0736163475907233\n",
      "  batch 40 loss: 0.06524865473035106\n",
      "  batch 50 loss: 0.06499796729003719\n",
      "  batch 60 loss: 0.09887292492057895\n",
      "  batch 70 loss: 0.1122888971314751\n",
      "  batch 80 loss: 0.13554548282681936\n",
      "  batch 90 loss: 0.2027952990738413\n",
      "  batch 100 loss: 0.21569108021067224\n",
      "  batch 110 loss: 0.1983411493485619\n",
      "  batch 120 loss: 0.10170768795796903\n",
      "  batch 130 loss: 0.049201384475600206\n",
      "  batch 140 loss: 0.2477762997172249\n",
      "  batch 150 loss: 0.26755357089332393\n",
      "  batch 160 loss: 0.13781476392073272\n",
      "  batch 170 loss: 0.1360053315942082\n",
      "  batch 180 loss: 0.13910643033177622\n",
      "  batch 190 loss: 0.12815322802416632\n",
      "LOSS train 0.12815322802416632 valid 0.2510062745566127\n",
      "EPOCH 353:\n",
      "  batch 10 loss: 0.11588217105354488\n",
      "  batch 20 loss: 0.034969971386817636\n",
      "  batch 30 loss: 0.13695338438919863\n",
      "  batch 40 loss: 0.06345031635792112\n",
      "  batch 50 loss: 0.06014889475336531\n",
      "  batch 60 loss: 0.05903424444913981\n",
      "  batch 70 loss: 0.1674470198510562\n",
      "  batch 80 loss: 0.24928884565324552\n",
      "  batch 90 loss: 0.08648269167038052\n",
      "  batch 100 loss: 0.03584542321282243\n",
      "  batch 110 loss: 0.2599282194378702\n",
      "  batch 120 loss: 0.38821764839335626\n",
      "  batch 130 loss: 0.10804306162967806\n",
      "  batch 140 loss: 0.5285481492669988\n",
      "  batch 150 loss: 0.1332552160344221\n",
      "  batch 160 loss: 0.13804788504639873\n",
      "  batch 170 loss: 0.12464266375154694\n",
      "  batch 180 loss: 0.07235487676953198\n",
      "  batch 190 loss: 0.11149420676406407\n",
      "LOSS train 0.11149420676406407 valid 0.1829730488558338\n",
      "EPOCH 354:\n",
      "  batch 10 loss: 0.08286998671464971\n",
      "  batch 20 loss: 0.13123573372067768\n",
      "  batch 30 loss: 0.20091956328724336\n",
      "  batch 40 loss: 0.25640162098294467\n",
      "  batch 50 loss: 0.1421279504776976\n",
      "  batch 60 loss: 0.049804293392503494\n",
      "  batch 70 loss: 0.1417793063003046\n",
      "  batch 80 loss: 0.02897666660592222\n",
      "  batch 90 loss: 0.1017256331480894\n",
      "  batch 100 loss: 0.0853821055465744\n",
      "  batch 110 loss: 0.11370398929907424\n",
      "  batch 120 loss: 0.11004989362045307\n",
      "  batch 130 loss: 0.08963309178170675\n",
      "  batch 140 loss: 0.10967565465289227\n",
      "  batch 150 loss: 0.019370292690655332\n",
      "  batch 160 loss: 0.43462946435206506\n",
      "  batch 170 loss: 0.046159770343183484\n",
      "  batch 180 loss: 0.037171764779242264\n",
      "  batch 190 loss: 0.13439660473522963\n",
      "LOSS train 0.13439660473522963 valid 0.22390207295561246\n",
      "EPOCH 355:\n",
      "  batch 10 loss: 0.10963302998534345\n",
      "  batch 20 loss: 0.2683851071124991\n",
      "  batch 30 loss: 0.09117513981836964\n",
      "  batch 40 loss: 0.059936095022385415\n",
      "  batch 50 loss: 0.14009815919533822\n",
      "  batch 60 loss: 0.14638761090636762\n",
      "  batch 70 loss: 0.02633157843592926\n",
      "  batch 80 loss: 0.1898423396929502\n",
      "  batch 90 loss: 0.07467353827071292\n",
      "  batch 100 loss: 0.03156162621716021\n",
      "  batch 110 loss: 0.09741654360827852\n",
      "  batch 120 loss: 0.0786887973008561\n",
      "  batch 130 loss: 0.17982732892055536\n",
      "  batch 140 loss: 0.1411563107079928\n",
      "  batch 150 loss: 0.09998421831896849\n",
      "  batch 160 loss: 0.7223078571640145\n",
      "  batch 170 loss: 0.39692634975881447\n",
      "  batch 180 loss: 0.09144164253411873\n",
      "  batch 190 loss: 0.1460512834569272\n",
      "LOSS train 0.1460512834569272 valid 0.7318723313321581\n",
      "EPOCH 356:\n",
      "  batch 10 loss: 0.2347011912605012\n",
      "  batch 20 loss: 0.11575945910926748\n",
      "  batch 30 loss: 0.28811581024456245\n",
      "  batch 40 loss: 0.3466665742445912\n",
      "  batch 50 loss: 0.02430253990296478\n",
      "  batch 60 loss: 0.17993995846900362\n",
      "  batch 70 loss: 0.03924171861019658\n",
      "  batch 80 loss: 0.05528311819944065\n",
      "  batch 90 loss: 0.09207149661569929\n",
      "  batch 100 loss: 0.047077201609545226\n",
      "  batch 110 loss: 0.12785068837001745\n",
      "  batch 120 loss: 0.053413205251854376\n",
      "  batch 130 loss: 0.0684611040833488\n",
      "  batch 140 loss: 0.0488212168780592\n",
      "  batch 150 loss: 0.061344377960540444\n",
      "  batch 160 loss: 0.0691348909846056\n",
      "  batch 170 loss: 0.10751597858652531\n",
      "  batch 180 loss: 0.049523244727402015\n",
      "  batch 190 loss: 0.421356569695854\n",
      "LOSS train 0.421356569695854 valid 0.26024246017660074\n",
      "EPOCH 357:\n",
      "  batch 10 loss: 0.056650959183116356\n",
      "  batch 20 loss: 0.24516364414261602\n",
      "  batch 30 loss: 0.09157867217500097\n",
      "  batch 40 loss: 0.04509051870603571\n",
      "  batch 50 loss: 0.08641708439636205\n",
      "  batch 60 loss: 0.1839820326131303\n",
      "  batch 70 loss: 0.14190104768881612\n",
      "  batch 80 loss: 0.09917388309523859\n",
      "  batch 90 loss: 0.04315086488472843\n",
      "  batch 100 loss: 0.07990065101848813\n",
      "  batch 110 loss: 0.1645424245996992\n",
      "  batch 120 loss: 0.23569945787403412\n",
      "  batch 130 loss: 0.10207838182668638\n",
      "  batch 140 loss: 0.0472272434201841\n",
      "  batch 150 loss: 0.11054843245083248\n",
      "  batch 160 loss: 0.17543390063817696\n",
      "  batch 170 loss: 0.6930080652909965\n",
      "  batch 180 loss: 0.046628263142679316\n",
      "  batch 190 loss: 0.0818128151289784\n",
      "LOSS train 0.0818128151289784 valid 0.2592312095797466\n",
      "EPOCH 358:\n",
      "  batch 10 loss: 0.22307830345557705\n",
      "  batch 20 loss: 0.041580323276320996\n",
      "  batch 30 loss: 0.21313689667877042\n",
      "  batch 40 loss: 0.225863005411793\n",
      "  batch 50 loss: 0.21064588073459162\n",
      "  batch 60 loss: 0.04178572694872855\n",
      "  batch 70 loss: 0.33703394879194093\n",
      "  batch 80 loss: 0.16169902122091884\n",
      "  batch 90 loss: 0.017731126081753246\n",
      "  batch 100 loss: 0.022418945357514986\n",
      "  batch 110 loss: 0.0930095717370932\n",
      "  batch 120 loss: 0.07066795803748391\n",
      "  batch 130 loss: 0.37570716565751355\n",
      "  batch 140 loss: 0.16260642830093275\n",
      "  batch 150 loss: 0.24392597442688385\n",
      "  batch 160 loss: 0.16531102079288756\n",
      "  batch 170 loss: 0.2259278207803618\n",
      "  batch 180 loss: 0.044162817796097896\n",
      "  batch 190 loss: 0.10964505577021555\n",
      "LOSS train 0.10964505577021555 valid 0.31593359247497177\n",
      "EPOCH 359:\n",
      "  batch 10 loss: 0.0663986301261957\n",
      "  batch 20 loss: 0.04290721927209233\n",
      "  batch 30 loss: 0.08149653804542822\n",
      "  batch 40 loss: 0.08325657991190383\n",
      "  batch 50 loss: 0.05592689087748113\n",
      "  batch 60 loss: 0.12077868210341762\n",
      "  batch 70 loss: 0.10085546877664911\n",
      "  batch 80 loss: 0.4362302475193161\n",
      "  batch 90 loss: 0.10604923787111034\n",
      "  batch 100 loss: 0.06309759694653394\n",
      "  batch 110 loss: 0.10450668363246222\n",
      "  batch 120 loss: 0.14255552273202737\n",
      "  batch 130 loss: 0.08409100711667179\n",
      "  batch 140 loss: 0.09626468757205657\n",
      "  batch 150 loss: 0.21478433383351786\n",
      "  batch 160 loss: 0.1549361692572063\n",
      "  batch 170 loss: 0.29464749380282457\n",
      "  batch 180 loss: 0.09106028151874171\n",
      "  batch 190 loss: 0.06826818900296985\n",
      "LOSS train 0.06826818900296985 valid 0.2767084115265007\n",
      "EPOCH 360:\n",
      "  batch 10 loss: 0.1767263488318349\n",
      "  batch 20 loss: 0.0501980166980502\n",
      "  batch 30 loss: 0.04865733727469888\n",
      "  batch 40 loss: 0.23229594553085917\n",
      "  batch 50 loss: 0.09607576926673574\n",
      "  batch 60 loss: 0.03974484215423217\n",
      "  batch 70 loss: 0.06255405660085671\n",
      "  batch 80 loss: 0.03648302063575102\n",
      "  batch 90 loss: 0.0754528366970817\n",
      "  batch 100 loss: 0.07672588457071469\n",
      "  batch 110 loss: 0.5524677899717971\n",
      "  batch 120 loss: 0.27014735603115697\n",
      "  batch 130 loss: 0.07298430686996653\n",
      "  batch 140 loss: 0.1359263578433456\n",
      "  batch 150 loss: 0.1814125726948987\n",
      "  batch 160 loss: 0.6629849752029017\n",
      "  batch 170 loss: 0.18877839561773727\n",
      "  batch 180 loss: 0.15675575005589054\n",
      "  batch 190 loss: 0.24427738829726878\n",
      "LOSS train 0.24427738829726878 valid 0.1992558522657442\n",
      "EPOCH 361:\n",
      "  batch 10 loss: 0.16503714796763233\n",
      "  batch 20 loss: 0.056360567120282215\n",
      "  batch 30 loss: 0.02778535160214233\n",
      "  batch 40 loss: 0.05192417527905491\n",
      "  batch 50 loss: 0.04501564485290146\n",
      "  batch 60 loss: 0.0795259410773724\n",
      "  batch 70 loss: 0.07728881205475772\n",
      "  batch 80 loss: 0.15548686539659684\n",
      "  batch 90 loss: 0.17367738187986106\n",
      "  batch 100 loss: 0.23526523063374044\n",
      "  batch 110 loss: 0.49916773664017455\n",
      "  batch 120 loss: 0.1615312646255916\n",
      "  batch 130 loss: 0.08383933982172494\n",
      "  batch 140 loss: 0.0750035019023926\n",
      "  batch 150 loss: 0.5013962464026236\n",
      "  batch 160 loss: 0.14775986696422477\n",
      "  batch 170 loss: 0.08529380835552729\n",
      "  batch 180 loss: 0.050095817424517006\n",
      "  batch 190 loss: 0.06952449738755036\n",
      "LOSS train 0.06952449738755036 valid 0.27709358255204\n",
      "EPOCH 362:\n",
      "  batch 10 loss: 0.1284093678924819\n",
      "  batch 20 loss: 0.14738985136154953\n",
      "  batch 30 loss: 0.13438446065074458\n",
      "  batch 40 loss: 0.04285979650958325\n",
      "  batch 50 loss: 0.24802621155404267\n",
      "  batch 60 loss: 0.3926474103471264\n",
      "  batch 70 loss: 0.18792133524002566\n",
      "  batch 80 loss: 0.16817742703915428\n",
      "  batch 90 loss: 0.1059730221401594\n",
      "  batch 100 loss: 0.16477783709074173\n",
      "  batch 110 loss: 0.194669192669744\n",
      "  batch 120 loss: 0.07554082740625745\n",
      "  batch 130 loss: 0.08729668467785814\n",
      "  batch 140 loss: 0.09813085259781928\n",
      "  batch 150 loss: 0.19690180247835087\n",
      "  batch 160 loss: 0.06340346188035255\n",
      "  batch 170 loss: 0.41613870018599075\n",
      "  batch 180 loss: 0.15176078679955934\n",
      "  batch 190 loss: 0.060937219525021645\n",
      "LOSS train 0.060937219525021645 valid 0.27702037292293913\n",
      "EPOCH 363:\n",
      "  batch 10 loss: 0.14549197484011528\n",
      "  batch 20 loss: 0.06554427543469502\n",
      "  batch 30 loss: 0.10592297651892295\n",
      "  batch 40 loss: 0.039967464246342385\n",
      "  batch 50 loss: 0.04089791037695249\n",
      "  batch 60 loss: 0.10360422932456004\n",
      "  batch 70 loss: 0.06844176625590989\n",
      "  batch 80 loss: 0.22180822705668107\n",
      "  batch 90 loss: 0.34163492068382767\n",
      "  batch 100 loss: 0.14880593938901257\n",
      "  batch 110 loss: 0.042085802060046265\n",
      "  batch 120 loss: 0.014633236950476202\n",
      "  batch 130 loss: 0.14595739540891373\n",
      "  batch 140 loss: 0.1838248332061994\n",
      "  batch 150 loss: 0.19504274095215807\n",
      "  batch 160 loss: 0.0855113291181624\n",
      "  batch 170 loss: 0.2548627900716383\n",
      "  batch 180 loss: 0.051063433902209\n",
      "  batch 190 loss: 0.06404921676121375\n",
      "LOSS train 0.06404921676121375 valid 0.21982232595719792\n",
      "EPOCH 364:\n",
      "  batch 10 loss: 0.08935924310524115\n",
      "  batch 20 loss: 0.13767462230807723\n",
      "  batch 30 loss: 0.10891084502177364\n",
      "  batch 40 loss: 0.12268030590262242\n",
      "  batch 50 loss: 0.24294841471437395\n",
      "  batch 60 loss: 0.08418243804539997\n",
      "  batch 70 loss: 0.037412170506877375\n",
      "  batch 80 loss: 0.27249891824817496\n",
      "  batch 90 loss: 0.092140263570991\n",
      "  batch 100 loss: 0.1466156380190114\n",
      "  batch 110 loss: 0.1613090641665053\n",
      "  batch 120 loss: 0.11430509421843453\n",
      "  batch 130 loss: 0.10398472779673398\n",
      "  batch 140 loss: 0.03351980399202148\n",
      "  batch 150 loss: 0.028617905889950633\n",
      "  batch 160 loss: 0.08730530971965891\n",
      "  batch 170 loss: 0.13543485423233506\n",
      "  batch 180 loss: 0.042114239559987256\n",
      "  batch 190 loss: 0.0185741872053768\n",
      "LOSS train 0.0185741872053768 valid 0.4026054024879602\n",
      "EPOCH 365:\n",
      "  batch 10 loss: 0.2576916818289874\n",
      "  batch 20 loss: 0.16347239752076348\n",
      "  batch 30 loss: 0.25773026899341855\n",
      "  batch 40 loss: 0.0664540457003568\n",
      "  batch 50 loss: 0.0705663851051213\n",
      "  batch 60 loss: 0.2403296296701683\n",
      "  batch 70 loss: 0.02633625565679267\n",
      "  batch 80 loss: 0.13454888109608873\n",
      "  batch 90 loss: 0.06405085940772323\n",
      "  batch 100 loss: 0.03306735401122296\n",
      "  batch 110 loss: 0.07980431430378303\n",
      "  batch 120 loss: 0.12462020623529498\n",
      "  batch 130 loss: 0.18625973948035152\n",
      "  batch 140 loss: 0.09098623903782936\n",
      "  batch 150 loss: 0.16089008517183742\n",
      "  batch 160 loss: 0.1373632833670399\n",
      "  batch 170 loss: 0.07211302094638086\n",
      "  batch 180 loss: 0.2945823126897153\n",
      "  batch 190 loss: 0.09652548467165616\n",
      "LOSS train 0.09652548467165616 valid 2.5530102816640303\n",
      "EPOCH 366:\n",
      "  batch 10 loss: 0.5661150670861389\n",
      "  batch 20 loss: 0.07360670555422075\n",
      "  batch 30 loss: 0.19867659881310828\n",
      "  batch 40 loss: 0.16379656723338484\n",
      "  batch 50 loss: 0.02518940573299915\n",
      "  batch 60 loss: 0.04059446551173096\n",
      "  batch 70 loss: 0.07751495746104524\n",
      "  batch 80 loss: 0.05424047985798097\n",
      "  batch 90 loss: 0.015197028364241305\n",
      "  batch 100 loss: 0.0752394970801106\n",
      "  batch 110 loss: 0.12524215655975013\n",
      "  batch 120 loss: 0.3494841981085415\n",
      "  batch 130 loss: 0.14318123044722597\n",
      "  batch 140 loss: 0.2185475095338461\n",
      "  batch 150 loss: 0.2966033088385302\n",
      "  batch 160 loss: 0.16743622945185735\n",
      "  batch 170 loss: 0.07609345217006194\n",
      "  batch 180 loss: 0.022651438336470164\n",
      "  batch 190 loss: 0.028044171343753987\n",
      "LOSS train 0.028044171343753987 valid 0.18744339357450512\n",
      "EPOCH 367:\n",
      "  batch 10 loss: 0.13843521116302782\n",
      "  batch 20 loss: 0.14768892927941124\n",
      "  batch 30 loss: 0.20336276430261932\n",
      "  batch 40 loss: 0.03585884922513287\n",
      "  batch 50 loss: 0.16841202794730634\n",
      "  batch 60 loss: 0.3384264353906474\n",
      "  batch 70 loss: 0.08060966300963628\n",
      "  batch 80 loss: 0.061916081870094786\n",
      "  batch 90 loss: 0.026040108026836605\n",
      "  batch 100 loss: 0.053545611894014654\n",
      "  batch 110 loss: 0.09133761675371374\n",
      "  batch 120 loss: 0.06336875281931498\n",
      "  batch 130 loss: 0.12054188419456295\n",
      "  batch 140 loss: 0.10258772236479671\n",
      "  batch 150 loss: 0.24605849068980207\n",
      "  batch 160 loss: 0.07273628653301785\n",
      "  batch 170 loss: 0.0711541321071536\n",
      "  batch 180 loss: 0.041537297962338474\n",
      "  batch 190 loss: 0.1471407770878841\n",
      "LOSS train 0.1471407770878841 valid 0.2414296796089741\n",
      "EPOCH 368:\n",
      "  batch 10 loss: 0.06460404397894308\n",
      "  batch 20 loss: 0.020336457180678735\n",
      "  batch 30 loss: 0.08192774589068677\n",
      "  batch 40 loss: 0.08120345118049954\n",
      "  batch 50 loss: 0.05893506825318582\n",
      "  batch 60 loss: 0.20446299812156213\n",
      "  batch 70 loss: 0.06970647718908367\n",
      "  batch 80 loss: 0.2996339886564556\n",
      "  batch 90 loss: 0.09060354300339099\n",
      "  batch 100 loss: 0.07344195488240075\n",
      "  batch 110 loss: 0.044285445849004644\n",
      "  batch 120 loss: 0.029938405657594556\n",
      "  batch 130 loss: 0.05465685850094815\n",
      "  batch 140 loss: 0.013428447045475878\n",
      "  batch 150 loss: 0.028643521316280385\n",
      "  batch 160 loss: 0.12957050367912187\n",
      "  batch 170 loss: 0.1885605211275106\n",
      "  batch 180 loss: 0.04389695366985506\n",
      "  batch 190 loss: 0.1765310538632491\n",
      "LOSS train 0.1765310538632491 valid 0.33665722468024284\n",
      "EPOCH 369:\n",
      "  batch 10 loss: 0.0833910198653939\n",
      "  batch 20 loss: 0.07084757185529042\n",
      "  batch 30 loss: 0.1262830345357088\n",
      "  batch 40 loss: 0.04910388729840633\n",
      "  batch 50 loss: 0.040749868195780435\n",
      "  batch 60 loss: 0.04337007543185791\n",
      "  batch 70 loss: 0.04630908356993189\n",
      "  batch 80 loss: 0.1165531816329917\n",
      "  batch 90 loss: 0.07115725113993676\n",
      "  batch 100 loss: 0.10399319135470933\n",
      "  batch 110 loss: 0.11892644739301658\n",
      "  batch 120 loss: 0.10893748985181446\n",
      "  batch 130 loss: 0.08285894337964236\n",
      "  batch 140 loss: 0.12109225590022561\n",
      "  batch 150 loss: 0.09573522254331693\n",
      "  batch 160 loss: 0.1256899480190441\n",
      "  batch 170 loss: 0.11257653245278335\n",
      "  batch 180 loss: 0.46089776181870545\n",
      "  batch 190 loss: 0.22794930417580872\n",
      "LOSS train 0.22794930417580872 valid 0.18661101527141\n",
      "EPOCH 370:\n",
      "  batch 10 loss: 0.07693450709572573\n",
      "  batch 20 loss: 0.10035280155551049\n",
      "  batch 30 loss: 0.0394664143404043\n",
      "  batch 40 loss: 0.2524816636709602\n",
      "  batch 50 loss: 0.11133522734307917\n",
      "  batch 60 loss: 0.09765856692665693\n",
      "  batch 70 loss: 0.029677066438352995\n",
      "  batch 80 loss: 0.029161921072318363\n",
      "  batch 90 loss: 0.06396938136920198\n",
      "  batch 100 loss: 0.12808990898665798\n",
      "  batch 110 loss: 0.03755912758388149\n",
      "  batch 120 loss: 0.14580028916097945\n",
      "  batch 130 loss: 0.07542773577525849\n",
      "  batch 140 loss: 0.04070151261294086\n",
      "  batch 150 loss: 0.23608232611504718\n",
      "  batch 160 loss: 0.09873563672141472\n",
      "  batch 170 loss: 0.3773874555254224\n",
      "  batch 180 loss: 0.1602254649306815\n",
      "  batch 190 loss: 0.1674800815920662\n",
      "LOSS train 0.1674800815920662 valid 0.22356387064543853\n",
      "EPOCH 371:\n",
      "  batch 10 loss: 0.1702812251024625\n",
      "  batch 20 loss: 0.11252872944282899\n",
      "  batch 30 loss: 0.04134243238831914\n",
      "  batch 40 loss: 0.061662645533442625\n",
      "  batch 50 loss: 0.1684783511439491\n",
      "  batch 60 loss: 0.17595296703240138\n",
      "  batch 70 loss: 0.18050651127032324\n",
      "  batch 80 loss: 0.07013996512605444\n",
      "  batch 90 loss: 0.23386152059993037\n",
      "  batch 100 loss: 0.08560482034708912\n",
      "  batch 110 loss: 0.155628557027444\n",
      "  batch 120 loss: 0.4382515140559008\n",
      "  batch 130 loss: 0.1117164103297\n",
      "  batch 140 loss: 0.08784540327069408\n",
      "  batch 150 loss: 0.08740110575963626\n",
      "  batch 160 loss: 0.09924548325780051\n",
      "  batch 170 loss: 0.5729700564851299\n",
      "  batch 180 loss: 0.057315932580968364\n",
      "  batch 190 loss: 0.10220019282351131\n",
      "LOSS train 0.10220019282351131 valid 0.22490487340391133\n",
      "EPOCH 372:\n",
      "  batch 10 loss: 0.09448280205232322\n",
      "  batch 20 loss: 0.08281612285745724\n",
      "  batch 30 loss: 0.04120719331887131\n",
      "  batch 40 loss: 0.023720380801842113\n",
      "  batch 50 loss: 0.13445097769717904\n",
      "  batch 60 loss: 0.12778290566579925\n",
      "  batch 70 loss: 0.16243925482889382\n",
      "  batch 80 loss: 0.05041915047531802\n",
      "  batch 90 loss: 0.15070089341279527\n",
      "  batch 100 loss: 0.04448100775871353\n",
      "  batch 110 loss: 0.1982701566736523\n",
      "  batch 120 loss: 0.061421638959200206\n",
      "  batch 130 loss: 0.16418507860339543\n",
      "  batch 140 loss: 0.06606266010891204\n",
      "  batch 150 loss: 0.13445004206841987\n",
      "  batch 160 loss: 0.06301378621135427\n",
      "  batch 170 loss: 0.0947043566055072\n",
      "  batch 180 loss: 0.12108826017743013\n",
      "  batch 190 loss: 0.061351665702107996\n",
      "LOSS train 0.061351665702107996 valid 0.19561796099524378\n",
      "EPOCH 373:\n",
      "  batch 10 loss: 0.26128072250039625\n",
      "  batch 20 loss: 0.10509877457407128\n",
      "  batch 30 loss: 0.06006141343877971\n",
      "  batch 40 loss: 0.05425300114584388\n",
      "  batch 50 loss: 0.08960095920149343\n",
      "  batch 60 loss: 0.07668677582132659\n",
      "  batch 70 loss: 0.11295030828896416\n",
      "  batch 80 loss: 0.26885168824501304\n",
      "  batch 90 loss: 0.1557648353931654\n",
      "  batch 100 loss: 0.10963248169286999\n",
      "  batch 110 loss: 0.16839900581153416\n",
      "  batch 120 loss: 0.15924467957752314\n",
      "  batch 130 loss: 0.10136757383631903\n",
      "  batch 140 loss: 0.08253095516338363\n",
      "  batch 150 loss: 0.07099759559789617\n",
      "  batch 160 loss: 0.10988974023703121\n",
      "  batch 170 loss: 0.35504570450043504\n",
      "  batch 180 loss: 0.1392391559246164\n",
      "  batch 190 loss: 0.12312531765392123\n",
      "LOSS train 0.12312531765392123 valid 0.21571566665263092\n",
      "EPOCH 374:\n",
      "  batch 10 loss: 0.07386295101132419\n",
      "  batch 20 loss: 0.39527717974696086\n",
      "  batch 30 loss: 0.07781112762495468\n",
      "  batch 40 loss: 0.23962333588683576\n",
      "  batch 50 loss: 0.10301287911706822\n",
      "  batch 60 loss: 0.21258055651935875\n",
      "  batch 70 loss: 0.113759915996161\n",
      "  batch 80 loss: 0.03362096829910115\n",
      "  batch 90 loss: 0.08481462428380837\n",
      "  batch 100 loss: 0.028669708830557285\n",
      "  batch 110 loss: 0.09180912275060109\n",
      "  batch 120 loss: 0.2158568753470263\n",
      "  batch 130 loss: 0.04727720500472969\n",
      "  batch 140 loss: 0.06962913221505004\n",
      "  batch 150 loss: 0.20470376800419388\n",
      "  batch 160 loss: 0.08162990501177773\n",
      "  batch 170 loss: 0.053246159060665835\n",
      "  batch 180 loss: 0.04145836676846102\n",
      "  batch 190 loss: 0.06066690860507151\n",
      "LOSS train 0.06066690860507151 valid 0.22459380647920532\n",
      "EPOCH 375:\n",
      "  batch 10 loss: 0.061181884752022596\n",
      "  batch 20 loss: 0.07618616073805243\n",
      "  batch 30 loss: 0.14478337817772627\n",
      "  batch 40 loss: 0.11120578718991965\n",
      "  batch 50 loss: 0.16279672234704776\n",
      "  batch 60 loss: 0.11061544667586531\n",
      "  batch 70 loss: 0.18375891375943637\n",
      "  batch 80 loss: 0.08341600438503746\n",
      "  batch 90 loss: 0.025241461515179253\n",
      "  batch 100 loss: 0.05593388829456671\n",
      "  batch 110 loss: 0.034976630967457824\n",
      "  batch 120 loss: 0.0383054797628688\n",
      "  batch 130 loss: 0.07562099787928674\n",
      "  batch 140 loss: 0.04592888768987961\n",
      "  batch 150 loss: 0.12438705053374406\n",
      "  batch 160 loss: 0.1887696128774678\n",
      "  batch 170 loss: 0.07011561675014946\n",
      "  batch 180 loss: 0.10924437448384197\n",
      "  batch 190 loss: 0.24481411565657255\n",
      "LOSS train 0.24481411565657255 valid 0.2648283383308551\n",
      "EPOCH 376:\n",
      "  batch 10 loss: 0.15992129117576043\n",
      "  batch 20 loss: 0.13228332612078475\n",
      "  batch 30 loss: 0.09464899440532122\n",
      "  batch 40 loss: 0.06316906469833157\n",
      "  batch 50 loss: 0.05780128238840234\n",
      "  batch 60 loss: 0.14956465835489324\n",
      "  batch 70 loss: 0.09186029557054098\n",
      "  batch 80 loss: 0.16186189264585665\n",
      "  batch 90 loss: 0.050680643848227194\n",
      "  batch 100 loss: 0.07478969774497272\n",
      "  batch 110 loss: 0.019910756904937443\n",
      "  batch 120 loss: 0.03136307467543702\n",
      "  batch 130 loss: 0.03318833748307952\n",
      "  batch 140 loss: 0.21271356873103286\n",
      "  batch 150 loss: 0.1331509089625797\n",
      "  batch 160 loss: 0.041042744068158755\n",
      "  batch 170 loss: 0.2062569381157573\n",
      "  batch 180 loss: 0.3389641976499206\n",
      "  batch 190 loss: 0.06134126769466093\n",
      "LOSS train 0.06134126769466093 valid 0.19446300386193682\n",
      "EPOCH 377:\n",
      "  batch 10 loss: 0.03232751347402427\n",
      "  batch 20 loss: 0.04450814316613787\n",
      "  batch 30 loss: 0.3102460099850305\n",
      "  batch 40 loss: 0.2773632904169915\n",
      "  batch 50 loss: 0.13021396544318123\n",
      "  batch 60 loss: 0.021297141458308032\n",
      "  batch 70 loss: 0.04071076259497204\n",
      "  batch 80 loss: 0.026728967202075182\n",
      "  batch 90 loss: 0.06637301287150876\n",
      "  batch 100 loss: 0.07379233087582122\n",
      "  batch 110 loss: 0.4073160113567155\n",
      "  batch 120 loss: 0.11044936167340894\n",
      "  batch 130 loss: 0.08021942150780888\n",
      "  batch 140 loss: 0.08347045448463178\n",
      "  batch 150 loss: 0.029949682915321317\n",
      "  batch 160 loss: 0.03757721323063379\n",
      "  batch 170 loss: 0.16447400818715324\n",
      "  batch 180 loss: 0.05897913542776223\n",
      "  batch 190 loss: 0.06462961325869401\n",
      "LOSS train 0.06462961325869401 valid 0.3840443086147266\n",
      "EPOCH 378:\n",
      "  batch 10 loss: 0.20733003390560042\n",
      "  batch 20 loss: 0.18261928898436963\n",
      "  batch 30 loss: 0.04025564086796294\n",
      "  batch 40 loss: 0.04456845567151504\n",
      "  batch 50 loss: 0.24271699646999423\n",
      "  batch 60 loss: 0.07028626921736532\n",
      "  batch 70 loss: 0.03785754569885853\n",
      "  batch 80 loss: 0.12350000730693864\n",
      "  batch 90 loss: 0.043581101607560416\n",
      "  batch 100 loss: 0.02624464849020569\n",
      "  batch 110 loss: 0.014069805871986318\n",
      "  batch 120 loss: 0.059843841909264485\n",
      "  batch 130 loss: 0.08050254741096978\n",
      "  batch 140 loss: 0.12578375642932543\n",
      "  batch 150 loss: 0.10522780585115185\n",
      "  batch 160 loss: 0.13102420672996687\n",
      "  batch 170 loss: 0.02783463530415702\n",
      "  batch 180 loss: 0.07035125231520852\n",
      "  batch 190 loss: 0.20488218517702989\n",
      "LOSS train 0.20488218517702989 valid 0.19990387755442732\n",
      "EPOCH 379:\n",
      "  batch 10 loss: 0.0691935286203261\n",
      "  batch 20 loss: 0.6531081296341881\n",
      "  batch 30 loss: 0.18090046018883754\n",
      "  batch 40 loss: 0.22274599851443783\n",
      "  batch 50 loss: 0.5260702930684147\n",
      "  batch 60 loss: 0.16337872346766744\n",
      "  batch 70 loss: 0.563823725617533\n",
      "  batch 80 loss: 0.1709313991128397\n",
      "  batch 90 loss: 0.028083818674735993\n",
      "  batch 100 loss: 0.38848473529042166\n",
      "  batch 110 loss: 0.11014087564590228\n",
      "  batch 120 loss: 0.10532192325122196\n",
      "  batch 130 loss: 0.10590681843768834\n",
      "  batch 140 loss: 0.5643699635740405\n",
      "  batch 150 loss: 0.4472761950156155\n",
      "  batch 160 loss: 0.20802644891409727\n",
      "  batch 170 loss: 0.05946856245718664\n",
      "  batch 180 loss: 0.05386354035791783\n",
      "  batch 190 loss: 0.07828983436611452\n",
      "LOSS train 0.07828983436611452 valid 0.16153129474851552\n",
      "EPOCH 380:\n",
      "  batch 10 loss: 0.23018981129813482\n",
      "  batch 20 loss: 0.20234216441313038\n",
      "  batch 30 loss: 0.07068946278436669\n",
      "  batch 40 loss: 0.2060510290246384\n",
      "  batch 50 loss: 0.026621013880526335\n",
      "  batch 60 loss: 0.054842327136680066\n",
      "  batch 70 loss: 0.24880827935558045\n",
      "  batch 80 loss: 0.18318982769287687\n",
      "  batch 90 loss: 0.026128049227600057\n",
      "  batch 100 loss: 0.050997305653572765\n",
      "  batch 110 loss: 0.018340297203394584\n",
      "  batch 120 loss: 0.047417015594737676\n",
      "  batch 130 loss: 0.04095073911963709\n",
      "  batch 140 loss: 0.04384176811072393\n",
      "  batch 150 loss: 0.16848375340032362\n",
      "  batch 160 loss: 0.08698760133165706\n",
      "  batch 170 loss: 0.026864556741293198\n",
      "  batch 180 loss: 0.17941207534277054\n",
      "  batch 190 loss: 0.021636219500930844\n",
      "LOSS train 0.021636219500930844 valid 0.22180732828803976\n",
      "EPOCH 381:\n",
      "  batch 10 loss: 0.032443056396095925\n",
      "  batch 20 loss: 0.05953900619823571\n",
      "  batch 30 loss: 0.06234037093323082\n",
      "  batch 40 loss: 0.11447315289965446\n",
      "  batch 50 loss: 0.1525085533902711\n",
      "  batch 60 loss: 0.07967531193067998\n",
      "  batch 70 loss: 0.13140255743519447\n",
      "  batch 80 loss: 0.06955945625613821\n",
      "  batch 90 loss: 0.07906666887955112\n",
      "  batch 100 loss: 0.07190268822068901\n",
      "  batch 110 loss: 0.035294305916806934\n",
      "  batch 120 loss: 0.04663943919590565\n",
      "  batch 130 loss: 0.05046143283220772\n",
      "  batch 140 loss: 0.32426109989781937\n",
      "  batch 150 loss: 0.045435407882541766\n",
      "  batch 160 loss: 0.04144265818777058\n",
      "  batch 170 loss: 0.0385804276795156\n",
      "  batch 180 loss: 0.026835105259760895\n",
      "  batch 190 loss: 0.5666413016694605\n",
      "LOSS train 0.5666413016694605 valid 0.24447794195331576\n",
      "EPOCH 382:\n",
      "  batch 10 loss: 0.11872659046397302\n",
      "  batch 20 loss: 0.13795644433967028\n",
      "  batch 30 loss: 0.05150699007172079\n",
      "  batch 40 loss: 0.16274812815290574\n",
      "  batch 50 loss: 0.044282825946356755\n",
      "  batch 60 loss: 0.13493600237662234\n",
      "  batch 70 loss: 0.017059356326535637\n",
      "  batch 80 loss: 0.1815857979026987\n",
      "  batch 90 loss: 0.07241511869501664\n",
      "  batch 100 loss: 0.26696244080812903\n",
      "  batch 110 loss: 0.020553002895940153\n",
      "  batch 120 loss: 0.044525716938568397\n",
      "  batch 130 loss: 0.09833878176959417\n",
      "  batch 140 loss: 0.04641765091000707\n",
      "  batch 150 loss: 0.15123463829504544\n",
      "  batch 160 loss: 0.048168325421374905\n",
      "  batch 170 loss: 0.1457967836595344\n",
      "  batch 180 loss: 0.030203072483618598\n",
      "  batch 190 loss: 0.08466732391184735\n",
      "LOSS train 0.08466732391184735 valid 0.23109435405355655\n",
      "EPOCH 383:\n",
      "  batch 10 loss: 0.07579381812358577\n",
      "  batch 20 loss: 0.056719475870897894\n",
      "  batch 30 loss: 0.03415530258848776\n",
      "  batch 40 loss: 0.046144752949112446\n",
      "  batch 50 loss: 0.1404436265617278\n",
      "  batch 60 loss: 0.051880971258924546\n",
      "  batch 70 loss: 0.025800492494636274\n",
      "  batch 80 loss: 0.0561939077772422\n",
      "  batch 90 loss: 0.08599016930253925\n",
      "  batch 100 loss: 0.01806304802203158\n",
      "  batch 110 loss: 0.03788730035065271\n",
      "  batch 120 loss: 0.10195498388551982\n",
      "  batch 130 loss: 0.08835585939896191\n",
      "  batch 140 loss: 0.07868105542580679\n",
      "  batch 150 loss: 0.2645697460030078\n",
      "  batch 160 loss: 0.26882616403663634\n",
      "  batch 170 loss: 0.11350126293509674\n",
      "  batch 180 loss: 0.09054959012314612\n",
      "  batch 190 loss: 0.18802589114284257\n",
      "LOSS train 0.18802589114284257 valid 0.17029595759698005\n",
      "EPOCH 384:\n",
      "  batch 10 loss: 0.10411884200470922\n",
      "  batch 20 loss: 0.05211100984452059\n",
      "  batch 30 loss: 0.0935099197130512\n",
      "  batch 40 loss: 0.06878267460015194\n",
      "  batch 50 loss: 0.03133906887992453\n",
      "  batch 60 loss: 0.1146234752230157\n",
      "  batch 70 loss: 0.10374683500381252\n",
      "  batch 80 loss: 0.1584339804168792\n",
      "  batch 90 loss: 0.08554403930235707\n",
      "  batch 100 loss: 0.03163104231744\n",
      "  batch 110 loss: 0.07010724938620569\n",
      "  batch 120 loss: 0.026296308198652696\n",
      "  batch 130 loss: 0.05357616624460206\n",
      "  batch 140 loss: 0.0682770774430935\n",
      "  batch 150 loss: 0.18887783089292043\n",
      "  batch 160 loss: 0.38374346559870104\n",
      "  batch 170 loss: 0.6205835730027729\n",
      "  batch 180 loss: 0.08881779299154005\n",
      "  batch 190 loss: 0.07669291120910202\n",
      "LOSS train 0.07669291120910202 valid 0.22827444986613699\n",
      "EPOCH 385:\n",
      "  batch 10 loss: 0.07547867581088212\n",
      "  batch 20 loss: 0.02243233813733241\n",
      "  batch 30 loss: 0.09608237034285594\n",
      "  batch 40 loss: 0.18437071401027083\n",
      "  batch 50 loss: 0.07797945151842214\n",
      "  batch 60 loss: 0.07658474277686764\n",
      "  batch 70 loss: 0.05537204119631269\n",
      "  batch 80 loss: 0.05315831113039167\n",
      "  batch 90 loss: 0.07576028075962996\n",
      "  batch 100 loss: 0.06646159666142921\n",
      "  batch 110 loss: 0.06232354396027802\n",
      "  batch 120 loss: 0.026424951389253694\n",
      "  batch 130 loss: 0.09502586854196124\n",
      "  batch 140 loss: 0.6315926726301768\n",
      "  batch 150 loss: 0.23496907883181847\n",
      "  batch 160 loss: 0.08311486858601284\n",
      "  batch 170 loss: 0.03647983568885138\n",
      "  batch 180 loss: 0.11340599575196393\n",
      "  batch 190 loss: 0.10319482401091165\n",
      "LOSS train 0.10319482401091165 valid 0.25577392215668393\n",
      "EPOCH 386:\n",
      "  batch 10 loss: 0.12244152268722246\n",
      "  batch 20 loss: 0.08715926476948539\n",
      "  batch 30 loss: 0.06007851133690565\n",
      "  batch 40 loss: 0.133225268590013\n",
      "  batch 50 loss: 0.08392342075339912\n",
      "  batch 60 loss: 0.0243076237495643\n",
      "  batch 70 loss: 0.08042882139766334\n",
      "  batch 80 loss: 0.0294669062273897\n",
      "  batch 90 loss: 0.03393475170759928\n",
      "  batch 100 loss: 0.030486626614765554\n",
      "  batch 110 loss: 0.16517967957815785\n",
      "  batch 120 loss: 0.10407247122691317\n",
      "  batch 130 loss: 0.04074163657114553\n",
      "  batch 140 loss: 0.0392623251461373\n",
      "  batch 150 loss: 0.14487801058796776\n",
      "  batch 160 loss: 0.28567733231466264\n",
      "  batch 170 loss: 0.07377006100677903\n",
      "  batch 180 loss: 0.28349223394216094\n",
      "  batch 190 loss: 0.07719053801672544\n",
      "LOSS train 0.07719053801672544 valid 0.25369870937402084\n",
      "EPOCH 387:\n",
      "  batch 10 loss: 0.11096578564356605\n",
      "  batch 20 loss: 0.15833395883338425\n",
      "  batch 30 loss: 0.03738976048628047\n",
      "  batch 40 loss: 0.07285667402338732\n",
      "  batch 50 loss: 0.18311415027055772\n",
      "  batch 60 loss: 0.08794950089068151\n",
      "  batch 70 loss: 0.02578802999714753\n",
      "  batch 80 loss: 0.03286277960987718\n",
      "  batch 90 loss: 0.15034549959073046\n",
      "  batch 100 loss: 0.1412363103685493\n",
      "  batch 110 loss: 0.10401746517438823\n",
      "  batch 120 loss: 0.3963270048026736\n",
      "  batch 130 loss: 0.10401231566419825\n",
      "  batch 140 loss: 0.07775701772216052\n",
      "  batch 150 loss: 0.1364111104150652\n",
      "  batch 160 loss: 0.05160489050808792\n",
      "  batch 170 loss: 0.07962329602105456\n",
      "  batch 180 loss: 0.12325275519033027\n",
      "  batch 190 loss: 0.026685770820859035\n",
      "LOSS train 0.026685770820859035 valid 0.2724199833783598\n",
      "EPOCH 388:\n",
      "  batch 10 loss: 0.10388266296413348\n",
      "  batch 20 loss: 0.024675343090439127\n",
      "  batch 30 loss: 0.07022406688419096\n",
      "  batch 40 loss: 0.04813431964366828\n",
      "  batch 50 loss: 0.06968216917757672\n",
      "  batch 60 loss: 0.0852984481562288\n",
      "  batch 70 loss: 0.1397260418219048\n",
      "  batch 80 loss: 0.028128967470956924\n",
      "  batch 90 loss: 0.1787025146407359\n",
      "  batch 100 loss: 0.02565475011624585\n",
      "  batch 110 loss: 0.0838487018210344\n",
      "  batch 120 loss: 0.23740688893994957\n",
      "  batch 130 loss: 0.0843441048064049\n",
      "  batch 140 loss: 0.04288437888053522\n",
      "  batch 150 loss: 0.06954952703053863\n",
      "  batch 160 loss: 0.04165417986841913\n",
      "  batch 170 loss: 0.07938264074660424\n",
      "  batch 180 loss: 0.25659150762242006\n",
      "  batch 190 loss: 0.1906635462552913\n",
      "LOSS train 0.1906635462552913 valid 0.1905240531079576\n",
      "EPOCH 389:\n",
      "  batch 10 loss: 0.06203148236225999\n",
      "  batch 20 loss: 0.12663602502436788\n",
      "  batch 30 loss: 0.1857004684650292\n",
      "  batch 40 loss: 0.06142567241276993\n",
      "  batch 50 loss: 0.1016740172957725\n",
      "  batch 60 loss: 0.10123055356798431\n",
      "  batch 70 loss: 0.11599139917789217\n",
      "  batch 80 loss: 0.5208723938352591\n",
      "  batch 90 loss: 0.215525499077944\n",
      "  batch 100 loss: 0.09235318041303327\n",
      "  batch 110 loss: 0.12329069791339861\n",
      "  batch 120 loss: 0.06321928060424398\n",
      "  batch 130 loss: 0.24405399336264963\n",
      "  batch 140 loss: 0.06214240523877379\n",
      "  batch 150 loss: 0.054765363611113574\n",
      "  batch 160 loss: 0.05790616329018121\n",
      "  batch 170 loss: 0.20400256386342336\n",
      "  batch 180 loss: 0.061163214984048864\n",
      "  batch 190 loss: 0.22305698282261802\n",
      "LOSS train 0.22305698282261802 valid 0.18046396687240746\n",
      "EPOCH 390:\n",
      "  batch 10 loss: 0.02933827460419707\n",
      "  batch 20 loss: 0.10105302437377475\n",
      "  batch 30 loss: 0.12761669917849758\n",
      "  batch 40 loss: 0.09782511395610527\n",
      "  batch 50 loss: 0.03865946000580607\n",
      "  batch 60 loss: 0.09451957781375313\n",
      "  batch 70 loss: 0.08873321373675935\n",
      "  batch 80 loss: 0.067702150239802\n",
      "  batch 90 loss: 0.03759176001540254\n",
      "  batch 100 loss: 0.08487729519601998\n",
      "  batch 110 loss: 0.05832994530317137\n",
      "  batch 120 loss: 0.03286546362487144\n",
      "  batch 130 loss: 0.01225968718663335\n",
      "  batch 140 loss: 0.2755723735426727\n",
      "  batch 150 loss: 0.14222722187651016\n",
      "  batch 160 loss: 0.09017180291179869\n",
      "  batch 170 loss: 0.0967756083668064\n",
      "  batch 180 loss: 0.18358128932677573\n",
      "  batch 190 loss: 0.16437932732868318\n",
      "LOSS train 0.16437932732868318 valid 0.5423038361749144\n",
      "EPOCH 391:\n",
      "  batch 10 loss: 0.15623898134545017\n",
      "  batch 20 loss: 0.12830055187787365\n",
      "  batch 30 loss: 0.2515378356108727\n",
      "  batch 40 loss: 0.05231787133450325\n",
      "  batch 50 loss: 0.1133535441681488\n",
      "  batch 60 loss: 0.036867201429367925\n",
      "  batch 70 loss: 0.1295170016450129\n",
      "  batch 80 loss: 0.03237875299560074\n",
      "  batch 90 loss: 0.017131566807165655\n",
      "  batch 100 loss: 0.1995661683846265\n",
      "  batch 110 loss: 0.20642616696795812\n",
      "  batch 120 loss: 0.049184802559116746\n",
      "  batch 130 loss: 0.03888498931787154\n",
      "  batch 140 loss: 0.0245309007834976\n",
      "  batch 150 loss: 0.041353517269544685\n",
      "  batch 160 loss: 0.019080333368174253\n",
      "  batch 170 loss: 0.10283683827947243\n",
      "  batch 180 loss: 0.024435799535831394\n",
      "  batch 190 loss: 0.07344653192085389\n",
      "LOSS train 0.07344653192085389 valid 0.25684447004421174\n",
      "EPOCH 392:\n",
      "  batch 10 loss: 0.15733121867069713\n",
      "  batch 20 loss: 0.06811169329717473\n",
      "  batch 30 loss: 0.07320689804645326\n",
      "  batch 40 loss: 0.1136864519471601\n",
      "  batch 50 loss: 0.3482048729885719\n",
      "  batch 60 loss: 0.1314410693435093\n",
      "  batch 70 loss: 0.04922514124600639\n",
      "  batch 80 loss: 0.10400669804330391\n",
      "  batch 90 loss: 0.06441678338933343\n",
      "  batch 100 loss: 0.038897227670304346\n",
      "  batch 110 loss: 0.05609048200617508\n",
      "  batch 120 loss: 0.052514422744562236\n",
      "  batch 130 loss: 0.11248989959374285\n",
      "  batch 140 loss: 0.06403595519432201\n",
      "  batch 150 loss: 0.10001318622109494\n",
      "  batch 160 loss: 0.06144456755940268\n",
      "  batch 170 loss: 0.15795233244202792\n",
      "  batch 180 loss: 0.0971306555800581\n",
      "  batch 190 loss: 0.04138828643435773\n",
      "LOSS train 0.04138828643435773 valid 0.24916143473530658\n",
      "EPOCH 393:\n",
      "  batch 10 loss: 0.049092556639448046\n",
      "  batch 20 loss: 0.06721187508998697\n",
      "  batch 30 loss: 0.1083873500669739\n",
      "  batch 40 loss: 0.12345852217090396\n",
      "  batch 50 loss: 0.13044683037405774\n",
      "  batch 60 loss: 0.06434942912292171\n",
      "  batch 70 loss: 0.12711583209675154\n",
      "  batch 80 loss: 0.05816267962291022\n",
      "  batch 90 loss: 0.03741829968546142\n",
      "  batch 100 loss: 0.02866606557737441\n",
      "  batch 110 loss: 0.0633125024697165\n",
      "  batch 120 loss: 0.014479459677795603\n",
      "  batch 130 loss: 0.11136436119918472\n",
      "  batch 140 loss: 0.14728298269026255\n",
      "  batch 150 loss: 0.06531388627252\n",
      "  batch 160 loss: 0.025502654927618097\n",
      "  batch 170 loss: 0.09135114248301761\n",
      "  batch 180 loss: 0.05089602165273845\n",
      "  batch 190 loss: 0.1000193740764189\n",
      "LOSS train 0.1000193740764189 valid 0.21607774084488413\n",
      "EPOCH 394:\n",
      "  batch 10 loss: 0.04335585959779564\n",
      "  batch 20 loss: 0.045261672461072065\n",
      "  batch 30 loss: 0.04107557990755595\n",
      "  batch 40 loss: 0.020495848807331463\n",
      "  batch 50 loss: 0.02195140391318091\n",
      "  batch 60 loss: 0.0775635018187387\n",
      "  batch 70 loss: 0.12960994946363086\n",
      "  batch 80 loss: 0.1261727441948551\n",
      "  batch 90 loss: 0.05933596847721674\n",
      "  batch 100 loss: 0.04297512750624719\n",
      "  batch 110 loss: 0.2858808555979181\n",
      "  batch 120 loss: 0.05548780305271066\n",
      "  batch 130 loss: 0.25325792629414534\n",
      "  batch 140 loss: 0.1949967673993342\n",
      "  batch 150 loss: 0.0865465162250075\n",
      "  batch 160 loss: 0.20785757576804825\n",
      "  batch 170 loss: 0.09985626729119304\n",
      "  batch 180 loss: 0.03906982159383006\n",
      "  batch 190 loss: 0.08345818209832032\n",
      "LOSS train 0.08345818209832032 valid 0.19113303734999276\n",
      "EPOCH 395:\n",
      "  batch 10 loss: 0.04302503417710568\n",
      "  batch 20 loss: 0.0810787148135205\n",
      "  batch 30 loss: 0.025083000559232006\n",
      "  batch 40 loss: 0.04100617943008729\n",
      "  batch 50 loss: 0.054806535145280576\n",
      "  batch 60 loss: 0.0901269531604612\n",
      "  batch 70 loss: 0.025186961401709597\n",
      "  batch 80 loss: 0.024946346675199037\n",
      "  batch 90 loss: 0.014600092965565636\n",
      "  batch 100 loss: 0.016765697295340942\n",
      "  batch 110 loss: 0.027503432621915458\n",
      "  batch 120 loss: 0.023054337237677202\n",
      "  batch 130 loss: 0.013549240996985646\n",
      "  batch 140 loss: 0.14584750140242592\n",
      "  batch 150 loss: 0.040164094903536805\n",
      "  batch 160 loss: 0.18940533105787835\n",
      "  batch 170 loss: 0.05413252790887313\n",
      "  batch 180 loss: 0.022221094006658858\n",
      "  batch 190 loss: 0.020113304207984583\n",
      "LOSS train 0.020113304207984583 valid 0.2121446295461096\n",
      "EPOCH 396:\n",
      "  batch 10 loss: 0.08946104498541559\n",
      "  batch 20 loss: 0.03574526093643708\n",
      "  batch 30 loss: 0.04083422857584083\n",
      "  batch 40 loss: 0.02152334951570083\n",
      "  batch 50 loss: 0.016580063183496918\n",
      "  batch 60 loss: 0.04019940720900195\n",
      "  batch 70 loss: 0.03424584564559154\n",
      "  batch 80 loss: 0.025684863743072127\n",
      "  batch 90 loss: 0.1603526744381126\n",
      "  batch 100 loss: 0.08925442314557586\n",
      "  batch 110 loss: 0.06343542207501969\n",
      "  batch 120 loss: 0.058274497793695444\n",
      "  batch 130 loss: 0.016105387174457066\n",
      "  batch 140 loss: 0.13467525219498314\n",
      "  batch 150 loss: 0.26905620017462295\n",
      "  batch 160 loss: 0.08491318937922117\n",
      "  batch 170 loss: 0.05515686149503836\n",
      "  batch 180 loss: 0.10437506893269984\n",
      "  batch 190 loss: 0.08289484225624619\n",
      "LOSS train 0.08289484225624619 valid 0.2146802608949073\n",
      "EPOCH 397:\n",
      "  batch 10 loss: 0.019591691956338763\n",
      "  batch 20 loss: 0.029074409551867575\n",
      "  batch 30 loss: 0.08199308478951935\n",
      "  batch 40 loss: 0.05240455674716014\n",
      "  batch 50 loss: 0.035988890343968905\n",
      "  batch 60 loss: 0.08253859482207275\n",
      "  batch 70 loss: 0.15626599285187695\n",
      "  batch 80 loss: 0.047058258997367375\n",
      "  batch 90 loss: 0.047674060272765925\n",
      "  batch 100 loss: 0.059755218315103775\n",
      "  batch 110 loss: 0.053128425866430004\n",
      "  batch 120 loss: 0.05598556168083633\n",
      "  batch 130 loss: 0.06273006872693258\n",
      "  batch 140 loss: 0.021932446109752847\n",
      "  batch 150 loss: 0.23587196431808408\n",
      "  batch 160 loss: 0.17920551006175173\n",
      "  batch 170 loss: 0.3311865208045333\n",
      "  batch 180 loss: 0.15251904747337902\n",
      "  batch 190 loss: 0.21457369913923685\n",
      "LOSS train 0.21457369913923685 valid 3.5631752723876375\n",
      "EPOCH 398:\n",
      "  batch 10 loss: 0.5311874958656062\n",
      "  batch 20 loss: 0.17298742929406216\n",
      "  batch 30 loss: 0.04626930245426593\n",
      "  batch 40 loss: 0.08399024502823522\n",
      "  batch 50 loss: 0.1552303746285361\n",
      "  batch 60 loss: 0.09585146981426078\n",
      "  batch 70 loss: 0.318458083462707\n",
      "  batch 80 loss: 0.2555861065085992\n",
      "  batch 90 loss: 0.01609327006883632\n",
      "  batch 100 loss: 0.031438140397355596\n",
      "  batch 110 loss: 0.037974097067990445\n",
      "  batch 120 loss: 0.11978493763217557\n",
      "  batch 130 loss: 0.07571288474791657\n",
      "  batch 140 loss: 0.07232059360271706\n",
      "  batch 150 loss: 0.0681182094330893\n",
      "  batch 160 loss: 0.10416172387617735\n",
      "  batch 170 loss: 0.07961079049709952\n",
      "  batch 180 loss: 0.5216813959495881\n",
      "  batch 190 loss: 0.16141304226830472\n",
      "LOSS train 0.16141304226830472 valid 0.3616479362197086\n",
      "EPOCH 399:\n",
      "  batch 10 loss: 0.3472977283925502\n",
      "  batch 20 loss: 0.10239441757694294\n",
      "  batch 30 loss: 0.07348120943415778\n",
      "  batch 40 loss: 0.16699616296755265\n",
      "  batch 50 loss: 0.03846684953498425\n",
      "  batch 60 loss: 0.03711441138061673\n",
      "  batch 70 loss: 0.03255695197667592\n",
      "  batch 80 loss: 0.028996919817546995\n",
      "  batch 90 loss: 0.01589758053414698\n",
      "  batch 100 loss: 0.06603410779407567\n",
      "  batch 110 loss: 0.021924311805560138\n",
      "  batch 120 loss: 0.08056245664793096\n",
      "  batch 130 loss: 0.09612859900400963\n",
      "  batch 140 loss: 0.05154372598435657\n",
      "  batch 150 loss: 0.06949082013354654\n",
      "  batch 160 loss: 0.250645488593727\n",
      "  batch 170 loss: 0.03289197350979407\n",
      "  batch 180 loss: 0.31702836548392954\n",
      "  batch 190 loss: 0.06877651053882801\n",
      "LOSS train 0.06877651053882801 valid 0.2758182497333949\n",
      "EPOCH 400:\n",
      "  batch 10 loss: 0.08687140435376861\n",
      "  batch 20 loss: 0.29975238787287706\n",
      "  batch 30 loss: 0.024594189760136942\n",
      "  batch 40 loss: 0.05815990321657409\n",
      "  batch 50 loss: 0.07825353993894169\n",
      "  batch 60 loss: 0.14465556619143172\n",
      "  batch 70 loss: 0.27543926206803915\n",
      "  batch 80 loss: 0.14614654160256962\n",
      "  batch 90 loss: 0.22881274790311182\n",
      "  batch 100 loss: 0.05167080431610884\n",
      "  batch 110 loss: 0.0639380999808509\n",
      "  batch 120 loss: 0.3338570576303027\n",
      "  batch 130 loss: 0.03884781543438294\n",
      "  batch 140 loss: 0.0793625835232433\n",
      "  batch 150 loss: 0.3491892624288383\n",
      "  batch 160 loss: 0.06201052201868151\n",
      "  batch 170 loss: 0.03994943024908935\n",
      "  batch 180 loss: 0.04171710184818948\n",
      "  batch 190 loss: 0.14157906433708406\n",
      "LOSS train 0.14157906433708406 valid 0.26982331995881953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x30f0d2d60>]"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "EPOCHS = 400\n",
    "best_vloss = 1_000_000.\n",
    "val_history = []\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch(curr_model=model)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_history.append(avg_vloss.detach().numpy())\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        torch.save(model, \"../models/model9.pth\")\n",
    "    epoch_number += 1\n",
    "\n",
    "plt.plot(range(EPOCHS), val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1615, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/model9.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total = len(validation_set)\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        inputs, labels = vdata\n",
    "        outputs = torch.argmax(model(inputs), dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct_count += (outputs==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741935483870968"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741935483870968"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.argmax(model(val_data).detach().numpy(), axis=1), np.argmax(val_labels.detach().numpy(), axis=1), average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5580645161290323"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_data.view(-1,21*2), train_labels)\n",
    "y_pred = knn.predict(val_data.view(-1,21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([699, 21, 2])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193548387096775"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RidgeClassifierCV()\n",
    "clf.fit(train_data.view(-1, 21*2), train_labels)\n",
    "y_pred = clf.predict(val_data.view(-1, 21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_86164/1044316036.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4573, 0.0484, 0.0896,  ..., 0.0850, 0.0911, 0.0797],\n",
       "        [0.4173, 0.0532, 0.1257,  ..., 0.0936, 0.0882, 0.0599],\n",
       "        [0.4564, 0.0462, 0.0918,  ..., 0.0855, 0.0899, 0.0812],\n",
       "        ...,\n",
       "        [0.0744, 0.0594, 0.1351,  ..., 0.1360, 0.1033, 0.1548],\n",
       "        [0.0907, 0.0860, 0.1948,  ..., 0.1134, 0.1084, 0.0952],\n",
       "        [0.0876, 0.0802, 0.1651,  ..., 0.1132, 0.1127, 0.1029]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193548387096775"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_data.view(-1, 21*2), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reetinav/anaconda3/envs/PIC16B/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(multi_class=\"ovr\")\n",
    "clf.fit(train_data.view(-1, 21*2), torch.argmax(train_labels, dim=1))\n",
    "clf.score(val_data.view(-1, 21*2), torch.argmax(val_labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5952, 0.0090, 0.0528,  ..., 0.0620, 0.0913, 0.0809],\n",
       "        [0.5499, 0.0127, 0.0850,  ..., 0.0669, 0.0768, 0.0737],\n",
       "        [0.5842, 0.0089, 0.0531,  ..., 0.0594, 0.0923, 0.0878],\n",
       "        ...,\n",
       "        [0.0690, 0.1502, 0.1629,  ..., 0.1562, 0.0389, 0.0815],\n",
       "        [0.0903, 0.0719, 0.4556,  ..., 0.0353, 0.1396, 0.1260],\n",
       "        [0.0648, 0.0662, 0.5327,  ..., 0.0554, 0.0969, 0.1004]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
