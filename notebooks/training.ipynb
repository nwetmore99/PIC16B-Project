{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\", \"right\", \"left\", \"background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.3, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@194170.327] global loadsave.cpp:248 findDecoder imread_('../training/down.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.327] global loadsave.cpp:248 findDecoder imread_('../training/down.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.328] global loadsave.cpp:248 findDecoder imread_('../training/down.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.329] global loadsave.cpp:248 findDecoder imread_('../training/down.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.330] global loadsave.cpp:248 findDecoder imread_('../training/down.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.330] global loadsave.cpp:248 findDecoder imread_('../training/down.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194170.330] global loadsave.cpp:248 findDecoder imread_('../training/down.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.494] global loadsave.cpp:248 findDecoder imread_('../training/up.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.495] global loadsave.cpp:248 findDecoder imread_('../training/up.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194175.496] global loadsave.cpp:248 findDecoder imread_('../training/up.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.405] global loadsave.cpp:248 findDecoder imread_('../training/stop.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.405] global loadsave.cpp:248 findDecoder imread_('../training/stop.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.406] global loadsave.cpp:248 findDecoder imread_('../training/stop.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194181.407] global loadsave.cpp:248 findDecoder imread_('../training/stop.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.452] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.453] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194186.454] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.538] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.539] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.540] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194191.541] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.536] global loadsave.cpp:248 findDecoder imread_('../training/right.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.536] global loadsave.cpp:248 findDecoder imread_('../training/right.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.536] global loadsave.cpp:248 findDecoder imread_('../training/right.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.537] global loadsave.cpp:248 findDecoder imread_('../training/right.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.538] global loadsave.cpp:248 findDecoder imread_('../training/right.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194196.539] global loadsave.cpp:248 findDecoder imread_('../training/right.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.479] global loadsave.cpp:248 findDecoder imread_('../training/left.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.480] global loadsave.cpp:248 findDecoder imread_('../training/left.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194201.481] global loadsave.cpp:248 findDecoder imread_('../training/left.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194209.909] global loadsave.cpp:248 findDecoder imread_('../training/background.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@194209.909] global loadsave.cpp:248 findDecoder imread_('../training/background.174.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(175):\n",
    "        try:\n",
    "            image = cv2.imread(f\"../training/{gesture_class}.{i}.jpg\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        except:\n",
    "            continue\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            train_label = np.zeros([len(classes)])\n",
    "            train_label[class_index] = 1\n",
    "            train_data.append(landmarks)\n",
    "            train_labels.append(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([785, 21, 2])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(train_data, train_labels)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_labels = []\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(40):\n",
    "        image = cv2.imread(f\"../validation/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            val_label = np.zeros([len(classes)])\n",
    "            val_label[class_index] = 1\n",
    "            val_data.append(landmarks)\n",
    "            val_labels.append(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(val_data, val_labels)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(curr_model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 2.0721105724573134\n",
      "  batch 20 loss: 2.0559220165014267\n",
      "  batch 30 loss: 2.0631752997636794\n",
      "  batch 40 loss: 2.0648324310779573\n",
      "  batch 50 loss: 2.093605932593346\n",
      "  batch 60 loss: 2.0566307067871095\n",
      "  batch 70 loss: 2.0613377660512926\n",
      "  batch 80 loss: 2.0752859473228455\n",
      "  batch 90 loss: 2.0545490473508834\n",
      "  batch 100 loss: 2.091955390572548\n",
      "  batch 110 loss: 2.0596517980098725\n",
      "  batch 120 loss: 2.08448840379715\n",
      "  batch 130 loss: 2.0160104781389236\n",
      "  batch 140 loss: 2.027228569984436\n",
      "  batch 150 loss: 2.0989886432886125\n",
      "  batch 160 loss: 2.073222240805626\n",
      "  batch 170 loss: 2.0963055342435837\n",
      "  batch 180 loss: 2.055954796075821\n",
      "  batch 190 loss: 2.028045791387558\n",
      "LOSS train 2.028045791387558 valid 2.0818058252334595\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 2.0153482228517534\n",
      "  batch 20 loss: 2.0596540540456774\n",
      "  batch 30 loss: 2.0913408517837526\n",
      "  batch 40 loss: 2.006839206814766\n",
      "  batch 50 loss: 2.085163101553917\n",
      "  batch 60 loss: 2.094262307882309\n",
      "  batch 70 loss: 2.063914266228676\n",
      "  batch 80 loss: 2.0445994555950167\n",
      "  batch 90 loss: 2.0725132405757902\n",
      "  batch 100 loss: 2.089619743824005\n",
      "  batch 110 loss: 2.0024950593709945\n",
      "  batch 120 loss: 1.983340311050415\n",
      "  batch 130 loss: 2.0212586909532546\n",
      "  batch 140 loss: 2.082469034194946\n",
      "  batch 150 loss: 2.0570606529712676\n",
      "  batch 160 loss: 2.0125841587781905\n",
      "  batch 170 loss: 2.0865782886743545\n",
      "  batch 180 loss: 2.033970722556114\n",
      "  batch 190 loss: 2.033421516418457\n",
      "LOSS train 2.033421516418457 valid 2.088720821035214\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 2.05899121761322\n",
      "  batch 20 loss: 2.072949668765068\n",
      "  batch 30 loss: 2.029633578658104\n",
      "  batch 40 loss: 2.02623488008976\n",
      "  batch 50 loss: 1.9912585586309433\n",
      "  batch 60 loss: 2.014220359921455\n",
      "  batch 70 loss: 2.062318268418312\n",
      "  batch 80 loss: 2.0751629114151\n",
      "  batch 90 loss: 2.045440712571144\n",
      "  batch 100 loss: 2.025450637936592\n",
      "  batch 110 loss: 2.0223094642162325\n",
      "  batch 120 loss: 2.0237697184085848\n",
      "  batch 130 loss: 2.0471954971551893\n",
      "  batch 140 loss: 2.0877552717924117\n",
      "  batch 150 loss: 2.058618575334549\n",
      "  batch 160 loss: 2.0566570281982424\n",
      "  batch 170 loss: 2.0731822341680526\n",
      "  batch 180 loss: 2.0538758665323256\n",
      "  batch 190 loss: 2.010049191117287\n",
      "LOSS train 2.010049191117287 valid 2.0926196498748584\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 2.0887807577848436\n",
      "  batch 20 loss: 2.059121251106262\n",
      "  batch 30 loss: 2.0675912618637087\n",
      "  batch 40 loss: 1.9941617846488953\n",
      "  batch 50 loss: 2.0208034574985505\n",
      "  batch 60 loss: 2.046676051616669\n",
      "  batch 70 loss: 2.073958772420883\n",
      "  batch 80 loss: 1.998452490568161\n",
      "  batch 90 loss: 1.9957505941390992\n",
      "  batch 100 loss: 2.0150632113218307\n",
      "  batch 110 loss: 2.0760462135076523\n",
      "  batch 120 loss: 2.0160413891077043\n",
      "  batch 130 loss: 2.034709084033966\n",
      "  batch 140 loss: 2.0444550067186356\n",
      "  batch 150 loss: 2.0716603606939317\n",
      "  batch 160 loss: 2.065612053871155\n",
      "  batch 170 loss: 2.0806248098611833\n",
      "  batch 180 loss: 2.0136150151491163\n",
      "  batch 190 loss: 2.061232495307922\n",
      "LOSS train 2.061232495307922 valid 2.095849408171116\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 2.0482904970645905\n",
      "  batch 20 loss: 1.9718156993389129\n",
      "  batch 30 loss: 2.0320308417081834\n",
      "  batch 40 loss: 2.0710972517728807\n",
      "  batch 50 loss: 2.04015499651432\n",
      "  batch 60 loss: 2.0078226208686827\n",
      "  batch 70 loss: 1.9927453696727753\n",
      "  batch 80 loss: 2.120279836654663\n",
      "  batch 90 loss: 2.031606563925743\n",
      "  batch 100 loss: 2.0322326362133025\n",
      "  batch 110 loss: 2.007870352268219\n",
      "  batch 120 loss: 2.010642495751381\n",
      "  batch 130 loss: 1.9950741052627563\n",
      "  batch 140 loss: 2.053865560889244\n",
      "  batch 150 loss: 2.057467946410179\n",
      "  batch 160 loss: 2.118379509449005\n",
      "  batch 170 loss: 2.114943364262581\n",
      "  batch 180 loss: 2.0372981786727906\n",
      "  batch 190 loss: 2.0527330160140993\n",
      "LOSS train 2.0527330160140993 valid 2.094687576859425\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 2.0783935457468035\n",
      "  batch 20 loss: 2.0257735013961793\n",
      "  batch 30 loss: 1.9989328384399414\n",
      "  batch 40 loss: 2.087158423662186\n",
      "  batch 50 loss: 2.0372772395610808\n",
      "  batch 60 loss: 2.0386028349399568\n",
      "  batch 70 loss: 1.9309380114078523\n",
      "  batch 80 loss: 2.039028161764145\n",
      "  batch 90 loss: 2.0185534834861754\n",
      "  batch 100 loss: 2.0542683720588686\n",
      "  batch 110 loss: 2.0345530182123186\n",
      "  batch 120 loss: 2.0562041461467744\n",
      "  batch 130 loss: 2.0718745201826096\n",
      "  batch 140 loss: 2.0094464123249054\n",
      "  batch 150 loss: 2.0732894778251647\n",
      "  batch 160 loss: 2.0345132112503053\n",
      "  batch 170 loss: 1.9898164689540863\n",
      "  batch 180 loss: 2.0139626294374464\n",
      "  batch 190 loss: 2.1189358592033387\n",
      "LOSS train 2.1189358592033387 valid 2.0952947498896184\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 2.0053796619176865\n",
      "  batch 20 loss: 2.066827616095543\n",
      "  batch 30 loss: 2.0284610480070113\n",
      "  batch 40 loss: 2.059913608431816\n",
      "  batch 50 loss: 2.030368188023567\n",
      "  batch 60 loss: 2.0874356061220167\n",
      "  batch 70 loss: 2.043040317296982\n",
      "  batch 80 loss: 2.03730066716671\n",
      "  batch 90 loss: 2.041115441918373\n",
      "  batch 100 loss: 1.963674619793892\n",
      "  batch 110 loss: 2.061930376291275\n",
      "  batch 120 loss: 1.9903858184814454\n",
      "  batch 130 loss: 2.05131411254406\n",
      "  batch 140 loss: 2.067864626646042\n",
      "  batch 150 loss: 2.013475838303566\n",
      "  batch 160 loss: 2.0242240607738493\n",
      "  batch 170 loss: 2.053806999325752\n",
      "  batch 180 loss: 2.0040478229522707\n",
      "  batch 190 loss: 2.003907245397568\n",
      "LOSS train 2.003907245397568 valid 2.0942048854552784\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 2.008412593603134\n",
      "  batch 20 loss: 1.980471470952034\n",
      "  batch 30 loss: 2.0746832519769667\n",
      "  batch 40 loss: 2.0405473947525024\n",
      "  batch 50 loss: 2.0779595255851744\n",
      "  batch 60 loss: 1.9585603177547455\n",
      "  batch 70 loss: 1.997439095377922\n",
      "  batch 80 loss: 2.0275656670331954\n",
      "  batch 90 loss: 2.048720473051071\n",
      "  batch 100 loss: 2.0081580013036726\n",
      "  batch 110 loss: 2.08421405851841\n",
      "  batch 120 loss: 2.0822116494178773\n",
      "  batch 130 loss: 2.0043361306190492\n",
      "  batch 140 loss: 2.0264809876680374\n",
      "  batch 150 loss: 2.0449934005737305\n",
      "  batch 160 loss: 2.0494571357965468\n",
      "  batch 170 loss: 2.0382107228040693\n",
      "  batch 180 loss: 1.9930680394172668\n",
      "  batch 190 loss: 2.0646603792905807\n",
      "LOSS train 2.0646603792905807 valid 2.0913412146843395\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 2.0609372556209564\n",
      "  batch 20 loss: 1.9631993889808654\n",
      "  batch 30 loss: 1.9813449919223785\n",
      "  batch 40 loss: 2.082478755712509\n",
      "  batch 50 loss: 1.9694297760725021\n",
      "  batch 60 loss: 1.988586875796318\n",
      "  batch 70 loss: 2.047047933936119\n",
      "  batch 80 loss: 1.9944212853908538\n",
      "  batch 90 loss: 2.0139291852712633\n",
      "  batch 100 loss: 2.032904413342476\n",
      "  batch 110 loss: 2.0504286885261536\n",
      "  batch 120 loss: 2.061899197101593\n",
      "  batch 130 loss: 2.052674597501755\n",
      "  batch 140 loss: 2.0597386687994\n",
      "  batch 150 loss: 2.005010908842087\n",
      "  batch 160 loss: 2.0237503558397294\n",
      "  batch 170 loss: 2.1428367525339125\n",
      "  batch 180 loss: 2.028112402558327\n",
      "  batch 190 loss: 1.98622967004776\n",
      "LOSS train 1.98622967004776 valid 2.0869837254285812\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 2.057851406931877\n",
      "  batch 20 loss: 1.9663884907960891\n",
      "  batch 30 loss: 1.966121345758438\n",
      "  batch 40 loss: 2.0540315568447114\n",
      "  batch 50 loss: 2.000861608982086\n",
      "  batch 60 loss: 2.058435022830963\n",
      "  batch 70 loss: 2.028144198656082\n",
      "  batch 80 loss: 2.012426194548607\n",
      "  batch 90 loss: 2.09393210709095\n",
      "  batch 100 loss: 1.9726311594247818\n",
      "  batch 110 loss: 1.9742984414100646\n",
      "  batch 120 loss: 2.087147542834282\n",
      "  batch 130 loss: 2.032330411672592\n",
      "  batch 140 loss: 1.869354459643364\n",
      "  batch 150 loss: 2.0649504512548447\n",
      "  batch 160 loss: 2.050547179579735\n",
      "  batch 170 loss: 2.06126583814621\n",
      "  batch 180 loss: 1.998684549331665\n",
      "  batch 190 loss: 2.0596592247486116\n",
      "LOSS train 2.0596592247486116 valid 2.0821712483198214\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 2.0409278839826586\n",
      "  batch 20 loss: 1.9579543262720107\n",
      "  batch 30 loss: 1.9832337826490403\n",
      "  batch 40 loss: 2.037656730413437\n",
      "  batch 50 loss: 2.0324132829904555\n",
      "  batch 60 loss: 2.068850389122963\n",
      "  batch 70 loss: 1.9785184115171432\n",
      "  batch 80 loss: 2.007033896446228\n",
      "  batch 90 loss: 2.0112113147974013\n",
      "  batch 100 loss: 2.0350300550460814\n",
      "  batch 110 loss: 2.037420946359634\n",
      "  batch 120 loss: 2.023424726724625\n",
      "  batch 130 loss: 2.030784696340561\n",
      "  batch 140 loss: 2.0019601047039033\n",
      "  batch 150 loss: 1.9639495015144348\n",
      "  batch 160 loss: 1.9941528379917144\n",
      "  batch 170 loss: 1.9868846744298936\n",
      "  batch 180 loss: 2.0282548367977142\n",
      "  batch 190 loss: 2.092644822597504\n",
      "LOSS train 2.092644822597504 valid 2.08030557173949\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 2.054566037654877\n",
      "  batch 20 loss: 2.011543941497803\n",
      "  batch 30 loss: 1.9995478004217149\n",
      "  batch 40 loss: 1.9460689544677734\n",
      "  batch 50 loss: 1.960834515094757\n",
      "  batch 60 loss: 1.9813568443059921\n",
      "  batch 70 loss: 2.1325117111206056\n",
      "  batch 80 loss: 2.082551184296608\n",
      "  batch 90 loss: 2.033719128370285\n",
      "  batch 100 loss: 2.0308638483285906\n",
      "  batch 110 loss: 1.9798401921987534\n",
      "  batch 120 loss: 2.0443510562181473\n",
      "  batch 130 loss: 2.040621793270111\n",
      "  batch 140 loss: 2.0505305886268617\n",
      "  batch 150 loss: 1.959054520726204\n",
      "  batch 160 loss: 1.9594246864318847\n",
      "  batch 170 loss: 1.9668971806764604\n",
      "  batch 180 loss: 2.0079378455877306\n",
      "  batch 190 loss: 1.9920057952404022\n",
      "LOSS train 1.9920057952404022 valid 2.07665159763434\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 2.025215798616409\n",
      "  batch 20 loss: 1.9894473254680634\n",
      "  batch 30 loss: 2.0359347999095916\n",
      "  batch 40 loss: 1.9645150393247603\n",
      "  batch 50 loss: 2.0141261726617814\n",
      "  batch 60 loss: 2.0941785484552384\n",
      "  batch 70 loss: 1.9642345279455184\n",
      "  batch 80 loss: 1.9662647277116776\n",
      "  batch 90 loss: 2.0389109790325164\n",
      "  batch 100 loss: 2.082798978686333\n",
      "  batch 110 loss: 1.989014235138893\n",
      "  batch 120 loss: 2.0097147792577745\n",
      "  batch 130 loss: 2.0134648829698563\n",
      "  batch 140 loss: 1.9299944519996644\n",
      "  batch 150 loss: 2.006531131267548\n",
      "  batch 160 loss: 2.005286228656769\n",
      "  batch 170 loss: 1.9838174283504486\n",
      "  batch 180 loss: 1.9736590772867202\n",
      "  batch 190 loss: 1.975923275947571\n",
      "LOSS train 1.975923275947571 valid 2.0684345914767337\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 2.0688471466302873\n",
      "  batch 20 loss: 1.993736946582794\n",
      "  batch 30 loss: 1.9262710779905319\n",
      "  batch 40 loss: 1.992953035235405\n",
      "  batch 50 loss: 1.957595944404602\n",
      "  batch 60 loss: 2.0321448177099226\n",
      "  batch 70 loss: 1.977415332198143\n",
      "  batch 80 loss: 2.0103445559740067\n",
      "  batch 90 loss: 1.9867748647928238\n",
      "  batch 100 loss: 1.999765831232071\n",
      "  batch 110 loss: 2.0070948243141173\n",
      "  batch 120 loss: 1.9385876446962356\n",
      "  batch 130 loss: 1.9621738821268082\n",
      "  batch 140 loss: 2.008724254369736\n",
      "  batch 150 loss: 1.9737506091594696\n",
      "  batch 160 loss: 1.9671831369400024\n",
      "  batch 170 loss: 1.8913319736719132\n",
      "  batch 180 loss: 1.972847929596901\n",
      "  batch 190 loss: 2.1325463742017745\n",
      "LOSS train 2.1325463742017745 valid 2.058080937235783\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 1.9807329148054122\n",
      "  batch 20 loss: 1.9730938822031021\n",
      "  batch 30 loss: 2.045726773142815\n",
      "  batch 40 loss: 2.072107145190239\n",
      "  batch 50 loss: 1.913984116911888\n",
      "  batch 60 loss: 1.943438211083412\n",
      "  batch 70 loss: 1.9173574417829513\n",
      "  batch 80 loss: 1.9332118719816207\n",
      "  batch 90 loss: 2.000769409537315\n",
      "  batch 100 loss: 1.993694657087326\n",
      "  batch 110 loss: 1.968689861893654\n",
      "  batch 120 loss: 2.0046086490154265\n",
      "  batch 130 loss: 1.9353852182626725\n",
      "  batch 140 loss: 2.051562103629112\n",
      "  batch 150 loss: 1.9365683019161224\n",
      "  batch 160 loss: 1.9401087164878845\n",
      "  batch 170 loss: 1.9938340395689012\n",
      "  batch 180 loss: 1.948409342765808\n",
      "  batch 190 loss: 2.03283186852932\n",
      "LOSS train 2.03283186852932 valid 2.0434788236251245\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 1.9679729431867599\n",
      "  batch 20 loss: 1.9439229249954224\n",
      "  batch 30 loss: 2.0063923090696334\n",
      "  batch 40 loss: 1.9486624121665954\n",
      "  batch 50 loss: 2.023679292201996\n",
      "  batch 60 loss: 1.9293306827545167\n",
      "  batch 70 loss: 1.9329436033964158\n",
      "  batch 80 loss: 1.9718457251787185\n",
      "  batch 90 loss: 1.9581338822841645\n",
      "  batch 100 loss: 2.107188066840172\n",
      "  batch 110 loss: 1.9797185391187668\n",
      "  batch 120 loss: 2.0435462802648545\n",
      "  batch 130 loss: 1.9360035240650177\n",
      "  batch 140 loss: 1.8927485018968582\n",
      "  batch 150 loss: 1.9832093983888626\n",
      "  batch 160 loss: 1.9659737586975097\n",
      "  batch 170 loss: 1.9711294323205948\n",
      "  batch 180 loss: 1.842844957113266\n",
      "  batch 190 loss: 1.8847729414701462\n",
      "LOSS train 1.8847729414701462 valid 2.042631428593244\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 1.9450786501169204\n",
      "  batch 20 loss: 2.040680208802223\n",
      "  batch 30 loss: 1.9727440416812896\n",
      "  batch 40 loss: 1.9881533950567245\n",
      "  batch 50 loss: 1.9977529764175415\n",
      "  batch 60 loss: 1.9597547888755797\n",
      "  batch 70 loss: 1.9415279567241668\n",
      "  batch 80 loss: 1.8949190616607665\n",
      "  batch 90 loss: 1.9239296048879624\n",
      "  batch 100 loss: 1.8726186573505401\n",
      "  batch 110 loss: 1.8474971294403075\n",
      "  batch 120 loss: 1.7905633732676507\n",
      "  batch 130 loss: 1.9127839252352714\n",
      "  batch 140 loss: 2.0067854136228562\n",
      "  batch 150 loss: 1.9702176466584205\n",
      "  batch 160 loss: 1.8737684696912766\n",
      "  batch 170 loss: 1.9567333996295928\n",
      "  batch 180 loss: 1.9419019669294357\n",
      "  batch 190 loss: 1.973832756280899\n",
      "LOSS train 1.973832756280899 valid 2.0048067067296076\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 1.819542533159256\n",
      "  batch 20 loss: 1.9487091913819312\n",
      "  batch 30 loss: 1.918393424153328\n",
      "  batch 40 loss: 2.0493396669626236\n",
      "  batch 50 loss: 1.8472564950585366\n",
      "  batch 60 loss: 1.7762186631560326\n",
      "  batch 70 loss: 1.8317443564534188\n",
      "  batch 80 loss: 1.903873535990715\n",
      "  batch 90 loss: 1.8972362563014031\n",
      "  batch 100 loss: 1.9897511661052705\n",
      "  batch 110 loss: 1.922555473446846\n",
      "  batch 120 loss: 1.8315837606787682\n",
      "  batch 130 loss: 1.881082147359848\n",
      "  batch 140 loss: 1.8695506900548935\n",
      "  batch 150 loss: 1.9042683243751526\n",
      "  batch 160 loss: 1.8727830454707146\n",
      "  batch 170 loss: 1.9519101589918137\n",
      "  batch 180 loss: 1.9645315319299699\n",
      "  batch 190 loss: 1.9353636384010315\n",
      "LOSS train 1.9353636384010315 valid 1.9445999247523456\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 1.9510143995285034\n",
      "  batch 20 loss: 1.8503877565264701\n",
      "  batch 30 loss: 1.8766042783856391\n",
      "  batch 40 loss: 1.837059284746647\n",
      "  batch 50 loss: 1.7854408204555512\n",
      "  batch 60 loss: 1.8422640725970267\n",
      "  batch 70 loss: 1.681453001499176\n",
      "  batch 80 loss: 1.8099269673228264\n",
      "  batch 90 loss: 1.8811624705791474\n",
      "  batch 100 loss: 1.7874148532748222\n",
      "  batch 110 loss: 1.9357229202985764\n",
      "  batch 120 loss: 1.9808716163039208\n",
      "  batch 130 loss: 1.8475032955408097\n",
      "  batch 140 loss: 1.9512022376060485\n",
      "  batch 150 loss: 1.8011220663785934\n",
      "  batch 160 loss: 1.8643233984708787\n",
      "  batch 170 loss: 1.810321068763733\n",
      "  batch 180 loss: 1.7989499539136886\n",
      "  batch 190 loss: 1.7337089791893958\n",
      "LOSS train 1.7337089791893958 valid 1.8601895660544052\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 1.7749319985508918\n",
      "  batch 20 loss: 1.6888503074645995\n",
      "  batch 30 loss: 1.944652783870697\n",
      "  batch 40 loss: 1.7217206969857215\n",
      "  batch 50 loss: 1.7600064814090728\n",
      "  batch 60 loss: 1.8798536658287048\n",
      "  batch 70 loss: 1.7540043458342551\n",
      "  batch 80 loss: 1.7301114365458488\n",
      "  batch 90 loss: 1.7742249116301536\n",
      "  batch 100 loss: 1.8678985446691514\n",
      "  batch 110 loss: 1.75857799872756\n",
      "  batch 120 loss: 1.7947478890419006\n",
      "  batch 130 loss: 1.788256011158228\n",
      "  batch 140 loss: 1.7600250847637653\n",
      "  batch 150 loss: 1.7232123449444772\n",
      "  batch 160 loss: 1.9001312136650086\n",
      "  batch 170 loss: 1.7891942486166954\n",
      "  batch 180 loss: 1.730650120601058\n",
      "  batch 190 loss: 1.6960905447602272\n",
      "LOSS train 1.6960905447602272 valid 1.7801879055989094\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 1.7858212247490883\n",
      "  batch 20 loss: 1.78327027708292\n",
      "  batch 30 loss: 1.7502023190259934\n",
      "  batch 40 loss: 1.751983480155468\n",
      "  batch 50 loss: 1.6133503302931786\n",
      "  batch 60 loss: 1.7453181937336921\n",
      "  batch 70 loss: 1.7882393941283226\n",
      "  batch 80 loss: 1.7057784646749496\n",
      "  batch 90 loss: 1.7466633439064025\n",
      "  batch 100 loss: 1.6795867085456848\n",
      "  batch 110 loss: 1.5562296450138091\n",
      "  batch 120 loss: 1.7119911581277847\n",
      "  batch 130 loss: 1.633399212360382\n",
      "  batch 140 loss: 1.7975894033908844\n",
      "  batch 150 loss: 1.5998315144330264\n",
      "  batch 160 loss: 1.8844439402222632\n",
      "  batch 170 loss: 1.5338885460048914\n",
      "  batch 180 loss: 1.7579061523079873\n",
      "  batch 190 loss: 1.7277389481663703\n",
      "LOSS train 1.7277389481663703 valid 1.7088030585302756\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 1.703915000706911\n",
      "  batch 20 loss: 1.6266006253659726\n",
      "  batch 30 loss: 1.7853550113737584\n",
      "  batch 40 loss: 1.4955986585468053\n",
      "  batch 50 loss: 1.8503173850476742\n",
      "  batch 60 loss: 1.5997441444545983\n",
      "  batch 70 loss: 1.4501202821731567\n",
      "  batch 80 loss: 1.5518412932753562\n",
      "  batch 90 loss: 1.76401801481843\n",
      "  batch 100 loss: 1.675626378506422\n",
      "  batch 110 loss: 1.5539166543632745\n",
      "  batch 120 loss: 1.6818660084158181\n",
      "  batch 130 loss: 1.7300132401287556\n",
      "  batch 140 loss: 1.7380175285041333\n",
      "  batch 150 loss: 1.5637988723814487\n",
      "  batch 160 loss: 1.694365105032921\n",
      "  batch 170 loss: 1.4846801046282052\n",
      "  batch 180 loss: 1.7015376381576062\n",
      "  batch 190 loss: 1.7492851339280606\n",
      "LOSS train 1.7492851339280606 valid 1.7483393219419014\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 1.6875472500920297\n",
      "  batch 20 loss: 1.559225445240736\n",
      "  batch 30 loss: 1.6653169192373753\n",
      "  batch 40 loss: 1.6290055423974992\n",
      "  batch 50 loss: 1.6738139882683754\n",
      "  batch 60 loss: 1.6568430162966252\n",
      "  batch 70 loss: 1.4587887395173311\n",
      "  batch 80 loss: 1.4802230961620808\n",
      "  batch 90 loss: 1.6826233372092247\n",
      "  batch 100 loss: 1.4848212964832783\n",
      "  batch 110 loss: 1.6979122787714005\n",
      "  batch 120 loss: 1.4614345386624337\n",
      "  batch 130 loss: 1.564462111517787\n",
      "  batch 140 loss: 1.6849556013941764\n",
      "  batch 150 loss: 1.5569417925551534\n",
      "  batch 160 loss: 1.6260180659592152\n",
      "  batch 170 loss: 1.7061748653650284\n",
      "  batch 180 loss: 1.6661359649151564\n",
      "  batch 190 loss: 1.7799264153465628\n",
      "LOSS train 1.7799264153465628 valid 1.8042055333558566\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 1.597255266085267\n",
      "  batch 20 loss: 1.5543444011360408\n",
      "  batch 30 loss: 1.4598901111632585\n",
      "  batch 40 loss: 1.601264188066125\n",
      "  batch 50 loss: 1.474422807432711\n",
      "  batch 60 loss: 1.705615970864892\n",
      "  batch 70 loss: 1.6791323767974973\n",
      "  batch 80 loss: 1.4143276089802383\n",
      "  batch 90 loss: 1.6513931952416896\n",
      "  batch 100 loss: 1.6865333821624517\n",
      "  batch 110 loss: 1.4828783482313157\n",
      "  batch 120 loss: 1.710485076904297\n",
      "  batch 130 loss: 1.6923027403652668\n",
      "  batch 140 loss: 1.658905890211463\n",
      "  batch 150 loss: 1.6685256652534008\n",
      "  batch 160 loss: 1.4979601640254259\n",
      "  batch 170 loss: 1.4395465221256019\n",
      "  batch 180 loss: 1.6134927067905664\n",
      "  batch 190 loss: 1.5986080147325992\n",
      "LOSS train 1.5986080147325992 valid 1.5609610547813086\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 1.6295088827610016\n",
      "  batch 20 loss: 1.4190900417044758\n",
      "  batch 30 loss: 1.609345969557762\n",
      "  batch 40 loss: 1.7724980000406503\n",
      "  batch 50 loss: 1.6745281025767327\n",
      "  batch 60 loss: 1.6643005065619945\n",
      "  batch 70 loss: 1.6508343257009983\n",
      "  batch 80 loss: 1.543055707961321\n",
      "  batch 90 loss: 1.4365292828530074\n",
      "  batch 100 loss: 1.5152160964906216\n",
      "  batch 110 loss: 1.627139324694872\n",
      "  batch 120 loss: 1.5327330113388598\n",
      "  batch 130 loss: 1.398330020532012\n",
      "  batch 140 loss: 1.4817404681816697\n",
      "  batch 150 loss: 1.602739323116839\n",
      "  batch 160 loss: 1.4898183036595583\n",
      "  batch 170 loss: 1.6706484228372573\n",
      "  batch 180 loss: 1.3633377151563764\n",
      "  batch 190 loss: 1.6011480251327157\n",
      "LOSS train 1.6011480251327157 valid 1.5245604060876827\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 1.746297388523817\n",
      "  batch 20 loss: 1.5276990242302417\n",
      "  batch 30 loss: 1.442345446255058\n",
      "  batch 40 loss: 1.451877430640161\n",
      "  batch 50 loss: 1.665830747783184\n",
      "  batch 60 loss: 1.5012607470154762\n",
      "  batch 70 loss: 1.6752322144806384\n",
      "  batch 80 loss: 1.5082330023869872\n",
      "  batch 90 loss: 1.132457160577178\n",
      "  batch 100 loss: 1.5399210369214416\n",
      "  batch 110 loss: 1.5957121428102254\n",
      "  batch 120 loss: 1.4095847411081195\n",
      "  batch 130 loss: 1.4717905204743147\n",
      "  batch 140 loss: 1.6731919094920158\n",
      "  batch 150 loss: 1.5754537515342235\n",
      "  batch 160 loss: 1.6789772221818566\n",
      "  batch 170 loss: 1.391634362190962\n",
      "  batch 180 loss: 1.7190494360402226\n",
      "  batch 190 loss: 1.4251001138240098\n",
      "LOSS train 1.4251001138240098 valid 1.4924208830612211\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 1.4976185634732246\n",
      "  batch 20 loss: 1.5664337582886219\n",
      "  batch 30 loss: 1.613640407472849\n",
      "  batch 40 loss: 1.703826238028705\n",
      "  batch 50 loss: 1.5643947733566166\n",
      "  batch 60 loss: 1.5143174717202783\n",
      "  batch 70 loss: 1.5269178241491317\n",
      "  batch 80 loss: 1.5393765565007924\n",
      "  batch 90 loss: 1.327903636172414\n",
      "  batch 100 loss: 1.3489420130848884\n",
      "  batch 110 loss: 1.5181156739592552\n",
      "  batch 120 loss: 1.482971489802003\n",
      "  batch 130 loss: 1.6926094099879265\n",
      "  batch 140 loss: 1.4289080895483495\n",
      "  batch 150 loss: 1.4452561970800162\n",
      "  batch 160 loss: 1.6102682381868363\n",
      "  batch 170 loss: 1.4652218796312808\n",
      "  batch 180 loss: 1.3891840800642967\n",
      "  batch 190 loss: 1.6174294291064144\n",
      "LOSS train 1.6174294291064144 valid 1.462020437973432\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 1.5915812328457832\n",
      "  batch 20 loss: 1.4784309066832066\n",
      "  batch 30 loss: 1.6433021867647768\n",
      "  batch 40 loss: 1.5070635717362166\n",
      "  batch 50 loss: 1.463387636654079\n",
      "  batch 60 loss: 1.6107637749053538\n",
      "  batch 70 loss: 1.4088949954137207\n",
      "  batch 80 loss: 1.2709448922425508\n",
      "  batch 90 loss: 1.4638837164267897\n",
      "  batch 100 loss: 1.5621744446456431\n",
      "  batch 110 loss: 1.6429560756310821\n",
      "  batch 120 loss: 1.322786852531135\n",
      "  batch 130 loss: 1.7108760530129075\n",
      "  batch 140 loss: 1.7340572463348507\n",
      "  batch 150 loss: 1.4437868095934392\n",
      "  batch 160 loss: 1.4800191232934594\n",
      "  batch 170 loss: 1.4468492506071926\n",
      "  batch 180 loss: 1.4378015587106348\n",
      "  batch 190 loss: 1.522052295319736\n",
      "LOSS train 1.522052295319736 valid 1.5032719421463134\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 1.486669666878879\n",
      "  batch 20 loss: 1.4074016708880663\n",
      "  batch 30 loss: 1.6686698976904153\n",
      "  batch 40 loss: 1.3375331934541463\n",
      "  batch 50 loss: 1.4176000552251935\n",
      "  batch 60 loss: 1.3343499347567558\n",
      "  batch 70 loss: 1.3927074044011534\n",
      "  batch 80 loss: 1.4708240799605847\n",
      "  batch 90 loss: 1.4446586456149817\n",
      "  batch 100 loss: 1.4950160827487706\n",
      "  batch 110 loss: 1.4239130925387145\n",
      "  batch 120 loss: 1.5724730649963021\n",
      "  batch 130 loss: 1.55293441247195\n",
      "  batch 140 loss: 1.373949340172112\n",
      "  batch 150 loss: 1.639057156071067\n",
      "  batch 160 loss: 1.4820321593433619\n",
      "  batch 170 loss: 1.3938032485544682\n",
      "  batch 180 loss: 1.6904517263174057\n",
      "  batch 190 loss: 1.7013565918430686\n",
      "LOSS train 1.7013565918430686 valid 1.389247486057381\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 1.6153627894818783\n",
      "  batch 20 loss: 1.4736557733267546\n",
      "  batch 30 loss: 1.5096242524683476\n",
      "  batch 40 loss: 1.334719104040414\n",
      "  batch 50 loss: 1.5707036739215254\n",
      "  batch 60 loss: 1.560859452560544\n",
      "  batch 70 loss: 1.5745706586167216\n",
      "  batch 80 loss: 1.5328028246760368\n",
      "  batch 90 loss: 1.392460421100259\n",
      "  batch 100 loss: 1.34355260822922\n",
      "  batch 110 loss: 1.4301413685083388\n",
      "  batch 120 loss: 1.570908421650529\n",
      "  batch 130 loss: 1.5163574770092965\n",
      "  batch 140 loss: 1.171252560801804\n",
      "  batch 150 loss: 1.1576813945546747\n",
      "  batch 160 loss: 1.7056815077550709\n",
      "  batch 170 loss: 1.4740011982619763\n",
      "  batch 180 loss: 1.320658153668046\n",
      "  batch 190 loss: 1.4102875065058469\n",
      "LOSS train 1.4102875065058469 valid 1.4801674224675083\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 1.4082495614886283\n",
      "  batch 20 loss: 1.5222685050219298\n",
      "  batch 30 loss: 1.3783458072692156\n",
      "  batch 40 loss: 1.3595704840496183\n",
      "  batch 50 loss: 1.5856228616088628\n",
      "  batch 60 loss: 1.4239386485889554\n",
      "  batch 70 loss: 1.6523251000791788\n",
      "  batch 80 loss: 1.3534340916201473\n",
      "  batch 90 loss: 1.3533247258514165\n",
      "  batch 100 loss: 1.3741237461566924\n",
      "  batch 110 loss: 1.5715608276426791\n",
      "  batch 120 loss: 1.4762316547334193\n",
      "  batch 130 loss: 1.4464907862246037\n",
      "  batch 140 loss: 1.5777664247900247\n",
      "  batch 150 loss: 1.2819972205907106\n",
      "  batch 160 loss: 1.3962714745663107\n",
      "  batch 170 loss: 1.5735558580607176\n",
      "  batch 180 loss: 1.3519034944474697\n",
      "  batch 190 loss: 1.1377967987209558\n",
      "LOSS train 1.1377967987209558 valid 1.4775318841999159\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 1.2743902191519738\n",
      "  batch 20 loss: 1.3425089363008738\n",
      "  batch 30 loss: 1.2874662486836315\n",
      "  batch 40 loss: 1.2815496399998665\n",
      "  batch 50 loss: 1.7194205708801746\n",
      "  batch 60 loss: 1.3993425785563887\n",
      "  batch 70 loss: 1.4959605932235718\n",
      "  batch 80 loss: 1.370784811489284\n",
      "  batch 90 loss: 1.242441288381815\n",
      "  batch 100 loss: 1.2623958474956454\n",
      "  batch 110 loss: 1.423453126102686\n",
      "  batch 120 loss: 1.294884636811912\n",
      "  batch 130 loss: 1.283780325949192\n",
      "  batch 140 loss: 1.3670996254310013\n",
      "  batch 150 loss: 1.527677951194346\n",
      "  batch 160 loss: 1.5747978195548058\n",
      "  batch 170 loss: 1.3346779266372324\n",
      "  batch 180 loss: 1.3753257273696362\n",
      "  batch 190 loss: 1.3824072294868528\n",
      "LOSS train 1.3824072294868528 valid 1.8133122458791313\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 1.3294465903192758\n",
      "  batch 20 loss: 1.3684149543754756\n",
      "  batch 30 loss: 1.381007304787636\n",
      "  batch 40 loss: 1.226098058000207\n",
      "  batch 50 loss: 1.579940322600305\n",
      "  batch 60 loss: 1.402018897421658\n",
      "  batch 70 loss: 1.2791491429321469\n",
      "  batch 80 loss: 1.570725904405117\n",
      "  batch 90 loss: 1.4119361482560635\n",
      "  batch 100 loss: 1.3493514770641923\n",
      "  batch 110 loss: 1.4195674372836948\n",
      "  batch 120 loss: 1.2245436089113355\n",
      "  batch 130 loss: 1.2988405847921967\n",
      "  batch 140 loss: 1.1690748206339776\n",
      "  batch 150 loss: 1.3903167733922601\n",
      "  batch 160 loss: 1.250872978195548\n",
      "  batch 170 loss: 1.0837479114532471\n",
      "  batch 180 loss: 1.4745568385347725\n",
      "  batch 190 loss: 1.5187820496037603\n",
      "LOSS train 1.5187820496037603 valid 1.4993469305097675\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 1.4350343879312277\n",
      "  batch 20 loss: 1.430364466831088\n",
      "  batch 30 loss: 1.356134069338441\n",
      "  batch 40 loss: 1.233635814487934\n",
      "  batch 50 loss: 1.3764064257033168\n",
      "  batch 60 loss: 1.3121659553609788\n",
      "  batch 70 loss: 1.2671199369244277\n",
      "  batch 80 loss: 1.1599433108232915\n",
      "  batch 90 loss: 1.4080103617161512\n",
      "  batch 100 loss: 1.3718624208122492\n",
      "  batch 110 loss: 1.1728852812200785\n",
      "  batch 120 loss: 1.309713988751173\n",
      "  batch 130 loss: 1.3493645191192627\n",
      "  batch 140 loss: 1.3533540150150656\n",
      "  batch 150 loss: 1.2817467670887708\n",
      "  batch 160 loss: 1.582772521302104\n",
      "  batch 170 loss: 1.516945663653314\n",
      "  batch 180 loss: 1.2067411574535072\n",
      "  batch 190 loss: 1.1906751728616656\n",
      "LOSS train 1.1906751728616656 valid 1.2746701187884004\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 1.3211786633357405\n",
      "  batch 20 loss: 1.3691628750413656\n",
      "  batch 30 loss: 1.5412079989910126\n",
      "  batch 40 loss: 1.4163749422878027\n",
      "  batch 50 loss: 1.2047966407146304\n",
      "  batch 60 loss: 1.3621337372809648\n",
      "  batch 70 loss: 1.182060185354203\n",
      "  batch 80 loss: 1.3220941838808358\n",
      "  batch 90 loss: 1.3079025528393686\n",
      "  batch 100 loss: 1.1132315721362829\n",
      "  batch 110 loss: 1.2633016959764063\n",
      "  batch 120 loss: 1.4025193883106113\n",
      "  batch 130 loss: 1.261896454077214\n",
      "  batch 140 loss: 1.4017836147919298\n",
      "  batch 150 loss: 1.2213896544650198\n",
      "  batch 160 loss: 1.261785395629704\n",
      "  batch 170 loss: 1.2717262035235763\n",
      "  batch 180 loss: 1.4200725328177213\n",
      "  batch 190 loss: 1.1469369121827184\n",
      "LOSS train 1.1469369121827184 valid 1.1742105677795525\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 1.0460417468566447\n",
      "  batch 20 loss: 1.2767445533536375\n",
      "  batch 30 loss: 1.1287688525393604\n",
      "  batch 40 loss: 1.3349960617721082\n",
      "  batch 50 loss: 1.2388517756015063\n",
      "  batch 60 loss: 1.3510462916456163\n",
      "  batch 70 loss: 1.1491546746343375\n",
      "  batch 80 loss: 1.3621562415733934\n",
      "  batch 90 loss: 1.180803898582235\n",
      "  batch 100 loss: 1.4968157887458802\n",
      "  batch 110 loss: 1.2133701947517692\n",
      "  batch 120 loss: 1.3801862246356904\n",
      "  batch 130 loss: 1.2423530450090765\n",
      "  batch 140 loss: 1.1638260568492114\n",
      "  batch 150 loss: 1.3548219265416264\n",
      "  batch 160 loss: 1.3778005599975587\n",
      "  batch 170 loss: 1.1072526052594185\n",
      "  batch 180 loss: 1.2322019290179014\n",
      "  batch 190 loss: 1.180330224893987\n",
      "LOSS train 1.180330224893987 valid 4.2380551838059315\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 1.6224480951204896\n",
      "  batch 20 loss: 1.2435637339949608\n",
      "  batch 30 loss: 1.3183007877320052\n",
      "  batch 40 loss: 1.3237908329814672\n",
      "  batch 50 loss: 1.2434454070404173\n",
      "  batch 60 loss: 1.2895641231909394\n",
      "  batch 70 loss: 1.2194602383300661\n",
      "  batch 80 loss: 1.310722517594695\n",
      "  batch 90 loss: 1.133307371288538\n",
      "  batch 100 loss: 1.1300048587843776\n",
      "  batch 110 loss: 1.3935410047881305\n",
      "  batch 120 loss: 1.2946964857168495\n",
      "  batch 130 loss: 1.061482261866331\n",
      "  batch 140 loss: 1.1268617124296725\n",
      "  batch 150 loss: 1.0720837357454003\n",
      "  batch 160 loss: 1.3493183189071716\n",
      "  batch 170 loss: 1.1637053310871124\n",
      "  batch 180 loss: 1.4404590552672745\n",
      "  batch 190 loss: 1.2444756142795086\n",
      "LOSS train 1.2444756142795086 valid 1.1772294995518258\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 1.290278127603233\n",
      "  batch 20 loss: 1.2475859817117452\n",
      "  batch 30 loss: 1.286180554330349\n",
      "  batch 40 loss: 1.1837335962802171\n",
      "  batch 50 loss: 1.1377727339044212\n",
      "  batch 60 loss: 1.1518689386546612\n",
      "  batch 70 loss: 1.3252149986103177\n",
      "  batch 80 loss: 1.2767169322818517\n",
      "  batch 90 loss: 1.0888224475085735\n",
      "  batch 100 loss: 1.2500434881076217\n",
      "  batch 110 loss: 1.1805744834244252\n",
      "  batch 120 loss: 1.1689991619437934\n",
      "  batch 130 loss: 0.997154462710023\n",
      "  batch 140 loss: 1.1717850353568793\n",
      "  batch 150 loss: 1.2452584287151693\n",
      "  batch 160 loss: 1.1528728540986777\n",
      "  batch 170 loss: 1.119496532343328\n",
      "  batch 180 loss: 1.1980338905472308\n",
      "  batch 190 loss: 1.210639246366918\n",
      "LOSS train 1.210639246366918 valid 1.2629650762507645\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 1.1938537667971105\n",
      "  batch 20 loss: 1.3630118496716022\n",
      "  batch 30 loss: 1.1232997114770114\n",
      "  batch 40 loss: 1.1633743460290134\n",
      "  batch 50 loss: 1.071626159735024\n",
      "  batch 60 loss: 1.348083980754018\n",
      "  batch 70 loss: 1.1976241976022721\n",
      "  batch 80 loss: 1.2230675601400436\n",
      "  batch 90 loss: 1.0081363135948778\n",
      "  batch 100 loss: 1.3651431784965098\n",
      "  batch 110 loss: 1.1195272811688484\n",
      "  batch 120 loss: 1.2480529823806137\n",
      "  batch 130 loss: 1.1622641630470754\n",
      "  batch 140 loss: 1.3073850268498064\n",
      "  batch 150 loss: 1.0887014745734631\n",
      "  batch 160 loss: 1.000001719314605\n",
      "  batch 170 loss: 0.9741065802052618\n",
      "  batch 180 loss: 1.1215282995253801\n",
      "  batch 190 loss: 0.9925653971731663\n",
      "LOSS train 0.9925653971731663 valid 1.1682279626958263\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 1.0766885402146726\n",
      "  batch 20 loss: 1.1522700992412864\n",
      "  batch 30 loss: 1.1261223023757339\n",
      "  batch 40 loss: 1.4847878879401832\n",
      "  batch 50 loss: 1.0669593145139515\n",
      "  batch 60 loss: 1.1630282166413963\n",
      "  batch 70 loss: 1.371006636414677\n",
      "  batch 80 loss: 1.1568814013153315\n",
      "  batch 90 loss: 1.3043567549437285\n",
      "  batch 100 loss: 1.181570169981569\n",
      "  batch 110 loss: 1.3003687862306834\n",
      "  batch 120 loss: 1.0384704352356493\n",
      "  batch 130 loss: 0.9913731947541237\n",
      "  batch 140 loss: 1.321736760996282\n",
      "  batch 150 loss: 1.089066167641431\n",
      "  batch 160 loss: 1.087845847941935\n",
      "  batch 170 loss: 1.0878844349645078\n",
      "  batch 180 loss: 1.0868208803236485\n",
      "  batch 190 loss: 1.3352856295183302\n",
      "LOSS train 1.3352856295183302 valid 1.1275738015627632\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.9994943623431027\n",
      "  batch 20 loss: 1.097301537077874\n",
      "  batch 30 loss: 1.1791156138293446\n",
      "  batch 40 loss: 1.0992312411777676\n",
      "  batch 50 loss: 1.1516827046871185\n",
      "  batch 60 loss: 1.2713340185582638\n",
      "  batch 70 loss: 1.390264101885259\n",
      "  batch 80 loss: 1.058140206709504\n",
      "  batch 90 loss: 1.113269928842783\n",
      "  batch 100 loss: 1.0140599749982357\n",
      "  batch 110 loss: 1.2429951102472843\n",
      "  batch 120 loss: 1.297567865252495\n",
      "  batch 130 loss: 1.0455584542825818\n",
      "  batch 140 loss: 1.0344883979298174\n",
      "  batch 150 loss: 1.0500110694672913\n",
      "  batch 160 loss: 0.9292057454586029\n",
      "  batch 170 loss: 0.9942522322759032\n",
      "  batch 180 loss: 1.2282009079121052\n",
      "  batch 190 loss: 0.8290888480842113\n",
      "LOSS train 0.8290888480842113 valid 0.9633339546764126\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 1.0253617847338319\n",
      "  batch 20 loss: 1.281277956813574\n",
      "  batch 30 loss: 1.1399740250781178\n",
      "  batch 40 loss: 1.121353558683768\n",
      "  batch 50 loss: 1.0542404344305396\n",
      "  batch 60 loss: 0.9617654701694847\n",
      "  batch 70 loss: 1.0424746142700314\n",
      "  batch 80 loss: 1.117810430098325\n",
      "  batch 90 loss: 1.1146823277696967\n",
      "  batch 100 loss: 0.7648049399722368\n",
      "  batch 110 loss: 1.3181343832984567\n",
      "  batch 120 loss: 1.1411062600323931\n",
      "  batch 130 loss: 0.9149096230044961\n",
      "  batch 140 loss: 0.8883685485459865\n",
      "  batch 150 loss: 1.1481425929814577\n",
      "  batch 160 loss: 1.097071156743914\n",
      "  batch 170 loss: 1.2614370531402528\n",
      "  batch 180 loss: 1.2503620127215982\n",
      "  batch 190 loss: 1.0624660635832697\n",
      "LOSS train 1.0624660635832697 valid 1.2315011524255028\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.9215840765740723\n",
      "  batch 20 loss: 0.8905261943116785\n",
      "  batch 30 loss: 1.0610566564369948\n",
      "  batch 40 loss: 1.1151109042111784\n",
      "  batch 50 loss: 1.1076222261413933\n",
      "  batch 60 loss: 1.118917397176847\n",
      "  batch 70 loss: 1.114490441698581\n",
      "  batch 80 loss: 1.0329587506130338\n",
      "  batch 90 loss: 1.0593186212703585\n",
      "  batch 100 loss: 0.9550061172805726\n",
      "  batch 110 loss: 1.249426500312984\n",
      "  batch 120 loss: 1.3552471794188024\n",
      "  batch 130 loss: 0.7989145167171955\n",
      "  batch 140 loss: 1.0317008197773248\n",
      "  batch 150 loss: 1.2137936404906213\n",
      "  batch 160 loss: 0.8533113923389465\n",
      "  batch 170 loss: 0.9228945433162152\n",
      "  batch 180 loss: 1.1230896648950874\n",
      "  batch 190 loss: 0.843462267331779\n",
      "LOSS train 0.843462267331779 valid 0.9304274859694908\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 1.356487838551402\n",
      "  batch 20 loss: 0.816381897777319\n",
      "  batch 30 loss: 0.9131775347050279\n",
      "  batch 40 loss: 1.0252577904611826\n",
      "  batch 50 loss: 1.0564972406486048\n",
      "  batch 60 loss: 1.078494343906641\n",
      "  batch 70 loss: 1.222537308675237\n",
      "  batch 80 loss: 1.1827581628225743\n",
      "  batch 90 loss: 1.3595489990431815\n",
      "  batch 100 loss: 0.9389610161539167\n",
      "  batch 110 loss: 1.1430654186755418\n",
      "  batch 120 loss: 0.7858626760542393\n",
      "  batch 130 loss: 1.1093943951185792\n",
      "  batch 140 loss: 1.05246712397784\n",
      "  batch 150 loss: 0.8675657537765801\n",
      "  batch 160 loss: 0.8938239055685699\n",
      "  batch 170 loss: 0.970981708727777\n",
      "  batch 180 loss: 0.9635701522231102\n",
      "  batch 190 loss: 0.9035980625310913\n",
      "LOSS train 0.9035980625310913 valid 0.9406297438336202\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.9601061542285606\n",
      "  batch 20 loss: 0.965392550965771\n",
      "  batch 30 loss: 1.0932365089654923\n",
      "  batch 40 loss: 0.9388627561740577\n",
      "  batch 50 loss: 1.1677989214193076\n",
      "  batch 60 loss: 0.8352303080260753\n",
      "  batch 70 loss: 0.9826058424077928\n",
      "  batch 80 loss: 1.0270940482616424\n",
      "  batch 90 loss: 0.9999976005405188\n",
      "  batch 100 loss: 0.9048009654972702\n",
      "  batch 110 loss: 0.7817020907532424\n",
      "  batch 120 loss: 1.046771446429193\n",
      "  batch 130 loss: 1.0651237051468343\n",
      "  batch 140 loss: 1.0172846858855338\n",
      "  batch 150 loss: 1.0097759414464236\n",
      "  batch 160 loss: 0.9650151750538498\n",
      "  batch 170 loss: 1.2380934715270997\n",
      "  batch 180 loss: 1.0134430485311896\n",
      "  batch 190 loss: 0.7988267306005582\n",
      "LOSS train 0.7988267306005582 valid 1.4768826968263453\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 1.0356756239198148\n",
      "  batch 20 loss: 0.9391690018353984\n",
      "  batch 30 loss: 0.9238123903051019\n",
      "  batch 40 loss: 0.9409628612687811\n",
      "  batch 50 loss: 0.9247935760300606\n",
      "  batch 60 loss: 0.8358862394001335\n",
      "  batch 70 loss: 0.7747820829972625\n",
      "  batch 80 loss: 0.9790346711874008\n",
      "  batch 90 loss: 0.9864542035385966\n",
      "  batch 100 loss: 0.9150015559047461\n",
      "  batch 110 loss: 0.9475515673868358\n",
      "  batch 120 loss: 1.11400389354676\n",
      "  batch 130 loss: 0.9010642844252288\n",
      "  batch 140 loss: 1.074444384686649\n",
      "  batch 150 loss: 1.0002336237579583\n",
      "  batch 160 loss: 0.9483407457824796\n",
      "  batch 170 loss: 1.0688811243278906\n",
      "  batch 180 loss: 0.9800770340487361\n",
      "  batch 190 loss: 1.097736183553934\n",
      "LOSS train 1.097736183553934 valid 0.9681694611417464\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.9353363924659789\n",
      "  batch 20 loss: 0.8680543839000165\n",
      "  batch 30 loss: 0.8868757211137563\n",
      "  batch 40 loss: 0.8729734419845044\n",
      "  batch 50 loss: 0.8413629768416285\n",
      "  batch 60 loss: 1.0420601509511471\n",
      "  batch 70 loss: 1.0742626302875578\n",
      "  batch 80 loss: 0.8354755603941157\n",
      "  batch 90 loss: 1.1628631150349975\n",
      "  batch 100 loss: 0.7770429489668459\n",
      "  batch 110 loss: 0.8729064431041479\n",
      "  batch 120 loss: 0.9197316034696996\n",
      "  batch 130 loss: 0.7456630425527692\n",
      "  batch 140 loss: 0.8893005955964327\n",
      "  batch 150 loss: 0.8001866458915174\n",
      "  batch 160 loss: 1.182428319333121\n",
      "  batch 170 loss: 0.9422426706645638\n",
      "  batch 180 loss: 0.8297206250019371\n",
      "  batch 190 loss: 0.8164450131356716\n",
      "LOSS train 0.8164450131356716 valid 1.1049716308241329\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 1.0606918415054678\n",
      "  batch 20 loss: 0.7246744767064228\n",
      "  batch 30 loss: 1.0673899587942288\n",
      "  batch 40 loss: 1.0852078033844008\n",
      "  batch 50 loss: 0.9328842172399163\n",
      "  batch 60 loss: 0.9816752391401679\n",
      "  batch 70 loss: 0.6733379001729191\n",
      "  batch 80 loss: 0.7921635544858873\n",
      "  batch 90 loss: 0.6181693119928241\n",
      "  batch 100 loss: 0.9904595333617181\n",
      "  batch 110 loss: 1.147385780257173\n",
      "  batch 120 loss: 0.7351812103763222\n",
      "  batch 130 loss: 1.1420927215833216\n",
      "  batch 140 loss: 0.8536085914820433\n",
      "  batch 150 loss: 0.792461067251861\n",
      "  batch 160 loss: 0.8685485771857202\n",
      "  batch 170 loss: 0.5635772848501801\n",
      "  batch 180 loss: 0.9820497187320143\n",
      "  batch 190 loss: 0.8248120308853686\n",
      "LOSS train 0.8248120308853686 valid 1.4203810800637047\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.8918567325919866\n",
      "  batch 20 loss: 0.9501071883365512\n",
      "  batch 30 loss: 0.8797890522051602\n",
      "  batch 40 loss: 0.8604536577244289\n",
      "  batch 50 loss: 0.8950366119388491\n",
      "  batch 60 loss: 0.6522365175653249\n",
      "  batch 70 loss: 0.7991971463430673\n",
      "  batch 80 loss: 0.7322061262093484\n",
      "  batch 90 loss: 0.9707707061665133\n",
      "  batch 100 loss: 0.8555263786576688\n",
      "  batch 110 loss: 0.7493800166295841\n",
      "  batch 120 loss: 0.812321508070454\n",
      "  batch 130 loss: 0.8650008592754602\n",
      "  batch 140 loss: 0.8241265676915646\n",
      "  batch 150 loss: 0.7948682276299224\n",
      "  batch 160 loss: 0.7301632684655488\n",
      "  batch 170 loss: 1.1142310365336017\n",
      "  batch 180 loss: 0.6747363313101232\n",
      "  batch 190 loss: 0.8608987067826093\n",
      "LOSS train 0.8608987067826093 valid 0.7702570404791726\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 1.0284467922640033\n",
      "  batch 20 loss: 0.9005602561868727\n",
      "  batch 30 loss: 0.8869701157556846\n",
      "  batch 40 loss: 0.7972695976961404\n",
      "  batch 50 loss: 0.8690426664426922\n",
      "  batch 60 loss: 0.8297393237240612\n",
      "  batch 70 loss: 0.9800247109495104\n",
      "  batch 80 loss: 1.088159551937133\n",
      "  batch 90 loss: 0.8701693822164088\n",
      "  batch 100 loss: 0.9634871246526018\n",
      "  batch 110 loss: 0.946486972575076\n",
      "  batch 120 loss: 0.727745400974527\n",
      "  batch 130 loss: 0.6426135767251253\n",
      "  batch 140 loss: 0.7086369970347732\n",
      "  batch 150 loss: 0.6640637920005247\n",
      "  batch 160 loss: 0.8180157550144941\n",
      "  batch 170 loss: 1.1158524999627843\n",
      "  batch 180 loss: 1.1333047836087644\n",
      "  batch 190 loss: 0.6657533704303205\n",
      "LOSS train 0.6657533704303205 valid 0.777870601190863\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.8005630210973322\n",
      "  batch 20 loss: 0.8002264365553856\n",
      "  batch 30 loss: 1.0827038260293194\n",
      "  batch 40 loss: 0.7059826329350471\n",
      "  batch 50 loss: 0.9236933060456067\n",
      "  batch 60 loss: 0.7950293627684004\n",
      "  batch 70 loss: 0.9947302815271541\n",
      "  batch 80 loss: 0.9446994993835688\n",
      "  batch 90 loss: 0.9392932940274477\n",
      "  batch 100 loss: 0.7625164655968547\n",
      "  batch 110 loss: 0.6748221536865457\n",
      "  batch 120 loss: 0.6975424792617559\n",
      "  batch 130 loss: 0.6529227464576252\n",
      "  batch 140 loss: 1.2818509165197611\n",
      "  batch 150 loss: 0.8434464947786182\n",
      "  batch 160 loss: 0.8791025187587366\n",
      "  batch 170 loss: 0.9530636885436252\n",
      "  batch 180 loss: 0.7552168237045407\n",
      "  batch 190 loss: 0.769325636595022\n",
      "LOSS train 0.769325636595022 valid 0.8609307269723967\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.898580328328535\n",
      "  batch 20 loss: 0.7999790706206114\n",
      "  batch 30 loss: 0.8505734824342653\n",
      "  batch 40 loss: 1.020711357612163\n",
      "  batch 50 loss: 0.8108752078376711\n",
      "  batch 60 loss: 0.8954446471296251\n",
      "  batch 70 loss: 1.0837226542644203\n",
      "  batch 80 loss: 1.0299812007229776\n",
      "  batch 90 loss: 0.9547486231196671\n",
      "  batch 100 loss: 1.1690787490457297\n",
      "  batch 110 loss: 0.6286397399846464\n",
      "  batch 120 loss: 0.9494800362735987\n",
      "  batch 130 loss: 0.8517167845508083\n",
      "  batch 140 loss: 0.5982959425309673\n",
      "  batch 150 loss: 0.7678606160450727\n",
      "  batch 160 loss: 0.6943478850647807\n",
      "  batch 170 loss: 0.7713385527487844\n",
      "  batch 180 loss: 0.8233352731913328\n",
      "  batch 190 loss: 0.7127825066680089\n",
      "LOSS train 0.7127825066680089 valid 1.6021611426198759\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.8594760341569782\n",
      "  batch 20 loss: 1.1866825385019184\n",
      "  batch 30 loss: 0.864803766575642\n",
      "  batch 40 loss: 0.6859504657913931\n",
      "  batch 50 loss: 0.6782099422998726\n",
      "  batch 60 loss: 1.0990592205896974\n",
      "  batch 70 loss: 0.6821223303093575\n",
      "  batch 80 loss: 0.7669106407207437\n",
      "  batch 90 loss: 1.486610724264756\n",
      "  batch 100 loss: 0.6754706410225481\n",
      "  batch 110 loss: 0.8064021573518403\n",
      "  batch 120 loss: 0.625271744839847\n",
      "  batch 130 loss: 0.8393228893866762\n",
      "  batch 140 loss: 0.6890681966207921\n",
      "  batch 150 loss: 0.7099592961370945\n",
      "  batch 160 loss: 0.7647220207378268\n",
      "  batch 170 loss: 0.7031250028871\n",
      "  batch 180 loss: 0.9271494426415302\n",
      "  batch 190 loss: 1.1320882820058613\n",
      "LOSS train 1.1320882820058613 valid 1.9814733719536797\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.9640725569799542\n",
      "  batch 20 loss: 0.7555683568003587\n",
      "  batch 30 loss: 0.8858814658597112\n",
      "  batch 40 loss: 0.7103649249649606\n",
      "  batch 50 loss: 0.7104860553052277\n",
      "  batch 60 loss: 0.9015810061595403\n",
      "  batch 70 loss: 0.7099329568911343\n",
      "  batch 80 loss: 0.7209029415156692\n",
      "  batch 90 loss: 0.6973930110922083\n",
      "  batch 100 loss: 0.6885081669315696\n",
      "  batch 110 loss: 0.7397104901028797\n",
      "  batch 120 loss: 1.135812458442524\n",
      "  batch 130 loss: 0.8609826628584415\n",
      "  batch 140 loss: 0.9724810497835279\n",
      "  batch 150 loss: 0.5565535825560801\n",
      "  batch 160 loss: 0.8868857624824159\n",
      "  batch 170 loss: 0.541743504605256\n",
      "  batch 180 loss: 0.8935053791035898\n",
      "  batch 190 loss: 0.9194528114516288\n",
      "LOSS train 0.9194528114516288 valid 0.6384729723810242\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.7411242667585611\n",
      "  batch 20 loss: 0.8017815771338064\n",
      "  batch 30 loss: 0.7769954212708399\n",
      "  batch 40 loss: 0.9574099547462538\n",
      "  batch 50 loss: 0.8564035136252641\n",
      "  batch 60 loss: 0.7106518349261023\n",
      "  batch 70 loss: 0.6746432040352375\n",
      "  batch 80 loss: 1.049052421329543\n",
      "  batch 90 loss: 0.6022511902498081\n",
      "  batch 100 loss: 0.8527006762567908\n",
      "  batch 110 loss: 0.9804516303818673\n",
      "  batch 120 loss: 0.9136658466653899\n",
      "  batch 130 loss: 0.6786165905650705\n",
      "  batch 140 loss: 0.6320148545317352\n",
      "  batch 150 loss: 0.6500577282044105\n",
      "  batch 160 loss: 0.9239218810107559\n",
      "  batch 170 loss: 0.7049345876090228\n",
      "  batch 180 loss: 0.7792913122801111\n",
      "  batch 190 loss: 0.7424753533909098\n",
      "LOSS train 0.7424753533909098 valid 0.5980493183477591\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.6027631919132546\n",
      "  batch 20 loss: 0.7629053777549416\n",
      "  batch 30 loss: 0.9811321324901655\n",
      "  batch 40 loss: 0.9684054092736915\n",
      "  batch 50 loss: 0.8210959267802537\n",
      "  batch 60 loss: 0.7779753103852272\n",
      "  batch 70 loss: 0.6090311862295493\n",
      "  batch 80 loss: 0.6183861844940111\n",
      "  batch 90 loss: 0.9286693801637739\n",
      "  batch 100 loss: 0.8413536213745829\n",
      "  batch 110 loss: 0.8115270804613829\n",
      "  batch 120 loss: 0.6192246902734041\n",
      "  batch 130 loss: 0.7679524321807548\n",
      "  batch 140 loss: 0.6701379668258596\n",
      "  batch 150 loss: 0.7320608995156362\n",
      "  batch 160 loss: 0.6092775305733085\n",
      "  batch 170 loss: 0.6312540641520172\n",
      "  batch 180 loss: 0.9570949408225715\n",
      "  batch 190 loss: 0.6402654637931846\n",
      "LOSS train 0.6402654637931846 valid 0.872916075806223\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 1.5068634086288513\n",
      "  batch 20 loss: 0.5648293703736271\n",
      "  batch 30 loss: 0.6839347947156057\n",
      "  batch 40 loss: 0.7141343150055036\n",
      "  batch 50 loss: 0.7053210916172248\n",
      "  batch 60 loss: 0.7397375463158824\n",
      "  batch 70 loss: 0.8141120102722198\n",
      "  batch 80 loss: 0.7536894918303005\n",
      "  batch 90 loss: 0.6326019009109587\n",
      "  batch 100 loss: 0.8415799048263579\n",
      "  batch 110 loss: 0.7591606036759913\n",
      "  batch 120 loss: 0.7322456966212485\n",
      "  batch 130 loss: 1.1470112974755466\n",
      "  batch 140 loss: 0.8490958543494344\n",
      "  batch 150 loss: 0.7353345995536074\n",
      "  batch 160 loss: 0.7881932663032785\n",
      "  batch 170 loss: 0.5392909879796207\n",
      "  batch 180 loss: 0.45448400075547396\n",
      "  batch 190 loss: 0.5700709611643106\n",
      "LOSS train 0.5700709611643106 valid 0.6765550290404533\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.5118593148421496\n",
      "  batch 20 loss: 0.617963403754402\n",
      "  batch 30 loss: 0.6617337303701788\n",
      "  batch 40 loss: 0.5422796428494621\n",
      "  batch 50 loss: 0.7636863819323481\n",
      "  batch 60 loss: 0.6053212442668154\n",
      "  batch 70 loss: 0.6817230408836622\n",
      "  batch 80 loss: 0.6219494115095585\n",
      "  batch 90 loss: 0.9777915586484596\n",
      "  batch 100 loss: 1.026014152955031\n",
      "  batch 110 loss: 0.6868764544604347\n",
      "  batch 120 loss: 0.7110598646337166\n",
      "  batch 130 loss: 0.8852187308017164\n",
      "  batch 140 loss: 0.659656498034019\n",
      "  batch 150 loss: 0.8397502815234474\n",
      "  batch 160 loss: 0.8438945764675736\n",
      "  batch 170 loss: 0.8230477032484487\n",
      "  batch 180 loss: 1.1256211223080754\n",
      "  batch 190 loss: 0.5853525270882528\n",
      "LOSS train 0.5853525270882528 valid 1.1151073975578094\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.7479650938999839\n",
      "  batch 20 loss: 0.528782747394871\n",
      "  batch 30 loss: 0.6976720989623573\n",
      "  batch 40 loss: 0.7325753640267066\n",
      "  batch 50 loss: 0.6560728399024811\n",
      "  batch 60 loss: 0.7883343093679287\n",
      "  batch 70 loss: 0.5684316356317141\n",
      "  batch 80 loss: 0.5832342176610836\n",
      "  batch 90 loss: 0.872156142804306\n",
      "  batch 100 loss: 0.9911657622549683\n",
      "  batch 110 loss: 0.9463814947288484\n",
      "  batch 120 loss: 1.1033376317936927\n",
      "  batch 130 loss: 0.6473309272667394\n",
      "  batch 140 loss: 1.051359820330981\n",
      "  batch 150 loss: 0.7515776242362335\n",
      "  batch 160 loss: 0.4865607596933842\n",
      "  batch 170 loss: 0.6426332376897335\n",
      "  batch 180 loss: 0.8254381563980132\n",
      "  batch 190 loss: 0.6384217228740454\n",
      "LOSS train 0.6384217228740454 valid 0.5983317809161515\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.5064846741966903\n",
      "  batch 20 loss: 0.6996951180044562\n",
      "  batch 30 loss: 1.030823623482138\n",
      "  batch 40 loss: 0.8474371755262837\n",
      "  batch 50 loss: 0.591998219746165\n",
      "  batch 60 loss: 0.4557683786726557\n",
      "  batch 70 loss: 0.7098334053298458\n",
      "  batch 80 loss: 0.47834385125897827\n",
      "  batch 90 loss: 0.7063735093921423\n",
      "  batch 100 loss: 0.6537206768640317\n",
      "  batch 110 loss: 0.9101516023161821\n",
      "  batch 120 loss: 0.6475716418819502\n",
      "  batch 130 loss: 0.7508653722819872\n",
      "  batch 140 loss: 0.6836238970106934\n",
      "  batch 150 loss: 0.7185030588880181\n",
      "  batch 160 loss: 0.9452490813680925\n",
      "  batch 170 loss: 0.5343031437078025\n",
      "  batch 180 loss: 0.5900490007479675\n",
      "  batch 190 loss: 0.8986188100185245\n",
      "LOSS train 0.8986188100185245 valid 0.6320797802510265\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.6911998176015913\n",
      "  batch 20 loss: 0.8243471583817155\n",
      "  batch 30 loss: 0.6261557568854187\n",
      "  batch 40 loss: 1.1492733021907042\n",
      "  batch 50 loss: 0.6945766739430838\n",
      "  batch 60 loss: 0.5809075001161546\n",
      "  batch 70 loss: 0.6565747302956879\n",
      "  batch 80 loss: 0.5891580816009082\n",
      "  batch 90 loss: 0.4374303576303646\n",
      "  batch 100 loss: 0.7227021098136902\n",
      "  batch 110 loss: 0.846726976078935\n",
      "  batch 120 loss: 0.682807543920353\n",
      "  batch 130 loss: 0.873373614018783\n",
      "  batch 140 loss: 0.6899344670004212\n",
      "  batch 150 loss: 0.4457448194167227\n",
      "  batch 160 loss: 0.6370015378459357\n",
      "  batch 170 loss: 0.6826950558461249\n",
      "  batch 180 loss: 0.6406241787102772\n",
      "  batch 190 loss: 0.8671055128856097\n",
      "LOSS train 0.8671055128856097 valid 0.8344583340207091\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.6755153535981663\n",
      "  batch 20 loss: 0.6272392929779016\n",
      "  batch 30 loss: 1.0310394071100746\n",
      "  batch 40 loss: 0.965888689039275\n",
      "  batch 50 loss: 0.8106247964547947\n",
      "  batch 60 loss: 0.7216597331222147\n",
      "  batch 70 loss: 0.7727269199676812\n",
      "  batch 80 loss: 0.6561015989631415\n",
      "  batch 90 loss: 0.6283397116581909\n",
      "  batch 100 loss: 0.6295151549158617\n",
      "  batch 110 loss: 0.47004102110513485\n",
      "  batch 120 loss: 0.5584896825777832\n",
      "  batch 130 loss: 0.7629379332647659\n",
      "  batch 140 loss: 0.5835471186903305\n",
      "  batch 150 loss: 0.7778343389276415\n",
      "  batch 160 loss: 1.026664071995765\n",
      "  batch 170 loss: 0.6128272236674093\n",
      "  batch 180 loss: 0.6574554878054186\n",
      "  batch 190 loss: 0.7680050280177966\n",
      "LOSS train 0.7680050280177966 valid 0.5597186233633413\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.6129860163666307\n",
      "  batch 20 loss: 0.5074283784022555\n",
      "  batch 30 loss: 0.5574913873628248\n",
      "  batch 40 loss: 0.43501092958031223\n",
      "  batch 50 loss: 0.4819230665219948\n",
      "  batch 60 loss: 0.60165222947835\n",
      "  batch 70 loss: 0.7867435834661591\n",
      "  batch 80 loss: 0.6695958242518827\n",
      "  batch 90 loss: 0.6254082839237526\n",
      "  batch 100 loss: 0.7805084708204959\n",
      "  batch 110 loss: 1.0554802412167192\n",
      "  batch 120 loss: 0.5568878413178027\n",
      "  batch 130 loss: 0.477427962445654\n",
      "  batch 140 loss: 0.7923556903144344\n",
      "  batch 150 loss: 0.6930039257742464\n",
      "  batch 160 loss: 0.4661713958135806\n",
      "  batch 170 loss: 0.7010080115636811\n",
      "  batch 180 loss: 0.6676933724898845\n",
      "  batch 190 loss: 0.619208303099731\n",
      "LOSS train 0.619208303099731 valid 1.0777892472923178\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.9668530594033655\n",
      "  batch 20 loss: 0.47823086609714666\n",
      "  batch 30 loss: 0.7629323352026404\n",
      "  batch 40 loss: 0.6495686217094772\n",
      "  batch 50 loss: 0.8425043065100908\n",
      "  batch 60 loss: 0.4514574859349523\n",
      "  batch 70 loss: 0.6304148728057044\n",
      "  batch 80 loss: 0.674264481139835\n",
      "  batch 90 loss: 0.6858064717263914\n",
      "  batch 100 loss: 0.6629451114684344\n",
      "  batch 110 loss: 0.6142936691830982\n",
      "  batch 120 loss: 0.6487429552827961\n",
      "  batch 130 loss: 0.44695769902318716\n",
      "  batch 140 loss: 0.5751397238287609\n",
      "  batch 150 loss: 0.7105326371849514\n",
      "  batch 160 loss: 0.5868854592525168\n",
      "  batch 170 loss: 0.6381468143837992\n",
      "  batch 180 loss: 0.6278385038895067\n",
      "  batch 190 loss: 0.701194812914764\n",
      "LOSS train 0.701194812914764 valid 0.9354133480735936\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.5405963936005719\n",
      "  batch 20 loss: 0.5306996634288226\n",
      "  batch 30 loss: 0.57257305267849\n",
      "  batch 40 loss: 0.6105553159140982\n",
      "  batch 50 loss: 0.9350645039696246\n",
      "  batch 60 loss: 1.0294355239253492\n",
      "  batch 70 loss: 0.5100995468557812\n",
      "  batch 80 loss: 0.6020110129524255\n",
      "  batch 90 loss: 0.5169440543861128\n",
      "  batch 100 loss: 0.5798235373746138\n",
      "  batch 110 loss: 0.6978613281971775\n",
      "  batch 120 loss: 0.6159552403143607\n",
      "  batch 130 loss: 0.8284805776609574\n",
      "  batch 140 loss: 0.7589831329358276\n",
      "  batch 150 loss: 0.6985015335958451\n",
      "  batch 160 loss: 0.5475992777384817\n",
      "  batch 170 loss: 0.7454195516998879\n",
      "  batch 180 loss: 0.5871143218129873\n",
      "  batch 190 loss: 0.6969838298042305\n",
      "LOSS train 0.6969838298042305 valid 0.5247288347962193\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.6799075269082095\n",
      "  batch 20 loss: 0.5573736843187362\n",
      "  batch 30 loss: 0.5759278613419155\n",
      "  batch 40 loss: 0.652203156700125\n",
      "  batch 50 loss: 0.5277642382134218\n",
      "  batch 60 loss: 0.43596004001738037\n",
      "  batch 70 loss: 0.5534043350024149\n",
      "  batch 80 loss: 0.6785924953961512\n",
      "  batch 90 loss: 0.5932731061562663\n",
      "  batch 100 loss: 0.6731710564345121\n",
      "  batch 110 loss: 0.575623992702458\n",
      "  batch 120 loss: 0.6871977701550349\n",
      "  batch 130 loss: 1.2360405433690176\n",
      "  batch 140 loss: 0.47534117139293813\n",
      "  batch 150 loss: 0.6726107038091869\n",
      "  batch 160 loss: 0.6196134786689071\n",
      "  batch 170 loss: 0.8102996346715372\n",
      "  batch 180 loss: 0.7323581340489909\n",
      "  batch 190 loss: 0.7866782292607241\n",
      "LOSS train 0.7866782292607241 valid 0.5249050269963053\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.5564551276911516\n",
      "  batch 20 loss: 0.3887078496583854\n",
      "  batch 30 loss: 0.5282790294673759\n",
      "  batch 40 loss: 0.8336121515603736\n",
      "  batch 50 loss: 0.7222865207004361\n",
      "  batch 60 loss: 0.6034635760355741\n",
      "  batch 70 loss: 0.46894802825991067\n",
      "  batch 80 loss: 0.840484961878974\n",
      "  batch 90 loss: 0.8743239722098224\n",
      "  batch 100 loss: 0.6530835598241538\n",
      "  batch 110 loss: 0.43857538466108964\n",
      "  batch 120 loss: 0.5497370972996578\n",
      "  batch 130 loss: 0.46966447769082154\n",
      "  batch 140 loss: 0.6854362432437483\n",
      "  batch 150 loss: 0.852577997406479\n",
      "  batch 160 loss: 0.611970408311754\n",
      "  batch 170 loss: 0.4249323566269595\n",
      "  batch 180 loss: 0.597267613268923\n",
      "  batch 190 loss: 1.023682900723361\n",
      "LOSS train 1.023682900723361 valid 2.035824749099377\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.9201317276369082\n",
      "  batch 20 loss: 0.598890538001433\n",
      "  batch 30 loss: 0.6568074897397309\n",
      "  batch 40 loss: 0.6021167972590774\n",
      "  batch 50 loss: 0.5818744700518437\n",
      "  batch 60 loss: 0.6301940274308435\n",
      "  batch 70 loss: 0.41891987580165735\n",
      "  batch 80 loss: 0.7318360634322744\n",
      "  batch 90 loss: 0.6384139207424596\n",
      "  batch 100 loss: 0.41543296939344143\n",
      "  batch 110 loss: 0.46631940428051166\n",
      "  batch 120 loss: 0.4499723981600255\n",
      "  batch 130 loss: 0.9154984945183969\n",
      "  batch 140 loss: 0.6225174954743125\n",
      "  batch 150 loss: 0.6122464576270431\n",
      "  batch 160 loss: 0.637519718054682\n",
      "  batch 170 loss: 0.5363851983318455\n",
      "  batch 180 loss: 0.49771992353489625\n",
      "  batch 190 loss: 0.6011706998804585\n",
      "LOSS train 0.6011706998804585 valid 0.7467426774397982\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.6912283418816514\n",
      "  batch 20 loss: 0.4917649437964428\n",
      "  batch 30 loss: 0.645259679236915\n",
      "  batch 40 loss: 0.5525802999618463\n",
      "  batch 50 loss: 0.5432126011466607\n",
      "  batch 60 loss: 0.942860457196366\n",
      "  batch 70 loss: 0.6760158560005948\n",
      "  batch 80 loss: 0.47595909888914323\n",
      "  batch 90 loss: 0.5198764570406638\n",
      "  batch 100 loss: 0.5468310136348009\n",
      "  batch 110 loss: 0.4764720874838531\n",
      "  batch 120 loss: 0.4399002599529922\n",
      "  batch 130 loss: 0.773294501734199\n",
      "  batch 140 loss: 0.5210624471015762\n",
      "  batch 150 loss: 0.4171275281914859\n",
      "  batch 160 loss: 0.53228938155371\n",
      "  batch 170 loss: 0.6378625557292252\n",
      "  batch 180 loss: 0.633374730005744\n",
      "  batch 190 loss: 0.8705373583827167\n",
      "LOSS train 0.8705373583827167 valid 0.5216273044389732\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.4759332703630207\n",
      "  batch 20 loss: 0.8232103103073314\n",
      "  batch 30 loss: 0.7720082230516709\n",
      "  batch 40 loss: 0.8484682488226098\n",
      "  batch 50 loss: 0.6754455280024558\n",
      "  batch 60 loss: 0.5193832139018923\n",
      "  batch 70 loss: 0.6101023682625965\n",
      "  batch 80 loss: 0.40433915606699883\n",
      "  batch 90 loss: 0.25368814687244595\n",
      "  batch 100 loss: 0.43413582890061664\n",
      "  batch 110 loss: 0.5035771403752733\n",
      "  batch 120 loss: 0.6758510929474142\n",
      "  batch 130 loss: 0.5164986617863179\n",
      "  batch 140 loss: 0.5877090790076182\n",
      "  batch 150 loss: 0.6833513253019191\n",
      "  batch 160 loss: 0.8379057473546709\n",
      "  batch 170 loss: 0.3161763475392945\n",
      "  batch 180 loss: 0.5418059617339168\n",
      "  batch 190 loss: 0.5490917809394886\n",
      "LOSS train 0.5490917809394886 valid 0.5064684283169276\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.5637935642153025\n",
      "  batch 20 loss: 0.6794161720201373\n",
      "  batch 30 loss: 0.5957591271377168\n",
      "  batch 40 loss: 0.5548299718444468\n",
      "  batch 50 loss: 0.4427323244584841\n",
      "  batch 60 loss: 0.6560269695393799\n",
      "  batch 70 loss: 0.5945444841287099\n",
      "  batch 80 loss: 0.32753807710832916\n",
      "  batch 90 loss: 0.667727360338904\n",
      "  batch 100 loss: 0.9300007405807265\n",
      "  batch 110 loss: 0.5797132970154053\n",
      "  batch 120 loss: 0.6475814671663102\n",
      "  batch 130 loss: 0.6544194468006026\n",
      "  batch 140 loss: 0.3232554735688609\n",
      "  batch 150 loss: 0.7199134909373243\n",
      "  batch 160 loss: 0.6014839594921796\n",
      "  batch 170 loss: 0.6802606047305744\n",
      "  batch 180 loss: 0.34492691879568155\n",
      "  batch 190 loss: 0.7784737394773401\n",
      "LOSS train 0.7784737394773401 valid 0.9653266746351549\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.670479133236222\n",
      "  batch 20 loss: 0.6609819778648671\n",
      "  batch 30 loss: 0.4485451129818102\n",
      "  batch 40 loss: 0.7440847191406647\n",
      "  batch 50 loss: 0.6886120010341983\n",
      "  batch 60 loss: 0.9651544778811513\n",
      "  batch 70 loss: 0.541550581320189\n",
      "  batch 80 loss: 0.5657719728536904\n",
      "  batch 90 loss: 0.6621760811365676\n",
      "  batch 100 loss: 0.6585304704785813\n",
      "  batch 110 loss: 0.5318695077905431\n",
      "  batch 120 loss: 0.628920877398923\n",
      "  batch 130 loss: 0.7374548284686171\n",
      "  batch 140 loss: 0.4686287498625461\n",
      "  batch 150 loss: 0.4685189776078914\n",
      "  batch 160 loss: 0.40492184162139894\n",
      "  batch 170 loss: 0.44784614530944966\n",
      "  batch 180 loss: 0.36389476919866864\n",
      "  batch 190 loss: 0.7536070539732463\n",
      "LOSS train 0.7536070539732463 valid 0.4736081031867518\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.7386085287274909\n",
      "  batch 20 loss: 0.6029563650954515\n",
      "  batch 30 loss: 0.6091566900722682\n",
      "  batch 40 loss: 0.5236409322649707\n",
      "  batch 50 loss: 0.39130036783753896\n",
      "  batch 60 loss: 0.5889375375667442\n",
      "  batch 70 loss: 0.7908622313596425\n",
      "  batch 80 loss: 0.8987767005804926\n",
      "  batch 90 loss: 0.7199945583939552\n",
      "  batch 100 loss: 0.5293286805303069\n",
      "  batch 110 loss: 0.600267424545018\n",
      "  batch 120 loss: 0.4374367652300862\n",
      "  batch 130 loss: 0.8951783313590568\n",
      "  batch 140 loss: 0.4530802817549556\n",
      "  batch 150 loss: 0.4951065719331382\n",
      "  batch 160 loss: 0.5115699439775199\n",
      "  batch 170 loss: 0.5044690660375636\n",
      "  batch 180 loss: 0.46057861753215545\n",
      "  batch 190 loss: 0.5976650119439\n",
      "LOSS train 0.5976650119439 valid 0.6728116171583772\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.9467593616165686\n",
      "  batch 20 loss: 0.573639150877716\n",
      "  batch 30 loss: 0.40055157607421277\n",
      "  batch 40 loss: 0.9453180228963902\n",
      "  batch 50 loss: 0.41268196028540843\n",
      "  batch 60 loss: 0.31586035526997874\n",
      "  batch 70 loss: 0.5784354189643637\n",
      "  batch 80 loss: 0.49487441098317503\n",
      "  batch 90 loss: 0.5099097384212655\n",
      "  batch 100 loss: 0.7392461087481934\n",
      "  batch 110 loss: 0.6752735416172072\n",
      "  batch 120 loss: 0.47992936824884963\n",
      "  batch 130 loss: 0.3400967270543333\n",
      "  batch 140 loss: 0.6422445376403629\n",
      "  batch 150 loss: 0.5934953587217023\n",
      "  batch 160 loss: 0.3351761773141334\n",
      "  batch 170 loss: 0.7368618400301784\n",
      "  batch 180 loss: 0.5165865418850444\n",
      "  batch 190 loss: 0.8478589675505646\n",
      "LOSS train 0.8478589675505646 valid 0.5662213529285509\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.6902563305571675\n",
      "  batch 20 loss: 0.48566218685009516\n",
      "  batch 30 loss: 0.4653650877327891\n",
      "  batch 40 loss: 0.6338181182742119\n",
      "  batch 50 loss: 0.6250639732847049\n",
      "  batch 60 loss: 0.6011903499311302\n",
      "  batch 70 loss: 0.9695608193287626\n",
      "  batch 80 loss: 0.29374942821450534\n",
      "  batch 90 loss: 0.5244364846439566\n",
      "  batch 100 loss: 0.5053654033865314\n",
      "  batch 110 loss: 0.4152376067824662\n",
      "  batch 120 loss: 0.5840008496306837\n",
      "  batch 130 loss: 0.8312350139429328\n",
      "  batch 140 loss: 0.5830584940646076\n",
      "  batch 150 loss: 0.9217012014225474\n",
      "  batch 160 loss: 0.5560097192676039\n",
      "  batch 170 loss: 0.8290565772709669\n",
      "  batch 180 loss: 0.45932624720735477\n",
      "  batch 190 loss: 0.5674858547339682\n",
      "LOSS train 0.5674858547339682 valid 0.7630403378742556\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.5410111739052809\n",
      "  batch 20 loss: 0.7697980664728675\n",
      "  batch 30 loss: 0.6332236150512471\n",
      "  batch 40 loss: 0.6025063430512091\n",
      "  batch 50 loss: 0.2719560027006082\n",
      "  batch 60 loss: 0.6347679042679374\n",
      "  batch 70 loss: 0.19453938457299955\n",
      "  batch 80 loss: 0.5118351465382147\n",
      "  batch 90 loss: 0.6664443614921766\n",
      "  batch 100 loss: 0.6473825476656202\n",
      "  batch 110 loss: 0.628242113586748\n",
      "  batch 120 loss: 0.442508835287299\n",
      "  batch 130 loss: 0.6940146910492331\n",
      "  batch 140 loss: 0.41932807317352855\n",
      "  batch 150 loss: 0.48261655835522105\n",
      "  batch 160 loss: 0.7041428417986026\n",
      "  batch 170 loss: 0.6546251761741587\n",
      "  batch 180 loss: 0.475991734379204\n",
      "  batch 190 loss: 0.4174007812340278\n",
      "LOSS train 0.4174007812340278 valid 0.4901077790050178\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 0.4244323792634532\n",
      "  batch 20 loss: 0.46366147901280785\n",
      "  batch 30 loss: 0.7754517590510659\n",
      "  batch 40 loss: 0.6519154618377797\n",
      "  batch 50 loss: 0.4650817563844612\n",
      "  batch 60 loss: 0.29265124330704567\n",
      "  batch 70 loss: 0.4574188081838656\n",
      "  batch 80 loss: 0.4560236370074563\n",
      "  batch 90 loss: 0.6707503082929179\n",
      "  batch 100 loss: 0.619577367277816\n",
      "  batch 110 loss: 0.46063969033421015\n",
      "  batch 120 loss: 0.5141002302800188\n",
      "  batch 130 loss: 0.6770599470517482\n",
      "  batch 140 loss: 1.019259950623382\n",
      "  batch 150 loss: 0.4249658050204744\n",
      "  batch 160 loss: 0.5334355601342395\n",
      "  batch 170 loss: 0.4245033573184628\n",
      "  batch 180 loss: 0.25806398868444375\n",
      "  batch 190 loss: 0.4934637803598889\n",
      "LOSS train 0.4934637803598889 valid 0.9183750922749869\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 0.6410840061806085\n",
      "  batch 20 loss: 0.9516255681868643\n",
      "  batch 30 loss: 0.5760352140641771\n",
      "  batch 40 loss: 0.2728716194891604\n",
      "  batch 50 loss: 0.4005979989815387\n",
      "  batch 60 loss: 0.3787303929158952\n",
      "  batch 70 loss: 0.5489260781905614\n",
      "  batch 80 loss: 0.35621739678899755\n",
      "  batch 90 loss: 0.4766926486045122\n",
      "  batch 100 loss: 0.8074292001343565\n",
      "  batch 110 loss: 0.40486824897816404\n",
      "  batch 120 loss: 0.534298008918995\n",
      "  batch 130 loss: 0.479602488965611\n",
      "  batch 140 loss: 0.7690093148732557\n",
      "  batch 150 loss: 0.4431539412587881\n",
      "  batch 160 loss: 0.5265941472083796\n",
      "  batch 170 loss: 0.39452545137610284\n",
      "  batch 180 loss: 0.61903722519055\n",
      "  batch 190 loss: 0.4441160547074105\n",
      "LOSS train 0.4441160547074105 valid 0.5058189110865127\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 0.6300971732009202\n",
      "  batch 20 loss: 0.6122041648079175\n",
      "  batch 30 loss: 0.41179578939918426\n",
      "  batch 40 loss: 0.42656872253865\n",
      "  batch 50 loss: 0.5807101868384053\n",
      "  batch 60 loss: 0.5301371397799812\n",
      "  batch 70 loss: 0.4150243153329939\n",
      "  batch 80 loss: 0.7402208761312068\n",
      "  batch 90 loss: 0.3832937514191144\n",
      "  batch 100 loss: 0.4573622527066618\n",
      "  batch 110 loss: 0.4501863228622824\n",
      "  batch 120 loss: 0.38514065875206144\n",
      "  batch 130 loss: 0.44822376433876343\n",
      "  batch 140 loss: 0.6526673761676648\n",
      "  batch 150 loss: 0.5258916448921809\n",
      "  batch 160 loss: 0.3754693678358308\n",
      "  batch 170 loss: 0.6727593254414387\n",
      "  batch 180 loss: 0.7797427212208277\n",
      "  batch 190 loss: 0.387551523768343\n",
      "LOSS train 0.387551523768343 valid 0.4493476399005512\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 0.4929813151495182\n",
      "  batch 20 loss: 0.633772189845331\n",
      "  batch 30 loss: 0.611682148433465\n",
      "  batch 40 loss: 0.2863190688229224\n",
      "  batch 50 loss: 0.3993991144190659\n",
      "  batch 60 loss: 0.8815577995992498\n",
      "  batch 70 loss: 0.4135183485894231\n",
      "  batch 80 loss: 0.49552604809287004\n",
      "  batch 90 loss: 0.7586390151962406\n",
      "  batch 100 loss: 0.43552725167537576\n",
      "  batch 110 loss: 0.4490311685192864\n",
      "  batch 120 loss: 0.8317498491145671\n",
      "  batch 130 loss: 0.4454860119149089\n",
      "  batch 140 loss: 0.4738784285960719\n",
      "  batch 150 loss: 0.6028493621619418\n",
      "  batch 160 loss: 0.43915433773363477\n",
      "  batch 170 loss: 0.6246046775369904\n",
      "  batch 180 loss: 0.29646931246097663\n",
      "  batch 190 loss: 0.5789864130783826\n",
      "LOSS train 0.5789864130783826 valid 0.5422382370813396\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 0.5729374565038597\n",
      "  batch 20 loss: 0.46414523174753414\n",
      "  batch 30 loss: 0.5139818188239588\n",
      "  batch 40 loss: 0.5437854956137016\n",
      "  batch 50 loss: 0.6058536392760289\n",
      "  batch 60 loss: 0.5260595211875625\n",
      "  batch 70 loss: 0.4492260065591836\n",
      "  batch 80 loss: 0.23733997092494974\n",
      "  batch 90 loss: 0.6187816661782563\n",
      "  batch 100 loss: 0.40061869818309787\n",
      "  batch 110 loss: 0.6330736158655782\n",
      "  batch 120 loss: 0.2765439402195625\n",
      "  batch 130 loss: 0.436022179601423\n",
      "  batch 140 loss: 0.32687752963684036\n",
      "  batch 150 loss: 0.48104676997754725\n",
      "  batch 160 loss: 0.5408963609312195\n",
      "  batch 170 loss: 0.6483822624060849\n",
      "  batch 180 loss: 0.64652502797544\n",
      "  batch 190 loss: 0.9910850673055392\n",
      "LOSS train 0.9910850673055392 valid 0.3707844886785796\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 0.23557343872089404\n",
      "  batch 20 loss: 0.4660448323658784\n",
      "  batch 30 loss: 0.9245484696191852\n",
      "  batch 40 loss: 0.45446619993672355\n",
      "  batch 50 loss: 0.5693347429303686\n",
      "  batch 60 loss: 0.5402083476539701\n",
      "  batch 70 loss: 0.28673617722815836\n",
      "  batch 80 loss: 0.4333519729145337\n",
      "  batch 90 loss: 0.5462587561341934\n",
      "  batch 100 loss: 0.49882820138009265\n",
      "  batch 110 loss: 0.6261346745592163\n",
      "  batch 120 loss: 0.48552873523149176\n",
      "  batch 130 loss: 0.39900437210453676\n",
      "  batch 140 loss: 0.642037707741838\n",
      "  batch 150 loss: 0.19032516017468878\n",
      "  batch 160 loss: 0.9536612418363802\n",
      "  batch 170 loss: 0.4358010558062233\n",
      "  batch 180 loss: 0.8114773948560469\n",
      "  batch 190 loss: 0.649683257273864\n",
      "LOSS train 0.649683257273864 valid 1.052264646393059\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 0.7153570043854416\n",
      "  batch 20 loss: 0.6448052848223597\n",
      "  batch 30 loss: 0.425679426593706\n",
      "  batch 40 loss: 0.3118043373833643\n",
      "  batch 50 loss: 0.36206309686749594\n",
      "  batch 60 loss: 0.6027978718979284\n",
      "  batch 70 loss: 0.5736758492304943\n",
      "  batch 80 loss: 0.48733638536359647\n",
      "  batch 90 loss: 0.359127509665268\n",
      "  batch 100 loss: 0.5404392633325188\n",
      "  batch 110 loss: 0.44791174099082126\n",
      "  batch 120 loss: 0.26041061535506743\n",
      "  batch 130 loss: 0.4757393931315164\n",
      "  batch 140 loss: 0.5488983982097124\n",
      "  batch 150 loss: 0.5818375069065951\n",
      "  batch 160 loss: 0.610434751881985\n",
      "  batch 170 loss: 0.30267664797938776\n",
      "  batch 180 loss: 0.3956738120410591\n",
      "  batch 190 loss: 0.6717416169238277\n",
      "LOSS train 0.6717416169238277 valid 0.4824884643556941\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 0.2898418289667461\n",
      "  batch 20 loss: 0.556675572891254\n",
      "  batch 30 loss: 0.498462506634678\n",
      "  batch 40 loss: 0.6884183527203277\n",
      "  batch 50 loss: 0.353372095755185\n",
      "  batch 60 loss: 0.10389103583984252\n",
      "  batch 70 loss: 0.4573661628775881\n",
      "  batch 80 loss: 0.3721026420284034\n",
      "  batch 90 loss: 0.4790352691714361\n",
      "  batch 100 loss: 0.6283643589064013\n",
      "  batch 110 loss: 0.5773643568420084\n",
      "  batch 120 loss: 0.3276346469472628\n",
      "  batch 130 loss: 0.693870676564984\n",
      "  batch 140 loss: 0.39053270815347785\n",
      "  batch 150 loss: 0.3446723025088431\n",
      "  batch 160 loss: 0.867308594356291\n",
      "  batch 170 loss: 0.5510095561650814\n",
      "  batch 180 loss: 0.5504043457563966\n",
      "  batch 190 loss: 0.8297217239451129\n",
      "LOSS train 0.8297217239451129 valid 1.0559742541204638\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 0.6564365796482889\n",
      "  batch 20 loss: 0.373342012410285\n",
      "  batch 30 loss: 0.4718586939998204\n",
      "  batch 40 loss: 0.4044410185975721\n",
      "  batch 50 loss: 0.3508415605640039\n",
      "  batch 60 loss: 0.4856809374818113\n",
      "  batch 70 loss: 0.6048347593517974\n",
      "  batch 80 loss: 0.2611483110813424\n",
      "  batch 90 loss: 0.2258583926508436\n",
      "  batch 100 loss: 0.5006245291660889\n",
      "  batch 110 loss: 0.7291212922951672\n",
      "  batch 120 loss: 0.38090142685396133\n",
      "  batch 130 loss: 0.5513692941807676\n",
      "  batch 140 loss: 0.8857911036815495\n",
      "  batch 150 loss: 0.4302644593743025\n",
      "  batch 160 loss: 0.4676405658770818\n",
      "  batch 170 loss: 0.33764981031708885\n",
      "  batch 180 loss: 0.5424901366408449\n",
      "  batch 190 loss: 0.5089544131158619\n",
      "LOSS train 0.5089544131158619 valid 0.6705847044962493\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 0.5875978056428721\n",
      "  batch 20 loss: 0.7233968420187011\n",
      "  batch 30 loss: 0.5021352320283767\n",
      "  batch 40 loss: 0.5308620795214665\n",
      "  batch 50 loss: 0.1382077623566147\n",
      "  batch 60 loss: 0.3159027945934213\n",
      "  batch 70 loss: 0.2364253297331743\n",
      "  batch 80 loss: 0.3002992892943439\n",
      "  batch 90 loss: 0.26458423242438583\n",
      "  batch 100 loss: 0.7121953818474139\n",
      "  batch 110 loss: 0.6815781018973212\n",
      "  batch 120 loss: 0.3586718769569416\n",
      "  batch 130 loss: 0.2802462183230091\n",
      "  batch 140 loss: 0.9134320099896286\n",
      "  batch 150 loss: 0.6463481629318266\n",
      "  batch 160 loss: 0.4735674318028032\n",
      "  batch 170 loss: 0.360125214481377\n",
      "  batch 180 loss: 0.6035625947610243\n",
      "  batch 190 loss: 0.7058203603955917\n",
      "LOSS train 0.7058203603955917 valid 0.37353248818740564\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 0.41137934535800014\n",
      "  batch 20 loss: 0.2997337974375114\n",
      "  batch 30 loss: 0.5601918038810254\n",
      "  batch 40 loss: 0.6003996254265076\n",
      "  batch 50 loss: 0.5632449716795236\n",
      "  batch 60 loss: 0.36225950001680757\n",
      "  batch 70 loss: 0.6164307034341618\n",
      "  batch 80 loss: 0.5794965723267523\n",
      "  batch 90 loss: 0.37341159154602793\n",
      "  batch 100 loss: 0.4891022016381612\n",
      "  batch 110 loss: 0.2763156934292056\n",
      "  batch 120 loss: 0.4229101021483075\n",
      "  batch 130 loss: 0.5957713390060235\n",
      "  batch 140 loss: 0.5335044095525519\n",
      "  batch 150 loss: 0.8889079234431847\n",
      "  batch 160 loss: 0.35377032809337833\n",
      "  batch 170 loss: 0.3464536936735385\n",
      "  batch 180 loss: 0.5456641575379763\n",
      "  batch 190 loss: 0.375786422501551\n",
      "LOSS train 0.375786422501551 valid 0.6165489233970174\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 0.4945824767608428\n",
      "  batch 20 loss: 0.6528229956282303\n",
      "  batch 30 loss: 0.6607896916684695\n",
      "  batch 40 loss: 0.5397427035466535\n",
      "  batch 50 loss: 0.29114421375561506\n",
      "  batch 60 loss: 0.49690410239272753\n",
      "  batch 70 loss: 0.4083122795273084\n",
      "  batch 80 loss: 0.5339712558983593\n",
      "  batch 90 loss: 0.475121740315808\n",
      "  batch 100 loss: 0.2904571752937045\n",
      "  batch 110 loss: 0.5587671780027449\n",
      "  batch 120 loss: 0.48824111868743786\n",
      "  batch 130 loss: 0.5734800001853728\n",
      "  batch 140 loss: 0.43708553063916045\n",
      "  batch 150 loss: 0.3065499268384883\n",
      "  batch 160 loss: 0.24102123637567274\n",
      "  batch 170 loss: 0.4231128834493575\n",
      "  batch 180 loss: 0.36694032688683365\n",
      "  batch 190 loss: 0.453825916632195\n",
      "LOSS train 0.453825916632195 valid 2.016950482443066\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 0.9555848458214313\n",
      "  batch 20 loss: 0.58423740673461\n",
      "  batch 30 loss: 0.46630146317183974\n",
      "  batch 40 loss: 0.408573299474665\n",
      "  batch 50 loss: 0.3221270296431612\n",
      "  batch 60 loss: 0.34765031111310235\n",
      "  batch 70 loss: 0.22484433915815316\n",
      "  batch 80 loss: 0.7356057761571719\n",
      "  batch 90 loss: 0.38908533438370796\n",
      "  batch 100 loss: 0.393529939220025\n",
      "  batch 110 loss: 0.6148229156671732\n",
      "  batch 120 loss: 0.8812904185324442\n",
      "  batch 130 loss: 0.6717476874619024\n",
      "  batch 140 loss: 0.5684933208336588\n",
      "  batch 150 loss: 0.5633345446636667\n",
      "  batch 160 loss: 0.8007003533595707\n",
      "  batch 170 loss: 0.7387773566122633\n",
      "  batch 180 loss: 0.29059491877269467\n",
      "  batch 190 loss: 0.30622865876648575\n",
      "LOSS train 0.30622865876648575 valid 0.41978325227682944\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 0.6016931323625613\n",
      "  batch 20 loss: 0.28867275138618426\n",
      "  batch 30 loss: 0.4034849824849516\n",
      "  batch 40 loss: 0.46228143842017744\n",
      "  batch 50 loss: 0.38243458637371075\n",
      "  batch 60 loss: 0.34655529888914316\n",
      "  batch 70 loss: 0.33887509036794655\n",
      "  batch 80 loss: 0.7178075732197613\n",
      "  batch 90 loss: 0.44805864457593997\n",
      "  batch 100 loss: 0.2983527799471631\n",
      "  batch 110 loss: 0.40718098857032603\n",
      "  batch 120 loss: 0.23163095734780653\n",
      "  batch 130 loss: 0.293738417331042\n",
      "  batch 140 loss: 0.6582207407569513\n",
      "  batch 150 loss: 0.5676960617565783\n",
      "  batch 160 loss: 0.48374052010476587\n",
      "  batch 170 loss: 0.49699918818951117\n",
      "  batch 180 loss: 0.5930132791982032\n",
      "  batch 190 loss: 0.38148740649048707\n",
      "LOSS train 0.38148740649048707 valid 0.3865409952764495\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 0.40901497534359804\n",
      "  batch 20 loss: 0.5279992935888004\n",
      "  batch 30 loss: 0.38314980980649127\n",
      "  batch 40 loss: 0.45951343982160325\n",
      "  batch 50 loss: 0.4441714869812131\n",
      "  batch 60 loss: 0.3777849340171088\n",
      "  batch 70 loss: 0.607274679816328\n",
      "  batch 80 loss: 0.5951937955105677\n",
      "  batch 90 loss: 0.8035847527818987\n",
      "  batch 100 loss: 0.3913083076797193\n",
      "  batch 110 loss: 0.5234959033841733\n",
      "  batch 120 loss: 0.6605055859836284\n",
      "  batch 130 loss: 0.3511419244110584\n",
      "  batch 140 loss: 0.47583528875256886\n",
      "  batch 150 loss: 0.33758142129227053\n",
      "  batch 160 loss: 0.49682941291976024\n",
      "  batch 170 loss: 0.44847716464428233\n",
      "  batch 180 loss: 0.5192453736148309\n",
      "  batch 190 loss: 0.17672105475212446\n",
      "LOSS train 0.17672105475212446 valid 0.5075355326728478\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 0.5178255873091985\n",
      "  batch 20 loss: 0.490384596286458\n",
      "  batch 30 loss: 0.3947079000296071\n",
      "  batch 40 loss: 0.23225997872068546\n",
      "  batch 50 loss: 0.40798722464824094\n",
      "  batch 60 loss: 0.2216421142962645\n",
      "  batch 70 loss: 0.6059613245888613\n",
      "  batch 80 loss: 0.252326445345534\n",
      "  batch 90 loss: 0.3993565157055855\n",
      "  batch 100 loss: 0.8603287243517116\n",
      "  batch 110 loss: 0.4880560662531934\n",
      "  batch 120 loss: 0.31682539960020223\n",
      "  batch 130 loss: 0.7444228116699378\n",
      "  batch 140 loss: 0.3093462975230068\n",
      "  batch 150 loss: 0.4605501594895031\n",
      "  batch 160 loss: 0.45442209258908406\n",
      "  batch 170 loss: 0.29720678744779433\n",
      "  batch 180 loss: 0.485786511504557\n",
      "  batch 190 loss: 0.43330809768740436\n",
      "LOSS train 0.43330809768740436 valid 0.530560897146083\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 0.5023553820035886\n",
      "  batch 20 loss: 0.43989204480312766\n",
      "  batch 30 loss: 0.45947508651879615\n",
      "  batch 40 loss: 0.5121390353306197\n",
      "  batch 50 loss: 0.3732499453239143\n",
      "  batch 60 loss: 0.44402009670156983\n",
      "  batch 70 loss: 0.5351772712456295\n",
      "  batch 80 loss: 0.3820365446968935\n",
      "  batch 90 loss: 0.26821178788086397\n",
      "  batch 100 loss: 0.6669102649437264\n",
      "  batch 110 loss: 0.30019291041098767\n",
      "  batch 120 loss: 0.422811193074449\n",
      "  batch 130 loss: 0.6708927045052405\n",
      "  batch 140 loss: 0.6042150948895142\n",
      "  batch 150 loss: 0.30806004721671343\n",
      "  batch 160 loss: 0.8665746159094851\n",
      "  batch 170 loss: 0.38198812170594465\n",
      "  batch 180 loss: 0.2886687264224747\n",
      "  batch 190 loss: 0.31879754160181617\n",
      "LOSS train 0.31879754160181617 valid 0.38031041667608617\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 0.17021732890279964\n",
      "  batch 20 loss: 0.3211953007557895\n",
      "  batch 30 loss: 0.46074637905694543\n",
      "  batch 40 loss: 0.787674479110865\n",
      "  batch 50 loss: 0.30513818993931635\n",
      "  batch 60 loss: 0.506086021871306\n",
      "  batch 70 loss: 0.4146717477589846\n",
      "  batch 80 loss: 0.3462474400730571\n",
      "  batch 90 loss: 0.29968554191145813\n",
      "  batch 100 loss: 0.34303121437405937\n",
      "  batch 110 loss: 0.44400645298301245\n",
      "  batch 120 loss: 0.6982164647393801\n",
      "  batch 130 loss: 0.4827234096592292\n",
      "  batch 140 loss: 0.4581756655708887\n",
      "  batch 150 loss: 0.43957145237363876\n",
      "  batch 160 loss: 0.5401848284556763\n",
      "  batch 170 loss: 0.5364263848634436\n",
      "  batch 180 loss: 0.34703450673259795\n",
      "  batch 190 loss: 0.2941428726961021\n",
      "LOSS train 0.2941428726961021 valid 0.75716014132829\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 0.8780907760665286\n",
      "  batch 20 loss: 0.42196928347839274\n",
      "  batch 30 loss: 0.6165207456768258\n",
      "  batch 40 loss: 0.44470923732733353\n",
      "  batch 50 loss: 0.5406309266225435\n",
      "  batch 60 loss: 0.5994977518304949\n",
      "  batch 70 loss: 0.61585100608645\n",
      "  batch 80 loss: 0.3740098965499783\n",
      "  batch 90 loss: 0.6362513607542496\n",
      "  batch 100 loss: 0.30125627145607725\n",
      "  batch 110 loss: 0.4993835596513236\n",
      "  batch 120 loss: 0.22749006293088314\n",
      "  batch 130 loss: 0.5219291014713235\n",
      "  batch 140 loss: 0.6205636768194381\n",
      "  batch 150 loss: 0.4435061921481974\n",
      "  batch 160 loss: 0.3427005524048582\n",
      "  batch 170 loss: 0.47462840484877233\n",
      "  batch 180 loss: 0.44984930658247324\n",
      "  batch 190 loss: 0.5566708591068164\n",
      "LOSS train 0.5566708591068164 valid 0.3565703636348376\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 0.3301618318539113\n",
      "  batch 20 loss: 0.15978918375585635\n",
      "  batch 30 loss: 0.3059199571609497\n",
      "  batch 40 loss: 0.7220185195998056\n",
      "  batch 50 loss: 0.449259340739809\n",
      "  batch 60 loss: 0.41778475185856223\n",
      "  batch 70 loss: 0.3600249908202386\n",
      "  batch 80 loss: 0.7476306265292806\n",
      "  batch 90 loss: 0.29806314508314247\n",
      "  batch 100 loss: 0.4604646617663093\n",
      "  batch 110 loss: 0.32201738946605474\n",
      "  batch 120 loss: 0.5739724997605663\n",
      "  batch 130 loss: 0.6051437568850815\n",
      "  batch 140 loss: 0.31748195563850456\n",
      "  batch 150 loss: 0.3246298290847335\n",
      "  batch 160 loss: 0.43734843415440994\n",
      "  batch 170 loss: 0.32863613568770234\n",
      "  batch 180 loss: 0.4768940816138638\n",
      "  batch 190 loss: 0.309777692813077\n",
      "LOSS train 0.309777692813077 valid 0.7087423962356973\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 0.5227956416463713\n",
      "  batch 20 loss: 0.4487508309015539\n",
      "  batch 30 loss: 0.5069337333552539\n",
      "  batch 40 loss: 0.37666117523476716\n",
      "  batch 50 loss: 0.3081736682084738\n",
      "  batch 60 loss: 0.5598195871512871\n",
      "  batch 70 loss: 0.5996748870122246\n",
      "  batch 80 loss: 0.4067366092902375\n",
      "  batch 90 loss: 0.5827962613198906\n",
      "  batch 100 loss: 0.3462163041695021\n",
      "  batch 110 loss: 0.21517051733389964\n",
      "  batch 120 loss: 0.4192947515723063\n",
      "  batch 130 loss: 0.2796087919472484\n",
      "  batch 140 loss: 0.46026961515890435\n",
      "  batch 150 loss: 0.2847606506082229\n",
      "  batch 160 loss: 0.4303880688472418\n",
      "  batch 170 loss: 0.5637802694691345\n",
      "  batch 180 loss: 0.4667957163474057\n",
      "  batch 190 loss: 0.6867917646857677\n",
      "LOSS train 0.6867917646857677 valid 0.9264402176463941\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 0.3283944339855225\n",
      "  batch 20 loss: 0.33917573114158583\n",
      "  batch 30 loss: 0.3468823427800089\n",
      "  batch 40 loss: 0.36211563695396765\n",
      "  batch 50 loss: 0.38925850007653934\n",
      "  batch 60 loss: 0.5028541973617393\n",
      "  batch 70 loss: 0.24199672359973193\n",
      "  batch 80 loss: 0.3489697355777025\n",
      "  batch 90 loss: 0.4173789981927257\n",
      "  batch 100 loss: 0.6478436113189673\n",
      "  batch 110 loss: 0.7240382689342368\n",
      "  batch 120 loss: 0.2388418293761788\n",
      "  batch 130 loss: 0.3293007664382458\n",
      "  batch 140 loss: 0.28104996594483966\n",
      "  batch 150 loss: 0.565977206005482\n",
      "  batch 160 loss: 0.5763134456065018\n",
      "  batch 170 loss: 0.2317480331817933\n",
      "  batch 180 loss: 0.3516754473093897\n",
      "  batch 190 loss: 0.3970404278144997\n",
      "LOSS train 0.3970404278144997 valid 0.3691942455022796\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 0.41046394491568206\n",
      "  batch 20 loss: 0.329144004965201\n",
      "  batch 30 loss: 0.1275907061295584\n",
      "  batch 40 loss: 0.39734777112607844\n",
      "  batch 50 loss: 0.9216697381751147\n",
      "  batch 60 loss: 0.9947010472922557\n",
      "  batch 70 loss: 0.36230562910204756\n",
      "  batch 80 loss: 0.37829182705027053\n",
      "  batch 90 loss: 0.6661933933239197\n",
      "  batch 100 loss: 0.5567172298324294\n",
      "  batch 110 loss: 0.19564046493323986\n",
      "  batch 120 loss: 0.5539472295196901\n",
      "  batch 130 loss: 0.34983665302861483\n",
      "  batch 140 loss: 0.31591415082803\n",
      "  batch 150 loss: 0.4112687229644507\n",
      "  batch 160 loss: 0.17284064745763317\n",
      "  batch 170 loss: 0.37060039915377274\n",
      "  batch 180 loss: 0.7907841929991264\n",
      "  batch 190 loss: 0.2905764740884479\n",
      "LOSS train 0.2905764740884479 valid 0.3168538065419386\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 0.52532974233618\n",
      "  batch 20 loss: 0.36970002568559723\n",
      "  batch 30 loss: 0.432962849951582\n",
      "  batch 40 loss: 0.13297775338287465\n",
      "  batch 50 loss: 0.5595994350733235\n",
      "  batch 60 loss: 0.542455340444576\n",
      "  batch 70 loss: 0.1997798866868834\n",
      "  batch 80 loss: 0.3674381217002519\n",
      "  batch 90 loss: 0.6337924187013414\n",
      "  batch 100 loss: 0.2599119682796299\n",
      "  batch 110 loss: 0.7301676390939974\n",
      "  batch 120 loss: 0.3280803360277787\n",
      "  batch 130 loss: 0.634920184945804\n",
      "  batch 140 loss: 0.3155120922441711\n",
      "  batch 150 loss: 0.4183737006911542\n",
      "  batch 160 loss: 0.44749893631087617\n",
      "  batch 170 loss: 0.34510359904670623\n",
      "  batch 180 loss: 0.7023065553279594\n",
      "  batch 190 loss: 0.3695033325755503\n",
      "LOSS train 0.3695033325755503 valid 0.3747358528484116\n",
      "EPOCH 101:\n",
      "  batch 10 loss: 0.32209848805796354\n",
      "  batch 20 loss: 0.4014298078851425\n",
      "  batch 30 loss: 0.38860393508221025\n",
      "  batch 40 loss: 0.794015944149578\n",
      "  batch 50 loss: 0.6086375512910308\n",
      "  batch 60 loss: 0.3436722245038254\n",
      "  batch 70 loss: 0.43187338413554244\n",
      "  batch 80 loss: 0.3248065814259462\n",
      "  batch 90 loss: 0.32802844947436827\n",
      "  batch 100 loss: 0.40952569472719913\n",
      "  batch 110 loss: 0.33007577514945297\n",
      "  batch 120 loss: 0.4813872229526169\n",
      "  batch 130 loss: 0.2686186238657683\n",
      "  batch 140 loss: 0.17069417462626008\n",
      "  batch 150 loss: 0.21408619152498432\n",
      "  batch 160 loss: 0.4681818659242708\n",
      "  batch 170 loss: 0.2602485482289921\n",
      "  batch 180 loss: 0.4785898146306863\n",
      "  batch 190 loss: 0.6455010124947875\n",
      "LOSS train 0.6455010124947875 valid 0.3801065851861121\n",
      "EPOCH 102:\n",
      "  batch 10 loss: 0.4856478206551401\n",
      "  batch 20 loss: 0.28211242256802505\n",
      "  batch 30 loss: 0.3133526462188456\n",
      "  batch 40 loss: 0.46339551687124186\n",
      "  batch 50 loss: 0.40003325911820864\n",
      "  batch 60 loss: 0.6170128042140277\n",
      "  batch 70 loss: 0.23313738144061064\n",
      "  batch 80 loss: 0.26512545768055135\n",
      "  batch 90 loss: 0.6944961876492016\n",
      "  batch 100 loss: 0.4578392465540674\n",
      "  batch 110 loss: 0.28925060781184586\n",
      "  batch 120 loss: 0.36565439606347355\n",
      "  batch 130 loss: 0.29196045857970604\n",
      "  batch 140 loss: 0.49713483118684965\n",
      "  batch 150 loss: 0.5190922567955567\n",
      "  batch 160 loss: 0.1597619797626976\n",
      "  batch 170 loss: 0.20667067991162186\n",
      "  batch 180 loss: 0.25715792497067014\n",
      "  batch 190 loss: 0.5669293039711192\n",
      "LOSS train 0.5669293039711192 valid 0.814517614517778\n",
      "EPOCH 103:\n",
      "  batch 10 loss: 0.4062595397590485\n",
      "  batch 20 loss: 0.27069249260239303\n",
      "  batch 30 loss: 0.38661376809468495\n",
      "  batch 40 loss: 0.31563053113641215\n",
      "  batch 50 loss: 0.3670424389601976\n",
      "  batch 60 loss: 0.6520989331795135\n",
      "  batch 70 loss: 0.43218597016530114\n",
      "  batch 80 loss: 0.4799833332770504\n",
      "  batch 90 loss: 0.6246856009995099\n",
      "  batch 100 loss: 0.3484075066488003\n",
      "  batch 110 loss: 0.3242021911544725\n",
      "  batch 120 loss: 0.49863579028751703\n",
      "  batch 130 loss: 0.43399977608351037\n",
      "  batch 140 loss: 0.4812506322574336\n",
      "  batch 150 loss: 0.22968950221547857\n",
      "  batch 160 loss: 0.44920618025353176\n",
      "  batch 170 loss: 0.40070187589735723\n",
      "  batch 180 loss: 0.3133494641384459\n",
      "  batch 190 loss: 0.4711629502577125\n",
      "LOSS train 0.4711629502577125 valid 0.3104374524836879\n",
      "EPOCH 104:\n",
      "  batch 10 loss: 0.18168409020872786\n",
      "  batch 20 loss: 0.3073363413626794\n",
      "  batch 30 loss: 0.46336315552034646\n",
      "  batch 40 loss: 0.6256980257341638\n",
      "  batch 50 loss: 0.38109036052483136\n",
      "  batch 60 loss: 0.4368061949935509\n",
      "  batch 70 loss: 0.20158509161847177\n",
      "  batch 80 loss: 0.4442790233413689\n",
      "  batch 90 loss: 0.4453391926021141\n",
      "  batch 100 loss: 0.4342055253437138\n",
      "  batch 110 loss: 0.4151927152997814\n",
      "  batch 120 loss: 0.37410731158015553\n",
      "  batch 130 loss: 0.5517385787534295\n",
      "  batch 140 loss: 0.333925517037278\n",
      "  batch 150 loss: 0.25769979377364505\n",
      "  batch 160 loss: 0.4601102392174653\n",
      "  batch 170 loss: 0.4695288350456394\n",
      "  batch 180 loss: 0.4452806911584048\n",
      "  batch 190 loss: 0.27530554589175155\n",
      "LOSS train 0.27530554589175155 valid 0.45175727022578005\n",
      "EPOCH 105:\n",
      "  batch 10 loss: 0.5375361690748832\n",
      "  batch 20 loss: 0.2130608989747998\n",
      "  batch 30 loss: 0.2611780281411484\n",
      "  batch 40 loss: 0.5190579045942286\n",
      "  batch 50 loss: 0.6355829913954949\n",
      "  batch 60 loss: 0.19980296615540283\n",
      "  batch 70 loss: 0.21616162011414417\n",
      "  batch 80 loss: 0.8490825266053434\n",
      "  batch 90 loss: 0.26904891060403313\n",
      "  batch 100 loss: 0.3639112806136836\n",
      "  batch 110 loss: 0.342749232485221\n",
      "  batch 120 loss: 0.5164888563216664\n",
      "  batch 130 loss: 0.4200092790400959\n",
      "  batch 140 loss: 0.5772186061818502\n",
      "  batch 150 loss: 0.5548526891354413\n",
      "  batch 160 loss: 0.3708944315701956\n",
      "  batch 170 loss: 0.35092592036562564\n",
      "  batch 180 loss: 0.4053303952459828\n",
      "  batch 190 loss: 0.45369380674455895\n",
      "LOSS train 0.45369380674455895 valid 0.3635657359626725\n",
      "EPOCH 106:\n",
      "  batch 10 loss: 0.4129128091095481\n",
      "  batch 20 loss: 0.2607658038767113\n",
      "  batch 30 loss: 0.8503148174437228\n",
      "  batch 40 loss: 0.48073424176836854\n",
      "  batch 50 loss: 0.47260305513627826\n",
      "  batch 60 loss: 0.4862272092897911\n",
      "  batch 70 loss: 0.428529587603407\n",
      "  batch 80 loss: 0.401999028516002\n",
      "  batch 90 loss: 0.392851131563657\n",
      "  batch 100 loss: 0.4003607710066717\n",
      "  batch 110 loss: 0.1829833517957013\n",
      "  batch 120 loss: 0.30689457043336005\n",
      "  batch 130 loss: 0.2774359421338886\n",
      "  batch 140 loss: 0.4895280954253394\n",
      "  batch 150 loss: 0.21925201571866637\n",
      "  batch 160 loss: 0.31699284754577095\n",
      "  batch 170 loss: 0.4195475536806043\n",
      "  batch 180 loss: 0.2858170520325075\n",
      "  batch 190 loss: 0.49307455587986626\n",
      "LOSS train 0.49307455587986626 valid 0.9633706712065293\n",
      "EPOCH 107:\n",
      "  batch 10 loss: 0.46278248251473997\n",
      "  batch 20 loss: 0.4542768616483954\n",
      "  batch 30 loss: 0.26732484093736275\n",
      "  batch 40 loss: 0.4603237896277278\n",
      "  batch 50 loss: 0.21613460647276952\n",
      "  batch 60 loss: 0.4515193612780422\n",
      "  batch 70 loss: 0.7349643068206205\n",
      "  batch 80 loss: 0.5170853678107961\n",
      "  batch 90 loss: 0.2839980441727675\n",
      "  batch 100 loss: 0.2786667676176876\n",
      "  batch 110 loss: 0.4942577148111013\n",
      "  batch 120 loss: 0.4939321778336307\n",
      "  batch 130 loss: 0.2777003196068108\n",
      "  batch 140 loss: 0.566526048880769\n",
      "  batch 150 loss: 0.4782173321553273\n",
      "  batch 160 loss: 0.26330612868769093\n",
      "  batch 170 loss: 0.3165999880817253\n",
      "  batch 180 loss: 0.21985216848552228\n",
      "  batch 190 loss: 0.49137999104277696\n",
      "LOSS train 0.49137999104277696 valid 1.0826678193130088\n",
      "EPOCH 108:\n",
      "  batch 10 loss: 0.6048551660263911\n",
      "  batch 20 loss: 0.1778266180452192\n",
      "  batch 30 loss: 0.7914696719817584\n",
      "  batch 40 loss: 0.8476071546727326\n",
      "  batch 50 loss: 0.36368408288108184\n",
      "  batch 60 loss: 0.45337893123505635\n",
      "  batch 70 loss: 0.38098915112786924\n",
      "  batch 80 loss: 0.35370125389890744\n",
      "  batch 90 loss: 0.26717083775729406\n",
      "  batch 100 loss: 0.29525611852295697\n",
      "  batch 110 loss: 0.46153142254333945\n",
      "  batch 120 loss: 0.36049463535309767\n",
      "  batch 130 loss: 0.25043161770736333\n",
      "  batch 140 loss: 0.41215953935170546\n",
      "  batch 150 loss: 0.29308725348091685\n",
      "  batch 160 loss: 0.21400600756169297\n",
      "  batch 170 loss: 0.5161270253360272\n",
      "  batch 180 loss: 0.6364586423311266\n",
      "  batch 190 loss: 0.2632813568241545\n",
      "LOSS train 0.2632813568241545 valid 0.3961430277204505\n",
      "EPOCH 109:\n",
      "  batch 10 loss: 0.3754997405601898\n",
      "  batch 20 loss: 0.369753120772657\n",
      "  batch 30 loss: 0.39920184449292717\n",
      "  batch 40 loss: 0.25536842047877145\n",
      "  batch 50 loss: 0.2596655929046392\n",
      "  batch 60 loss: 0.6394435320260528\n",
      "  batch 70 loss: 0.23347934891935437\n",
      "  batch 80 loss: 0.39145624411466995\n",
      "  batch 90 loss: 0.2911067847831873\n",
      "  batch 100 loss: 0.1279757834454358\n",
      "  batch 110 loss: 0.2353505768114701\n",
      "  batch 120 loss: 0.25726475760893663\n",
      "  batch 130 loss: 0.35019627068541015\n",
      "  batch 140 loss: 0.5907328478700947\n",
      "  batch 150 loss: 0.20426747477904428\n",
      "  batch 160 loss: 0.2186126727669034\n",
      "  batch 170 loss: 0.6090206692679203\n",
      "  batch 180 loss: 0.2489854790153913\n",
      "  batch 190 loss: 0.5516694550286048\n",
      "LOSS train 0.5516694550286048 valid 0.3160298058582367\n",
      "EPOCH 110:\n",
      "  batch 10 loss: 0.3510859385263757\n",
      "  batch 20 loss: 0.40396536876942263\n",
      "  batch 30 loss: 0.7643718885316048\n",
      "  batch 40 loss: 0.4362655552395154\n",
      "  batch 50 loss: 0.23400361582025653\n",
      "  batch 60 loss: 0.2380196389160119\n",
      "  batch 70 loss: 0.46519516543630746\n",
      "  batch 80 loss: 0.315859635209199\n",
      "  batch 90 loss: 0.25063272528204833\n",
      "  batch 100 loss: 0.2911263435249566\n",
      "  batch 110 loss: 0.36724853546038505\n",
      "  batch 120 loss: 0.3286132241715677\n",
      "  batch 130 loss: 0.33759049063373825\n",
      "  batch 140 loss: 0.369289416182437\n",
      "  batch 150 loss: 0.33408448692644016\n",
      "  batch 160 loss: 0.3653529208502732\n",
      "  batch 170 loss: 0.3712690734770149\n",
      "  batch 180 loss: 0.335469422157621\n",
      "  batch 190 loss: 0.37081621926045044\n",
      "LOSS train 0.37081621926045044 valid 0.43610134470187545\n",
      "EPOCH 111:\n",
      "  batch 10 loss: 0.44415692507609494\n",
      "  batch 20 loss: 0.8752365039174037\n",
      "  batch 30 loss: 0.4301549525727751\n",
      "  batch 40 loss: 0.33120647860632746\n",
      "  batch 50 loss: 0.4029447069624439\n",
      "  batch 60 loss: 0.21573525062412954\n",
      "  batch 70 loss: 0.3893415654994897\n",
      "  batch 80 loss: 0.3952719069522573\n",
      "  batch 90 loss: 0.22584811388514936\n",
      "  batch 100 loss: 0.34538089004636274\n",
      "  batch 110 loss: 0.31041818219964623\n",
      "  batch 120 loss: 0.31848746676550943\n",
      "  batch 130 loss: 0.5057363418571186\n",
      "  batch 140 loss: 0.32236296373594087\n",
      "  batch 150 loss: 0.46306186590663856\n",
      "  batch 160 loss: 0.34171534283377697\n",
      "  batch 170 loss: 0.29406276519875973\n",
      "  batch 180 loss: 0.5143097607360687\n",
      "  batch 190 loss: 0.26492791090568063\n",
      "LOSS train 0.26492791090568063 valid 0.31191169808274327\n",
      "EPOCH 112:\n",
      "  batch 10 loss: 0.36369178661261686\n",
      "  batch 20 loss: 0.38038373055096597\n",
      "  batch 30 loss: 0.2731000248451892\n",
      "  batch 40 loss: 0.318635795051523\n",
      "  batch 50 loss: 0.3365958958653209\n",
      "  batch 60 loss: 0.3403493019344751\n",
      "  batch 70 loss: 0.1546903426242352\n",
      "  batch 80 loss: 0.39045263470325153\n",
      "  batch 90 loss: 0.6764278808841482\n",
      "  batch 100 loss: 0.2589756676898105\n",
      "  batch 110 loss: 0.27105146285612136\n",
      "  batch 120 loss: 0.16775761494936886\n",
      "  batch 130 loss: 0.35953212628519393\n",
      "  batch 140 loss: 0.7044574248138815\n",
      "  batch 150 loss: 0.21809124669234733\n",
      "  batch 160 loss: 0.39013374159694647\n",
      "  batch 170 loss: 0.19998497683955066\n",
      "  batch 180 loss: 0.2313941387285013\n",
      "  batch 190 loss: 0.5219166637310991\n",
      "LOSS train 0.5219166637310991 valid 0.36638366237102904\n",
      "EPOCH 113:\n",
      "  batch 10 loss: 0.5393252125577419\n",
      "  batch 20 loss: 0.2861967806995381\n",
      "  batch 30 loss: 0.3212394000918721\n",
      "  batch 40 loss: 0.23993696005636594\n",
      "  batch 50 loss: 0.3751549774897285\n",
      "  batch 60 loss: 0.33332588203338676\n",
      "  batch 70 loss: 0.49584953878074883\n",
      "  batch 80 loss: 0.6109074038002291\n",
      "  batch 90 loss: 0.3353305958793499\n",
      "  batch 100 loss: 0.2796273601954454\n",
      "  batch 110 loss: 0.34888716947243664\n",
      "  batch 120 loss: 0.4890004583809059\n",
      "  batch 130 loss: 0.34152581528178416\n",
      "  batch 140 loss: 0.270503472591372\n",
      "  batch 150 loss: 0.358942524518352\n",
      "  batch 160 loss: 0.421482293997542\n",
      "  batch 170 loss: 0.30288992572168355\n",
      "  batch 180 loss: 0.4667726773986942\n",
      "  batch 190 loss: 0.37632344382000155\n",
      "LOSS train 0.37632344382000155 valid 0.29987060540532606\n",
      "EPOCH 114:\n",
      "  batch 10 loss: 0.24932533997343853\n",
      "  batch 20 loss: 0.18953094779863022\n",
      "  batch 30 loss: 0.12879769502033014\n",
      "  batch 40 loss: 0.4391807691019494\n",
      "  batch 50 loss: 0.43188571403661624\n",
      "  batch 60 loss: 0.23085216993204086\n",
      "  batch 70 loss: 0.3707499906886369\n",
      "  batch 80 loss: 0.2734101503941929\n",
      "  batch 90 loss: 0.32335798109561437\n",
      "  batch 100 loss: 0.32093625038396567\n",
      "  batch 110 loss: 0.29641564789344554\n",
      "  batch 120 loss: 0.2963885951437987\n",
      "  batch 130 loss: 0.29872545754878954\n",
      "  batch 140 loss: 0.28061824083852116\n",
      "  batch 150 loss: 0.12458803633926437\n",
      "  batch 160 loss: 0.26334018021589145\n",
      "  batch 170 loss: 0.4746055273746606\n",
      "  batch 180 loss: 0.4056854029331589\n",
      "  batch 190 loss: 0.3228456895252748\n",
      "LOSS train 0.3228456895252748 valid 0.3997493643480247\n",
      "EPOCH 115:\n",
      "  batch 10 loss: 0.3952915549365571\n",
      "  batch 20 loss: 0.32143219961872094\n",
      "  batch 30 loss: 0.18833987115795026\n",
      "  batch 40 loss: 0.27903798279803593\n",
      "  batch 50 loss: 0.5353210422967095\n",
      "  batch 60 loss: 0.46531296173634473\n",
      "  batch 70 loss: 0.27316042381571604\n",
      "  batch 80 loss: 0.3404320568428375\n",
      "  batch 90 loss: 0.21564971148618498\n",
      "  batch 100 loss: 0.11308748898009072\n",
      "  batch 110 loss: 0.3487271026708186\n",
      "  batch 120 loss: 0.46570817804422404\n",
      "  batch 130 loss: 0.6161096954412641\n",
      "  batch 140 loss: 0.20504113424467504\n",
      "  batch 150 loss: 0.4911771692452021\n",
      "  batch 160 loss: 0.3031134231336182\n",
      "  batch 170 loss: 0.37117622199875766\n",
      "  batch 180 loss: 0.23094999065942828\n",
      "  batch 190 loss: 0.2515474236919545\n",
      "LOSS train 0.2515474236919545 valid 0.37292609210504823\n",
      "EPOCH 116:\n",
      "  batch 10 loss: 0.2247283944787341\n",
      "  batch 20 loss: 0.28646884817280804\n",
      "  batch 30 loss: 0.2860678601107793\n",
      "  batch 40 loss: 0.26468500355840663\n",
      "  batch 50 loss: 0.3648940353130456\n",
      "  batch 60 loss: 0.5992605961480877\n",
      "  batch 70 loss: 0.37210539844236334\n",
      "  batch 80 loss: 0.26164143024216174\n",
      "  batch 90 loss: 0.25257415040614434\n",
      "  batch 100 loss: 0.6323738964565564\n",
      "  batch 110 loss: 0.6508019826607778\n",
      "  batch 120 loss: 0.19415935345168692\n",
      "  batch 130 loss: 0.42876908900070704\n",
      "  batch 140 loss: 0.5356762183335377\n",
      "  batch 150 loss: 0.3123778528017283\n",
      "  batch 160 loss: 0.2719181421110989\n",
      "  batch 170 loss: 0.23991615490340337\n",
      "  batch 180 loss: 0.1999438373131852\n",
      "  batch 190 loss: 0.3111734214115131\n",
      "LOSS train 0.3111734214115131 valid 0.3686806621309911\n",
      "EPOCH 117:\n",
      "  batch 10 loss: 0.30856441381183686\n",
      "  batch 20 loss: 0.4644776091634412\n",
      "  batch 30 loss: 0.5236500184983015\n",
      "  batch 40 loss: 0.44564323156664615\n",
      "  batch 50 loss: 0.3044085755092965\n",
      "  batch 60 loss: 0.19460235366132111\n",
      "  batch 70 loss: 0.31483984244114255\n",
      "  batch 80 loss: 0.3059747745617642\n",
      "  batch 90 loss: 0.17518091957172147\n",
      "  batch 100 loss: 0.23681248995526402\n",
      "  batch 110 loss: 0.39597739083110356\n",
      "  batch 120 loss: 0.3225096907000989\n",
      "  batch 130 loss: 0.31159896015597044\n",
      "  batch 140 loss: 0.7727381641394458\n",
      "  batch 150 loss: 0.3484392870333977\n",
      "  batch 160 loss: 0.20083352388646744\n",
      "  batch 170 loss: 0.2555438573635911\n",
      "  batch 180 loss: 0.19424542079941604\n",
      "  batch 190 loss: 0.36821547756480866\n",
      "LOSS train 0.36821547756480866 valid 0.4153751661521174\n",
      "EPOCH 118:\n",
      "  batch 10 loss: 0.21211195290816248\n",
      "  batch 20 loss: 0.5137049782017129\n",
      "  batch 30 loss: 0.306078261112998\n",
      "  batch 40 loss: 0.16740474699181504\n",
      "  batch 50 loss: 0.1514706748755998\n",
      "  batch 60 loss: 0.16729859537445008\n",
      "  batch 70 loss: 0.4594387114440906\n",
      "  batch 80 loss: 0.3050145292007073\n",
      "  batch 90 loss: 0.23353510171873496\n",
      "  batch 100 loss: 0.3790728228748776\n",
      "  batch 110 loss: 0.1058085963333724\n",
      "  batch 120 loss: 0.43112064119195564\n",
      "  batch 130 loss: 0.6262911939847982\n",
      "  batch 140 loss: 0.5440476440708153\n",
      "  batch 150 loss: 0.33518976377963555\n",
      "  batch 160 loss: 0.4081280746788252\n",
      "  batch 170 loss: 0.4514544625184499\n",
      "  batch 180 loss: 0.12363967678829794\n",
      "  batch 190 loss: 0.19935563179751625\n",
      "LOSS train 0.19935563179751625 valid 0.424187255298677\n",
      "EPOCH 119:\n",
      "  batch 10 loss: 0.3599392915595672\n",
      "  batch 20 loss: 0.5986785882530967\n",
      "  batch 30 loss: 0.27805279902822805\n",
      "  batch 40 loss: 0.4276812286298082\n",
      "  batch 50 loss: 0.27400058299463126\n",
      "  batch 60 loss: 0.24350945950900496\n",
      "  batch 70 loss: 0.4549651540059131\n",
      "  batch 80 loss: 0.3419976072800637\n",
      "  batch 90 loss: 0.23374817840594914\n",
      "  batch 100 loss: 0.17234483460197225\n",
      "  batch 110 loss: 0.3887550286861369\n",
      "  batch 120 loss: 0.4685894125897903\n",
      "  batch 130 loss: 0.3306277580850292\n",
      "  batch 140 loss: 0.5686974606229341\n",
      "  batch 150 loss: 0.21405934286303818\n",
      "  batch 160 loss: 0.26749873743974606\n",
      "  batch 170 loss: 0.416308085671335\n",
      "  batch 180 loss: 0.09062826426234097\n",
      "  batch 190 loss: 0.47535476864286463\n",
      "LOSS train 0.47535476864286463 valid 0.4684758038454861\n",
      "EPOCH 120:\n",
      "  batch 10 loss: 0.40371808437048456\n",
      "  batch 20 loss: 0.12949714727292302\n",
      "  batch 30 loss: 0.16973719049419742\n",
      "  batch 40 loss: 0.2967726898896217\n",
      "  batch 50 loss: 0.33972023459355116\n",
      "  batch 60 loss: 0.2092669704928994\n",
      "  batch 70 loss: 0.27266584988537945\n",
      "  batch 80 loss: 0.6310248236637562\n",
      "  batch 90 loss: 0.18870284625008935\n",
      "  batch 100 loss: 0.23656204653088936\n",
      "  batch 110 loss: 0.41194269307670767\n",
      "  batch 120 loss: 0.704817364126211\n",
      "  batch 130 loss: 0.5364202522265259\n",
      "  batch 140 loss: 0.2631991854279477\n",
      "  batch 150 loss: 0.18458263748179887\n",
      "  batch 160 loss: 0.5133086802256003\n",
      "  batch 170 loss: 0.5422069819796889\n",
      "  batch 180 loss: 0.32611119844950737\n",
      "  batch 190 loss: 0.429590644367272\n",
      "LOSS train 0.429590644367272 valid 0.4975124210806173\n",
      "EPOCH 121:\n",
      "  batch 10 loss: 0.5719908565282822\n",
      "  batch 20 loss: 0.5277098112594103\n",
      "  batch 30 loss: 0.22511837126221507\n",
      "  batch 40 loss: 0.2054304730372678\n",
      "  batch 50 loss: 0.4512203936421429\n",
      "  batch 60 loss: 0.3192961272899993\n",
      "  batch 70 loss: 0.6072033261632896\n",
      "  batch 80 loss: 0.21856564528425224\n",
      "  batch 90 loss: 0.36698649447353093\n",
      "  batch 100 loss: 0.2547383989658556\n",
      "  batch 110 loss: 0.20349004350064206\n",
      "  batch 120 loss: 0.2720021898014238\n",
      "  batch 130 loss: 0.2252424795879051\n",
      "  batch 140 loss: 0.4423911775869783\n",
      "  batch 150 loss: 0.6285142092641763\n",
      "  batch 160 loss: 0.5519316562465975\n",
      "  batch 170 loss: 0.2778969675113331\n",
      "  batch 180 loss: 0.2107374035862449\n",
      "  batch 190 loss: 0.2036584031433449\n",
      "LOSS train 0.2036584031433449 valid 0.851604210337824\n",
      "EPOCH 122:\n",
      "  batch 10 loss: 0.5221669972044765\n",
      "  batch 20 loss: 0.3994807438342832\n",
      "  batch 30 loss: 0.22796062039560638\n",
      "  batch 40 loss: 0.22538677562479278\n",
      "  batch 50 loss: 0.21050434565986506\n",
      "  batch 60 loss: 0.22810304816666757\n",
      "  batch 70 loss: 0.5578817732603056\n",
      "  batch 80 loss: 0.2709404890789301\n",
      "  batch 90 loss: 0.22430500745467724\n",
      "  batch 100 loss: 0.49482530773821054\n",
      "  batch 110 loss: 0.526081936902483\n",
      "  batch 120 loss: 0.28425770597968947\n",
      "  batch 130 loss: 0.38133848672368914\n",
      "  batch 140 loss: 0.5022244840743951\n",
      "  batch 150 loss: 0.3568238968931837\n",
      "  batch 160 loss: 0.21648043775348924\n",
      "  batch 170 loss: 0.25362041978514754\n",
      "  batch 180 loss: 0.19473682857787936\n",
      "  batch 190 loss: 0.35003737085717146\n",
      "LOSS train 0.35003737085717146 valid 0.2915881087073313\n",
      "EPOCH 123:\n",
      "  batch 10 loss: 0.43296175192808734\n",
      "  batch 20 loss: 0.20048466343650945\n",
      "  batch 30 loss: 0.26556408544856824\n",
      "  batch 40 loss: 0.3860833944461774\n",
      "  batch 50 loss: 0.317497024309705\n",
      "  batch 60 loss: 0.14328696243610467\n",
      "  batch 70 loss: 0.24398740854812787\n",
      "  batch 80 loss: 0.39793287000284183\n",
      "  batch 90 loss: 0.12602346254425356\n",
      "  batch 100 loss: 0.34287468116672243\n",
      "  batch 110 loss: 0.23055097384858528\n",
      "  batch 120 loss: 0.3890572260148474\n",
      "  batch 130 loss: 0.31663258545158895\n",
      "  batch 140 loss: 0.18761660334930638\n",
      "  batch 150 loss: 0.2768956502688525\n",
      "  batch 160 loss: 0.5259784373054572\n",
      "  batch 170 loss: 0.1945475913111295\n",
      "  batch 180 loss: 0.33048480049474166\n",
      "  batch 190 loss: 0.2055808165765484\n",
      "LOSS train 0.2055808165765484 valid 0.3526781174091994\n",
      "EPOCH 124:\n",
      "  batch 10 loss: 0.3043604500388028\n",
      "  batch 20 loss: 0.2032437283152831\n",
      "  batch 30 loss: 0.37436230725616043\n",
      "  batch 40 loss: 0.1902901876193937\n",
      "  batch 50 loss: 0.11210118810486165\n",
      "  batch 60 loss: 0.32206281253056657\n",
      "  batch 70 loss: 0.4067879643815104\n",
      "  batch 80 loss: 0.6170177945856267\n",
      "  batch 90 loss: 0.2021093344395922\n",
      "  batch 100 loss: 0.3574682497324829\n",
      "  batch 110 loss: 0.16200247698070597\n",
      "  batch 120 loss: 0.20274801719970129\n",
      "  batch 130 loss: 0.3861319934410858\n",
      "  batch 140 loss: 0.4141625571734039\n",
      "  batch 150 loss: 0.3343877985746076\n",
      "  batch 160 loss: 0.20964471339611918\n",
      "  batch 170 loss: 0.3704800659310422\n",
      "  batch 180 loss: 0.1672393052613188\n",
      "  batch 190 loss: 0.39791930992214475\n",
      "LOSS train 0.39791930992214475 valid 0.4997305586740624\n",
      "EPOCH 125:\n",
      "  batch 10 loss: 0.2069857985668932\n",
      "  batch 20 loss: 0.5156056850697496\n",
      "  batch 30 loss: 0.3462169963400811\n",
      "  batch 40 loss: 0.36739192062959775\n",
      "  batch 50 loss: 0.31070849976094905\n",
      "  batch 60 loss: 0.10444405848829774\n",
      "  batch 70 loss: 0.43480408343020827\n",
      "  batch 80 loss: 0.28711734402459116\n",
      "  batch 90 loss: 0.3075071718500112\n",
      "  batch 100 loss: 0.16830378074664623\n",
      "  batch 110 loss: 0.2943175334512489\n",
      "  batch 120 loss: 0.4276335660139011\n",
      "  batch 130 loss: 0.1984526630549226\n",
      "  batch 140 loss: 0.3526391770938062\n",
      "  batch 150 loss: 0.21394070836831816\n",
      "  batch 160 loss: 0.2634213979355991\n",
      "  batch 170 loss: 0.4885449044653797\n",
      "  batch 180 loss: 0.2581603087121039\n",
      "  batch 190 loss: 0.6137490942976\n",
      "LOSS train 0.6137490942976 valid 0.6912665652249089\n",
      "EPOCH 126:\n",
      "  batch 10 loss: 0.6914674417115748\n",
      "  batch 20 loss: 0.29615748060750774\n",
      "  batch 30 loss: 0.4410013635380892\n",
      "  batch 40 loss: 0.17671603554845206\n",
      "  batch 50 loss: 0.1563774119160371\n",
      "  batch 60 loss: 0.20587939243978326\n",
      "  batch 70 loss: 0.4203020586588536\n",
      "  batch 80 loss: 0.26119135471526533\n",
      "  batch 90 loss: 0.18848728139419108\n",
      "  batch 100 loss: 0.19751738463783114\n",
      "  batch 110 loss: 0.2480618923160364\n",
      "  batch 120 loss: 0.36746495519328165\n",
      "  batch 130 loss: 0.25725928238534834\n",
      "  batch 140 loss: 0.4715728236638824\n",
      "  batch 150 loss: 0.3318850164549076\n",
      "  batch 160 loss: 0.25445083889644593\n",
      "  batch 170 loss: 0.5094213395554107\n",
      "  batch 180 loss: 0.23776181018620263\n",
      "  batch 190 loss: 0.16507066933627357\n",
      "LOSS train 0.16507066933627357 valid 0.3769673837316357\n",
      "EPOCH 127:\n",
      "  batch 10 loss: 0.32098661611089485\n",
      "  batch 20 loss: 0.2839703559060581\n",
      "  batch 30 loss: 0.39002148213330656\n",
      "  batch 40 loss: 0.6385332279707654\n",
      "  batch 50 loss: 0.1719242134829983\n",
      "  batch 60 loss: 0.2796310663135955\n",
      "  batch 70 loss: 0.3403914520604303\n",
      "  batch 80 loss: 0.4172113164611801\n",
      "  batch 90 loss: 0.49640395917522256\n",
      "  batch 100 loss: 0.5131472310109529\n",
      "  batch 110 loss: 0.5324497881141724\n",
      "  batch 120 loss: 0.42910365224233826\n",
      "  batch 130 loss: 0.058751089264842446\n",
      "  batch 140 loss: 0.4675053356182616\n",
      "  batch 150 loss: 0.273638986773949\n",
      "  batch 160 loss: 0.21481206819589715\n",
      "  batch 170 loss: 0.2569007457699627\n",
      "  batch 180 loss: 0.15526712418486568\n",
      "  batch 190 loss: 0.23111846849496942\n",
      "LOSS train 0.23111846849496942 valid 0.3541122837495305\n",
      "EPOCH 128:\n",
      "  batch 10 loss: 0.33076468137478515\n",
      "  batch 20 loss: 0.28169355927238937\n",
      "  batch 30 loss: 0.6863013566777226\n",
      "  batch 40 loss: 0.2752617997070047\n",
      "  batch 50 loss: 0.23061287431046368\n",
      "  batch 60 loss: 0.20855580965871923\n",
      "  batch 70 loss: 0.4704096581619524\n",
      "  batch 80 loss: 0.13952486436319306\n",
      "  batch 90 loss: 0.31002002553723285\n",
      "  batch 100 loss: 0.3349594822328072\n",
      "  batch 110 loss: 0.3055188944912516\n",
      "  batch 120 loss: 0.4842540339624975\n",
      "  batch 130 loss: 0.10303621584898792\n",
      "  batch 140 loss: 0.21738117185595912\n",
      "  batch 150 loss: 0.13709794339811196\n",
      "  batch 160 loss: 0.2013613857794553\n",
      "  batch 170 loss: 0.35248020800572705\n",
      "  batch 180 loss: 0.17443771631660637\n",
      "  batch 190 loss: 0.2932941743540141\n",
      "LOSS train 0.2932941743540141 valid 0.5405517585724084\n",
      "EPOCH 129:\n",
      "  batch 10 loss: 0.26085543048793625\n",
      "  batch 20 loss: 0.3136932526111195\n",
      "  batch 30 loss: 0.27211712099378926\n",
      "  batch 40 loss: 0.13383188422449166\n",
      "  batch 50 loss: 0.39535854659770847\n",
      "  batch 60 loss: 0.21493345967974165\n",
      "  batch 70 loss: 0.08915711895970162\n",
      "  batch 80 loss: 0.2193718102906132\n",
      "  batch 90 loss: 0.2205572512029903\n",
      "  batch 100 loss: 0.15504660757978855\n",
      "  batch 110 loss: 0.2612345929228468\n",
      "  batch 120 loss: 0.23744335006231268\n",
      "  batch 130 loss: 0.43082110818941144\n",
      "  batch 140 loss: 0.2605505042534787\n",
      "  batch 150 loss: 0.43911845693364737\n",
      "  batch 160 loss: 0.3036060259666556\n",
      "  batch 170 loss: 0.24275238915142835\n",
      "  batch 180 loss: 0.4571382435358828\n",
      "  batch 190 loss: 0.2744327800221072\n",
      "LOSS train 0.2744327800221072 valid 0.2795059944132495\n",
      "EPOCH 130:\n",
      "  batch 10 loss: 0.3591904969311145\n",
      "  batch 20 loss: 0.3078304470604053\n",
      "  batch 30 loss: 0.24841684902348787\n",
      "  batch 40 loss: 0.49377074831973006\n",
      "  batch 50 loss: 0.5763560428866186\n",
      "  batch 60 loss: 0.18539617662900126\n",
      "  batch 70 loss: 0.440603568906954\n",
      "  batch 80 loss: 0.26589329320158867\n",
      "  batch 90 loss: 0.2240716542739392\n",
      "  batch 100 loss: 0.5725923707374022\n",
      "  batch 110 loss: 0.2255697138258256\n",
      "  batch 120 loss: 0.24897055965848267\n",
      "  batch 130 loss: 0.20432883498433513\n",
      "  batch 140 loss: 0.2672954434368876\n",
      "  batch 150 loss: 0.28149707480988584\n",
      "  batch 160 loss: 0.2871370695633232\n",
      "  batch 170 loss: 0.2455687419653259\n",
      "  batch 180 loss: 0.23473027009895303\n",
      "  batch 190 loss: 0.4141276391172141\n",
      "LOSS train 0.4141276391172141 valid 0.5809593499148972\n",
      "EPOCH 131:\n",
      "  batch 10 loss: 0.22971502935979515\n",
      "  batch 20 loss: 0.2993581218946929\n",
      "  batch 30 loss: 0.2976164995376166\n",
      "  batch 40 loss: 0.17935084933124018\n",
      "  batch 50 loss: 0.4337525520772033\n",
      "  batch 60 loss: 0.10179949558660155\n",
      "  batch 70 loss: 0.26842557049239985\n",
      "  batch 80 loss: 0.21610047323920298\n",
      "  batch 90 loss: 0.2695325447275536\n",
      "  batch 100 loss: 0.2329100667659077\n",
      "  batch 110 loss: 0.5083709564440142\n",
      "  batch 120 loss: 0.47271419849130325\n",
      "  batch 130 loss: 0.34943908958630343\n",
      "  batch 140 loss: 0.14380141283472767\n",
      "  batch 150 loss: 0.3828978104458656\n",
      "  batch 160 loss: 0.3300049577796017\n",
      "  batch 170 loss: 0.1673721077502705\n",
      "  batch 180 loss: 0.3032974483059661\n",
      "  batch 190 loss: 0.3503223444284231\n",
      "LOSS train 0.3503223444284231 valid 0.33812141885955965\n",
      "EPOCH 132:\n",
      "  batch 10 loss: 0.36601351559875184\n",
      "  batch 20 loss: 0.2566763724287739\n",
      "  batch 30 loss: 0.10219703147340624\n",
      "  batch 40 loss: 0.1929550193279283\n",
      "  batch 50 loss: 0.28594469930449123\n",
      "  batch 60 loss: 0.13390895910852124\n",
      "  batch 70 loss: 0.23602222260378766\n",
      "  batch 80 loss: 0.32268990079210197\n",
      "  batch 90 loss: 0.15179070963931734\n",
      "  batch 100 loss: 0.46625434134039095\n",
      "  batch 110 loss: 0.2923065430190036\n",
      "  batch 120 loss: 0.3167269247609511\n",
      "  batch 130 loss: 0.20320171220464545\n",
      "  batch 140 loss: 0.44068950391811085\n",
      "  batch 150 loss: 0.3279404975131911\n",
      "  batch 160 loss: 0.32702248875539225\n",
      "  batch 170 loss: 0.3527346554925316\n",
      "  batch 180 loss: 0.18297405616904144\n",
      "  batch 190 loss: 0.22242550875162123\n",
      "LOSS train 0.22242550875162123 valid 0.32122772049688836\n",
      "EPOCH 133:\n",
      "  batch 10 loss: 0.22750716457394446\n",
      "  batch 20 loss: 0.2800699710245681\n",
      "  batch 30 loss: 0.46549496857369377\n",
      "  batch 40 loss: 0.21782437643341837\n",
      "  batch 50 loss: 0.3532266349735437\n",
      "  batch 60 loss: 0.15212916193995624\n",
      "  batch 70 loss: 0.4845970084599685\n",
      "  batch 80 loss: 0.2705609514734533\n",
      "  batch 90 loss: 0.119283725299465\n",
      "  batch 100 loss: 0.16729528802170535\n",
      "  batch 110 loss: 0.3451839036293677\n",
      "  batch 120 loss: 0.5040767993927148\n",
      "  batch 130 loss: 0.22345356425721546\n",
      "  batch 140 loss: 0.5041116443142528\n",
      "  batch 150 loss: 0.2500101671950688\n",
      "  batch 160 loss: 0.20763865319459002\n",
      "  batch 170 loss: 0.12749420121181174\n",
      "  batch 180 loss: 0.3061916858467157\n",
      "  batch 190 loss: 0.30054825370316396\n",
      "LOSS train 0.30054825370316396 valid 0.31369239745375427\n",
      "EPOCH 134:\n",
      "  batch 10 loss: 0.25958857326022555\n",
      "  batch 20 loss: 0.42817894581785365\n",
      "  batch 30 loss: 0.22185492646458443\n",
      "  batch 40 loss: 0.20892293401593634\n",
      "  batch 50 loss: 0.5166975265135989\n",
      "  batch 60 loss: 0.31306027441387413\n",
      "  batch 70 loss: 0.33256533700041474\n",
      "  batch 80 loss: 0.16759913073765348\n",
      "  batch 90 loss: 0.2005526858894882\n",
      "  batch 100 loss: 0.09514903702256561\n",
      "  batch 110 loss: 0.2793940849449427\n",
      "  batch 120 loss: 0.16290598772557133\n",
      "  batch 130 loss: 0.4727111160682398\n",
      "  batch 140 loss: 0.38072930244379677\n",
      "  batch 150 loss: 0.3467426097020507\n",
      "  batch 160 loss: 0.20729187306824315\n",
      "  batch 170 loss: 0.25629376881261123\n",
      "  batch 180 loss: 0.23467635592387523\n",
      "  batch 190 loss: 0.4191983207032536\n",
      "LOSS train 0.4191983207032536 valid 0.39113716125305076\n",
      "EPOCH 135:\n",
      "  batch 10 loss: 0.2715478645524854\n",
      "  batch 20 loss: 0.20739123607054352\n",
      "  batch 30 loss: 0.13962707162281732\n",
      "  batch 40 loss: 0.21465000688913277\n",
      "  batch 50 loss: 0.23393957399894133\n",
      "  batch 60 loss: 0.5478327392571373\n",
      "  batch 70 loss: 0.12068678539435496\n",
      "  batch 80 loss: 0.14855151508090786\n",
      "  batch 90 loss: 0.20463443198532333\n",
      "  batch 100 loss: 0.31697067450004396\n",
      "  batch 110 loss: 0.25084445182219495\n",
      "  batch 120 loss: 0.20304544011305553\n",
      "  batch 130 loss: 0.33335422536329135\n",
      "  batch 140 loss: 0.7173871163875447\n",
      "  batch 150 loss: 0.4643692577972615\n",
      "  batch 160 loss: 0.4116850180016627\n",
      "  batch 170 loss: 0.28787978133768777\n",
      "  batch 180 loss: 0.3969083597999997\n",
      "  batch 190 loss: 0.43847187490937356\n",
      "LOSS train 0.43847187490937356 valid 0.35789567964321833\n",
      "EPOCH 136:\n",
      "  batch 10 loss: 0.2680936799977644\n",
      "  batch 20 loss: 0.12705139222525758\n",
      "  batch 30 loss: 0.9572007419745205\n",
      "  batch 40 loss: 0.30372707521964915\n",
      "  batch 50 loss: 0.6768283519661054\n",
      "  batch 60 loss: 0.2671160924714059\n",
      "  batch 70 loss: 0.14252760510062218\n",
      "  batch 80 loss: 0.16746029602072668\n",
      "  batch 90 loss: 0.13115189208765515\n",
      "  batch 100 loss: 0.22775528033234876\n",
      "  batch 110 loss: 0.31955069338291653\n",
      "  batch 120 loss: 0.14656737056502606\n",
      "  batch 130 loss: 0.1827377126781357\n",
      "  batch 140 loss: 0.2881680864134978\n",
      "  batch 150 loss: 0.21798633269827405\n",
      "  batch 160 loss: 0.4842589893796685\n",
      "  batch 170 loss: 0.9855745942142675\n",
      "  batch 180 loss: 0.6999540128395892\n",
      "  batch 190 loss: 0.1335044779068994\n",
      "LOSS train 0.1335044779068994 valid 0.38844507315307286\n",
      "EPOCH 137:\n",
      "  batch 10 loss: 0.320103509062028\n",
      "  batch 20 loss: 0.1308585710801708\n",
      "  batch 30 loss: 0.21007784462235576\n",
      "  batch 40 loss: 0.2550765287815011\n",
      "  batch 50 loss: 0.28491281340830027\n",
      "  batch 60 loss: 0.49541261978738477\n",
      "  batch 70 loss: 0.29371437004847395\n",
      "  batch 80 loss: 0.25224168092645416\n",
      "  batch 90 loss: 0.19981326142078615\n",
      "  batch 100 loss: 0.22410848614454154\n",
      "  batch 110 loss: 0.15009663932578404\n",
      "  batch 120 loss: 0.2799015439901268\n",
      "  batch 130 loss: 0.31551202475675383\n",
      "  batch 140 loss: 0.25218077576500947\n",
      "  batch 150 loss: 0.1353788389264082\n",
      "  batch 160 loss: 0.4322213319435832\n",
      "  batch 170 loss: 0.25983853618072317\n",
      "  batch 180 loss: 0.20092289995081955\n",
      "  batch 190 loss: 0.30655907407344785\n",
      "LOSS train 0.30655907407344785 valid 0.2677183313001665\n",
      "EPOCH 138:\n",
      "  batch 10 loss: 0.2992084860203249\n",
      "  batch 20 loss: 0.2858007431748774\n",
      "  batch 30 loss: 0.13793729313010772\n",
      "  batch 40 loss: 0.20011089583204011\n",
      "  batch 50 loss: 0.2410919670893236\n",
      "  batch 60 loss: 0.07299478190961964\n",
      "  batch 70 loss: 0.16487412328278878\n",
      "  batch 80 loss: 0.24788151251705132\n",
      "  batch 90 loss: 0.34197273374747966\n",
      "  batch 100 loss: 0.25519701193952643\n",
      "  batch 110 loss: 0.5069504017679719\n",
      "  batch 120 loss: 0.3913630615672446\n",
      "  batch 130 loss: 0.3835459064041061\n",
      "  batch 140 loss: 0.12132222701402498\n",
      "  batch 150 loss: 0.4240009130066028\n",
      "  batch 160 loss: 0.27731212181861337\n",
      "  batch 170 loss: 0.4175714294106001\n",
      "  batch 180 loss: 0.13523674521493376\n",
      "  batch 190 loss: 0.1378152769184453\n",
      "LOSS train 0.1378152769184453 valid 0.486118985594751\n",
      "EPOCH 139:\n",
      "  batch 10 loss: 0.3470647425507195\n",
      "  batch 20 loss: 0.5732218931152602\n",
      "  batch 30 loss: 0.2961233054007607\n",
      "  batch 40 loss: 0.20486936367378802\n",
      "  batch 50 loss: 0.1933946660683432\n",
      "  batch 60 loss: 0.11439265669760061\n",
      "  batch 70 loss: 0.34209259498456956\n",
      "  batch 80 loss: 0.373016418561383\n",
      "  batch 90 loss: 0.5548756027652416\n",
      "  batch 100 loss: 0.19456376764683228\n",
      "  batch 110 loss: 0.21115120826634665\n",
      "  batch 120 loss: 0.23183356307727082\n",
      "  batch 130 loss: 0.15215687194995553\n",
      "  batch 140 loss: 0.36343084700783945\n",
      "  batch 150 loss: 0.22151458192138307\n",
      "  batch 160 loss: 0.09640211022888252\n",
      "  batch 170 loss: 0.2967738557155826\n",
      "  batch 180 loss: 0.28753105336218143\n",
      "  batch 190 loss: 0.244766618093945\n",
      "LOSS train 0.244766618093945 valid 0.46042467961613\n",
      "EPOCH 140:\n",
      "  batch 10 loss: 0.22213073924031052\n",
      "  batch 20 loss: 0.48105322966730457\n",
      "  batch 30 loss: 0.1558241146052751\n",
      "  batch 40 loss: 0.21109923259718927\n",
      "  batch 50 loss: 0.11636759939792682\n",
      "  batch 60 loss: 0.24221755808976014\n",
      "  batch 70 loss: 0.29591452905442567\n",
      "  batch 80 loss: 0.5037943384715617\n",
      "  batch 90 loss: 0.6035283195946249\n",
      "  batch 100 loss: 0.17676208240764027\n",
      "  batch 110 loss: 0.09749213630529993\n",
      "  batch 120 loss: 0.43652882655514985\n",
      "  batch 130 loss: 0.5909115456498512\n",
      "  batch 140 loss: 0.2180687680709525\n",
      "  batch 150 loss: 0.606597148552828\n",
      "  batch 160 loss: 0.2120791105565331\n",
      "  batch 170 loss: 0.6796314202799294\n",
      "  batch 180 loss: 0.13512586405558977\n",
      "  batch 190 loss: 0.15668475541679072\n",
      "LOSS train 0.15668475541679072 valid 0.30101605467774417\n",
      "EPOCH 141:\n",
      "  batch 10 loss: 0.28050086631483284\n",
      "  batch 20 loss: 0.13646017753999332\n",
      "  batch 30 loss: 0.050447515917767305\n",
      "  batch 40 loss: 0.45794369819741404\n",
      "  batch 50 loss: 0.4386920219367312\n",
      "  batch 60 loss: 0.14877950666304968\n",
      "  batch 70 loss: 0.24405154274063534\n",
      "  batch 80 loss: 0.2542437631971552\n",
      "  batch 90 loss: 0.1400679024758574\n",
      "  batch 100 loss: 0.1587786398929893\n",
      "  batch 110 loss: 0.23033668979387584\n",
      "  batch 120 loss: 0.12007215961584734\n",
      "  batch 130 loss: 0.4034481969953049\n",
      "  batch 140 loss: 0.17194672907498898\n",
      "  batch 150 loss: 0.47623933487266185\n",
      "  batch 160 loss: 0.17607234477372913\n",
      "  batch 170 loss: 0.468452661190895\n",
      "  batch 180 loss: 0.32266617025670713\n",
      "  batch 190 loss: 0.488973891751084\n",
      "LOSS train 0.488973891751084 valid 0.4464239897142528\n",
      "EPOCH 142:\n",
      "  batch 10 loss: 0.32340653360952276\n",
      "  batch 20 loss: 0.17423533414839767\n",
      "  batch 30 loss: 0.2936126354899898\n",
      "  batch 40 loss: 0.26202628187602384\n",
      "  batch 50 loss: 0.2425704690082057\n",
      "  batch 60 loss: 0.2246297849706025\n",
      "  batch 70 loss: 0.11176022098516114\n",
      "  batch 80 loss: 0.27388180833222575\n",
      "  batch 90 loss: 0.2907826242750161\n",
      "  batch 100 loss: 0.10794739525790646\n",
      "  batch 110 loss: 0.3530121297175356\n",
      "  batch 120 loss: 0.15476035270439753\n",
      "  batch 130 loss: 0.5513361530640395\n",
      "  batch 140 loss: 0.21784020383129246\n",
      "  batch 150 loss: 0.2724660526559092\n",
      "  batch 160 loss: 0.13194753123207192\n",
      "  batch 170 loss: 0.1875527086762304\n",
      "  batch 180 loss: 0.15222434261431772\n",
      "  batch 190 loss: 0.20798826210739207\n",
      "LOSS train 0.20798826210739207 valid 2.2944037451166084\n",
      "EPOCH 143:\n",
      "  batch 10 loss: 0.4271991090314259\n",
      "  batch 20 loss: 0.49915208989186793\n",
      "  batch 30 loss: 0.24342486300593008\n",
      "  batch 40 loss: 0.2763582680563559\n",
      "  batch 50 loss: 0.18256198961025802\n",
      "  batch 60 loss: 0.18112552533784765\n",
      "  batch 70 loss: 0.24018174342418205\n",
      "  batch 80 loss: 0.1242569208257919\n",
      "  batch 90 loss: 0.1327882911562483\n",
      "  batch 100 loss: 0.28517759403130183\n",
      "  batch 110 loss: 0.27327877256684585\n",
      "  batch 120 loss: 0.15333719761893008\n",
      "  batch 130 loss: 0.3782429971292004\n",
      "  batch 140 loss: 0.2971904062498652\n",
      "  batch 150 loss: 0.5438989749833126\n",
      "  batch 160 loss: 0.27179924301999564\n",
      "  batch 170 loss: 0.08840259584903834\n",
      "  batch 180 loss: 0.27187192083656553\n",
      "  batch 190 loss: 0.3609089352369665\n",
      "LOSS train 0.3609089352369665 valid 0.5635763995701265\n",
      "EPOCH 144:\n",
      "  batch 10 loss: 0.2542958114408975\n",
      "  batch 20 loss: 0.9815277558394883\n",
      "  batch 30 loss: 0.4185980538019976\n",
      "  batch 40 loss: 0.23411758007277966\n",
      "  batch 50 loss: 0.16537678945933293\n",
      "  batch 60 loss: 0.2658793063630583\n",
      "  batch 70 loss: 0.20501632872110348\n",
      "  batch 80 loss: 0.2846863714468782\n",
      "  batch 90 loss: 0.2762384386078338\n",
      "  batch 100 loss: 0.40580528528953436\n",
      "  batch 110 loss: 0.2119381148557295\n",
      "  batch 120 loss: 0.25227480804460356\n",
      "  batch 130 loss: 0.2221048589752172\n",
      "  batch 140 loss: 0.28169551441606017\n",
      "  batch 150 loss: 0.27734827994572697\n",
      "  batch 160 loss: 0.31083232935197885\n",
      "  batch 170 loss: 0.1811572860024171\n",
      "  batch 180 loss: 0.09929034449669416\n",
      "  batch 190 loss: 0.15252083405157463\n",
      "LOSS train 0.15252083405157463 valid 0.25390741723950694\n",
      "EPOCH 145:\n",
      "  batch 10 loss: 0.142436750914203\n",
      "  batch 20 loss: 0.18019659045967273\n",
      "  batch 30 loss: 0.17400331335302327\n",
      "  batch 40 loss: 0.158122095480212\n",
      "  batch 50 loss: 0.14419532116116898\n",
      "  batch 60 loss: 0.20113562596225165\n",
      "  batch 70 loss: 0.5741224595956738\n",
      "  batch 80 loss: 0.3008663329965202\n",
      "  batch 90 loss: 0.18052999834262665\n",
      "  batch 100 loss: 0.0735405963510857\n",
      "  batch 110 loss: 0.31280879897203706\n",
      "  batch 120 loss: 0.2855273220833624\n",
      "  batch 130 loss: 0.1815190885547054\n",
      "  batch 140 loss: 0.33463444554727173\n",
      "  batch 150 loss: 0.3653473892811235\n",
      "  batch 160 loss: 0.09162348417030444\n",
      "  batch 170 loss: 0.21408458897785748\n",
      "  batch 180 loss: 0.24021783324878926\n",
      "  batch 190 loss: 0.24656153073265158\n",
      "LOSS train 0.24656153073265158 valid 0.4512094269371818\n",
      "EPOCH 146:\n",
      "  batch 10 loss: 0.6775771884247661\n",
      "  batch 20 loss: 0.2684137624302821\n",
      "  batch 30 loss: 0.400820507993194\n",
      "  batch 40 loss: 0.14483580324922513\n",
      "  batch 50 loss: 0.11942604585783556\n",
      "  batch 60 loss: 0.16737581002817023\n",
      "  batch 70 loss: 0.16446954315724724\n",
      "  batch 80 loss: 0.807480877434864\n",
      "  batch 90 loss: 0.4447125942853745\n",
      "  batch 100 loss: 0.34740477521845603\n",
      "  batch 110 loss: 0.3916508349262585\n",
      "  batch 120 loss: 0.332831735144282\n",
      "  batch 130 loss: 0.5658629654382821\n",
      "  batch 140 loss: 0.27015145876357566\n",
      "  batch 150 loss: 0.150948683371098\n",
      "  batch 160 loss: 0.20540835873580363\n",
      "  batch 170 loss: 0.07044846632197732\n",
      "  batch 180 loss: 0.22513610606547446\n",
      "  batch 190 loss: 0.2483603074622806\n",
      "LOSS train 0.2483603074622806 valid 0.30831831378551355\n",
      "EPOCH 147:\n",
      "  batch 10 loss: 0.15686253460298757\n",
      "  batch 20 loss: 0.41859831776237116\n",
      "  batch 30 loss: 0.10495988888142165\n",
      "  batch 40 loss: 0.2659007035601462\n",
      "  batch 50 loss: 0.313689789819\n",
      "  batch 60 loss: 0.37784008107555567\n",
      "  batch 70 loss: 0.16571359648442013\n",
      "  batch 80 loss: 0.2777472304109324\n",
      "  batch 90 loss: 0.12929102221551148\n",
      "  batch 100 loss: 0.15931326867348616\n",
      "  batch 110 loss: 0.15900536913104588\n",
      "  batch 120 loss: 0.16629722782890893\n",
      "  batch 130 loss: 0.15669971614970563\n",
      "  batch 140 loss: 0.4320778799294203\n",
      "  batch 150 loss: 0.3695818756699737\n",
      "  batch 160 loss: 0.14512737705317705\n",
      "  batch 170 loss: 0.20687024899834797\n",
      "  batch 180 loss: 0.1761648155981675\n",
      "  batch 190 loss: 0.4025793836801313\n",
      "LOSS train 0.4025793836801313 valid 0.33753405932663016\n",
      "EPOCH 148:\n",
      "  batch 10 loss: 0.4117715428092197\n",
      "  batch 20 loss: 0.4961465955489984\n",
      "  batch 30 loss: 0.23511338178504956\n",
      "  batch 40 loss: 0.10091636103388738\n",
      "  batch 50 loss: 0.21519726454989724\n",
      "  batch 60 loss: 0.3466406530696986\n",
      "  batch 70 loss: 0.5419117939945863\n",
      "  batch 80 loss: 0.1542587630930939\n",
      "  batch 90 loss: 0.08485986782616237\n",
      "  batch 100 loss: 0.155118047635915\n",
      "  batch 110 loss: 0.0804288026451104\n",
      "  batch 120 loss: 0.35927294822722616\n",
      "  batch 130 loss: 0.15768713739144005\n",
      "  batch 140 loss: 0.25313701885024786\n",
      "  batch 150 loss: 0.25091944452997267\n",
      "  batch 160 loss: 0.17305236659012735\n",
      "  batch 170 loss: 0.2072096218871593\n",
      "  batch 180 loss: 0.36161821448476983\n",
      "  batch 190 loss: 0.358520695705738\n",
      "LOSS train 0.358520695705738 valid 1.0619115702159037\n",
      "EPOCH 149:\n",
      "  batch 10 loss: 0.3179614816544927\n",
      "  batch 20 loss: 0.3499495701038541\n",
      "  batch 30 loss: 0.4649799080507364\n",
      "  batch 40 loss: 0.20920610458124428\n",
      "  batch 50 loss: 0.33915776844896756\n",
      "  batch 60 loss: 0.16919808704442402\n",
      "  batch 70 loss: 0.2323819093741804\n",
      "  batch 80 loss: 0.05374451630123076\n",
      "  batch 90 loss: 0.18058529852232824\n",
      "  batch 100 loss: 0.2159424670933731\n",
      "  batch 110 loss: 0.2044751874602298\n",
      "  batch 120 loss: 0.3079453758604359\n",
      "  batch 130 loss: 0.20160406405611866\n",
      "  batch 140 loss: 0.35631873814272697\n",
      "  batch 150 loss: 0.2671040590321354\n",
      "  batch 160 loss: 0.1918134727384313\n",
      "  batch 170 loss: 0.25310804061628006\n",
      "  batch 180 loss: 0.2730742217027\n",
      "  batch 190 loss: 0.3299892705072125\n",
      "LOSS train 0.3299892705072125 valid 0.35307322309500494\n",
      "EPOCH 150:\n",
      "  batch 10 loss: 0.39537553556356214\n",
      "  batch 20 loss: 0.1473071343238189\n",
      "  batch 30 loss: 0.2637419519473042\n",
      "  batch 40 loss: 0.2695518170336982\n",
      "  batch 50 loss: 0.40937081677984677\n",
      "  batch 60 loss: 0.09535004768404179\n",
      "  batch 70 loss: 0.12631296484469204\n",
      "  batch 80 loss: 0.09142152004133094\n",
      "  batch 90 loss: 0.2789426561761502\n",
      "  batch 100 loss: 0.2024068583967164\n",
      "  batch 110 loss: 0.20370197742759047\n",
      "  batch 120 loss: 0.13340169365801557\n",
      "  batch 130 loss: 0.3742469196353341\n",
      "  batch 140 loss: 0.33953305057693794\n",
      "  batch 150 loss: 0.29396608234783345\n",
      "  batch 160 loss: 0.3582485787446785\n",
      "  batch 170 loss: 0.286158998090832\n",
      "  batch 180 loss: 0.19514561237374436\n",
      "  batch 190 loss: 0.303398708786699\n",
      "LOSS train 0.303398708786699 valid 0.2595777189782007\n",
      "EPOCH 151:\n",
      "  batch 10 loss: 0.10952467599418014\n",
      "  batch 20 loss: 0.19409447315265424\n",
      "  batch 30 loss: 0.11499440304905875\n",
      "  batch 40 loss: 0.1534093640937499\n",
      "  batch 50 loss: 0.27305686079707814\n",
      "  batch 60 loss: 0.09165102643382853\n",
      "  batch 70 loss: 0.08492942852317356\n",
      "  batch 80 loss: 0.1422685210054624\n",
      "  batch 90 loss: 0.11458773474041664\n",
      "  batch 100 loss: 0.48452736455110423\n",
      "  batch 110 loss: 0.48433406321837535\n",
      "  batch 120 loss: 0.07654812509099429\n",
      "  batch 130 loss: 0.18987141353936748\n",
      "  batch 140 loss: 0.06307538670007488\n",
      "  batch 150 loss: 0.1716168888480752\n",
      "  batch 160 loss: 0.4615776216607628\n",
      "  batch 170 loss: 0.4202814666965423\n",
      "  batch 180 loss: 0.5600282045685162\n",
      "  batch 190 loss: 0.20458074219623085\n",
      "LOSS train 0.20458074219623085 valid 0.47050069122113386\n",
      "EPOCH 152:\n",
      "  batch 10 loss: 0.21466293611847503\n",
      "  batch 20 loss: 0.21118659157582442\n",
      "  batch 30 loss: 0.3909771261507558\n",
      "  batch 40 loss: 0.18704776559534367\n",
      "  batch 50 loss: 0.20065302993352815\n",
      "  batch 60 loss: 0.08218675142343272\n",
      "  batch 70 loss: 0.22940301399794408\n",
      "  batch 80 loss: 0.1497355086634343\n",
      "  batch 90 loss: 0.19316890664467792\n",
      "  batch 100 loss: 0.13808563721686368\n",
      "  batch 110 loss: 0.15351090088333877\n",
      "  batch 120 loss: 0.295468227697711\n",
      "  batch 130 loss: 0.20096728811586217\n",
      "  batch 140 loss: 0.3229038135279552\n",
      "  batch 150 loss: 0.23011517464601638\n",
      "  batch 160 loss: 0.11868543177406536\n",
      "  batch 170 loss: 0.43282144677941686\n",
      "  batch 180 loss: 0.4146135681090527\n",
      "  batch 190 loss: 0.32971339459109006\n",
      "LOSS train 0.32971339459109006 valid 0.32433283216988246\n",
      "EPOCH 153:\n",
      "  batch 10 loss: 0.35276894779744905\n",
      "  batch 20 loss: 0.2048328257536923\n",
      "  batch 30 loss: 0.20608680245204597\n",
      "  batch 40 loss: 0.06279307772347238\n",
      "  batch 50 loss: 0.5107953484948666\n",
      "  batch 60 loss: 0.16479646155348746\n",
      "  batch 70 loss: 0.16486432058409264\n",
      "  batch 80 loss: 0.08926863919914467\n",
      "  batch 90 loss: 0.2637349992819509\n",
      "  batch 100 loss: 0.228619051608257\n",
      "  batch 110 loss: 0.2208339260523644\n",
      "  batch 120 loss: 0.3368167448834356\n",
      "  batch 130 loss: 0.2732026098861752\n",
      "  batch 140 loss: 0.365993023772171\n",
      "  batch 150 loss: 0.17611165356938727\n",
      "  batch 160 loss: 0.39858806078445924\n",
      "  batch 170 loss: 0.1531881149385299\n",
      "  batch 180 loss: 0.12550827663653763\n",
      "  batch 190 loss: 0.242064982854572\n",
      "LOSS train 0.242064982854572 valid 0.25918062254808877\n",
      "EPOCH 154:\n",
      "  batch 10 loss: 0.10275313209531305\n",
      "  batch 20 loss: 0.5305462830729084\n",
      "  batch 30 loss: 0.12383653997821967\n",
      "  batch 40 loss: 0.19158444254426285\n",
      "  batch 50 loss: 0.24730548641446148\n",
      "  batch 60 loss: 0.1503141258622236\n",
      "  batch 70 loss: 0.23821795983676566\n",
      "  batch 80 loss: 0.2886015137315553\n",
      "  batch 90 loss: 0.20476611014746596\n",
      "  batch 100 loss: 0.43581180502806094\n",
      "  batch 110 loss: 0.2262255593246664\n",
      "  batch 120 loss: 0.20192684287248994\n",
      "  batch 130 loss: 0.3352159962756559\n",
      "  batch 140 loss: 0.37806409943150354\n",
      "  batch 150 loss: 0.23506956529490708\n",
      "  batch 160 loss: 0.23187294595900312\n",
      "  batch 170 loss: 0.37997159270926206\n",
      "  batch 180 loss: 0.18180382932478095\n",
      "  batch 190 loss: 0.39953042948109213\n",
      "LOSS train 0.39953042948109213 valid 0.2711280022657518\n",
      "EPOCH 155:\n",
      "  batch 10 loss: 0.3862340257346659\n",
      "  batch 20 loss: 0.3577868350505014\n",
      "  batch 30 loss: 0.12395499446138274\n",
      "  batch 40 loss: 0.17620010597165675\n",
      "  batch 50 loss: 0.21144925081171095\n",
      "  batch 60 loss: 0.5396886417787755\n",
      "  batch 70 loss: 0.3955714447060018\n",
      "  batch 80 loss: 0.36085941878700395\n",
      "  batch 90 loss: 0.39051466112396155\n",
      "  batch 100 loss: 0.2082465765197412\n",
      "  batch 110 loss: 0.14685567785491002\n",
      "  batch 120 loss: 0.19562364734883886\n",
      "  batch 130 loss: 0.18724492017790909\n",
      "  batch 140 loss: 0.2249894307475188\n",
      "  batch 150 loss: 0.13344986475494808\n",
      "  batch 160 loss: 0.2963007708411169\n",
      "  batch 170 loss: 0.22520754285433214\n",
      "  batch 180 loss: 0.22530738519781152\n",
      "  batch 190 loss: 0.10374711532931542\n",
      "LOSS train 0.10374711532931542 valid 0.30676397201676037\n",
      "EPOCH 156:\n",
      "  batch 10 loss: 0.14219303326754015\n",
      "  batch 20 loss: 0.07519798021239694\n",
      "  batch 30 loss: 0.28309867767311514\n",
      "  batch 40 loss: 0.21765965763333953\n",
      "  batch 50 loss: 0.45484465122535767\n",
      "  batch 60 loss: 0.24221468056603043\n",
      "  batch 70 loss: 0.1390106314463992\n",
      "  batch 80 loss: 0.13837299196165986\n",
      "  batch 90 loss: 0.239093042072318\n",
      "  batch 100 loss: 0.7289205784763908\n",
      "  batch 110 loss: 0.3691738069028361\n",
      "  batch 120 loss: 0.3765549387346255\n",
      "  batch 130 loss: 0.3997765863623499\n",
      "  batch 140 loss: 0.15945266995113344\n",
      "  batch 150 loss: 0.13790111047273967\n",
      "  batch 160 loss: 0.07148066884656146\n",
      "  batch 170 loss: 0.13307565926370443\n",
      "  batch 180 loss: 0.3386769812799685\n",
      "  batch 190 loss: 0.0596766666003532\n",
      "LOSS train 0.0596766666003532 valid 0.26809145938920825\n",
      "EPOCH 157:\n",
      "  batch 10 loss: 0.2812077226008114\n",
      "  batch 20 loss: 0.14195904954249272\n",
      "  batch 30 loss: 0.22406001047165774\n",
      "  batch 40 loss: 0.2068596876243646\n",
      "  batch 50 loss: 0.14195936199741938\n",
      "  batch 60 loss: 0.19022528587811394\n",
      "  batch 70 loss: 0.31980323690841034\n",
      "  batch 80 loss: 0.28200904361783613\n",
      "  batch 90 loss: 0.1595417434224146\n",
      "  batch 100 loss: 0.1490498056897195\n",
      "  batch 110 loss: 0.13084552516156692\n",
      "  batch 120 loss: 0.160329966635436\n",
      "  batch 130 loss: 0.10658150402960018\n",
      "  batch 140 loss: 0.07979074253926229\n",
      "  batch 150 loss: 0.05643192305306002\n",
      "  batch 160 loss: 0.36604964651237426\n",
      "  batch 170 loss: 0.5947118070576835\n",
      "  batch 180 loss: 0.23507763387970043\n",
      "  batch 190 loss: 0.23365484672685852\n",
      "LOSS train 0.23365484672685852 valid 0.3684550500010795\n",
      "EPOCH 158:\n",
      "  batch 10 loss: 0.18615980439935811\n",
      "  batch 20 loss: 0.73417096291887\n",
      "  batch 30 loss: 0.2774894783333366\n",
      "  batch 40 loss: 0.13214421312040942\n",
      "  batch 50 loss: 0.1703668433437997\n",
      "  batch 60 loss: 0.1387992465921343\n",
      "  batch 70 loss: 0.14584769631328526\n",
      "  batch 80 loss: 0.21170787535229466\n",
      "  batch 90 loss: 0.09032377942548919\n",
      "  batch 100 loss: 0.22656603673176506\n",
      "  batch 110 loss: 0.268503267473352\n",
      "  batch 120 loss: 0.16200417455220303\n",
      "  batch 130 loss: 0.17028291525343775\n",
      "  batch 140 loss: 0.28762375202495605\n",
      "  batch 150 loss: 0.13205027417207021\n",
      "  batch 160 loss: 0.23318466646014713\n",
      "  batch 170 loss: 0.11814718168006948\n",
      "  batch 180 loss: 0.09965817462261839\n",
      "  batch 190 loss: 0.5794558746734765\n",
      "LOSS train 0.5794558746734765 valid 0.2775820079270302\n",
      "EPOCH 159:\n",
      "  batch 10 loss: 0.05939756350562675\n",
      "  batch 20 loss: 0.18638495374962077\n",
      "  batch 30 loss: 0.21500390776400308\n",
      "  batch 40 loss: 0.10579109278405667\n",
      "  batch 50 loss: 0.20477969121711795\n",
      "  batch 60 loss: 0.2206274250842398\n",
      "  batch 70 loss: 0.06845822545910778\n",
      "  batch 80 loss: 0.11573444985733658\n",
      "  batch 90 loss: 0.5658218969081645\n",
      "  batch 100 loss: 0.2863832782702957\n",
      "  batch 110 loss: 0.19500547720017494\n",
      "  batch 120 loss: 0.2536480883725744\n",
      "  batch 130 loss: 0.11533354987041093\n",
      "  batch 140 loss: 0.083100528747309\n",
      "  batch 150 loss: 0.5785917285702453\n",
      "  batch 160 loss: 0.26690562627500186\n",
      "  batch 170 loss: 0.6936139728572016\n",
      "  batch 180 loss: 0.19452449068558053\n",
      "  batch 190 loss: 0.21190080805813521\n",
      "LOSS train 0.21190080805813521 valid 0.2712736501439045\n",
      "EPOCH 160:\n",
      "  batch 10 loss: 0.23467176393951378\n",
      "  batch 20 loss: 0.13987539906192978\n",
      "  batch 30 loss: 0.1753222638490115\n",
      "  batch 40 loss: 0.19100497922627255\n",
      "  batch 50 loss: 0.4596059448636879\n",
      "  batch 60 loss: 0.2966847080191656\n",
      "  batch 70 loss: 0.2424319869838655\n",
      "  batch 80 loss: 0.13875371222529792\n",
      "  batch 90 loss: 0.37679564330828724\n",
      "  batch 100 loss: 0.31507919745054097\n",
      "  batch 110 loss: 0.26408729994836905\n",
      "  batch 120 loss: 0.05231306503756059\n",
      "  batch 130 loss: 0.08115160184906926\n",
      "  batch 140 loss: 0.05302873728214763\n",
      "  batch 150 loss: 0.18400986247725087\n",
      "  batch 160 loss: 0.15074038744542123\n",
      "  batch 170 loss: 0.5496080827564583\n",
      "  batch 180 loss: 0.5309067785858133\n",
      "  batch 190 loss: 0.33276262586105076\n",
      "LOSS train 0.33276262586105076 valid 0.3934780825730822\n",
      "EPOCH 161:\n",
      "  batch 10 loss: 0.5180642512301347\n",
      "  batch 20 loss: 0.40468928475088434\n",
      "  batch 30 loss: 0.13395846721541602\n",
      "  batch 40 loss: 0.14124651441161404\n",
      "  batch 50 loss: 0.2692870470396883\n",
      "  batch 60 loss: 0.3534588699514643\n",
      "  batch 70 loss: 0.288172439961636\n",
      "  batch 80 loss: 0.1414571564819198\n",
      "  batch 90 loss: 0.12439253228367306\n",
      "  batch 100 loss: 0.14255849384971953\n",
      "  batch 110 loss: 0.20195886211367906\n",
      "  batch 120 loss: 0.3617331403064782\n",
      "  batch 130 loss: 0.10568571916610381\n",
      "  batch 140 loss: 0.07423954071573462\n",
      "  batch 150 loss: 0.19626441413392967\n",
      "  batch 160 loss: 0.21837061290789278\n",
      "  batch 170 loss: 0.23462219127468414\n",
      "  batch 180 loss: 0.11658486353626359\n",
      "  batch 190 loss: 0.2794759587080534\n",
      "LOSS train 0.2794759587080534 valid 0.3403128157593785\n",
      "EPOCH 162:\n",
      "  batch 10 loss: 0.23582598390057682\n",
      "  batch 20 loss: 0.4485801405146731\n",
      "  batch 30 loss: 0.2887984038262402\n",
      "  batch 40 loss: 0.08017067196342395\n",
      "  batch 50 loss: 0.23847012466058004\n",
      "  batch 60 loss: 0.14058796963990972\n",
      "  batch 70 loss: 0.1993867099787167\n",
      "  batch 80 loss: 0.13085284941407735\n",
      "  batch 90 loss: 0.06838640320929698\n",
      "  batch 100 loss: 0.07787451769736435\n",
      "  batch 110 loss: 0.25080161309597315\n",
      "  batch 120 loss: 0.1306226110767966\n",
      "  batch 130 loss: 0.3683937240290106\n",
      "  batch 140 loss: 0.1912208628370081\n",
      "  batch 150 loss: 0.0777105623843454\n",
      "  batch 160 loss: 0.15457801036245655\n",
      "  batch 170 loss: 0.3794055165883492\n",
      "  batch 180 loss: 0.2942945234220133\n",
      "  batch 190 loss: 0.34130236403980235\n",
      "LOSS train 0.34130236403980235 valid 1.5934273979477531\n",
      "EPOCH 163:\n",
      "  batch 10 loss: 0.41337731170569897\n",
      "  batch 20 loss: 0.12032306323671946\n",
      "  batch 30 loss: 0.29011753934209994\n",
      "  batch 40 loss: 0.11341789607686223\n",
      "  batch 50 loss: 0.17064657011396775\n",
      "  batch 60 loss: 0.4900656329067715\n",
      "  batch 70 loss: 0.24112209626509867\n",
      "  batch 80 loss: 0.10769243540917159\n",
      "  batch 90 loss: 0.10177104129234067\n",
      "  batch 100 loss: 0.12667570195480948\n",
      "  batch 110 loss: 0.17232286500202462\n",
      "  batch 120 loss: 0.3467906249077714\n",
      "  batch 130 loss: 0.26067346532654484\n",
      "  batch 140 loss: 0.19566625828811085\n",
      "  batch 150 loss: 0.11924223162641283\n",
      "  batch 160 loss: 0.13518238431097415\n",
      "  batch 170 loss: 0.10396065578916022\n",
      "  batch 180 loss: 0.2322011957039649\n",
      "  batch 190 loss: 0.378979379791781\n",
      "LOSS train 0.378979379791781 valid 0.2879436377953448\n",
      "EPOCH 164:\n",
      "  batch 10 loss: 0.16257406937529595\n",
      "  batch 20 loss: 0.2696788835542975\n",
      "  batch 30 loss: 0.15015882903826422\n",
      "  batch 40 loss: 0.1560271090404058\n",
      "  batch 50 loss: 0.1627085414238536\n",
      "  batch 60 loss: 0.1192235131497\n",
      "  batch 70 loss: 0.15619601085090834\n",
      "  batch 80 loss: 0.0453464485501172\n",
      "  batch 90 loss: 0.11431344153907048\n",
      "  batch 100 loss: 0.2823208197749409\n",
      "  batch 110 loss: 0.9982786806816876\n",
      "  batch 120 loss: 0.2953672146826648\n",
      "  batch 130 loss: 0.2863787748627146\n",
      "  batch 140 loss: 0.1940840071449202\n",
      "  batch 150 loss: 0.1809874180733459\n",
      "  batch 160 loss: 0.1487807859200984\n",
      "  batch 170 loss: 0.08094814048017725\n",
      "  batch 180 loss: 0.1530358679337951\n",
      "  batch 190 loss: 0.45840787417109824\n",
      "LOSS train 0.45840787417109824 valid 0.3770287525742391\n",
      "EPOCH 165:\n",
      "  batch 10 loss: 0.2409333581548708\n",
      "  batch 20 loss: 0.12370353867263476\n",
      "  batch 30 loss: 0.15094327799452004\n",
      "  batch 40 loss: 0.29554298787957123\n",
      "  batch 50 loss: 0.20877672368951608\n",
      "  batch 60 loss: 0.07549661997963994\n",
      "  batch 70 loss: 0.30996329342815443\n",
      "  batch 80 loss: 0.127964688600332\n",
      "  batch 90 loss: 0.13393461017258232\n",
      "  batch 100 loss: 0.20688102004241954\n",
      "  batch 110 loss: 0.147267700680095\n",
      "  batch 120 loss: 0.12980572574451799\n",
      "  batch 130 loss: 0.2679074522384326\n",
      "  batch 140 loss: 0.3992494602634906\n",
      "  batch 150 loss: 0.06467511775554158\n",
      "  batch 160 loss: 0.1997438194724964\n",
      "  batch 170 loss: 0.11756107434994192\n",
      "  batch 180 loss: 0.18728419930776\n",
      "  batch 190 loss: 0.5615159472235064\n",
      "LOSS train 0.5615159472235064 valid 0.2783559670751157\n",
      "EPOCH 166:\n",
      "  batch 10 loss: 0.11044844610969448\n",
      "  batch 20 loss: 0.05900183714111336\n",
      "  batch 30 loss: 0.29039814222087446\n",
      "  batch 40 loss: 0.11655198768385162\n",
      "  batch 50 loss: 0.182467217562953\n",
      "  batch 60 loss: 0.1611251342163996\n",
      "  batch 70 loss: 0.15464089145461912\n",
      "  batch 80 loss: 0.11536538999644108\n",
      "  batch 90 loss: 0.23172539323568345\n",
      "  batch 100 loss: 0.052190707896988896\n",
      "  batch 110 loss: 0.3860441921344318\n",
      "  batch 120 loss: 0.417391880271316\n",
      "  batch 130 loss: 0.19441720784870994\n",
      "  batch 140 loss: 0.3598372928550816\n",
      "  batch 150 loss: 0.23503999004651632\n",
      "  batch 160 loss: 0.2874016884246885\n",
      "  batch 170 loss: 0.32825341653224316\n",
      "  batch 180 loss: 0.1113555475118119\n",
      "  batch 190 loss: 0.17111273972968774\n",
      "LOSS train 0.17111273972968774 valid 5.438027331169848\n",
      "EPOCH 167:\n",
      "  batch 10 loss: 1.961742958129753\n",
      "  batch 20 loss: 0.3204978548863437\n",
      "  batch 30 loss: 0.23647168115712702\n",
      "  batch 40 loss: 0.2574354652286274\n",
      "  batch 50 loss: 0.35145862086355917\n",
      "  batch 60 loss: 0.305686572232662\n",
      "  batch 70 loss: 0.4516551251344936\n",
      "  batch 80 loss: 0.17908549984858838\n",
      "  batch 90 loss: 0.2209572496722103\n",
      "  batch 100 loss: 0.0988142779966438\n",
      "  batch 110 loss: 0.26258448669905193\n",
      "  batch 120 loss: 0.1025591602534405\n",
      "  batch 130 loss: 0.28053168586047833\n",
      "  batch 140 loss: 0.19411060789061593\n",
      "  batch 150 loss: 0.09012238141731359\n",
      "  batch 160 loss: 0.1844976410829986\n",
      "  batch 170 loss: 0.18816572917858138\n",
      "  batch 180 loss: 0.08304904676842853\n",
      "  batch 190 loss: 0.4128265092302172\n",
      "LOSS train 0.4128265092302172 valid 0.42296967683734155\n",
      "EPOCH 168:\n",
      "  batch 10 loss: 0.16173341528392485\n",
      "  batch 20 loss: 0.3356202184600988\n",
      "  batch 30 loss: 0.10205112650437513\n",
      "  batch 40 loss: 0.0802048876614208\n",
      "  batch 50 loss: 0.11244092981032736\n",
      "  batch 60 loss: 0.20446399539323465\n",
      "  batch 70 loss: 0.30999920286340055\n",
      "  batch 80 loss: 0.08058576763942256\n",
      "  batch 90 loss: 0.054837928296728934\n",
      "  batch 100 loss: 0.46064846735680476\n",
      "  batch 110 loss: 0.09041616190806963\n",
      "  batch 120 loss: 0.14545144671574234\n",
      "  batch 130 loss: 0.1776070679530676\n",
      "  batch 140 loss: 0.20888023848201556\n",
      "  batch 150 loss: 0.6758565539324991\n",
      "  batch 160 loss: 0.20324399808741872\n",
      "  batch 170 loss: 0.29707673412740404\n",
      "  batch 180 loss: 0.16115888647036627\n",
      "  batch 190 loss: 0.20271806303608172\n",
      "LOSS train 0.20271806303608172 valid 0.29832776783190734\n",
      "EPOCH 169:\n",
      "  batch 10 loss: 0.1735513894702308\n",
      "  batch 20 loss: 0.26960076736031624\n",
      "  batch 30 loss: 0.19289397380198353\n",
      "  batch 40 loss: 0.15627280511544087\n",
      "  batch 50 loss: 0.3274037857212534\n",
      "  batch 60 loss: 0.15433654464650318\n",
      "  batch 70 loss: 0.0934183457349718\n",
      "  batch 80 loss: 0.0636164734067279\n",
      "  batch 90 loss: 0.12108409916254459\n",
      "  batch 100 loss: 0.7124301917931006\n",
      "  batch 110 loss: 0.3287355134569225\n",
      "  batch 120 loss: 0.07615284260791669\n",
      "  batch 130 loss: 0.4923460754923326\n",
      "  batch 140 loss: 0.12140232217634547\n",
      "  batch 150 loss: 0.28457104186072685\n",
      "  batch 160 loss: 0.21275996533295255\n",
      "  batch 170 loss: 0.23939838865553612\n",
      "  batch 180 loss: 0.25138574735901786\n",
      "  batch 190 loss: 0.16205891048884952\n",
      "LOSS train 0.16205891048884952 valid 0.2604932855183273\n",
      "EPOCH 170:\n",
      "  batch 10 loss: 0.21183184745605105\n",
      "  batch 20 loss: 0.12523515098982899\n",
      "  batch 30 loss: 0.45295534835440776\n",
      "  batch 40 loss: 0.26544018644526657\n",
      "  batch 50 loss: 0.23055132302652054\n",
      "  batch 60 loss: 0.2777875937533281\n",
      "  batch 70 loss: 0.12245190243374963\n",
      "  batch 80 loss: 0.17369726710467148\n",
      "  batch 90 loss: 0.11115969868296816\n",
      "  batch 100 loss: 0.15510984365378136\n",
      "  batch 110 loss: 0.20927776832140807\n",
      "  batch 120 loss: 0.23923922647159088\n",
      "  batch 130 loss: 0.10922196311876178\n",
      "  batch 140 loss: 0.04990169163197607\n",
      "  batch 150 loss: 0.04993073106211341\n",
      "  batch 160 loss: 0.04078795014338539\n",
      "  batch 170 loss: 0.23820986408009048\n",
      "  batch 180 loss: 0.15181234296360344\n",
      "  batch 190 loss: 0.3470014783910301\n",
      "LOSS train 0.3470014783910301 valid 1.078843822420962\n",
      "EPOCH 171:\n",
      "  batch 10 loss: 0.382457879073263\n",
      "  batch 20 loss: 0.2556208117699498\n",
      "  batch 30 loss: 0.08095980892958324\n",
      "  batch 40 loss: 0.10613558324985206\n",
      "  batch 50 loss: 0.33139993493805375\n",
      "  batch 60 loss: 0.13971017638205013\n",
      "  batch 70 loss: 0.1745357235418851\n",
      "  batch 80 loss: 0.5233455267578393\n",
      "  batch 90 loss: 0.16671730932539502\n",
      "  batch 100 loss: 0.34847746850864497\n",
      "  batch 110 loss: 0.10352436567845871\n",
      "  batch 120 loss: 0.10370152167806737\n",
      "  batch 130 loss: 0.1890875785076787\n",
      "  batch 140 loss: 0.32242210692877504\n",
      "  batch 150 loss: 0.31658278964623604\n",
      "  batch 160 loss: 0.2245698580842145\n",
      "  batch 170 loss: 0.380190666125236\n",
      "  batch 180 loss: 0.20000488142322864\n",
      "  batch 190 loss: 0.2679464668792207\n",
      "LOSS train 0.2679464668792207 valid 1.464127599415056\n",
      "EPOCH 172:\n",
      "  batch 10 loss: 0.5547269421454984\n",
      "  batch 20 loss: 0.18932937441295508\n",
      "  batch 30 loss: 0.17731751108076424\n",
      "  batch 40 loss: 0.07245965573156354\n",
      "  batch 50 loss: 0.3217512181325219\n",
      "  batch 60 loss: 0.16885851439801627\n",
      "  batch 70 loss: 0.2904615797880979\n",
      "  batch 80 loss: 0.09365946419966349\n",
      "  batch 90 loss: 0.30228881690600246\n",
      "  batch 100 loss: 0.26907112007791056\n",
      "  batch 110 loss: 0.19708483700651414\n",
      "  batch 120 loss: 0.2643540489654697\n",
      "  batch 130 loss: 0.23515491489106352\n",
      "  batch 140 loss: 0.6226486317675153\n",
      "  batch 150 loss: 0.23018082190092173\n",
      "  batch 160 loss: 0.1266138092330948\n",
      "  batch 170 loss: 0.0877347187293708\n",
      "  batch 180 loss: 0.09267965535509574\n",
      "  batch 190 loss: 0.04730388522148132\n",
      "LOSS train 0.04730388522148132 valid 0.28067566586721043\n",
      "EPOCH 173:\n",
      "  batch 10 loss: 0.03780011116423339\n",
      "  batch 20 loss: 0.1350420621180092\n",
      "  batch 30 loss: 0.2990129579229688\n",
      "  batch 40 loss: 0.2221728575117595\n",
      "  batch 50 loss: 0.39598277268632953\n",
      "  batch 60 loss: 0.15960734445015987\n",
      "  batch 70 loss: 0.2296807713471935\n",
      "  batch 80 loss: 0.23260733145070844\n",
      "  batch 90 loss: 0.09665524955707952\n",
      "  batch 100 loss: 0.2787043077281851\n",
      "  batch 110 loss: 0.19323639568683576\n",
      "  batch 120 loss: 0.17122568480681366\n",
      "  batch 130 loss: 0.18607306701997003\n",
      "  batch 140 loss: 0.11205754767543112\n",
      "  batch 150 loss: 0.12477845070643526\n",
      "  batch 160 loss: 0.44314604402570695\n",
      "  batch 170 loss: 0.15536170806699373\n",
      "  batch 180 loss: 0.11187077868316919\n",
      "  batch 190 loss: 0.1032951785300611\n",
      "LOSS train 0.1032951785300611 valid 0.6340050314521506\n",
      "EPOCH 174:\n",
      "  batch 10 loss: 0.5359245974712394\n",
      "  batch 20 loss: 0.09050302128089242\n",
      "  batch 30 loss: 0.29164081606650144\n",
      "  batch 40 loss: 0.4233809539156027\n",
      "  batch 50 loss: 0.21064212181299807\n",
      "  batch 60 loss: 0.11051050753567324\n",
      "  batch 70 loss: 0.2853393980311921\n",
      "  batch 80 loss: 0.06932190065981558\n",
      "  batch 90 loss: 0.19404631396782862\n",
      "  batch 100 loss: 0.23230671696883293\n",
      "  batch 110 loss: 0.07850655897018441\n",
      "  batch 120 loss: 0.3769494662923307\n",
      "  batch 130 loss: 0.18940633801048534\n",
      "  batch 140 loss: 0.09073987759875309\n",
      "  batch 150 loss: 0.06769554499442165\n",
      "  batch 160 loss: 0.26288844141872686\n",
      "  batch 170 loss: 0.1337243369729549\n",
      "  batch 180 loss: 0.19156824684505408\n",
      "  batch 190 loss: 0.10565220094731557\n",
      "LOSS train 0.10565220094731557 valid 0.5337973857934301\n",
      "EPOCH 175:\n",
      "  batch 10 loss: 0.12231328770212713\n",
      "  batch 20 loss: 0.04401099816814167\n",
      "  batch 30 loss: 0.09109303977456876\n",
      "  batch 40 loss: 0.1299883270796272\n",
      "  batch 50 loss: 0.17114130265981659\n",
      "  batch 60 loss: 0.10396384226150986\n",
      "  batch 70 loss: 0.5091251169931639\n",
      "  batch 80 loss: 0.13982603715437564\n",
      "  batch 90 loss: 0.1226369031329341\n",
      "  batch 100 loss: 0.11536896670504575\n",
      "  batch 110 loss: 0.07350465081603944\n",
      "  batch 120 loss: 0.7283806618083872\n",
      "  batch 130 loss: 0.2696775834353957\n",
      "  batch 140 loss: 0.17865411541115464\n",
      "  batch 150 loss: 0.19785437560658464\n",
      "  batch 160 loss: 0.15461573037646303\n",
      "  batch 170 loss: 0.4878244354184062\n",
      "  batch 180 loss: 0.19417642599412374\n",
      "  batch 190 loss: 0.11300643612412387\n",
      "LOSS train 0.11300643612412387 valid 0.7399666446460509\n",
      "EPOCH 176:\n",
      "  batch 10 loss: 0.3355463511459675\n",
      "  batch 20 loss: 0.19484335590204865\n",
      "  batch 30 loss: 0.20562544639833505\n",
      "  batch 40 loss: 0.13566878571709823\n",
      "  batch 50 loss: 0.18802850099909846\n",
      "  batch 60 loss: 0.09655838060716633\n",
      "  batch 70 loss: 0.2046818427076687\n",
      "  batch 80 loss: 0.11296114809592836\n",
      "  batch 90 loss: 0.1649913560026107\n",
      "  batch 100 loss: 0.15425930778692418\n",
      "  batch 110 loss: 0.34327238309160746\n",
      "  batch 120 loss: 0.2208687632647525\n",
      "  batch 130 loss: 0.15342855598682945\n",
      "  batch 140 loss: 0.07660820084638545\n",
      "  batch 150 loss: 0.15379794516984474\n",
      "  batch 160 loss: 0.24275254847161704\n",
      "  batch 170 loss: 0.09515299456543289\n",
      "  batch 180 loss: 0.3354499519427918\n",
      "  batch 190 loss: 0.15621027988181596\n",
      "LOSS train 0.15621027988181596 valid 0.31792703706598613\n",
      "EPOCH 177:\n",
      "  batch 10 loss: 0.2819928614066157\n",
      "  batch 20 loss: 0.09457735146861523\n",
      "  batch 30 loss: 0.2955694587002654\n",
      "  batch 40 loss: 0.11716618032787665\n",
      "  batch 50 loss: 0.19890063887801263\n",
      "  batch 60 loss: 0.16136260922430665\n",
      "  batch 70 loss: 0.16076742821460357\n",
      "  batch 80 loss: 0.06783547581021594\n",
      "  batch 90 loss: 0.30362135691480036\n",
      "  batch 100 loss: 0.24751177561902296\n",
      "  batch 110 loss: 0.3105997895470864\n",
      "  batch 120 loss: 0.144575157660438\n",
      "  batch 130 loss: 0.3125069095996878\n",
      "  batch 140 loss: 0.20663936919913795\n",
      "  batch 150 loss: 0.1243041638452496\n",
      "  batch 160 loss: 0.1454448213438809\n",
      "  batch 170 loss: 0.09429274385984172\n",
      "  batch 180 loss: 0.21168064368389422\n",
      "  batch 190 loss: 0.17999431495106819\n",
      "LOSS train 0.17999431495106819 valid 0.3478868878697476\n",
      "EPOCH 178:\n",
      "  batch 10 loss: 0.22784009894348856\n",
      "  batch 20 loss: 0.4666862644200137\n",
      "  batch 30 loss: 0.11436158966043877\n",
      "  batch 40 loss: 0.4300090620954961\n",
      "  batch 50 loss: 0.08065977023761661\n",
      "  batch 60 loss: 0.15102063582871778\n",
      "  batch 70 loss: 0.17510575740361672\n",
      "  batch 80 loss: 0.23912219427120363\n",
      "  batch 90 loss: 0.25529336460922425\n",
      "  batch 100 loss: 0.24864321407595752\n",
      "  batch 110 loss: 0.11598874539195095\n",
      "  batch 120 loss: 0.0945933615166723\n",
      "  batch 130 loss: 0.09024846018182871\n",
      "  batch 140 loss: 0.29143967407644594\n",
      "  batch 150 loss: 0.1474036418079777\n",
      "  batch 160 loss: 0.13969771766187478\n",
      "  batch 170 loss: 0.08215106291399935\n",
      "  batch 180 loss: 0.2681898526603163\n",
      "  batch 190 loss: 0.18413519600435393\n",
      "LOSS train 0.18413519600435393 valid 0.28231446754221695\n",
      "EPOCH 179:\n",
      "  batch 10 loss: 0.1465680655877577\n",
      "  batch 20 loss: 0.7770691763002106\n",
      "  batch 30 loss: 0.7362758496888773\n",
      "  batch 40 loss: 0.1751459083199734\n",
      "  batch 50 loss: 0.19952624060824747\n",
      "  batch 60 loss: 0.14991711327202212\n",
      "  batch 70 loss: 0.23034265996247996\n",
      "  batch 80 loss: 0.25695166387304197\n",
      "  batch 90 loss: 0.22928209749188683\n",
      "  batch 100 loss: 0.18151988946719938\n",
      "  batch 110 loss: 0.12289463165070628\n",
      "  batch 120 loss: 0.22429341034703612\n",
      "  batch 130 loss: 0.24334827243656038\n",
      "  batch 140 loss: 0.06476967698854423\n",
      "  batch 150 loss: 0.05380553281229368\n",
      "  batch 160 loss: 0.16186136302624163\n",
      "  batch 170 loss: 0.38583217877239806\n",
      "  batch 180 loss: 0.18040990275367222\n",
      "  batch 190 loss: 0.245978136247777\n",
      "LOSS train 0.245978136247777 valid 0.27401807890526425\n",
      "EPOCH 180:\n",
      "  batch 10 loss: 0.2934549386118306\n",
      "  batch 20 loss: 0.23354894789372338\n",
      "  batch 30 loss: 0.13792234585253027\n",
      "  batch 40 loss: 0.23102359959484603\n",
      "  batch 50 loss: 0.12145594460707798\n",
      "  batch 60 loss: 0.0703567411640961\n",
      "  batch 70 loss: 0.20832223346515094\n",
      "  batch 80 loss: 0.2722897854876237\n",
      "  batch 90 loss: 0.2656085839640582\n",
      "  batch 100 loss: 0.11468062060084776\n",
      "  batch 110 loss: 0.098118010111466\n",
      "  batch 120 loss: 0.22462461531304143\n",
      "  batch 130 loss: 0.3038985080164821\n",
      "  batch 140 loss: 0.18791280895379714\n",
      "  batch 150 loss: 0.10955622052892977\n",
      "  batch 160 loss: 0.3095674731059262\n",
      "  batch 170 loss: 0.174220902601337\n",
      "  batch 180 loss: 0.11316455081960157\n",
      "  batch 190 loss: 0.1292461796213047\n",
      "LOSS train 0.1292461796213047 valid 0.6076914617376326\n",
      "EPOCH 181:\n",
      "  batch 10 loss: 0.24420127807261452\n",
      "  batch 20 loss: 0.1707608065993554\n",
      "  batch 30 loss: 0.24284223535960336\n",
      "  batch 40 loss: 0.06779822218122718\n",
      "  batch 50 loss: 0.13842633032472804\n",
      "  batch 60 loss: 0.4650126737464234\n",
      "  batch 70 loss: 0.07646973677328787\n",
      "  batch 80 loss: 0.03657834694331541\n",
      "  batch 90 loss: 0.15015465593660338\n",
      "  batch 100 loss: 0.25498075181476454\n",
      "  batch 110 loss: 0.051507470785782064\n",
      "  batch 120 loss: 0.07601737022341695\n",
      "  batch 130 loss: 0.2646719340988966\n",
      "  batch 140 loss: 0.0914989447564949\n",
      "  batch 150 loss: 0.12643350424607433\n",
      "  batch 160 loss: 0.3549190356210602\n",
      "  batch 170 loss: 0.22050718992741167\n",
      "  batch 180 loss: 0.11405557644247892\n",
      "  batch 190 loss: 0.2835506315741213\n",
      "LOSS train 0.2835506315741213 valid 0.33924172392678487\n",
      "EPOCH 182:\n",
      "  batch 10 loss: 0.13167586988674884\n",
      "  batch 20 loss: 0.21242607195126767\n",
      "  batch 30 loss: 0.138751236741291\n",
      "  batch 40 loss: 0.26341421317956704\n",
      "  batch 50 loss: 0.1533619782839196\n",
      "  batch 60 loss: 0.515759495194834\n",
      "  batch 70 loss: 0.1550655162935982\n",
      "  batch 80 loss: 0.07326849874521031\n",
      "  batch 90 loss: 0.5048684773400055\n",
      "  batch 100 loss: 0.36791658548843315\n",
      "  batch 110 loss: 0.07245562836105819\n",
      "  batch 120 loss: 0.26508467855551315\n",
      "  batch 130 loss: 0.1134026367264596\n",
      "  batch 140 loss: 0.3225621409278062\n",
      "  batch 150 loss: 0.2744095328594881\n",
      "  batch 160 loss: 0.17712730739272048\n",
      "  batch 170 loss: 0.2338914446581839\n",
      "  batch 180 loss: 0.07500260160882136\n",
      "  batch 190 loss: 0.05442580880212518\n",
      "LOSS train 0.05442580880212518 valid 0.2854695740764339\n",
      "EPOCH 183:\n",
      "  batch 10 loss: 0.39220114305153403\n",
      "  batch 20 loss: 0.3019563729609217\n",
      "  batch 30 loss: 0.12346983765833101\n",
      "  batch 40 loss: 0.3458835378471122\n",
      "  batch 50 loss: 0.1478657032186675\n",
      "  batch 60 loss: 0.18958743034818326\n",
      "  batch 70 loss: 0.2132467361221643\n",
      "  batch 80 loss: 0.16654146638338715\n",
      "  batch 90 loss: 0.08944484060484684\n",
      "  batch 100 loss: 0.20926228519911091\n",
      "  batch 110 loss: 0.3251547897087221\n",
      "  batch 120 loss: 0.12383038259531531\n",
      "  batch 130 loss: 0.09373220282191141\n",
      "  batch 140 loss: 0.1075838948439923\n",
      "  batch 150 loss: 0.13491503669174562\n",
      "  batch 160 loss: 0.11312369725467306\n",
      "  batch 170 loss: 0.1777907982007491\n",
      "  batch 180 loss: 0.10748478434552453\n",
      "  batch 190 loss: 0.24701205657811443\n",
      "LOSS train 0.24701205657811443 valid 0.27099132251937474\n",
      "EPOCH 184:\n",
      "  batch 10 loss: 0.8401126095812828\n",
      "  batch 20 loss: 0.2595182312887118\n",
      "  batch 30 loss: 0.12152029524295357\n",
      "  batch 40 loss: 0.05408833833753306\n",
      "  batch 50 loss: 0.05008294879226014\n",
      "  batch 60 loss: 0.2507618983922839\n",
      "  batch 70 loss: 0.2487363828863181\n",
      "  batch 80 loss: 0.26633452304135974\n",
      "  batch 90 loss: 0.13142648532011664\n",
      "  batch 100 loss: 0.08300131553442043\n",
      "  batch 110 loss: 0.05859787621154737\n",
      "  batch 120 loss: 0.2402714754356566\n",
      "  batch 130 loss: 0.19764269541210525\n",
      "  batch 140 loss: 0.13560788785030126\n",
      "  batch 150 loss: 0.10368949856401741\n",
      "  batch 160 loss: 0.15940782247780588\n",
      "  batch 170 loss: 0.10724954011748196\n",
      "  batch 180 loss: 0.05398895107314274\n",
      "  batch 190 loss: 0.08224799936933777\n",
      "LOSS train 0.08224799936933777 valid 0.43206764749121274\n",
      "EPOCH 185:\n",
      "  batch 10 loss: 0.08685687038766901\n",
      "  batch 20 loss: 0.05021436734655253\n",
      "  batch 30 loss: 0.06005171092137971\n",
      "  batch 40 loss: 0.09218310017740805\n",
      "  batch 50 loss: 0.08452699311817469\n",
      "  batch 60 loss: 0.24212870733026648\n",
      "  batch 70 loss: 0.20250486570478188\n",
      "  batch 80 loss: 0.2341756085475936\n",
      "  batch 90 loss: 0.10951659677139106\n",
      "  batch 100 loss: 0.19309657226403942\n",
      "  batch 110 loss: 0.806544031092676\n",
      "  batch 120 loss: 0.22548821376567502\n",
      "  batch 130 loss: 0.3947264930080564\n",
      "  batch 140 loss: 0.09798587041987047\n",
      "  batch 150 loss: 0.16692140250124793\n",
      "  batch 160 loss: 0.16551662735333822\n",
      "  batch 170 loss: 0.05355470055046681\n",
      "  batch 180 loss: 0.14654621001845952\n",
      "  batch 190 loss: 0.10238627998519405\n",
      "LOSS train 0.10238627998519405 valid 0.3447281611508795\n",
      "EPOCH 186:\n",
      "  batch 10 loss: 0.10180059688389065\n",
      "  batch 20 loss: 0.25355125918572413\n",
      "  batch 30 loss: 0.10212044299787522\n",
      "  batch 40 loss: 0.07933827585284234\n",
      "  batch 50 loss: 0.16420763950877698\n",
      "  batch 60 loss: 0.29470911926036936\n",
      "  batch 70 loss: 0.482814909100307\n",
      "  batch 80 loss: 0.2519265983678906\n",
      "  batch 90 loss: 0.17456003164152206\n",
      "  batch 100 loss: 0.34543265405918644\n",
      "  batch 110 loss: 0.24951556962914764\n",
      "  batch 120 loss: 0.0993641887776903\n",
      "  batch 130 loss: 0.20320171525977457\n",
      "  batch 140 loss: 0.04387546329644465\n",
      "  batch 150 loss: 0.28867605459445256\n",
      "  batch 160 loss: 0.16350437191256334\n",
      "  batch 170 loss: 0.17458726034610664\n",
      "  batch 180 loss: 0.19627849658372726\n",
      "  batch 190 loss: 0.20144088273718808\n",
      "LOSS train 0.20144088273718808 valid 0.2721197035767452\n",
      "EPOCH 187:\n",
      "  batch 10 loss: 0.1900939167855995\n",
      "  batch 20 loss: 0.03467951453785645\n",
      "  batch 30 loss: 0.080013950588841\n",
      "  batch 40 loss: 0.09099195784478979\n",
      "  batch 50 loss: 0.042341320504783654\n",
      "  batch 60 loss: 0.12710735544333146\n",
      "  batch 70 loss: 0.10512987622514629\n",
      "  batch 80 loss: 0.10350810661393553\n",
      "  batch 90 loss: 0.13062107677278617\n",
      "  batch 100 loss: 0.12578170967144614\n",
      "  batch 110 loss: 0.0950259647338271\n",
      "  batch 120 loss: 0.14608137426766915\n",
      "  batch 130 loss: 0.19500761245908507\n",
      "  batch 140 loss: 0.6766860983780134\n",
      "  batch 150 loss: 0.43242495125793995\n",
      "  batch 160 loss: 0.13042092403593414\n",
      "  batch 170 loss: 0.08441280658153119\n",
      "  batch 180 loss: 0.2998241310251615\n",
      "  batch 190 loss: 0.27914427610846815\n",
      "LOSS train 0.27914427610846815 valid 0.26422138516418503\n",
      "EPOCH 188:\n",
      "  batch 10 loss: 0.27380535315605814\n",
      "  batch 20 loss: 0.1364903495501494\n",
      "  batch 30 loss: 0.10699671313313956\n",
      "  batch 40 loss: 0.22409549871536\n",
      "  batch 50 loss: 0.08995650517217654\n",
      "  batch 60 loss: 0.12969675642252695\n",
      "  batch 70 loss: 0.17174386586848414\n",
      "  batch 80 loss: 0.16183775827403224\n",
      "  batch 90 loss: 0.18881424077044356\n",
      "  batch 100 loss: 0.2251712871900054\n",
      "  batch 110 loss: 0.033841502191535255\n",
      "  batch 120 loss: 0.09882620177390891\n",
      "  batch 130 loss: 0.12886165423537932\n",
      "  batch 140 loss: 0.21158184573050676\n",
      "  batch 150 loss: 0.15039379030436067\n",
      "  batch 160 loss: 0.7926082233128454\n",
      "  batch 170 loss: 0.22022657548077404\n",
      "  batch 180 loss: 0.2559884972648433\n",
      "  batch 190 loss: 0.29565666332127877\n",
      "LOSS train 0.29565666332127877 valid 0.29022532776820625\n",
      "EPOCH 189:\n",
      "  batch 10 loss: 0.040935489052753835\n",
      "  batch 20 loss: 0.15047196769774018\n",
      "  batch 30 loss: 0.10200897558802353\n",
      "  batch 40 loss: 0.1995063594385101\n",
      "  batch 50 loss: 0.2969895202235989\n",
      "  batch 60 loss: 0.16132688574075474\n",
      "  batch 70 loss: 0.07594804357041766\n",
      "  batch 80 loss: 0.19223003534589225\n",
      "  batch 90 loss: 0.22806278626351287\n",
      "  batch 100 loss: 0.09908660163309832\n",
      "  batch 110 loss: 0.3065931942901898\n",
      "  batch 120 loss: 0.18920798477533934\n",
      "  batch 130 loss: 0.2285587019947343\n",
      "  batch 140 loss: 0.18752456013135088\n",
      "  batch 150 loss: 0.18168032794726513\n",
      "  batch 160 loss: 0.19864940860613842\n",
      "  batch 170 loss: 0.09153853386537776\n",
      "  batch 180 loss: 0.13030924848901576\n",
      "  batch 190 loss: 0.1456045784779235\n",
      "LOSS train 0.1456045784779235 valid 0.49700419852587613\n",
      "EPOCH 190:\n",
      "  batch 10 loss: 0.08508007163181901\n",
      "  batch 20 loss: 0.13436496184322094\n",
      "  batch 30 loss: 0.34189841956977035\n",
      "  batch 40 loss: 0.12801053419425443\n",
      "  batch 50 loss: 0.2015654715134815\n",
      "  batch 60 loss: 0.16657094548900203\n",
      "  batch 70 loss: 0.10264611183083616\n",
      "  batch 80 loss: 0.3804610446574316\n",
      "  batch 90 loss: 0.24420703696423515\n",
      "  batch 100 loss: 0.2207266167954458\n",
      "  batch 110 loss: 0.03384751355986282\n",
      "  batch 120 loss: 0.08903227190094185\n",
      "  batch 130 loss: 0.3327837954871029\n",
      "  batch 140 loss: 0.24544403916962665\n",
      "  batch 150 loss: 0.03749992533703335\n",
      "  batch 160 loss: 0.11052812936918599\n",
      "  batch 170 loss: 0.16275488032324575\n",
      "  batch 180 loss: 0.4060161897603393\n",
      "  batch 190 loss: 0.4074648695170254\n",
      "LOSS train 0.4074648695170254 valid 0.2943129654175745\n",
      "EPOCH 191:\n",
      "  batch 10 loss: 0.17739262676568615\n",
      "  batch 20 loss: 0.13016793018650788\n",
      "  batch 30 loss: 0.37794647046830504\n",
      "  batch 40 loss: 0.11121948646812144\n",
      "  batch 50 loss: 0.19482711410787487\n",
      "  batch 60 loss: 0.1444532950325083\n",
      "  batch 70 loss: 0.1395918631593304\n",
      "  batch 80 loss: 0.05483442284748889\n",
      "  batch 90 loss: 0.11939417200042043\n",
      "  batch 100 loss: 0.45711272230892064\n",
      "  batch 110 loss: 0.0800885158316305\n",
      "  batch 120 loss: 0.10812233111273599\n",
      "  batch 130 loss: 0.04417627851507859\n",
      "  batch 140 loss: 0.04059411010475742\n",
      "  batch 150 loss: 0.24969347411970375\n",
      "  batch 160 loss: 0.3357580490890541\n",
      "  batch 170 loss: 0.10337294042819849\n",
      "  batch 180 loss: 0.07531316516586913\n",
      "  batch 190 loss: 0.19727346026411396\n",
      "LOSS train 0.19727346026411396 valid 0.36573148257703464\n",
      "EPOCH 192:\n",
      "  batch 10 loss: 0.18218427179890567\n",
      "  batch 20 loss: 0.11042302034759928\n",
      "  batch 30 loss: 0.09801094781528263\n",
      "  batch 40 loss: 0.16408195895671723\n",
      "  batch 50 loss: 0.051368943720990504\n",
      "  batch 60 loss: 0.15183483648834226\n",
      "  batch 70 loss: 0.2952643478041864\n",
      "  batch 80 loss: 0.252830937922954\n",
      "  batch 90 loss: 0.2525523238747155\n",
      "  batch 100 loss: 0.0956188650821332\n",
      "  batch 110 loss: 0.1388542411612434\n",
      "  batch 120 loss: 0.11672293877472839\n",
      "  batch 130 loss: 0.0668730593593864\n",
      "  batch 140 loss: 0.2506377634289493\n",
      "  batch 150 loss: 0.09886627772993961\n",
      "  batch 160 loss: 0.2139698918846989\n",
      "  batch 170 loss: 0.2332087005522226\n",
      "  batch 180 loss: 0.38445093991440443\n",
      "  batch 190 loss: 0.7277063607700256\n",
      "LOSS train 0.7277063607700256 valid 0.34462369200086984\n",
      "EPOCH 193:\n",
      "  batch 10 loss: 0.3193056781852647\n",
      "  batch 20 loss: 0.06228799444579636\n",
      "  batch 30 loss: 0.06481558721352484\n",
      "  batch 40 loss: 0.13811644213965338\n",
      "  batch 50 loss: 0.07376371892132738\n",
      "  batch 60 loss: 0.06867981325121945\n",
      "  batch 70 loss: 0.22383842873968546\n",
      "  batch 80 loss: 0.14517341368537018\n",
      "  batch 90 loss: 0.08088826927050832\n",
      "  batch 100 loss: 0.0586183389843427\n",
      "  batch 110 loss: 0.3196664546903776\n",
      "  batch 120 loss: 0.13542864161208854\n",
      "  batch 130 loss: 0.1102598156001477\n",
      "  batch 140 loss: 0.13216390456204863\n",
      "  batch 150 loss: 0.32881606077571635\n",
      "  batch 160 loss: 0.07500505686039105\n",
      "  batch 170 loss: 0.31894472559215503\n",
      "  batch 180 loss: 0.6048823863095094\n",
      "  batch 190 loss: 0.1442617487005009\n",
      "LOSS train 0.1442617487005009 valid 0.879440113861739\n",
      "EPOCH 194:\n",
      "  batch 10 loss: 0.3600424140720861\n",
      "  batch 20 loss: 0.35497071069930825\n",
      "  batch 30 loss: 0.25618861954985733\n",
      "  batch 40 loss: 0.050992261140345364\n",
      "  batch 50 loss: 0.4130167117112705\n",
      "  batch 60 loss: 0.12262414618598996\n",
      "  batch 70 loss: 0.203356610595074\n",
      "  batch 80 loss: 0.07930274858917982\n",
      "  batch 90 loss: 0.14002330156290554\n",
      "  batch 100 loss: 0.11791734544676728\n",
      "  batch 110 loss: 0.313577284632629\n",
      "  batch 120 loss: 0.04659484996755055\n",
      "  batch 130 loss: 0.1886489970567709\n",
      "  batch 140 loss: 0.22878226458956305\n",
      "  batch 150 loss: 0.10045870809262851\n",
      "  batch 160 loss: 0.061492265319748184\n",
      "  batch 170 loss: 0.1923093670239723\n",
      "  batch 180 loss: 0.18688114986664459\n",
      "  batch 190 loss: 0.1211212384278042\n",
      "LOSS train 0.1211212384278042 valid 0.3084807057820609\n",
      "EPOCH 195:\n",
      "  batch 10 loss: 0.1454121464524178\n",
      "  batch 20 loss: 0.08409598263729094\n",
      "  batch 30 loss: 0.02508738719423036\n",
      "  batch 40 loss: 0.15651857893672058\n",
      "  batch 50 loss: 0.12819947114649038\n",
      "  batch 60 loss: 0.23558063041522245\n",
      "  batch 70 loss: 0.05532208136646659\n",
      "  batch 80 loss: 0.24288700952347425\n",
      "  batch 90 loss: 0.21190091784928883\n",
      "  batch 100 loss: 0.1547776842618987\n",
      "  batch 110 loss: 0.08196877826485434\n",
      "  batch 120 loss: 0.08397496617239994\n",
      "  batch 130 loss: 0.20864827615023387\n",
      "  batch 140 loss: 0.09006079029552438\n",
      "  batch 150 loss: 0.05791440352350037\n",
      "  batch 160 loss: 0.3411782421371754\n",
      "  batch 170 loss: 0.2823449916773825\n",
      "  batch 180 loss: 0.13980770581692922\n",
      "  batch 190 loss: 0.36572597434460474\n",
      "LOSS train 0.36572597434460474 valid 0.24921511496176535\n",
      "EPOCH 196:\n",
      "  batch 10 loss: 0.08509888182234135\n",
      "  batch 20 loss: 0.28091993747684685\n",
      "  batch 30 loss: 0.1264492832443466\n",
      "  batch 40 loss: 0.14308935801782355\n",
      "  batch 50 loss: 0.23715131639617085\n",
      "  batch 60 loss: 0.13643207374261693\n",
      "  batch 70 loss: 0.1632791457647727\n",
      "  batch 80 loss: 0.15747442713211512\n",
      "  batch 90 loss: 0.09613917861570372\n",
      "  batch 100 loss: 0.04835234250090252\n",
      "  batch 110 loss: 0.2167714022177279\n",
      "  batch 120 loss: 0.09613504896333325\n",
      "  batch 130 loss: 0.14472054311077046\n",
      "  batch 140 loss: 0.5881652357064013\n",
      "  batch 150 loss: 0.0922492415736997\n",
      "  batch 160 loss: 0.1869174359414046\n",
      "  batch 170 loss: 0.17266442169766377\n",
      "  batch 180 loss: 0.15456435401902127\n",
      "  batch 190 loss: 0.24572485904864153\n",
      "LOSS train 0.24572485904864153 valid 0.2563333388653075\n",
      "EPOCH 197:\n",
      "  batch 10 loss: 0.16566505375740234\n",
      "  batch 20 loss: 0.18318678155483212\n",
      "  batch 30 loss: 0.09275398506929378\n",
      "  batch 40 loss: 0.11436815488850698\n",
      "  batch 50 loss: 0.07138665629843217\n",
      "  batch 60 loss: 0.10045574825999211\n",
      "  batch 70 loss: 0.3338266286644284\n",
      "  batch 80 loss: 0.08494208849642745\n",
      "  batch 90 loss: 0.31899584848520135\n",
      "  batch 100 loss: 0.15084076676275798\n",
      "  batch 110 loss: 0.21062824314694809\n",
      "  batch 120 loss: 0.18656540482802483\n",
      "  batch 130 loss: 0.16925701739507987\n",
      "  batch 140 loss: 0.1728062666099504\n",
      "  batch 150 loss: 0.09981946471725678\n",
      "  batch 160 loss: 0.13510459405133587\n",
      "  batch 170 loss: 0.2562326501051757\n",
      "  batch 180 loss: 0.22636517266746523\n",
      "  batch 190 loss: 0.2222325720882509\n",
      "LOSS train 0.2222325720882509 valid 0.3089161141096424\n",
      "EPOCH 198:\n",
      "  batch 10 loss: 0.35084453367634294\n",
      "  batch 20 loss: 0.20491359367388212\n",
      "  batch 30 loss: 0.05524469110589507\n",
      "  batch 40 loss: 0.3541933655855246\n",
      "  batch 50 loss: 0.1869664608827236\n",
      "  batch 60 loss: 0.0794300566869424\n",
      "  batch 70 loss: 0.08220168037141776\n",
      "  batch 80 loss: 0.18647825832667878\n",
      "  batch 90 loss: 0.3121124785791835\n",
      "  batch 100 loss: 0.1495209185715794\n",
      "  batch 110 loss: 0.12935995903499134\n",
      "  batch 120 loss: 0.04655425027740421\n",
      "  batch 130 loss: 0.05004194507455395\n",
      "  batch 140 loss: 0.46891121475091496\n",
      "  batch 150 loss: 0.0946406148201561\n",
      "  batch 160 loss: 0.09583281558658427\n",
      "  batch 170 loss: 0.033645290957429096\n",
      "  batch 180 loss: 0.17544230857383808\n",
      "  batch 190 loss: 0.6607035641952279\n",
      "LOSS train 0.6607035641952279 valid 0.41971469507304904\n",
      "EPOCH 199:\n",
      "  batch 10 loss: 0.1796079941498192\n",
      "  batch 20 loss: 0.19899969270491055\n",
      "  batch 30 loss: 0.16743184880797343\n",
      "  batch 40 loss: 0.17038837758955197\n",
      "  batch 50 loss: 0.08662067706627567\n",
      "  batch 60 loss: 0.05319576917445375\n",
      "  batch 70 loss: 0.08057883303495146\n",
      "  batch 80 loss: 0.09625840068711114\n",
      "  batch 90 loss: 0.16456066060945887\n",
      "  batch 100 loss: 0.03103050260906457\n",
      "  batch 110 loss: 0.11066565899272973\n",
      "  batch 120 loss: 0.18861081680952338\n",
      "  batch 130 loss: 0.24545975836489617\n",
      "  batch 140 loss: 0.3639877126279657\n",
      "  batch 150 loss: 0.11066425532953872\n",
      "  batch 160 loss: 0.3305037687283857\n",
      "  batch 170 loss: 0.11077375381742058\n",
      "  batch 180 loss: 0.1562396607066603\n",
      "  batch 190 loss: 0.2688893421155626\n",
      "LOSS train 0.2688893421155626 valid 0.37344916449496735\n",
      "EPOCH 200:\n",
      "  batch 10 loss: 0.33291155499623243\n",
      "  batch 20 loss: 0.3809158467091038\n",
      "  batch 30 loss: 0.136369574622222\n",
      "  batch 40 loss: 0.10159375529001409\n",
      "  batch 50 loss: 0.04330500462385771\n",
      "  batch 60 loss: 0.13940168429762706\n",
      "  batch 70 loss: 0.08365876248772111\n",
      "  batch 80 loss: 0.05900463077655331\n",
      "  batch 90 loss: 0.13434216864220616\n",
      "  batch 100 loss: 0.05127405297371297\n",
      "  batch 110 loss: 0.10206364545915676\n",
      "  batch 120 loss: 0.21775768487577807\n",
      "  batch 130 loss: 0.12353272923387522\n",
      "  batch 140 loss: 0.08061014949726086\n",
      "  batch 150 loss: 0.14218087298327192\n",
      "  batch 160 loss: 0.11299408651648264\n",
      "  batch 170 loss: 0.10103657116069371\n",
      "  batch 180 loss: 0.2751598778493644\n",
      "  batch 190 loss: 0.34727801994554286\n",
      "LOSS train 0.34727801994554286 valid 0.27041532009978614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2fe399af0>]"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "EPOCHS = 200\n",
    "best_vloss = 1_000_000.\n",
    "val_history = []\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch(curr_model=model)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_history.append(avg_vloss.detach().numpy())\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        torch.save(model, \"../models/model8.pth\")\n",
    "    epoch_number += 1\n",
    "\n",
    "plt.plot(range(EPOCHS), val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2492, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/model8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total = len(validation_set)\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        inputs, labels = vdata\n",
    "        outputs = torch.argmax(model(inputs), dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct_count += (outputs==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9451612903225807"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387096774193548"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.argmax(model(val_data).detach().numpy(), axis=1), np.argmax(val_labels.detach().numpy(), axis=1), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"../models/model6.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../models/model6.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5580645161290323"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_data.view(-1,21*2), train_labels)\n",
    "y_pred = knn.predict(val_data.view(-1,21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([699, 21, 2])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193548387096775"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RidgeClassifierCV()\n",
    "clf.fit(train_data.view(-1, 21*2), train_labels)\n",
    "y_pred = clf.predict(val_data.view(-1, 21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_86164/1044316036.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4573, 0.0484, 0.0896,  ..., 0.0850, 0.0911, 0.0797],\n",
       "        [0.4173, 0.0532, 0.1257,  ..., 0.0936, 0.0882, 0.0599],\n",
       "        [0.4564, 0.0462, 0.0918,  ..., 0.0855, 0.0899, 0.0812],\n",
       "        ...,\n",
       "        [0.0744, 0.0594, 0.1351,  ..., 0.1360, 0.1033, 0.1548],\n",
       "        [0.0907, 0.0860, 0.1948,  ..., 0.1134, 0.1084, 0.0952],\n",
       "        [0.0876, 0.0802, 0.1651,  ..., 0.1132, 0.1127, 0.1029]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193548387096775"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_data.view(-1, 21*2), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reetinav/anaconda3/envs/PIC16B/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(multi_class=\"ovr\")\n",
    "clf.fit(train_data.view(-1, 21*2), torch.argmax(train_labels, dim=1))\n",
    "clf.score(val_data.view(-1, 21*2), torch.argmax(val_labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5952, 0.0090, 0.0528,  ..., 0.0620, 0.0913, 0.0809],\n",
       "        [0.5499, 0.0127, 0.0850,  ..., 0.0669, 0.0768, 0.0737],\n",
       "        [0.5842, 0.0089, 0.0531,  ..., 0.0594, 0.0923, 0.0878],\n",
       "        ...,\n",
       "        [0.0690, 0.1502, 0.1629,  ..., 0.1562, 0.0389, 0.0815],\n",
       "        [0.0903, 0.0719, 0.4556,  ..., 0.0353, 0.1396, 0.1260],\n",
       "        [0.0648, 0.0662, 0.5327,  ..., 0.0554, 0.0969, 0.1004]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
