{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Dataset at 0x17d6fb0d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.data.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../training/down.0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, static_image_mode=False, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "image.flags.writeable = False\n",
    "results = hands.process(image) # this makes the actual detections\n",
    "image.flags.writeable = True\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = []\n",
    "if results.multi_hand_landmarks:\n",
    "    for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "        x, y = landmark.x, landmark.y\n",
    "        landmarks.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.array(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_tensor = torch.tensor(landmarks, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2628, 0.3843],\n",
       "        [0.3132, 0.4501],\n",
       "        [0.3475, 0.5395],\n",
       "        [0.3496, 0.6470],\n",
       "        [0.3377, 0.7126],\n",
       "        [0.3652, 0.4491],\n",
       "        [0.3603, 0.5908],\n",
       "        [0.3541, 0.6657],\n",
       "        [0.3461, 0.7193],\n",
       "        [0.3246, 0.4260],\n",
       "        [0.3129, 0.5904],\n",
       "        [0.3077, 0.5611],\n",
       "        [0.3129, 0.5146],\n",
       "        [0.2826, 0.4209],\n",
       "        [0.2775, 0.5821],\n",
       "        [0.2794, 0.5434],\n",
       "        [0.2843, 0.4964],\n",
       "        [0.2459, 0.4240],\n",
       "        [0.2426, 0.5528],\n",
       "        [0.2482, 0.5272],\n",
       "        [0.2535, 0.4854]])"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y_vals = []\n",
    "for i in range(50):\n",
    "    image = cv2.imread(f\"../training/down.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(0)\n",
    "\n",
    "for i in range(50):\n",
    "    image = cv2.imread(f\"../training/up.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(1)\n",
    "\n",
    "for i in range(50):\n",
    "    image = cv2.imread(f\"../training/thumb.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"thumbs up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data)\n",
    "y_vals = torch.tensor(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(data, y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y_vals = []\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"../validation/down.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(0)\n",
    "\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"../validation/up.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(1)\n",
    "\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"../validation/thumb.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data)\n",
    "y_vals = torch.tensor(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2])"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(data, y_vals)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0393,  0.0584, -0.0740]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(landmarks_tensor.view(-1, 21, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 0.03257557181641459\n",
      "  batch 20 loss: 0.05546635733917356\n",
      "  batch 30 loss: 0.019798521883785725\n",
      "LOSS train 0.019798521883785725 valid 0.02325417287647724\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.014236991945654153\n",
      "  batch 20 loss: 0.041099155927076934\n",
      "  batch 30 loss: 0.052653033100068566\n",
      "LOSS train 0.052653033100068566 valid 0.02325417287647724\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.022774085216224194\n",
      "  batch 20 loss: 0.036964822374284265\n",
      "  batch 30 loss: 0.04879928370937705\n",
      "LOSS train 0.04879928370937705 valid 0.02325417287647724\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.019898624811321496\n",
      "  batch 20 loss: 0.06049192948266864\n",
      "  batch 30 loss: 0.029163545928895473\n",
      "LOSS train 0.029163545928895473 valid 0.02325417287647724\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.05918706860393286\n",
      "  batch 20 loss: 0.03175804233178496\n",
      "  batch 30 loss: 0.017756244633346796\n",
      "LOSS train 0.017756244633346796 valid 0.02325417287647724\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.06509706024080515\n",
      "  batch 20 loss: 0.02771271257661283\n",
      "  batch 30 loss: 0.014937029825523496\n",
      "LOSS train 0.014937029825523496 valid 0.02325417287647724\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.07554256515577436\n",
      "  batch 20 loss: 0.01461706468835473\n",
      "  batch 30 loss: 0.017861549369990827\n",
      "LOSS train 0.017861549369990827 valid 0.02325417287647724\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.057059417851269244\n",
      "  batch 20 loss: 0.026945017371326686\n",
      "  batch 30 loss: 0.022914879862219094\n",
      "LOSS train 0.022914879862219094 valid 0.02325417287647724\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.039829245675355195\n",
      "  batch 20 loss: 0.017602581158280374\n",
      "  batch 30 loss: 0.04566033580340445\n",
      "LOSS train 0.04566033580340445 valid 0.02325417287647724\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.022567675355821847\n",
      "  batch 20 loss: 0.040814845077693465\n",
      "  batch 30 loss: 0.046342895925045015\n",
      "LOSS train 0.046342895925045015 valid 0.02325417287647724\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        #torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model.pkl\", \"rb\") as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frameCounter = 0\n",
    "prevTime = 0\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, static_image_mode=False, max_num_hands=1) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        landmarks = []\n",
    "        # Detections\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if cv2.waitKey(1) &0xFF == ord('p'):\n",
    "            if results.multi_hand_landmarks:\n",
    "                for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                    x, y = landmark.x, landmark.y\n",
    "                    landmarks.append([x,y])\n",
    "                landmarks = torch.tensor(landmarks)\n",
    "                out = torch.argmax(model(landmarks.view(-1,21,2)))\n",
    "                print(f\"Prediction: {classes[out]}\")\n",
    "\n",
    "        # Rendering results\n",
    "        \n",
    "        # Print fps\n",
    "        currTime = time.time()\n",
    "        fps = 1 / (currTime-prevTime)\n",
    "        prevTime = currTime\n",
    "        image = cv2.flip(image,1)\n",
    "        cv2.putText(image, f\"FPS: {fps}\", (20,70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 196, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Hand Tracking\", image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=42, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=42, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=42, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
