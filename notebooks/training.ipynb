{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Dataset at 0x131c70460>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.data.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"../training/down.0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, static_image_mode=False, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "image.flags.writeable = False\n",
    "results = hands.process(image) # this makes the actual detections\n",
    "image.flags.writeable = True\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = []\n",
    "if results.multi_hand_landmarks:\n",
    "    for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "        x, y = landmark.x, landmark.y\n",
    "        landmarks.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.array(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_tensor = torch.tensor(landmarks, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2628, 0.3843],\n",
       "        [0.3132, 0.4501],\n",
       "        [0.3475, 0.5395],\n",
       "        [0.3496, 0.6470],\n",
       "        [0.3377, 0.7126],\n",
       "        [0.3652, 0.4491],\n",
       "        [0.3603, 0.5908],\n",
       "        [0.3541, 0.6657],\n",
       "        [0.3461, 0.7193],\n",
       "        [0.3246, 0.4260],\n",
       "        [0.3129, 0.5904],\n",
       "        [0.3077, 0.5611],\n",
       "        [0.3129, 0.5146],\n",
       "        [0.2826, 0.4209],\n",
       "        [0.2775, 0.5821],\n",
       "        [0.2794, 0.5434],\n",
       "        [0.2843, 0.4964],\n",
       "        [0.2459, 0.4240],\n",
       "        [0.2426, 0.5528],\n",
       "        [0.2482, 0.5272],\n",
       "        [0.2535, 0.4854]])"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y_vals = []\n",
    "for i in range(50):\n",
    "    image = cv2.imread(f\"../training/down.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(0)\n",
    "\n",
    "for i in range(50):\n",
    "    image = cv2.imread(f\"../training/up.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(1)\n",
    "\n",
    "for i in range(50):\n",
    "    image = cv2.imread(f\"../training/thumb.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"thumbs up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data)\n",
    "y_vals = torch.tensor(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(data, y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y_vals = []\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"../validation/down.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(0)\n",
    "\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"../validation/up.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(1)\n",
    "\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"../validation/thumb.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data)\n",
    "y_vals = torch.tensor(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2])"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(data, y_vals)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0393,  0.0584, -0.0740]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(landmarks_tensor.view(-1, 21, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.098435151576996\n",
      "  batch 20 loss: 1.102132999897003\n",
      "  batch 30 loss: 1.0812122821807861\n",
      "LOSS train 1.0812122821807861 valid 1.0540964603424072\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 1.0329448938369752\n",
      "  batch 20 loss: 1.0461229801177978\n",
      "  batch 30 loss: 0.9691039204597474\n",
      "LOSS train 0.9691039204597474 valid 0.9211711883544922\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.8859965085983277\n",
      "  batch 20 loss: 0.7656323194503785\n",
      "  batch 30 loss: 0.6515930503606796\n",
      "LOSS train 0.6515930503606796 valid 0.622218132019043\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.6019847258925438\n",
      "  batch 20 loss: 0.48264782726764677\n",
      "  batch 30 loss: 0.515526881814003\n",
      "LOSS train 0.515526881814003 valid 0.7677610516548157\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.4053989380598068\n",
      "  batch 20 loss: 1.289748278260231\n",
      "  batch 30 loss: 0.6489023745059967\n",
      "LOSS train 0.6489023745059967 valid 0.5538231134414673\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.45072523355484007\n",
      "  batch 20 loss: 0.3419716484844685\n",
      "  batch 30 loss: 0.2696025773882866\n",
      "LOSS train 0.2696025773882866 valid 1.1866512298583984\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.3664066195487976\n",
      "  batch 20 loss: 0.2516997754573822\n",
      "  batch 30 loss: 0.13505120165646076\n",
      "LOSS train 0.13505120165646076 valid 0.35815882682800293\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.19881654307246208\n",
      "  batch 20 loss: 0.09016566947102547\n",
      "  batch 30 loss: 0.21749539989978076\n",
      "LOSS train 0.21749539989978076 valid 0.08179266005754471\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.06224514823406935\n",
      "  batch 20 loss: 0.06449626991525292\n",
      "  batch 30 loss: 0.04105175081640482\n",
      "LOSS train 0.04105175081640482 valid 0.9322674870491028\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.28334723468869927\n",
      "  batch 20 loss: 0.07889751037582755\n",
      "  batch 30 loss: 0.0961719882208854\n",
      "LOSS train 0.0961719882208854 valid 0.02325417287647724\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: down\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n",
      "Prediction: thumbs up\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "frameCounter = 0\n",
    "prevTime = 0\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, static_image_mode=False, max_num_hands=1) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        landmarks = []\n",
    "        # Detections\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if cv2.waitKey(1) &0xFF == ord('p'):\n",
    "            if results.multi_hand_landmarks:\n",
    "                for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                    x, y = landmark.x, landmark.y\n",
    "                    landmarks.append([x,y])\n",
    "                landmarks = torch.tensor(landmarks)\n",
    "                out = torch.argmax(model(landmarks.view(-1,21,2)))\n",
    "                print(f\"Prediction: {classes[out]}\")\n",
    "\n",
    "        # Rendering results\n",
    "        \n",
    "        # Print fps\n",
    "        currTime = time.time()\n",
    "        fps = 1 / (currTime-prevTime)\n",
    "        prevTime = currTime\n",
    "        image = cv2.flip(image,1)\n",
    "        cv2.putText(image, f\"FPS: {fps}\", (20,70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 196, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Hand Tracking\", image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=42, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=42, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=42, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
