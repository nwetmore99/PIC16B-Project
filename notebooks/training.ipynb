{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"training/down.0.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5, static_image_mode=False, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "image.flags.writeable = False\n",
    "results = hands.process(image) # this makes the actual detections\n",
    "image.flags.writeable = True\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = []\n",
    "if results.multi_hand_landmarks:\n",
    "    for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "        x, y, z = landmark.x, landmark.y, landmark.z\n",
    "        landmarks.append([x,y,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.array(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_tensor = torch.tensor(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 3])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6169e-01,  3.5162e-01,  5.2388e-07],\n",
       "        [ 3.1173e-01,  4.0588e-01, -1.9637e-02],\n",
       "        [ 3.4277e-01,  5.1171e-01, -4.9626e-02],\n",
       "        [ 3.4353e-01,  6.2118e-01, -7.7466e-02],\n",
       "        [ 3.4383e-01,  6.8985e-01, -1.0540e-01],\n",
       "        [ 3.6291e-01,  4.3615e-01, -8.1996e-02],\n",
       "        [ 3.5962e-01,  5.8429e-01, -1.1503e-01],\n",
       "        [ 3.5429e-01,  6.6624e-01, -1.3190e-01],\n",
       "        [ 3.4618e-01,  7.2988e-01, -1.4240e-01],\n",
       "        [ 3.2038e-01,  4.1795e-01, -9.0673e-02],\n",
       "        [ 3.1555e-01,  5.9374e-01, -1.2266e-01],\n",
       "        [ 3.1004e-01,  5.6808e-01, -1.1885e-01],\n",
       "        [ 3.1328e-01,  5.2203e-01, -1.1607e-01],\n",
       "        [ 2.7819e-01,  4.1597e-01, -9.8668e-02],\n",
       "        [ 2.8027e-01,  5.8395e-01, -1.1922e-01],\n",
       "        [ 2.7786e-01,  5.5161e-01, -9.8715e-02],\n",
       "        [ 2.8025e-01,  5.0518e-01, -8.4991e-02],\n",
       "        [ 2.4063e-01,  4.2561e-01, -1.0939e-01],\n",
       "        [ 2.4922e-01,  5.6303e-01, -1.1770e-01],\n",
       "        [ 2.5351e-01,  5.4196e-01, -1.0227e-01],\n",
       "        [ 2.5585e-01,  4.9809e-01, -9.1983e-02]], dtype=torch.float64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "y_vals = []\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"training/down.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(0)\n",
    "\n",
    "for i in range(10):\n",
    "    image = cv2.imread(f\"training/up.{i}.jpg\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image) # this makes the actual detections\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "            x, y = landmark.x, landmark.y\n",
    "            landmarks.append([x,y])\n",
    "        data.append(landmarks)\n",
    "        y_vals.append(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 21, 2])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Flatten(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(data[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten(0)\n",
    "        self.fc1 = nn.Linear(42, 100)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0601, 0.1512], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
