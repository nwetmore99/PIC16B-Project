{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\", \"right\", \"left\", \"background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.3, static_image_mode=True, max_num_hands=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "[ WARN:0@15.644] global loadsave.cpp:248 findDecoder imread_('../training/down.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.647] global loadsave.cpp:248 findDecoder imread_('../training/down.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.648] global loadsave.cpp:248 findDecoder imread_('../training/down.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.649] global loadsave.cpp:248 findDecoder imread_('../training/down.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.650] global loadsave.cpp:248 findDecoder imread_('../training/down.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15.651] global loadsave.cpp:248 findDecoder imread_('../training/down.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.846] global loadsave.cpp:248 findDecoder imread_('../training/up.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.846] global loadsave.cpp:248 findDecoder imread_('../training/up.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.846] global loadsave.cpp:248 findDecoder imread_('../training/up.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.846] global loadsave.cpp:248 findDecoder imread_('../training/up.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.846] global loadsave.cpp:248 findDecoder imread_('../training/up.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.846] global loadsave.cpp:248 findDecoder imread_('../training/up.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.847] global loadsave.cpp:248 findDecoder imread_('../training/up.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.848] global loadsave.cpp:248 findDecoder imread_('../training/up.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@20.849] global loadsave.cpp:248 findDecoder imread_('../training/up.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.884] global loadsave.cpp:248 findDecoder imread_('../training/stop.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.885] global loadsave.cpp:248 findDecoder imread_('../training/stop.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.886] global loadsave.cpp:248 findDecoder imread_('../training/stop.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.887] global loadsave.cpp:248 findDecoder imread_('../training/stop.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@26.887] global loadsave.cpp:248 findDecoder imread_('../training/stop.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.017] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.018] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@32.019] global loadsave.cpp:248 findDecoder imread_('../training/thumbright.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.151] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.151] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.152] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.153] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@37.154] global loadsave.cpp:248 findDecoder imread_('../training/thumbleft.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.116] global loadsave.cpp:248 findDecoder imread_('../training/right.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.117] global loadsave.cpp:248 findDecoder imread_('../training/right.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.118] global loadsave.cpp:248 findDecoder imread_('../training/right.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@42.119] global loadsave.cpp:248 findDecoder imread_('../training/right.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.170] global loadsave.cpp:248 findDecoder imread_('../training/left.100.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.170] global loadsave.cpp:248 findDecoder imread_('../training/left.101.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.102.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.103.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.104.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.105.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.106.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.107.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.108.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.109.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.110.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.111.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.112.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.113.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.114.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.115.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.116.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.117.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.118.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.119.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.120.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.121.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.122.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.123.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.124.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.125.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.126.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.127.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.128.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.129.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.130.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.131.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.132.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.133.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.134.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.171] global loadsave.cpp:248 findDecoder imread_('../training/left.135.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.136.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.137.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.138.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.139.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.140.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.141.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.142.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.143.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.144.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.145.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.146.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.147.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.148.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.149.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.150.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.151.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.152.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.153.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.154.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.155.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.156.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.157.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.158.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.159.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.160.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.161.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.162.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.163.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.164.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.165.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.166.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.167.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.168.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.169.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.170.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.171.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.172.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@47.172] global loadsave.cpp:248 findDecoder imread_('../training/left.174.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@55.727] global loadsave.cpp:248 findDecoder imread_('../training/background.173.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@55.727] global loadsave.cpp:248 findDecoder imread_('../training/background.174.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(175):\n",
    "        try:\n",
    "            image = cv2.imread(f\"../training/{gesture_class}.{i}.jpg\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        except:\n",
    "            continue\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            train_label = np.zeros([len(classes)])\n",
    "            train_label[class_index] = 1\n",
    "            train_data.append(landmarks)\n",
    "            train_labels.append(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_15404/3266268609.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1695391825142/work/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  train_labels = torch.tensor(train_labels)\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.tensor(train_data)\n",
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([785, 21, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LandmarksDataset(train_data, train_labels)\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "val_labels = []\n",
    "for class_index, gesture_class in enumerate(classes):\n",
    "    for i in range(40):\n",
    "        image = cv2.imread(f\"../validation/{gesture_class}.{i}.jpg\")\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image) # this makes the actual detections\n",
    "        \n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for landmark in results.multi_hand_landmarks[0].landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                landmarks.append([x,y])\n",
    "            val_label = np.zeros([len(classes)])\n",
    "            val_label[class_index] = 1\n",
    "            val_data.append(landmarks)\n",
    "            val_labels.append(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.tensor(val_data)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = LandmarksDataset(val_data, val_labels)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([310, 21, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fc1 = nn.Linear(42, 120)\n",
    "        self.fc2 = nn.Linear(120, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, len(classes))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HandNetwork()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(curr_model):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = curr_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # calculate the gradients\n",
    "        optimizer.step() # update the params\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 10-1:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print(f'  batch {i+1} loss: {last_loss}')\n",
    "            running_loss = 0\n",
    "    \n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 2.081457620859146\n",
      "  batch 20 loss: 2.0688407838344576\n",
      "  batch 30 loss: 2.0353876769542696\n",
      "  batch 40 loss: 2.08460813164711\n",
      "  batch 50 loss: 2.0626399725675584\n",
      "  batch 60 loss: 2.099316856265068\n",
      "  batch 70 loss: 2.0558640241622923\n",
      "  batch 80 loss: 2.0721993893384933\n",
      "  batch 90 loss: 2.0843223571777343\n",
      "  batch 100 loss: 2.074992221593857\n",
      "  batch 110 loss: 2.0453262120485305\n",
      "  batch 120 loss: 2.0834028840065004\n",
      "  batch 130 loss: 2.041154000163078\n",
      "  batch 140 loss: 2.047263702750206\n",
      "  batch 150 loss: 2.0400151938199995\n",
      "  batch 160 loss: 2.0504810363054276\n",
      "  batch 170 loss: 2.0422505706548693\n",
      "  batch 180 loss: 2.002999100089073\n",
      "  batch 190 loss: 2.044433891773224\n",
      "LOSS train 2.044433891773224 valid 2.084791893760363\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 2.061880576610565\n",
      "  batch 20 loss: 2.0585823982954023\n",
      "  batch 30 loss: 2.0669133365154266\n",
      "  batch 40 loss: 2.050375735759735\n",
      "  batch 50 loss: 1.9722982108592988\n",
      "  batch 60 loss: 2.01523971259594\n",
      "  batch 70 loss: 2.073643261194229\n",
      "  batch 80 loss: 2.0154240250587465\n",
      "  batch 90 loss: 2.045998293161392\n",
      "  batch 100 loss: 2.063548168540001\n",
      "  batch 110 loss: 2.114790776371956\n",
      "  batch 120 loss: 2.115109860897064\n",
      "  batch 130 loss: 2.013978934288025\n",
      "  batch 140 loss: 2.03905374109745\n",
      "  batch 150 loss: 2.0270913749933244\n",
      "  batch 160 loss: 1.9773247480392455\n",
      "  batch 170 loss: 2.111058387160301\n",
      "  batch 180 loss: 2.050817680358887\n",
      "  batch 190 loss: 2.024252462387085\n",
      "LOSS train 2.024252462387085 valid 2.091649525058575\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 2.0849772185087203\n",
      "  batch 20 loss: 2.051295304298401\n",
      "  batch 30 loss: 2.0747212946414946\n",
      "  batch 40 loss: 2.06018950343132\n",
      "  batch 50 loss: 2.0752712696790696\n",
      "  batch 60 loss: 2.024456486105919\n",
      "  batch 70 loss: 2.0487615287303926\n",
      "  batch 80 loss: 2.085581398010254\n",
      "  batch 90 loss: 2.0707052141427993\n",
      "  batch 100 loss: 1.9830537110567092\n",
      "  batch 110 loss: 2.052649822831154\n",
      "  batch 120 loss: 1.944713070988655\n",
      "  batch 130 loss: 1.9889922529458999\n",
      "  batch 140 loss: 2.029027298092842\n",
      "  batch 150 loss: 2.023833844065666\n",
      "  batch 160 loss: 2.0757122308015825\n",
      "  batch 170 loss: 2.039497348666191\n",
      "  batch 180 loss: 2.027563124895096\n",
      "  batch 190 loss: 2.077588278055191\n",
      "LOSS train 2.077588278055191 valid 2.0956278336353793\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 2.1024986118078233\n",
      "  batch 20 loss: 2.061332580447197\n",
      "  batch 30 loss: 2.0743079602718355\n",
      "  batch 40 loss: 2.091059610247612\n",
      "  batch 50 loss: 2.056657022237778\n",
      "  batch 60 loss: 2.009095329046249\n",
      "  batch 70 loss: 2.0416260778903963\n",
      "  batch 80 loss: 2.027476340532303\n",
      "  batch 90 loss: 1.9987564474344253\n",
      "  batch 100 loss: 2.0360991448163985\n",
      "  batch 110 loss: 2.027864822745323\n",
      "  batch 120 loss: 2.078202545642853\n",
      "  batch 130 loss: 2.0242369741201403\n",
      "  batch 140 loss: 1.9713719636201859\n",
      "  batch 150 loss: 1.9798950880765915\n",
      "  batch 160 loss: 2.0580730110406877\n",
      "  batch 170 loss: 2.0716847240924836\n",
      "  batch 180 loss: 2.028293418884277\n",
      "  batch 190 loss: 2.0412106245756148\n",
      "LOSS train 2.0412106245756148 valid 2.0969260866061235\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 2.0639741629362107\n",
      "  batch 20 loss: 2.0333101034164427\n",
      "  batch 30 loss: 2.024090775847435\n",
      "  batch 40 loss: 2.0731306046247484\n",
      "  batch 50 loss: 2.0769616961479187\n",
      "  batch 60 loss: 1.9837719649076462\n",
      "  batch 70 loss: 2.051440900564194\n",
      "  batch 80 loss: 2.0015979528427126\n",
      "  batch 90 loss: 1.9809736758470535\n",
      "  batch 100 loss: 2.0734147399663927\n",
      "  batch 110 loss: 2.0131955295801163\n",
      "  batch 120 loss: 2.0133358031511306\n",
      "  batch 130 loss: 2.0639462202787398\n",
      "  batch 140 loss: 2.078502595424652\n",
      "  batch 150 loss: 1.9766164630651475\n",
      "  batch 160 loss: 2.067131307721138\n",
      "  batch 170 loss: 2.032689207792282\n",
      "  batch 180 loss: 2.0108721762895585\n",
      "  batch 190 loss: 2.087384191155434\n",
      "LOSS train 2.087384191155434 valid 2.092385301223168\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 2.044467145204544\n",
      "  batch 20 loss: 1.9906755566596985\n",
      "  batch 30 loss: 2.017564061284065\n",
      "  batch 40 loss: 2.116481375694275\n",
      "  batch 50 loss: 2.002384293079376\n",
      "  batch 60 loss: 2.0431050300598144\n",
      "  batch 70 loss: 2.0454846560955047\n",
      "  batch 80 loss: 2.104585346579552\n",
      "  batch 90 loss: 1.994257378578186\n",
      "  batch 100 loss: 2.0685607939958572\n",
      "  batch 110 loss: 2.0489829897880556\n",
      "  batch 120 loss: 2.0655791729688646\n",
      "  batch 130 loss: 1.9954340934753418\n",
      "  batch 140 loss: 2.055785194039345\n",
      "  batch 150 loss: 2.050540116429329\n",
      "  batch 160 loss: 2.02658988237381\n",
      "  batch 170 loss: 1.97352776825428\n",
      "  batch 180 loss: 1.9719206601381303\n",
      "  batch 190 loss: 2.0576536774635317\n",
      "LOSS train 2.0576536774635317 valid 2.094851315021515\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 1.9927034199237823\n",
      "  batch 20 loss: 2.0167436093091964\n",
      "  batch 30 loss: 1.9893288105726241\n",
      "  batch 40 loss: 2.0521411180496214\n",
      "  batch 50 loss: 2.033748123049736\n",
      "  batch 60 loss: 1.9996708273887633\n",
      "  batch 70 loss: 2.080636003613472\n",
      "  batch 80 loss: 2.04859119951725\n",
      "  batch 90 loss: 2.0384664058685305\n",
      "  batch 100 loss: 2.109269866347313\n",
      "  batch 110 loss: 1.9398097187280654\n",
      "  batch 120 loss: 2.048021614551544\n",
      "  batch 130 loss: 2.054789036512375\n",
      "  batch 140 loss: 1.9587911188602447\n",
      "  batch 150 loss: 2.043753018975258\n",
      "  batch 160 loss: 2.0861455261707307\n",
      "  batch 170 loss: 2.05784912109375\n",
      "  batch 180 loss: 2.043254539370537\n",
      "  batch 190 loss: 2.044554391503334\n",
      "LOSS train 2.044554391503334 valid 2.0884082431976614\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 2.0210875540971758\n",
      "  batch 20 loss: 2.005024027824402\n",
      "  batch 30 loss: 2.0095501869916914\n",
      "  batch 40 loss: 2.0700693666934966\n",
      "  batch 50 loss: 2.0128031373023987\n",
      "  batch 60 loss: 2.050995719432831\n",
      "  batch 70 loss: 1.9900952488183976\n",
      "  batch 80 loss: 2.0655520886182783\n",
      "  batch 90 loss: 2.0743405878543855\n",
      "  batch 100 loss: 1.9994892090559007\n",
      "  batch 110 loss: 2.038534754514694\n",
      "  batch 120 loss: 2.034607881307602\n",
      "  batch 130 loss: 2.0849819809198378\n",
      "  batch 140 loss: 2.113810604810715\n",
      "  batch 150 loss: 1.996997857093811\n",
      "  batch 160 loss: 2.084754395484924\n",
      "  batch 170 loss: 1.9916671752929687\n",
      "  batch 180 loss: 2.0147425502538683\n",
      "  batch 190 loss: 1.8883521556854248\n",
      "LOSS train 1.8883521556854248 valid 2.0910096118847528\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 2.0687010258436205\n",
      "  batch 20 loss: 2.0335378378629683\n",
      "  batch 30 loss: 2.017066317796707\n",
      "  batch 40 loss: 1.9768132030963899\n",
      "  batch 50 loss: 1.987922155857086\n",
      "  batch 60 loss: 2.076590910553932\n",
      "  batch 70 loss: 2.0444132536649704\n",
      "  batch 80 loss: 2.0749077409505845\n",
      "  batch 90 loss: 1.9959178984165191\n",
      "  batch 100 loss: 2.0125723510980604\n",
      "  batch 110 loss: 2.0559103965759276\n",
      "  batch 120 loss: 1.9746111243963242\n",
      "  batch 130 loss: 2.022141432762146\n",
      "  batch 140 loss: 2.100498008728027\n",
      "  batch 150 loss: 1.9561984956264495\n",
      "  batch 160 loss: 1.9928500056266785\n",
      "  batch 170 loss: 2.0427231073379515\n",
      "  batch 180 loss: 2.0240350157022475\n",
      "  batch 190 loss: 2.0230761140584947\n",
      "LOSS train 2.0230761140584947 valid 2.082042572590021\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 2.007091850042343\n",
      "  batch 20 loss: 2.018865552544594\n",
      "  batch 30 loss: 2.0299300283193586\n",
      "  batch 40 loss: 2.0852488785982133\n",
      "  batch 50 loss: 2.0331977128982546\n",
      "  batch 60 loss: 2.014032208919525\n",
      "  batch 70 loss: 2.0637032479047774\n",
      "  batch 80 loss: 2.0572902470827104\n",
      "  batch 90 loss: 2.0870577335357665\n",
      "  batch 100 loss: 2.0010525614023207\n",
      "  batch 110 loss: 1.937970769405365\n",
      "  batch 120 loss: 1.9915047645568849\n",
      "  batch 130 loss: 1.9922891050577163\n",
      "  batch 140 loss: 1.9760399967432023\n",
      "  batch 150 loss: 1.9713556617498398\n",
      "  batch 160 loss: 2.038416105508804\n",
      "  batch 170 loss: 1.986753162741661\n",
      "  batch 180 loss: 1.9781632035970689\n",
      "  batch 190 loss: 2.052949067950249\n",
      "LOSS train 2.052949067950249 valid 2.075432939789234\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 2.1053442686796187\n",
      "  batch 20 loss: 1.979966163635254\n",
      "  batch 30 loss: 2.012681132555008\n",
      "  batch 40 loss: 1.9072469174861908\n",
      "  batch 50 loss: 2.0577572107315065\n",
      "  batch 60 loss: 2.078185522556305\n",
      "  batch 70 loss: 1.9338347494602204\n",
      "  batch 80 loss: 1.9632919192314149\n",
      "  batch 90 loss: 2.06275580227375\n",
      "  batch 100 loss: 2.0462725818157197\n",
      "  batch 110 loss: 1.9821579992771148\n",
      "  batch 120 loss: 2.015590763092041\n",
      "  batch 130 loss: 2.0391233623027802\n",
      "  batch 140 loss: 2.0128036379814147\n",
      "  batch 150 loss: 1.9390995711088181\n",
      "  batch 160 loss: 2.0137086004018783\n",
      "  batch 170 loss: 1.995093509554863\n",
      "  batch 180 loss: 1.9892451226711274\n",
      "  batch 190 loss: 2.0130849093198777\n",
      "LOSS train 2.0130849093198777 valid 2.0637505039190636\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 2.0808014631271363\n",
      "  batch 20 loss: 2.006893214583397\n",
      "  batch 30 loss: 2.025044909119606\n",
      "  batch 40 loss: 2.0000060528516768\n",
      "  batch 50 loss: 1.979488581418991\n",
      "  batch 60 loss: 2.00089311003685\n",
      "  batch 70 loss: 1.8688360393047332\n",
      "  batch 80 loss: 2.03817717730999\n",
      "  batch 90 loss: 2.005128154158592\n",
      "  batch 100 loss: 1.9268827229738235\n",
      "  batch 110 loss: 2.0746488511562347\n",
      "  batch 120 loss: 1.9506596118211745\n",
      "  batch 130 loss: 1.9696471512317657\n",
      "  batch 140 loss: 2.025804418325424\n",
      "  batch 150 loss: 1.9381892949342727\n",
      "  batch 160 loss: 2.014778012037277\n",
      "  batch 170 loss: 2.0222706884145736\n",
      "  batch 180 loss: 1.936554953455925\n",
      "  batch 190 loss: 2.0287274032831193\n",
      "LOSS train 2.0287274032831193 valid 2.047334679808372\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 2.057751166820526\n",
      "  batch 20 loss: 1.9616840124130248\n",
      "  batch 30 loss: 1.9870700746774674\n",
      "  batch 40 loss: 1.9903041630983354\n",
      "  batch 50 loss: 1.935345748066902\n",
      "  batch 60 loss: 1.9353044390678407\n",
      "  batch 70 loss: 2.0043867617845534\n",
      "  batch 80 loss: 2.044871833920479\n",
      "  batch 90 loss: 2.042244464159012\n",
      "  batch 100 loss: 1.9692674577236176\n",
      "  batch 110 loss: 1.9435816168785096\n",
      "  batch 120 loss: 1.910511639714241\n",
      "  batch 130 loss: 1.8934505790472032\n",
      "  batch 140 loss: 1.9787202388048173\n",
      "  batch 150 loss: 1.947784361243248\n",
      "  batch 160 loss: 1.882632076740265\n",
      "  batch 170 loss: 2.0234110921621324\n",
      "  batch 180 loss: 2.0181505143642426\n",
      "  batch 190 loss: 1.9996123701333999\n",
      "LOSS train 1.9996123701333999 valid 2.020930704780114\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 1.8582782924175263\n",
      "  batch 20 loss: 1.9955344676971436\n",
      "  batch 30 loss: 1.979578933119774\n",
      "  batch 40 loss: 2.0118391513824463\n",
      "  batch 50 loss: 1.97437903881073\n",
      "  batch 60 loss: 1.924487292766571\n",
      "  batch 70 loss: 1.8788011640310287\n",
      "  batch 80 loss: 1.9947349727153778\n",
      "  batch 90 loss: 1.9130786418914796\n",
      "  batch 100 loss: 2.033736398816109\n",
      "  batch 110 loss: 1.9495727241039276\n",
      "  batch 120 loss: 1.899489364027977\n",
      "  batch 130 loss: 1.9009371459484101\n",
      "  batch 140 loss: 1.9250929087400437\n",
      "  batch 150 loss: 1.8743972182273865\n",
      "  batch 160 loss: 2.000635153055191\n",
      "  batch 170 loss: 1.9145723819732665\n",
      "  batch 180 loss: 1.8916714519262314\n",
      "  batch 190 loss: 1.9266918644309043\n",
      "LOSS train 1.9266918644309043 valid 1.9819718094972463\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 1.8805802032351493\n",
      "  batch 20 loss: 1.9044207364320755\n",
      "  batch 30 loss: 1.928050810098648\n",
      "  batch 40 loss: 1.9373218327760697\n",
      "  batch 50 loss: 1.9511770725250244\n",
      "  batch 60 loss: 1.967696726322174\n",
      "  batch 70 loss: 1.9773009479045869\n",
      "  batch 80 loss: 1.8445273846387864\n",
      "  batch 90 loss: 1.932069969177246\n",
      "  batch 100 loss: 1.8987772643566132\n",
      "  batch 110 loss: 1.7417466789484024\n",
      "  batch 120 loss: 1.931582471728325\n",
      "  batch 130 loss: 1.9257040917873383\n",
      "  batch 140 loss: 1.8261930420994759\n",
      "  batch 150 loss: 1.8777365550398826\n",
      "  batch 160 loss: 1.9168121069669724\n",
      "  batch 170 loss: 1.8423359632492065\n",
      "  batch 180 loss: 1.9077100723981857\n",
      "  batch 190 loss: 1.8717574611306191\n",
      "LOSS train 1.8717574611306191 valid 1.9218033105134964\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 1.867043088376522\n",
      "  batch 20 loss: 1.9480758741497994\n",
      "  batch 30 loss: 1.8593260005116463\n",
      "  batch 40 loss: 2.0429367423057556\n",
      "  batch 50 loss: 1.877375864982605\n",
      "  batch 60 loss: 1.8857292354106903\n",
      "  batch 70 loss: 1.8516348734498025\n",
      "  batch 80 loss: 1.7868488430976868\n",
      "  batch 90 loss: 1.890570083260536\n",
      "  batch 100 loss: 1.855032978951931\n",
      "  batch 110 loss: 1.7904265105724335\n",
      "  batch 120 loss: 1.7836383894085883\n",
      "  batch 130 loss: 1.7571519702672957\n",
      "  batch 140 loss: 1.7658818721771241\n",
      "  batch 150 loss: 1.8173848554491996\n",
      "  batch 160 loss: 1.762535072863102\n",
      "  batch 170 loss: 1.8878965109586716\n",
      "  batch 180 loss: 1.895215529203415\n",
      "  batch 190 loss: 1.761347594857216\n",
      "LOSS train 1.761347594857216 valid 1.9092993971246939\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 1.8461317539215087\n",
      "  batch 20 loss: 1.8549015164375304\n",
      "  batch 30 loss: 1.6490999653935432\n",
      "  batch 40 loss: 1.8083718582987784\n",
      "  batch 50 loss: 1.7914919584989548\n",
      "  batch 60 loss: 1.6608570516109467\n",
      "  batch 70 loss: 1.816548466682434\n",
      "  batch 80 loss: 1.6860881090164184\n",
      "  batch 90 loss: 1.756289367377758\n",
      "  batch 100 loss: 1.7313705056905746\n",
      "  batch 110 loss: 1.8594221740961074\n",
      "  batch 120 loss: 1.754174430668354\n",
      "  batch 130 loss: 1.7445471182465553\n",
      "  batch 140 loss: 1.8463040061295033\n",
      "  batch 150 loss: 1.787841258943081\n",
      "  batch 160 loss: 1.720919243991375\n",
      "  batch 170 loss: 1.7274285204708577\n",
      "  batch 180 loss: 1.8172293819487095\n",
      "  batch 190 loss: 1.8383743047714234\n",
      "LOSS train 1.8383743047714234 valid 2.151434873827757\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 1.895031575858593\n",
      "  batch 20 loss: 1.8283049866557122\n",
      "  batch 30 loss: 1.7160263687372208\n",
      "  batch 40 loss: 1.592013992369175\n",
      "  batch 50 loss: 1.9048808678984641\n",
      "  batch 60 loss: 1.6754966363310815\n",
      "  batch 70 loss: 1.6502710901200772\n",
      "  batch 80 loss: 1.6380451008677483\n",
      "  batch 90 loss: 1.7311613261699677\n",
      "  batch 100 loss: 1.6789906341582537\n",
      "  batch 110 loss: 1.5556065693497658\n",
      "  batch 120 loss: 1.5476522013545035\n",
      "  batch 130 loss: 1.7030714929103852\n",
      "  batch 140 loss: 1.7117150098085403\n",
      "  batch 150 loss: 1.781127256900072\n",
      "  batch 160 loss: 1.6496212467551232\n",
      "  batch 170 loss: 1.6744454957544803\n",
      "  batch 180 loss: 1.7316841430962087\n",
      "  batch 190 loss: 1.7925916202366352\n",
      "LOSS train 1.7925916202366352 valid 1.6813525847899609\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 1.5593884371221065\n",
      "  batch 20 loss: 1.5692603185772895\n",
      "  batch 30 loss: 1.5674981251358986\n",
      "  batch 40 loss: 1.7433616653084756\n",
      "  batch 50 loss: 1.7744308523833752\n",
      "  batch 60 loss: 1.6305444337427617\n",
      "  batch 70 loss: 1.6760339736938477\n",
      "  batch 80 loss: 1.7425689436495304\n",
      "  batch 90 loss: 1.6514299467206002\n",
      "  batch 100 loss: 1.6536279425024987\n",
      "  batch 110 loss: 1.7163974702358247\n",
      "  batch 120 loss: 1.6126978781074286\n",
      "  batch 130 loss: 1.6768197409808636\n",
      "  batch 140 loss: 1.7057482868432998\n",
      "  batch 150 loss: 1.8287524417042733\n",
      "  batch 160 loss: 1.5007811199873686\n",
      "  batch 170 loss: 1.6486739877611398\n",
      "  batch 180 loss: 1.6263248778879642\n",
      "  batch 190 loss: 1.6704044461250305\n",
      "LOSS train 1.6704044461250305 valid 1.8812721585616088\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 1.701177717745304\n",
      "  batch 20 loss: 1.6330097526311875\n",
      "  batch 30 loss: 1.7076693154871463\n",
      "  batch 40 loss: 1.6674145579338073\n",
      "  batch 50 loss: 1.8152537688612937\n",
      "  batch 60 loss: 1.7223432287573814\n",
      "  batch 70 loss: 1.6214311640709638\n",
      "  batch 80 loss: 1.3700910929590464\n",
      "  batch 90 loss: 1.5527716916054488\n",
      "  batch 100 loss: 1.6538533009588718\n",
      "  batch 110 loss: 1.4647479563951493\n",
      "  batch 120 loss: 1.5907469242811203\n",
      "  batch 130 loss: 1.8046536773443222\n",
      "  batch 140 loss: 1.6544519178569317\n",
      "  batch 150 loss: 1.5315005742013454\n",
      "  batch 160 loss: 1.4634431768208742\n",
      "  batch 170 loss: 1.8320883652195334\n",
      "  batch 180 loss: 1.7015606340020895\n",
      "  batch 190 loss: 1.5118352983146905\n",
      "LOSS train 1.5118352983146905 valid 1.6184796177041836\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 1.5887669891119003\n",
      "  batch 20 loss: 1.5559895038604736\n",
      "  batch 30 loss: 1.543094313889742\n",
      "  batch 40 loss: 1.5345541764050723\n",
      "  batch 50 loss: 1.5914794683456421\n",
      "  batch 60 loss: 1.6175648659467696\n",
      "  batch 70 loss: 1.6655270002782345\n",
      "  batch 80 loss: 1.6307035110890866\n",
      "  batch 90 loss: 1.737013977766037\n",
      "  batch 100 loss: 1.7551175110042094\n",
      "  batch 110 loss: 1.597364889830351\n",
      "  batch 120 loss: 1.4373371470719576\n",
      "  batch 130 loss: 1.6389697462320327\n",
      "  batch 140 loss: 1.5942139849066734\n",
      "  batch 150 loss: 1.3442494977265596\n",
      "  batch 160 loss: 1.522945436090231\n",
      "  batch 170 loss: 1.645704911649227\n",
      "  batch 180 loss: 1.782469469681382\n",
      "  batch 190 loss: 1.5242014702409505\n",
      "LOSS train 1.5242014702409505 valid 1.6911490311702857\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 1.5975336495786905\n",
      "  batch 20 loss: 1.6001701712608338\n",
      "  batch 30 loss: 1.5914163660258054\n",
      "  batch 40 loss: 1.6038453366607428\n",
      "  batch 50 loss: 1.595233727991581\n",
      "  batch 60 loss: 1.5385735783725978\n",
      "  batch 70 loss: 1.628993407264352\n",
      "  batch 80 loss: 1.7646527651697397\n",
      "  batch 90 loss: 1.4149202074855567\n",
      "  batch 100 loss: 1.6850705809891224\n",
      "  batch 110 loss: 1.3488381333649158\n",
      "  batch 120 loss: 1.6312169117853046\n",
      "  batch 130 loss: 1.4951274149119853\n",
      "  batch 140 loss: 1.6121415615081787\n",
      "  batch 150 loss: 1.6625918038189411\n",
      "  batch 160 loss: 1.591933410987258\n",
      "  batch 170 loss: 1.4159002628177404\n",
      "  batch 180 loss: 1.4791296591982246\n",
      "  batch 190 loss: 1.742953109741211\n",
      "LOSS train 1.742953109741211 valid 2.418443482649775\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 1.5682991184294224\n",
      "  batch 20 loss: 1.8128613781183958\n",
      "  batch 30 loss: 1.6388352438807487\n",
      "  batch 40 loss: 1.5644699133932591\n",
      "  batch 50 loss: 1.7755110077559948\n",
      "  batch 60 loss: 1.5337457086890935\n",
      "  batch 70 loss: 1.697646652907133\n",
      "  batch 80 loss: 1.436463099345565\n",
      "  batch 90 loss: 1.5892548847943544\n",
      "  batch 100 loss: 1.499010972864926\n",
      "  batch 110 loss: 1.6025707092136146\n",
      "  batch 120 loss: 1.3344689613208174\n",
      "  batch 130 loss: 1.5692065425217152\n",
      "  batch 140 loss: 1.5271665222011506\n",
      "  batch 150 loss: 1.6160255956463516\n",
      "  batch 160 loss: 1.5700860442593694\n",
      "  batch 170 loss: 1.4875899970531463\n",
      "  batch 180 loss: 1.498883467912674\n",
      "  batch 190 loss: 1.4972994530573487\n",
      "LOSS train 1.4972994530573487 valid 1.5256380863631\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 1.4173163887113334\n",
      "  batch 20 loss: 1.2926948968321086\n",
      "  batch 30 loss: 1.434191761724651\n",
      "  batch 40 loss: 1.4166818283498288\n",
      "  batch 50 loss: 1.7063908904790879\n",
      "  batch 60 loss: 1.6174943050369621\n",
      "  batch 70 loss: 1.5635530393570662\n",
      "  batch 80 loss: 1.6990471873432398\n",
      "  batch 90 loss: 1.5006699625402689\n",
      "  batch 100 loss: 1.6950275104492902\n",
      "  batch 110 loss: 1.6876713789999485\n",
      "  batch 120 loss: 1.7360039874911308\n",
      "  batch 130 loss: 1.5533527690917253\n",
      "  batch 140 loss: 1.342207127995789\n",
      "  batch 150 loss: 1.5241245117038489\n",
      "  batch 160 loss: 1.4435884419828653\n",
      "  batch 170 loss: 1.4866436509415508\n",
      "  batch 180 loss: 1.5732457868754863\n",
      "  batch 190 loss: 1.6323059808462859\n",
      "LOSS train 1.6323059808462859 valid 1.5740051013536942\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 1.5351295994594694\n",
      "  batch 20 loss: 1.5584771528840065\n",
      "  batch 30 loss: 1.364809986948967\n",
      "  batch 40 loss: 1.6841819318011404\n",
      "  batch 50 loss: 1.2819044414907694\n",
      "  batch 60 loss: 1.4313260439783335\n",
      "  batch 70 loss: 1.6368956124410032\n",
      "  batch 80 loss: 1.3452381851151585\n",
      "  batch 90 loss: 1.519696393236518\n",
      "  batch 100 loss: 1.5755413113161922\n",
      "  batch 110 loss: 1.7199253872036935\n",
      "  batch 120 loss: 1.6483959659934044\n",
      "  batch 130 loss: 1.3546203125268221\n",
      "  batch 140 loss: 1.6817579105496407\n",
      "  batch 150 loss: 1.6219141256064176\n",
      "  batch 160 loss: 1.330112273991108\n",
      "  batch 170 loss: 1.5095210835337638\n",
      "  batch 180 loss: 1.6363610371947288\n",
      "  batch 190 loss: 1.4655565213412047\n",
      "LOSS train 1.4655565213412047 valid 2.2018053165565314\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 1.5702353607863189\n",
      "  batch 20 loss: 1.4286406828090548\n",
      "  batch 30 loss: 1.4351547060534358\n",
      "  batch 40 loss: 1.4041324064135552\n",
      "  batch 50 loss: 1.459927112609148\n",
      "  batch 60 loss: 1.3851254677399993\n",
      "  batch 70 loss: 1.6960265873000027\n",
      "  batch 80 loss: 1.5899082653224468\n",
      "  batch 90 loss: 1.7218036632984877\n",
      "  batch 100 loss: 1.6322341162711382\n",
      "  batch 110 loss: 1.7064454145729542\n",
      "  batch 120 loss: 1.3576425909996033\n",
      "  batch 130 loss: 1.5858764097094535\n",
      "  batch 140 loss: 1.4078152058646083\n",
      "  batch 150 loss: 1.4726685032248497\n",
      "  batch 160 loss: 1.3217214733362197\n",
      "  batch 170 loss: 1.4936528358608485\n",
      "  batch 180 loss: 1.5404159320518374\n",
      "  batch 190 loss: 1.6722831269726157\n",
      "LOSS train 1.6722831269726157 valid 1.7004436331633956\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 1.455539545789361\n",
      "  batch 20 loss: 1.435958793759346\n",
      "  batch 30 loss: 1.7704961322247983\n",
      "  batch 40 loss: 1.4109459813684224\n",
      "  batch 50 loss: 1.5316296396777034\n",
      "  batch 60 loss: 1.388586075976491\n",
      "  batch 70 loss: 1.4292599374428392\n",
      "  batch 80 loss: 1.3565235074609518\n",
      "  batch 90 loss: 1.3799823811277747\n",
      "  batch 100 loss: 1.442332254908979\n",
      "  batch 110 loss: 1.6813297621905803\n",
      "  batch 120 loss: 1.5744061468169093\n",
      "  batch 130 loss: 1.5560357907786966\n",
      "  batch 140 loss: 1.5883404774591328\n",
      "  batch 150 loss: 1.6767273258417845\n",
      "  batch 160 loss: 1.5497642751783132\n",
      "  batch 170 loss: 1.5127161925658583\n",
      "  batch 180 loss: 1.5947123125195504\n",
      "  batch 190 loss: 1.3493951082229614\n",
      "LOSS train 1.3493951082229614 valid 1.4278683477344039\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 1.6213691694661976\n",
      "  batch 20 loss: 1.4649458924308418\n",
      "  batch 30 loss: 1.2992835067212583\n",
      "  batch 40 loss: 1.2927843015640974\n",
      "  batch 50 loss: 1.5651547994464636\n",
      "  batch 60 loss: 1.3035623853094875\n",
      "  batch 70 loss: 1.6116240471601486\n",
      "  batch 80 loss: 1.502040250506252\n",
      "  batch 90 loss: 1.3986823810264468\n",
      "  batch 100 loss: 1.5024288399145007\n",
      "  batch 110 loss: 1.5037587542086839\n",
      "  batch 120 loss: 1.3716456822119654\n",
      "  batch 130 loss: 1.3995551835745572\n",
      "  batch 140 loss: 1.582625995017588\n",
      "  batch 150 loss: 1.5120559468865395\n",
      "  batch 160 loss: 1.2717761686071754\n",
      "  batch 170 loss: 1.7230004025623202\n",
      "  batch 180 loss: 1.7238806542009115\n",
      "  batch 190 loss: 1.552342352643609\n",
      "LOSS train 1.552342352643609 valid 1.439675632195595\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 1.2393096778541803\n",
      "  batch 20 loss: 1.6161898642778396\n",
      "  batch 30 loss: 1.4120790261775256\n",
      "  batch 40 loss: 1.540583484992385\n",
      "  batch 50 loss: 1.2897158145904541\n",
      "  batch 60 loss: 1.7030800819396972\n",
      "  batch 70 loss: 1.5056474588811397\n",
      "  batch 80 loss: 1.5722762616351247\n",
      "  batch 90 loss: 1.241644418798387\n",
      "  batch 100 loss: 1.3940800013020636\n",
      "  batch 110 loss: 1.4222734745591878\n",
      "  batch 120 loss: 1.598411875963211\n",
      "  batch 130 loss: 1.2749179093167187\n",
      "  batch 140 loss: 1.5835824806243182\n",
      "  batch 150 loss: 1.2822870628908276\n",
      "  batch 160 loss: 1.526500960625708\n",
      "  batch 170 loss: 1.4761475110426545\n",
      "  batch 180 loss: 1.4928931746631862\n",
      "  batch 190 loss: 1.5276739737018943\n",
      "LOSS train 1.5276739737018943 valid 1.4366343652781768\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 1.4949275888502598\n",
      "  batch 20 loss: 1.4090418649837375\n",
      "  batch 30 loss: 1.5039433053694666\n",
      "  batch 40 loss: 1.6012670446187258\n",
      "  batch 50 loss: 1.5583416866138577\n",
      "  batch 60 loss: 1.235443550348282\n",
      "  batch 70 loss: 1.4094529524445534\n",
      "  batch 80 loss: 1.4088397399522363\n",
      "  batch 90 loss: 1.5819188973866403\n",
      "  batch 100 loss: 1.2519165631383657\n",
      "  batch 110 loss: 1.4658505868166685\n",
      "  batch 120 loss: 1.5129557644948364\n",
      "  batch 130 loss: 1.399762324243784\n",
      "  batch 140 loss: 1.3851310569792985\n",
      "  batch 150 loss: 1.4192495431751013\n",
      "  batch 160 loss: 1.317210822366178\n",
      "  batch 170 loss: 1.523094704374671\n",
      "  batch 180 loss: 1.4325031895190476\n",
      "  batch 190 loss: 1.3001645672135056\n",
      "LOSS train 1.3001645672135056 valid 1.4097921836118286\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 1.5867326721549033\n",
      "  batch 20 loss: 1.4897104468196631\n",
      "  batch 30 loss: 1.284195147268474\n",
      "  batch 40 loss: 1.58590801153332\n",
      "  batch 50 loss: 1.3023088198155164\n",
      "  batch 60 loss: 1.392377407476306\n",
      "  batch 70 loss: 1.4175815782509744\n",
      "  batch 80 loss: 1.4723624154925345\n",
      "  batch 90 loss: 1.4107492376118898\n",
      "  batch 100 loss: 1.594647766649723\n",
      "  batch 110 loss: 1.300303616374731\n",
      "  batch 120 loss: 1.3849043199792503\n",
      "  batch 130 loss: 1.349962118268013\n",
      "  batch 140 loss: 1.1807879268191754\n",
      "  batch 150 loss: 1.4758315990678965\n",
      "  batch 160 loss: 1.5211390234529971\n",
      "  batch 170 loss: 1.5601728841662408\n",
      "  batch 180 loss: 1.429503153078258\n",
      "  batch 190 loss: 1.2454278364777565\n",
      "LOSS train 1.2454278364777565 valid 1.3870122955323985\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 1.567304198257625\n",
      "  batch 20 loss: 1.303122728690505\n",
      "  batch 30 loss: 1.3641686690971255\n",
      "  batch 40 loss: 1.547733948379755\n",
      "  batch 50 loss: 1.1184628604911269\n",
      "  batch 60 loss: 1.4354555631056427\n",
      "  batch 70 loss: 1.2840851943939924\n",
      "  batch 80 loss: 1.6512867519631982\n",
      "  batch 90 loss: 1.3202402213588358\n",
      "  batch 100 loss: 1.2862937072291971\n",
      "  batch 110 loss: 1.6127753261476756\n",
      "  batch 120 loss: 1.3649198079481721\n",
      "  batch 130 loss: 1.2569260405376554\n",
      "  batch 140 loss: 1.5258100269362331\n",
      "  batch 150 loss: 1.2128338223323225\n",
      "  batch 160 loss: 1.5373377626761795\n",
      "  batch 170 loss: 1.2270538963377475\n",
      "  batch 180 loss: 1.2964087139815093\n",
      "  batch 190 loss: 1.4310066484846176\n",
      "LOSS train 1.4310066484846176 valid 1.417460081860041\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 1.522650316171348\n",
      "  batch 20 loss: 1.4648503629490732\n",
      "  batch 30 loss: 1.2305988848209382\n",
      "  batch 40 loss: 1.2426644179970026\n",
      "  batch 50 loss: 1.4262047335505486\n",
      "  batch 60 loss: 1.342615645378828\n",
      "  batch 70 loss: 1.5165093736723065\n",
      "  batch 80 loss: 1.2730372412130238\n",
      "  batch 90 loss: 1.510157084465027\n",
      "  batch 100 loss: 1.1025836249813437\n",
      "  batch 110 loss: 1.1196362350136042\n",
      "  batch 120 loss: 1.318898937664926\n",
      "  batch 130 loss: 1.4754921674728394\n",
      "  batch 140 loss: 1.5221470812335611\n",
      "  batch 150 loss: 1.284193352982402\n",
      "  batch 160 loss: 1.3076442293822765\n",
      "  batch 170 loss: 1.518859599158168\n",
      "  batch 180 loss: 1.1845417438074946\n",
      "  batch 190 loss: 1.290296688862145\n",
      "LOSS train 1.290296688862145 valid 1.2845430786076646\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 1.2628960481844842\n",
      "  batch 20 loss: 1.384143067151308\n",
      "  batch 30 loss: 1.257602186407894\n",
      "  batch 40 loss: 1.5487656507641077\n",
      "  batch 50 loss: 1.3001108936965466\n",
      "  batch 60 loss: 1.3132461989298463\n",
      "  batch 70 loss: 1.294198358245194\n",
      "  batch 80 loss: 1.3020490985363722\n",
      "  batch 90 loss: 1.3017031706869602\n",
      "  batch 100 loss: 1.3594355590641498\n",
      "  batch 110 loss: 1.2772081023082138\n",
      "  batch 120 loss: 1.2713097020983697\n",
      "  batch 130 loss: 1.387735121976584\n",
      "  batch 140 loss: 1.3545333111658693\n",
      "  batch 150 loss: 1.139854376297444\n",
      "  batch 160 loss: 1.2016901690978556\n",
      "  batch 170 loss: 1.3064657427370547\n",
      "  batch 180 loss: 1.2764604231342673\n",
      "  batch 190 loss: 1.407863362133503\n",
      "LOSS train 1.407863362133503 valid 1.7228176389367154\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 1.3519535375759006\n",
      "  batch 20 loss: 1.2631961384788155\n",
      "  batch 30 loss: 1.4448017824441195\n",
      "  batch 40 loss: 1.1871408438310027\n",
      "  batch 50 loss: 1.2634289234876632\n",
      "  batch 60 loss: 1.170558669604361\n",
      "  batch 70 loss: 0.9990486699156463\n",
      "  batch 80 loss: 1.2723068743944168\n",
      "  batch 90 loss: 1.3335035550408065\n",
      "  batch 100 loss: 1.406853398308158\n",
      "  batch 110 loss: 1.3390888519585133\n",
      "  batch 120 loss: 1.2062307050917298\n",
      "  batch 130 loss: 1.1196088997647167\n",
      "  batch 140 loss: 1.161955091636628\n",
      "  batch 150 loss: 1.3641748374328018\n",
      "  batch 160 loss: 1.3291889579966665\n",
      "  batch 170 loss: 1.169639902561903\n",
      "  batch 180 loss: 1.5275720477104187\n",
      "  batch 190 loss: 1.2838491207920015\n",
      "LOSS train 1.2838491207920015 valid 1.1801347895405996\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 1.32783334068954\n",
      "  batch 20 loss: 1.175092712417245\n",
      "  batch 30 loss: 1.238239973410964\n",
      "  batch 40 loss: 1.1125373909249903\n",
      "  batch 50 loss: 1.2487238544970751\n",
      "  batch 60 loss: 1.323247180879116\n",
      "  batch 70 loss: 1.1765367303974927\n",
      "  batch 80 loss: 1.0735523007810115\n",
      "  batch 90 loss: 0.991018018964678\n",
      "  batch 100 loss: 1.1453785561025143\n",
      "  batch 110 loss: 1.419066465459764\n",
      "  batch 120 loss: 1.3934447519481181\n",
      "  batch 130 loss: 0.9991960239596664\n",
      "  batch 140 loss: 1.475646619312465\n",
      "  batch 150 loss: 1.2468408841639758\n",
      "  batch 160 loss: 1.1834088552743196\n",
      "  batch 170 loss: 1.129766516201198\n",
      "  batch 180 loss: 1.3596783384680748\n",
      "  batch 190 loss: 1.1289410473778845\n",
      "LOSS train 1.1289410473778845 valid 1.1864216179849627\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 1.3016765896230935\n",
      "  batch 20 loss: 1.449290818721056\n",
      "  batch 30 loss: 1.1345335513353347\n",
      "  batch 40 loss: 1.1137836859561503\n",
      "  batch 50 loss: 1.0689789053052663\n",
      "  batch 60 loss: 1.1366996195167303\n",
      "  batch 70 loss: 1.100832610577345\n",
      "  batch 80 loss: 1.350113285332918\n",
      "  batch 90 loss: 1.062711527198553\n",
      "  batch 100 loss: 1.0568857621401548\n",
      "  batch 110 loss: 1.1319708953611554\n",
      "  batch 120 loss: 1.1636240556836128\n",
      "  batch 130 loss: 1.1661574616096915\n",
      "  batch 140 loss: 1.0295074603520333\n",
      "  batch 150 loss: 1.1430171251296997\n",
      "  batch 160 loss: 1.5176860663108527\n",
      "  batch 170 loss: 1.285168425925076\n",
      "  batch 180 loss: 1.2195817053318023\n",
      "  batch 190 loss: 1.1463699340820312\n",
      "LOSS train 1.1463699340820312 valid 1.8686131385083382\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 1.19868496209383\n",
      "  batch 20 loss: 1.2034659542143344\n",
      "  batch 30 loss: 1.093950535543263\n",
      "  batch 40 loss: 1.4275627043098211\n",
      "  batch 50 loss: 1.2308786039240658\n",
      "  batch 60 loss: 1.0962807454168797\n",
      "  batch 70 loss: 1.310486786440015\n",
      "  batch 80 loss: 1.1305853409692646\n",
      "  batch 90 loss: 1.0429793499410152\n",
      "  batch 100 loss: 1.3320230416953565\n",
      "  batch 110 loss: 1.1682304974645377\n",
      "  batch 120 loss: 1.199574600160122\n",
      "  batch 130 loss: 1.227025135792792\n",
      "  batch 140 loss: 1.0867612589150668\n",
      "  batch 150 loss: 1.0416710505262017\n",
      "  batch 160 loss: 1.0856706347316503\n",
      "  batch 170 loss: 1.3105102917179465\n",
      "  batch 180 loss: 0.9799147570505738\n",
      "  batch 190 loss: 1.293791113421321\n",
      "LOSS train 1.293791113421321 valid 1.0303218438863182\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 1.1163519103080035\n",
      "  batch 20 loss: 1.095437124185264\n",
      "  batch 30 loss: 1.2526184175163508\n",
      "  batch 40 loss: 1.1202432370744646\n",
      "  batch 50 loss: 1.1435876470059156\n",
      "  batch 60 loss: 1.127688906621188\n",
      "  batch 70 loss: 1.3398641953244805\n",
      "  batch 80 loss: 1.002639376744628\n",
      "  batch 90 loss: 1.3361291222274303\n",
      "  batch 100 loss: 1.1882834166288376\n",
      "  batch 110 loss: 0.9171266726218164\n",
      "  batch 120 loss: 1.1206948892213404\n",
      "  batch 130 loss: 1.236280227266252\n",
      "  batch 140 loss: 0.9338189523667098\n",
      "  batch 150 loss: 1.2506362438201903\n",
      "  batch 160 loss: 0.8830140529200434\n",
      "  batch 170 loss: 1.0994217901490628\n",
      "  batch 180 loss: 1.0697465527337044\n",
      "  batch 190 loss: 1.203455831296742\n",
      "LOSS train 1.203455831296742 valid 1.0352105116232848\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.937748546525836\n",
      "  batch 20 loss: 1.1243814221583306\n",
      "  batch 30 loss: 0.9802174676209688\n",
      "  batch 40 loss: 1.0428886633366345\n",
      "  batch 50 loss: 1.3103861863724888\n",
      "  batch 60 loss: 1.0045503201894461\n",
      "  batch 70 loss: 1.1252446783706547\n",
      "  batch 80 loss: 1.1311550372280181\n",
      "  batch 90 loss: 0.9056954838335514\n",
      "  batch 100 loss: 1.29619035795331\n",
      "  batch 110 loss: 0.9808444522321225\n",
      "  batch 120 loss: 1.3279518677853048\n",
      "  batch 130 loss: 1.0297529889270662\n",
      "  batch 140 loss: 1.1257620037533342\n",
      "  batch 150 loss: 1.1531035624444486\n",
      "  batch 160 loss: 1.1338629296049476\n",
      "  batch 170 loss: 1.1364942201413215\n",
      "  batch 180 loss: 0.9402421079576015\n",
      "  batch 190 loss: 1.1930584502406418\n",
      "LOSS train 1.1930584502406418 valid 0.9778972870718019\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.7303036482073366\n",
      "  batch 20 loss: 0.9604058579076081\n",
      "  batch 30 loss: 1.4613241857849062\n",
      "  batch 40 loss: 1.033535483572632\n",
      "  batch 50 loss: 1.1805033128708602\n",
      "  batch 60 loss: 0.8227027581073344\n",
      "  batch 70 loss: 0.9844197102822363\n",
      "  batch 80 loss: 1.0566223669797181\n",
      "  batch 90 loss: 1.1191283414140343\n",
      "  batch 100 loss: 0.9841381685808301\n",
      "  batch 110 loss: 1.0358031576499342\n",
      "  batch 120 loss: 1.3500434271991253\n",
      "  batch 130 loss: 1.0199404742568732\n",
      "  batch 140 loss: 1.3767064830288291\n",
      "  batch 150 loss: 1.1582922879606485\n",
      "  batch 160 loss: 0.9614733182825148\n",
      "  batch 170 loss: 0.9284302616491914\n",
      "  batch 180 loss: 0.9783904540352524\n",
      "  batch 190 loss: 1.295806481130421\n",
      "LOSS train 1.295806481130421 valid 1.0650753303645895\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 1.076906837336719\n",
      "  batch 20 loss: 0.9645596580579877\n",
      "  batch 30 loss: 1.2546719434671103\n",
      "  batch 40 loss: 1.0893151365220546\n",
      "  batch 50 loss: 1.175362959690392\n",
      "  batch 60 loss: 1.1737590626813472\n",
      "  batch 70 loss: 1.2606731174513697\n",
      "  batch 80 loss: 1.0800532087683679\n",
      "  batch 90 loss: 1.0969738466665149\n",
      "  batch 100 loss: 1.0337882350198924\n",
      "  batch 110 loss: 1.068460556678474\n",
      "  batch 120 loss: 0.9194109329022467\n",
      "  batch 130 loss: 1.0175772430375218\n",
      "  batch 140 loss: 0.8808455966413021\n",
      "  batch 150 loss: 1.0730599608272313\n",
      "  batch 160 loss: 1.2112515587359667\n",
      "  batch 170 loss: 0.8086349733173848\n",
      "  batch 180 loss: 1.034848933853209\n",
      "  batch 190 loss: 1.157557146344334\n",
      "LOSS train 1.157557146344334 valid 1.0858647163169315\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.8889463065192104\n",
      "  batch 20 loss: 0.8773251567967236\n",
      "  batch 30 loss: 1.1469430474564433\n",
      "  batch 40 loss: 0.9410358252003789\n",
      "  batch 50 loss: 1.232771010324359\n",
      "  batch 60 loss: 0.7921800738200545\n",
      "  batch 70 loss: 0.82007075259462\n",
      "  batch 80 loss: 0.9716921930201352\n",
      "  batch 90 loss: 0.8314698568545282\n",
      "  batch 100 loss: 1.2866207890212535\n",
      "  batch 110 loss: 1.0619597879238427\n",
      "  batch 120 loss: 1.0129187870770693\n",
      "  batch 130 loss: 1.194340824894607\n",
      "  batch 140 loss: 1.1044989498332143\n",
      "  batch 150 loss: 0.9168111545965075\n",
      "  batch 160 loss: 1.2637338606640696\n",
      "  batch 170 loss: 1.2096687371842563\n",
      "  batch 180 loss: 0.8308688066899776\n",
      "  batch 190 loss: 1.1974129434674978\n",
      "LOSS train 1.1974129434674978 valid 0.8509022211655974\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.8970838543027639\n",
      "  batch 20 loss: 1.0549549490213395\n",
      "  batch 30 loss: 0.9354030912276358\n",
      "  batch 40 loss: 0.8987535874359309\n",
      "  batch 50 loss: 0.9864257711917161\n",
      "  batch 60 loss: 1.4718997963704168\n",
      "  batch 70 loss: 0.7665794003754854\n",
      "  batch 80 loss: 0.894927965477109\n",
      "  batch 90 loss: 0.9231108073145151\n",
      "  batch 100 loss: 1.0137932154349982\n",
      "  batch 110 loss: 1.114960760436952\n",
      "  batch 120 loss: 0.9275665059685707\n",
      "  batch 130 loss: 0.9762057077139616\n",
      "  batch 140 loss: 1.036934631690383\n",
      "  batch 150 loss: 0.9768104398623109\n",
      "  batch 160 loss: 0.9093582877889276\n",
      "  batch 170 loss: 1.165807238034904\n",
      "  batch 180 loss: 1.208870763797313\n",
      "  batch 190 loss: 0.8624936799518764\n",
      "LOSS train 0.8624936799518764 valid 1.022327668296221\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 1.001709371060133\n",
      "  batch 20 loss: 1.0650594379752873\n",
      "  batch 30 loss: 0.9902189093641937\n",
      "  batch 40 loss: 0.738846972770989\n",
      "  batch 50 loss: 0.996251107007265\n",
      "  batch 60 loss: 0.9979410314932465\n",
      "  batch 70 loss: 1.0068406578153373\n",
      "  batch 80 loss: 0.9722843534313143\n",
      "  batch 90 loss: 1.1075228028930724\n",
      "  batch 100 loss: 1.0828310552984477\n",
      "  batch 110 loss: 0.8936412008479238\n",
      "  batch 120 loss: 0.847003610432148\n",
      "  batch 130 loss: 0.7721466651186347\n",
      "  batch 140 loss: 1.0947471148334444\n",
      "  batch 150 loss: 0.9325544062070549\n",
      "  batch 160 loss: 0.9064538210630417\n",
      "  batch 170 loss: 0.9388061310630291\n",
      "  batch 180 loss: 1.0131795268505812\n",
      "  batch 190 loss: 1.1027518531307579\n",
      "LOSS train 1.1027518531307579 valid 0.8199676615186036\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.7463437098078429\n",
      "  batch 20 loss: 0.8752368515357375\n",
      "  batch 30 loss: 0.9273945970460773\n",
      "  batch 40 loss: 1.0716695099137723\n",
      "  batch 50 loss: 1.0262952250428499\n",
      "  batch 60 loss: 0.8606598720885813\n",
      "  batch 70 loss: 0.8959039974026382\n",
      "  batch 80 loss: 1.1888821786269546\n",
      "  batch 90 loss: 0.946044531930238\n",
      "  batch 100 loss: 1.0630764356814324\n",
      "  batch 110 loss: 1.1660359730944037\n",
      "  batch 120 loss: 0.8724125666543842\n",
      "  batch 130 loss: 0.9600744860246777\n",
      "  batch 140 loss: 1.2111828114837409\n",
      "  batch 150 loss: 0.8626469809561967\n",
      "  batch 160 loss: 0.9232375593855977\n",
      "  batch 170 loss: 0.7725033310241998\n",
      "  batch 180 loss: 1.012487322371453\n",
      "  batch 190 loss: 1.186181220319122\n",
      "LOSS train 1.186181220319122 valid 1.9179802370400956\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 1.345466567762196\n",
      "  batch 20 loss: 1.2034393526613711\n",
      "  batch 30 loss: 0.8818746598437428\n",
      "  batch 40 loss: 0.8013699108734726\n",
      "  batch 50 loss: 0.9330085035413503\n",
      "  batch 60 loss: 0.8580119635909795\n",
      "  batch 70 loss: 0.8116027704440057\n",
      "  batch 80 loss: 0.8487015089951455\n",
      "  batch 90 loss: 0.8443447057157755\n",
      "  batch 100 loss: 1.0239554459229112\n",
      "  batch 110 loss: 0.9893967937678099\n",
      "  batch 120 loss: 0.9413475993089377\n",
      "  batch 130 loss: 0.7282483726739883\n",
      "  batch 140 loss: 1.2914485509507359\n",
      "  batch 150 loss: 1.0026626576669515\n",
      "  batch 160 loss: 1.0325307201594114\n",
      "  batch 170 loss: 0.9141324745491147\n",
      "  batch 180 loss: 0.8384040033444762\n",
      "  batch 190 loss: 0.8599828528240323\n",
      "LOSS train 0.8599828528240323 valid 0.8672587180163902\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.841044713370502\n",
      "  batch 20 loss: 1.0676112628076226\n",
      "  batch 30 loss: 1.0017950180917978\n",
      "  batch 40 loss: 0.9649622224271297\n",
      "  batch 50 loss: 0.7191035089083015\n",
      "  batch 60 loss: 0.8783621326088905\n",
      "  batch 70 loss: 1.2385466879233717\n",
      "  batch 80 loss: 1.1511191665194929\n",
      "  batch 90 loss: 0.9604664707556367\n",
      "  batch 100 loss: 0.8513454088941217\n",
      "  batch 110 loss: 0.94569710358046\n",
      "  batch 120 loss: 1.0116388631984592\n",
      "  batch 130 loss: 1.0245900028850883\n",
      "  batch 140 loss: 0.8945914289914072\n",
      "  batch 150 loss: 1.0290398727171124\n",
      "  batch 160 loss: 0.7741210607811808\n",
      "  batch 170 loss: 0.9309647652320564\n",
      "  batch 180 loss: 0.9708953652530908\n",
      "  batch 190 loss: 0.6334200521931053\n",
      "LOSS train 0.6334200521931053 valid 1.3176904850734923\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.8012393048033118\n",
      "  batch 20 loss: 0.7837222043424845\n",
      "  batch 30 loss: 1.0238905008882284\n",
      "  batch 40 loss: 0.631581677030772\n",
      "  batch 50 loss: 0.9721769689582288\n",
      "  batch 60 loss: 0.976969807036221\n",
      "  batch 70 loss: 0.9000516759231687\n",
      "  batch 80 loss: 0.6796768298372626\n",
      "  batch 90 loss: 0.974460431560874\n",
      "  batch 100 loss: 0.8370953650213778\n",
      "  batch 110 loss: 0.911487579997629\n",
      "  batch 120 loss: 0.6998682837001979\n",
      "  batch 130 loss: 1.0775635937228798\n",
      "  batch 140 loss: 0.8964828262105584\n",
      "  batch 150 loss: 1.0825546491891145\n",
      "  batch 160 loss: 0.9230901202186942\n",
      "  batch 170 loss: 0.7728605075739324\n",
      "  batch 180 loss: 0.7809543513692916\n",
      "  batch 190 loss: 0.9620621006935834\n",
      "LOSS train 0.9620621006935834 valid 0.7080085548834923\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.8216885556466877\n",
      "  batch 20 loss: 0.8391663942486047\n",
      "  batch 30 loss: 0.9842730931006372\n",
      "  batch 40 loss: 0.9797379786614329\n",
      "  batch 50 loss: 0.8929181938059628\n",
      "  batch 60 loss: 0.6138640893623233\n",
      "  batch 70 loss: 0.9978679856285453\n",
      "  batch 80 loss: 0.7507434705272317\n",
      "  batch 90 loss: 0.9721465209964663\n",
      "  batch 100 loss: 1.1470334656536578\n",
      "  batch 110 loss: 0.7969422600232065\n",
      "  batch 120 loss: 0.8099366249516606\n",
      "  batch 130 loss: 0.7988214144483209\n",
      "  batch 140 loss: 0.7420545497909188\n",
      "  batch 150 loss: 0.879529644921422\n",
      "  batch 160 loss: 0.7695460302755237\n",
      "  batch 170 loss: 0.6774613639339805\n",
      "  batch 180 loss: 0.7991362331435085\n",
      "  batch 190 loss: 1.0169365022331476\n",
      "LOSS train 1.0169365022331476 valid 1.0274144283686883\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.9178614432923495\n",
      "  batch 20 loss: 0.9315047380514443\n",
      "  batch 30 loss: 0.8035244096070528\n",
      "  batch 40 loss: 1.0538230725564062\n",
      "  batch 50 loss: 0.8181273073889315\n",
      "  batch 60 loss: 0.7592494475655258\n",
      "  batch 70 loss: 0.7449951056391001\n",
      "  batch 80 loss: 0.961273041088134\n",
      "  batch 90 loss: 0.8204776586033404\n",
      "  batch 100 loss: 0.8555429439991713\n",
      "  batch 110 loss: 0.8516475340351463\n",
      "  batch 120 loss: 0.6121943781152368\n",
      "  batch 130 loss: 0.6999602746218443\n",
      "  batch 140 loss: 0.7873352366499603\n",
      "  batch 150 loss: 0.7975609917193651\n",
      "  batch 160 loss: 0.6626142906490713\n",
      "  batch 170 loss: 0.810418251156807\n",
      "  batch 180 loss: 0.6900667388923466\n",
      "  batch 190 loss: 0.7063937048893422\n",
      "LOSS train 0.7063937048893422 valid 0.8962870046484451\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.7201048312708735\n",
      "  batch 20 loss: 0.8543073265813291\n",
      "  batch 30 loss: 0.5595617873594165\n",
      "  batch 40 loss: 0.6134509100578726\n",
      "  batch 50 loss: 0.7713867522310466\n",
      "  batch 60 loss: 0.6516267034923657\n",
      "  batch 70 loss: 1.1571248532971368\n",
      "  batch 80 loss: 1.096903280634433\n",
      "  batch 90 loss: 0.777889690734446\n",
      "  batch 100 loss: 0.7947362400591373\n",
      "  batch 110 loss: 0.7690759197808802\n",
      "  batch 120 loss: 0.6474360386840999\n",
      "  batch 130 loss: 0.797198386862874\n",
      "  batch 140 loss: 0.9272167044691741\n",
      "  batch 150 loss: 0.9507193794474006\n",
      "  batch 160 loss: 0.7919976853765547\n",
      "  batch 170 loss: 0.939323004335165\n",
      "  batch 180 loss: 0.8606543494388461\n",
      "  batch 190 loss: 0.8181020784657449\n",
      "LOSS train 0.8181020784657449 valid 0.7624995644944601\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.7118287372402847\n",
      "  batch 20 loss: 0.889873204100877\n",
      "  batch 30 loss: 0.9159284911118448\n",
      "  batch 40 loss: 0.9432858329266309\n",
      "  batch 50 loss: 0.7892765318974853\n",
      "  batch 60 loss: 0.6805646235123277\n",
      "  batch 70 loss: 0.6668264672160149\n",
      "  batch 80 loss: 0.9621392039116472\n",
      "  batch 90 loss: 0.7649485088419169\n",
      "  batch 100 loss: 0.6425833946326748\n",
      "  batch 110 loss: 0.8430062304250896\n",
      "  batch 120 loss: 0.7252316135913134\n",
      "  batch 130 loss: 0.8551857798825949\n",
      "  batch 140 loss: 0.7671173493377864\n",
      "  batch 150 loss: 0.7439742886926979\n",
      "  batch 160 loss: 0.7142543036490678\n",
      "  batch 170 loss: 0.8878624614328146\n",
      "  batch 180 loss: 0.875402614288032\n",
      "  batch 190 loss: 0.7554776624776423\n",
      "LOSS train 0.7554776624776423 valid 0.6631690136168916\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.707304849801585\n",
      "  batch 20 loss: 0.8383390734903514\n",
      "  batch 30 loss: 0.7393004014156759\n",
      "  batch 40 loss: 0.6830690082162618\n",
      "  batch 50 loss: 0.8256073287688196\n",
      "  batch 60 loss: 0.569746668660082\n",
      "  batch 70 loss: 0.6770737083163112\n",
      "  batch 80 loss: 0.7602008626796305\n",
      "  batch 90 loss: 0.8849607389420271\n",
      "  batch 100 loss: 0.795026880223304\n",
      "  batch 110 loss: 0.9357100773602725\n",
      "  batch 120 loss: 0.6581220678053796\n",
      "  batch 130 loss: 0.9082826896570623\n",
      "  batch 140 loss: 0.8475381209515035\n",
      "  batch 150 loss: 0.8012274825014174\n",
      "  batch 160 loss: 0.5997039337642491\n",
      "  batch 170 loss: 0.6794188089668751\n",
      "  batch 180 loss: 0.7064330123364926\n",
      "  batch 190 loss: 0.7588191198185086\n",
      "LOSS train 0.7588191198185086 valid 0.590596709269075\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.7966063914820551\n",
      "  batch 20 loss: 0.9965644671116024\n",
      "  batch 30 loss: 0.7581607255153358\n",
      "  batch 40 loss: 0.8165698711760342\n",
      "  batch 50 loss: 0.9354155664797872\n",
      "  batch 60 loss: 0.6603092825971544\n",
      "  batch 70 loss: 0.6314837484620511\n",
      "  batch 80 loss: 0.7278302950784564\n",
      "  batch 90 loss: 0.971184714557603\n",
      "  batch 100 loss: 0.8065382976084947\n",
      "  batch 110 loss: 0.7823321853764356\n",
      "  batch 120 loss: 0.8694867464480922\n",
      "  batch 130 loss: 0.8086958903819322\n",
      "  batch 140 loss: 0.7002826099749655\n",
      "  batch 150 loss: 0.7723596266470849\n",
      "  batch 160 loss: 0.4309582208283246\n",
      "  batch 170 loss: 0.8110500441864132\n",
      "  batch 180 loss: 0.8415436859242618\n",
      "  batch 190 loss: 0.5037032938562334\n",
      "LOSS train 0.5037032938562334 valid 0.6511319801211357\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.7241654743440449\n",
      "  batch 20 loss: 0.5281540125142783\n",
      "  batch 30 loss: 0.4193377105053514\n",
      "  batch 40 loss: 0.5721481525339186\n",
      "  batch 50 loss: 0.7330595163162797\n",
      "  batch 60 loss: 0.7336383385583758\n",
      "  batch 70 loss: 0.5351569228805602\n",
      "  batch 80 loss: 0.8514698371291161\n",
      "  batch 90 loss: 0.771194333746098\n",
      "  batch 100 loss: 0.8077713339123875\n",
      "  batch 110 loss: 1.076290343888104\n",
      "  batch 120 loss: 0.6294810543535277\n",
      "  batch 130 loss: 0.8381823996081949\n",
      "  batch 140 loss: 0.9198361810296773\n",
      "  batch 150 loss: 0.871507917996496\n",
      "  batch 160 loss: 0.6505127952434122\n",
      "  batch 170 loss: 1.0342752499505878\n",
      "  batch 180 loss: 0.7293613398447633\n",
      "  batch 190 loss: 0.7229916321113705\n",
      "LOSS train 0.7229916321113705 valid 1.1850965243573182\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 0.588421429367736\n",
      "  batch 20 loss: 0.5988331691361963\n",
      "  batch 30 loss: 0.6040412398055196\n",
      "  batch 40 loss: 0.6930719105526805\n",
      "  batch 50 loss: 0.7953139978460968\n",
      "  batch 60 loss: 0.7053930213209242\n",
      "  batch 70 loss: 0.45513157721143216\n",
      "  batch 80 loss: 0.7218694660812617\n",
      "  batch 90 loss: 0.9348986526951194\n",
      "  batch 100 loss: 0.8669261145405471\n",
      "  batch 110 loss: 0.634880868671462\n",
      "  batch 120 loss: 0.5690550749655813\n",
      "  batch 130 loss: 0.8395674305036664\n",
      "  batch 140 loss: 0.8091426496161148\n",
      "  batch 150 loss: 0.6182666531763971\n",
      "  batch 160 loss: 0.6929470640141517\n",
      "  batch 170 loss: 0.7479994720779359\n",
      "  batch 180 loss: 0.7325977453030645\n",
      "  batch 190 loss: 0.6123613672330975\n",
      "LOSS train 0.6123613672330975 valid 0.899361239047721\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.620805301796645\n",
      "  batch 20 loss: 0.6635924151632935\n",
      "  batch 30 loss: 0.49476075554266574\n",
      "  batch 40 loss: 0.7293336369097233\n",
      "  batch 50 loss: 0.5704142085975036\n",
      "  batch 60 loss: 0.7018447034060955\n",
      "  batch 70 loss: 0.6136320278514177\n",
      "  batch 80 loss: 0.7353783852420748\n",
      "  batch 90 loss: 0.8214806273113936\n",
      "  batch 100 loss: 0.8581803989596665\n",
      "  batch 110 loss: 0.682466248003766\n",
      "  batch 120 loss: 0.9393015366978943\n",
      "  batch 130 loss: 0.9316181735135615\n",
      "  batch 140 loss: 0.5390070117078721\n",
      "  batch 150 loss: 0.48808916537091135\n",
      "  batch 160 loss: 0.4440493464935571\n",
      "  batch 170 loss: 0.9800907374359668\n",
      "  batch 180 loss: 0.7284153383225203\n",
      "  batch 190 loss: 0.9108317932114005\n",
      "LOSS train 0.9108317932114005 valid 0.607892557626399\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.7547916404902935\n",
      "  batch 20 loss: 0.7616321960929782\n",
      "  batch 30 loss: 0.6265483890660107\n",
      "  batch 40 loss: 0.584344285633415\n",
      "  batch 50 loss: 0.6729562048800289\n",
      "  batch 60 loss: 0.4942911206511781\n",
      "  batch 70 loss: 0.7993547768099234\n",
      "  batch 80 loss: 0.8149905178695918\n",
      "  batch 90 loss: 0.819592468906194\n",
      "  batch 100 loss: 0.782875605346635\n",
      "  batch 110 loss: 0.6704465801361948\n",
      "  batch 120 loss: 0.7248187255579979\n",
      "  batch 130 loss: 0.6017974268179387\n",
      "  batch 140 loss: 0.8212288371287286\n",
      "  batch 150 loss: 0.4127109056804329\n",
      "  batch 160 loss: 0.7564868024550379\n",
      "  batch 170 loss: 0.8580630405340344\n",
      "  batch 180 loss: 0.9390237188898027\n",
      "  batch 190 loss: 0.8899126741103828\n",
      "LOSS train 0.8899126741103828 valid 0.745315308873661\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.6303341680206358\n",
      "  batch 20 loss: 0.7968551020137966\n",
      "  batch 30 loss: 0.736386031191796\n",
      "  batch 40 loss: 0.7591824400238693\n",
      "  batch 50 loss: 0.64947146801278\n",
      "  batch 60 loss: 0.7655802404507994\n",
      "  batch 70 loss: 0.6418033439666033\n",
      "  batch 80 loss: 0.6354173267260194\n",
      "  batch 90 loss: 0.9407350514084101\n",
      "  batch 100 loss: 0.6185895841568708\n",
      "  batch 110 loss: 0.8855136360973119\n",
      "  batch 120 loss: 0.5259848915040493\n",
      "  batch 130 loss: 0.5175130520481617\n",
      "  batch 140 loss: 0.5331228451337665\n",
      "  batch 150 loss: 0.6824937676079571\n",
      "  batch 160 loss: 0.8519870302174241\n",
      "  batch 170 loss: 0.5635789457708598\n",
      "  batch 180 loss: 0.6252281379885971\n",
      "  batch 190 loss: 0.6128502690931782\n",
      "LOSS train 0.6128502690931782 valid 0.505284536173209\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.5815622415393591\n",
      "  batch 20 loss: 0.7644677311182022\n",
      "  batch 30 loss: 0.8096177546307445\n",
      "  batch 40 loss: 0.5150163740850985\n",
      "  batch 50 loss: 0.7436860418878496\n",
      "  batch 60 loss: 0.5559245271608233\n",
      "  batch 70 loss: 0.734730242472142\n",
      "  batch 80 loss: 0.8524341884069144\n",
      "  batch 90 loss: 0.6860457825940103\n",
      "  batch 100 loss: 0.6306890700012445\n",
      "  batch 110 loss: 0.6270104594994337\n",
      "  batch 120 loss: 0.6572217613924295\n",
      "  batch 130 loss: 0.7559164224425331\n",
      "  batch 140 loss: 0.5918230942916125\n",
      "  batch 150 loss: 0.2918378305388615\n",
      "  batch 160 loss: 0.603400411538314\n",
      "  batch 170 loss: 0.7350777758751065\n",
      "  batch 180 loss: 0.5512412671465426\n",
      "  batch 190 loss: 0.7942037569358945\n",
      "LOSS train 0.7942037569358945 valid 0.7593745441289428\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.7150584485381841\n",
      "  batch 20 loss: 0.5355845930986106\n",
      "  batch 30 loss: 0.7279762825462968\n",
      "  batch 40 loss: 1.1474017772357912\n",
      "  batch 50 loss: 0.7157532963436097\n",
      "  batch 60 loss: 0.5987575277686119\n",
      "  batch 70 loss: 0.5953650766983628\n",
      "  batch 80 loss: 0.5380847943481057\n",
      "  batch 90 loss: 0.7608385375235229\n",
      "  batch 100 loss: 0.7402999570593238\n",
      "  batch 110 loss: 0.7921062100445851\n",
      "  batch 120 loss: 0.6908323449082673\n",
      "  batch 130 loss: 0.8268528578337282\n",
      "  batch 140 loss: 0.5239698490127921\n",
      "  batch 150 loss: 0.5237542316783219\n",
      "  batch 160 loss: 0.639312204811722\n",
      "  batch 170 loss: 0.7098646295722574\n",
      "  batch 180 loss: 0.6196268229279667\n",
      "  batch 190 loss: 0.6621715665794909\n",
      "LOSS train 0.6621715665794909 valid 0.6633164753438905\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.58439389183186\n",
      "  batch 20 loss: 0.5999882789794355\n",
      "  batch 30 loss: 0.6589873362332582\n",
      "  batch 40 loss: 0.6349821571260691\n",
      "  batch 50 loss: 0.6655319163575768\n",
      "  batch 60 loss: 0.6749773451127112\n",
      "  batch 70 loss: 0.820732709672302\n",
      "  batch 80 loss: 0.644649719260633\n",
      "  batch 90 loss: 0.5035793881397694\n",
      "  batch 100 loss: 0.5713298581540585\n",
      "  batch 110 loss: 0.35538189704529943\n",
      "  batch 120 loss: 0.6898827758617699\n",
      "  batch 130 loss: 0.546785181434825\n",
      "  batch 140 loss: 0.47666637958027425\n",
      "  batch 150 loss: 0.9368470944231376\n",
      "  batch 160 loss: 0.6441528559662402\n",
      "  batch 170 loss: 0.6135227160295471\n",
      "  batch 180 loss: 0.604010414192453\n",
      "  batch 190 loss: 0.6828374379314482\n",
      "LOSS train 0.6828374379314482 valid 0.853501700479776\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.6241893036291003\n",
      "  batch 20 loss: 0.876615573791787\n",
      "  batch 30 loss: 0.39790409873239696\n",
      "  batch 40 loss: 0.5244577058125287\n",
      "  batch 50 loss: 0.5417744506616146\n",
      "  batch 60 loss: 0.5141617998480796\n",
      "  batch 70 loss: 0.453627364942804\n",
      "  batch 80 loss: 0.6079964824486523\n",
      "  batch 90 loss: 0.7116415986791254\n",
      "  batch 100 loss: 0.6949221485527233\n",
      "  batch 110 loss: 0.5826776090078056\n",
      "  batch 120 loss: 0.8603110386291519\n",
      "  batch 130 loss: 0.7110140973702073\n",
      "  batch 140 loss: 0.807569976663217\n",
      "  batch 150 loss: 0.5176719823852182\n",
      "  batch 160 loss: 0.9893049338366836\n",
      "  batch 170 loss: 0.5381332537857816\n",
      "  batch 180 loss: 0.7148554637795315\n",
      "  batch 190 loss: 0.4510791857726872\n",
      "LOSS train 0.4510791857726872 valid 0.5110059719628248\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.6503569429274648\n",
      "  batch 20 loss: 0.6264057321939618\n",
      "  batch 30 loss: 0.6429333330597728\n",
      "  batch 40 loss: 0.5339099748292938\n",
      "  batch 50 loss: 0.564077487634495\n",
      "  batch 60 loss: 0.5070203757612035\n",
      "  batch 70 loss: 0.5314612799324095\n",
      "  batch 80 loss: 0.7720651599345729\n",
      "  batch 90 loss: 0.5582896646345035\n",
      "  batch 100 loss: 0.7164670592173934\n",
      "  batch 110 loss: 0.9167852654820308\n",
      "  batch 120 loss: 0.5460725420620293\n",
      "  batch 130 loss: 0.5668330766260624\n",
      "  batch 140 loss: 0.7516793192364275\n",
      "  batch 150 loss: 0.5085262103471905\n",
      "  batch 160 loss: 0.5758510317653418\n",
      "  batch 170 loss: 0.5066046015359461\n",
      "  batch 180 loss: 0.47741496870294214\n",
      "  batch 190 loss: 0.49074737587943673\n",
      "LOSS train 0.49074737587943673 valid 0.6387126044734405\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.6468484387500212\n",
      "  batch 20 loss: 0.4771713517839089\n",
      "  batch 30 loss: 0.8436807627789676\n",
      "  batch 40 loss: 0.46364285266026856\n",
      "  batch 50 loss: 0.7438171758316457\n",
      "  batch 60 loss: 0.47791557912714777\n",
      "  batch 70 loss: 0.5308345217723399\n",
      "  batch 80 loss: 0.6615163972601295\n",
      "  batch 90 loss: 0.6470537255052478\n",
      "  batch 100 loss: 0.846733380202204\n",
      "  batch 110 loss: 0.4702604630030692\n",
      "  batch 120 loss: 1.0716876741236774\n",
      "  batch 130 loss: 0.48187065769452603\n",
      "  batch 140 loss: 0.598935485072434\n",
      "  batch 150 loss: 0.7629200393101201\n",
      "  batch 160 loss: 0.6711007416015491\n",
      "  batch 170 loss: 0.5586446249391883\n",
      "  batch 180 loss: 0.8059297755360604\n",
      "  batch 190 loss: 0.8720579031622038\n",
      "LOSS train 0.8720579031622038 valid 2.097825390824045\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.9324443478835747\n",
      "  batch 20 loss: 0.7410799977136776\n",
      "  batch 30 loss: 0.5916932875290513\n",
      "  batch 40 loss: 0.6869875806150958\n",
      "  batch 50 loss: 0.7220688134431839\n",
      "  batch 60 loss: 0.6828673783689737\n",
      "  batch 70 loss: 0.6269037386402487\n",
      "  batch 80 loss: 0.551749310316518\n",
      "  batch 90 loss: 0.5786270583979786\n",
      "  batch 100 loss: 0.6197371767368167\n",
      "  batch 110 loss: 0.5144230705453083\n",
      "  batch 120 loss: 0.5843427085317672\n",
      "  batch 130 loss: 0.5628862742101773\n",
      "  batch 140 loss: 0.3761833128402941\n",
      "  batch 150 loss: 0.4490184482652694\n",
      "  batch 160 loss: 0.9177101457491517\n",
      "  batch 170 loss: 0.5681942020193673\n",
      "  batch 180 loss: 0.6881707394029946\n",
      "  batch 190 loss: 0.5579409361351282\n",
      "LOSS train 0.5579409361351282 valid 0.45700163384063697\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.6535820750985295\n",
      "  batch 20 loss: 0.7554645859636366\n",
      "  batch 30 loss: 0.6094280734658242\n",
      "  batch 40 loss: 0.8174728371202946\n",
      "  batch 50 loss: 0.5922645219601691\n",
      "  batch 60 loss: 0.5544405318796635\n",
      "  batch 70 loss: 0.4673642362933606\n",
      "  batch 80 loss: 0.5952963239513338\n",
      "  batch 90 loss: 0.5463610130362213\n",
      "  batch 100 loss: 0.4520875313784927\n",
      "  batch 110 loss: 0.3812760927248746\n",
      "  batch 120 loss: 0.546889342344366\n",
      "  batch 130 loss: 0.7472444016253575\n",
      "  batch 140 loss: 0.5610222244169563\n",
      "  batch 150 loss: 0.5992301886901259\n",
      "  batch 160 loss: 0.5986081406008452\n",
      "  batch 170 loss: 0.7100217942846939\n",
      "  batch 180 loss: 0.46413462442578746\n",
      "  batch 190 loss: 0.5835201164707542\n",
      "LOSS train 0.5835201164707542 valid 0.4600317574162872\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.8291952685918659\n",
      "  batch 20 loss: 0.3887341420631856\n",
      "  batch 30 loss: 0.45517130959779023\n",
      "  batch 40 loss: 0.4243254702305421\n",
      "  batch 50 loss: 0.44889349297154696\n",
      "  batch 60 loss: 0.5803881403291598\n",
      "  batch 70 loss: 1.0687614114023745\n",
      "  batch 80 loss: 0.5656019892543555\n",
      "  batch 90 loss: 0.554269818495959\n",
      "  batch 100 loss: 0.564339368394576\n",
      "  batch 110 loss: 0.3470493972999975\n",
      "  batch 120 loss: 0.5686865534633399\n",
      "  batch 130 loss: 0.6284563299734145\n",
      "  batch 140 loss: 0.7043693041894585\n",
      "  batch 150 loss: 0.6199229847639799\n",
      "  batch 160 loss: 0.721019752509892\n",
      "  batch 170 loss: 0.5090729534393177\n",
      "  batch 180 loss: 0.5264457201119512\n",
      "  batch 190 loss: 0.39838904244825246\n",
      "LOSS train 0.39838904244825246 valid 1.1769496870954008\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.648091040935833\n",
      "  batch 20 loss: 0.6427696599857882\n",
      "  batch 30 loss: 0.6480566815240308\n",
      "  batch 40 loss: 0.5406677685678005\n",
      "  batch 50 loss: 0.5684785075020045\n",
      "  batch 60 loss: 0.5504242062103003\n",
      "  batch 70 loss: 0.5844178291037678\n",
      "  batch 80 loss: 0.5527672309544869\n",
      "  batch 90 loss: 0.41479400265961885\n",
      "  batch 100 loss: 0.5419929166790098\n",
      "  batch 110 loss: 0.5881275780731812\n",
      "  batch 120 loss: 0.480774052196648\n",
      "  batch 130 loss: 0.48289336934685706\n",
      "  batch 140 loss: 0.8368560508592054\n",
      "  batch 150 loss: 0.5032893467810936\n",
      "  batch 160 loss: 0.5129778672475368\n",
      "  batch 170 loss: 0.526323988288641\n",
      "  batch 180 loss: 0.6072224521776661\n",
      "  batch 190 loss: 1.0733744559809566\n",
      "LOSS train 1.0733744559809566 valid 0.48057883015821856\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.8942606202326715\n",
      "  batch 20 loss: 0.6286972435656935\n",
      "  batch 30 loss: 0.4394013148965314\n",
      "  batch 40 loss: 0.5485072619747371\n",
      "  batch 50 loss: 0.5861258342862129\n",
      "  batch 60 loss: 0.438573864614591\n",
      "  batch 70 loss: 0.4894293858669698\n",
      "  batch 80 loss: 0.3315939322812483\n",
      "  batch 90 loss: 0.49678288733121007\n",
      "  batch 100 loss: 0.3851322282920592\n",
      "  batch 110 loss: 0.6535491439281031\n",
      "  batch 120 loss: 0.8076266278978437\n",
      "  batch 130 loss: 0.5962291451985948\n",
      "  batch 140 loss: 0.6079686566721648\n",
      "  batch 150 loss: 0.6468407733365893\n",
      "  batch 160 loss: 0.4740903104189783\n",
      "  batch 170 loss: 0.45604139069328087\n",
      "  batch 180 loss: 0.620197705202736\n",
      "  batch 190 loss: 0.32077972118277104\n",
      "LOSS train 0.32077972118277104 valid 0.8623965591078815\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.5662113470491021\n",
      "  batch 20 loss: 0.8143655190011486\n",
      "  batch 30 loss: 0.3969586895895191\n",
      "  batch 40 loss: 0.5506531741470099\n",
      "  batch 50 loss: 0.5885826325044036\n",
      "  batch 60 loss: 0.6250230986624956\n",
      "  batch 70 loss: 0.5027032184647396\n",
      "  batch 80 loss: 0.5482473727781325\n",
      "  batch 90 loss: 0.4040754935820587\n",
      "  batch 100 loss: 0.5752149848500266\n",
      "  batch 110 loss: 0.6269635023083537\n",
      "  batch 120 loss: 0.5652204932877793\n",
      "  batch 130 loss: 0.8111550809815526\n",
      "  batch 140 loss: 0.5820775294560008\n",
      "  batch 150 loss: 0.8814854937139899\n",
      "  batch 160 loss: 0.4275636687525548\n",
      "  batch 170 loss: 0.4711594480322674\n",
      "  batch 180 loss: 0.4699299014057033\n",
      "  batch 190 loss: 0.535034637217177\n",
      "LOSS train 0.535034637217177 valid 0.6539753562828992\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.7661107883322984\n",
      "  batch 20 loss: 0.542014224716695\n",
      "  batch 30 loss: 0.6334017714951188\n",
      "  batch 40 loss: 0.4518756018136628\n",
      "  batch 50 loss: 0.7172718984074891\n",
      "  batch 60 loss: 0.4754045282723382\n",
      "  batch 70 loss: 0.37327232997631654\n",
      "  batch 80 loss: 0.6090359763475135\n",
      "  batch 90 loss: 0.5257056556351017\n",
      "  batch 100 loss: 0.3972507679311093\n",
      "  batch 110 loss: 0.4281953839585185\n",
      "  batch 120 loss: 0.9663144349586219\n",
      "  batch 130 loss: 0.39397495952434836\n",
      "  batch 140 loss: 0.6463926494587213\n",
      "  batch 150 loss: 0.7636972747277468\n",
      "  batch 160 loss: 0.5826831380953081\n",
      "  batch 170 loss: 0.596886025229469\n",
      "  batch 180 loss: 0.5497227658051997\n",
      "  batch 190 loss: 0.5110777979716659\n",
      "LOSS train 0.5110777979716659 valid 2.4029298598660924\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.8425061266869307\n",
      "  batch 20 loss: 0.7459844796219841\n",
      "  batch 30 loss: 0.6059592057019472\n",
      "  batch 40 loss: 0.5041126389987767\n",
      "  batch 50 loss: 0.5012450980255381\n",
      "  batch 60 loss: 0.4351579851936549\n",
      "  batch 70 loss: 0.5092922337120399\n",
      "  batch 80 loss: 0.2636319063487463\n",
      "  batch 90 loss: 0.8928660260047764\n",
      "  batch 100 loss: 0.5227331172209233\n",
      "  batch 110 loss: 0.4956908302148804\n",
      "  batch 120 loss: 0.6061741148587316\n",
      "  batch 130 loss: 0.2907925613108091\n",
      "  batch 140 loss: 0.5667659601662308\n",
      "  batch 150 loss: 0.488554706168361\n",
      "  batch 160 loss: 0.6636788337025792\n",
      "  batch 170 loss: 0.4813696567900479\n",
      "  batch 180 loss: 0.587860889453441\n",
      "  batch 190 loss: 0.664737602788955\n",
      "LOSS train 0.664737602788955 valid 0.46736515798152256\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.3508283764589578\n",
      "  batch 20 loss: 0.5786517507163808\n",
      "  batch 30 loss: 0.5171683756867423\n",
      "  batch 40 loss: 0.5762023117160424\n",
      "  batch 50 loss: 0.7926438571419567\n",
      "  batch 60 loss: 0.44057182800024747\n",
      "  batch 70 loss: 0.2772619224851951\n",
      "  batch 80 loss: 0.5006767032900825\n",
      "  batch 90 loss: 0.6616297423606738\n",
      "  batch 100 loss: 0.6935847523040138\n",
      "  batch 110 loss: 0.5653249371796847\n",
      "  batch 120 loss: 0.7096067719394341\n",
      "  batch 130 loss: 0.4742188544245437\n",
      "  batch 140 loss: 0.41725183876696975\n",
      "  batch 150 loss: 0.4115893295384012\n",
      "  batch 160 loss: 0.3916863657766953\n",
      "  batch 170 loss: 0.4674385254038498\n",
      "  batch 180 loss: 0.5776824434520677\n",
      "  batch 190 loss: 0.47264229225111193\n",
      "LOSS train 0.47264229225111193 valid 0.4042626553430007\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.7024315326707438\n",
      "  batch 20 loss: 0.42245376695063896\n",
      "  batch 30 loss: 0.8195836499799043\n",
      "  batch 40 loss: 0.565209208172746\n",
      "  batch 50 loss: 0.5052458859572653\n",
      "  batch 60 loss: 0.501265416026581\n",
      "  batch 70 loss: 0.4162585540441796\n",
      "  batch 80 loss: 0.49493941897526383\n",
      "  batch 90 loss: 0.4607441721251234\n",
      "  batch 100 loss: 0.4840371795231476\n",
      "  batch 110 loss: 0.5899609093437903\n",
      "  batch 120 loss: 0.3662294440087862\n",
      "  batch 130 loss: 0.3792167131090537\n",
      "  batch 140 loss: 0.557760887988843\n",
      "  batch 150 loss: 0.7001645505661145\n",
      "  batch 160 loss: 0.5157866564462893\n",
      "  batch 170 loss: 0.5468794198241085\n",
      "  batch 180 loss: 0.3321681855130009\n",
      "  batch 190 loss: 0.48674258957616984\n",
      "LOSS train 0.48674258957616984 valid 0.5987480268791282\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 0.8315641212044284\n",
      "  batch 20 loss: 0.35108031288255004\n",
      "  batch 30 loss: 0.6231930525740609\n",
      "  batch 40 loss: 0.5630161499720998\n",
      "  batch 50 loss: 0.7226185557257849\n",
      "  batch 60 loss: 0.5654785595950671\n",
      "  batch 70 loss: 0.3602857007877901\n",
      "  batch 80 loss: 0.4873853125842288\n",
      "  batch 90 loss: 0.6970006383024157\n",
      "  batch 100 loss: 0.5281158664263785\n",
      "  batch 110 loss: 0.4578546360717155\n",
      "  batch 120 loss: 0.7160046414472163\n",
      "  batch 130 loss: 0.5138995154411532\n",
      "  batch 140 loss: 0.763755642407341\n",
      "  batch 150 loss: 0.5168391204904765\n",
      "  batch 160 loss: 0.43282370634842665\n",
      "  batch 170 loss: 0.9224838259338867\n",
      "  batch 180 loss: 0.4317235143389553\n",
      "  batch 190 loss: 0.3690033895196393\n",
      "LOSS train 0.3690033895196393 valid 0.48123817182540035\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 0.5583988273981959\n",
      "  batch 20 loss: 0.4169696519849822\n",
      "  batch 30 loss: 0.4303755781031214\n",
      "  batch 40 loss: 0.33708994707558304\n",
      "  batch 50 loss: 0.3721673662133981\n",
      "  batch 60 loss: 0.397573084582109\n",
      "  batch 70 loss: 0.417071207833942\n",
      "  batch 80 loss: 0.6938547327416018\n",
      "  batch 90 loss: 0.6287806808482855\n",
      "  batch 100 loss: 0.33784772056387735\n",
      "  batch 110 loss: 0.4456014687428251\n",
      "  batch 120 loss: 0.352574471681146\n",
      "  batch 130 loss: 0.48023373874602837\n",
      "  batch 140 loss: 0.7064765389310196\n",
      "  batch 150 loss: 0.5040338717284613\n",
      "  batch 160 loss: 0.5273906659567729\n",
      "  batch 170 loss: 0.680488086398691\n",
      "  batch 180 loss: 0.6281584829906933\n",
      "  batch 190 loss: 0.5741499380441383\n",
      "LOSS train 0.5741499380441383 valid 0.536036816512933\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 0.7542687930166722\n",
      "  batch 20 loss: 0.4225731692800764\n",
      "  batch 30 loss: 0.52564312905306\n",
      "  batch 40 loss: 0.39519856829138006\n",
      "  batch 50 loss: 0.1430580671032658\n",
      "  batch 60 loss: 0.47266565593890847\n",
      "  batch 70 loss: 0.6007862790487707\n",
      "  batch 80 loss: 0.6178997931303456\n",
      "  batch 90 loss: 0.46291403961367905\n",
      "  batch 100 loss: 0.5339030148461461\n",
      "  batch 110 loss: 0.364528206421528\n",
      "  batch 120 loss: 0.5646786135504953\n",
      "  batch 130 loss: 0.4341176840243861\n",
      "  batch 140 loss: 0.5796951662749052\n",
      "  batch 150 loss: 0.4598924637073651\n",
      "  batch 160 loss: 0.4542867703363299\n",
      "  batch 170 loss: 0.2865810234565288\n",
      "  batch 180 loss: 0.7360962700564414\n",
      "  batch 190 loss: 0.8609863631310872\n",
      "LOSS train 0.8609863631310872 valid 0.49385612406350005\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 0.3073544666636735\n",
      "  batch 20 loss: 0.3648128633387387\n",
      "  batch 30 loss: 0.3663253334350884\n",
      "  batch 40 loss: 0.5408623574534431\n",
      "  batch 50 loss: 0.6735145081474911\n",
      "  batch 60 loss: 0.4718933185446076\n",
      "  batch 70 loss: 0.34778031598543746\n",
      "  batch 80 loss: 0.4570025702705607\n",
      "  batch 90 loss: 0.45667428635060786\n",
      "  batch 100 loss: 0.5647912667365744\n",
      "  batch 110 loss: 0.627266808564309\n",
      "  batch 120 loss: 0.33284928102220873\n",
      "  batch 130 loss: 0.4110558104584925\n",
      "  batch 140 loss: 0.5996605622814968\n",
      "  batch 150 loss: 0.4656403580505867\n",
      "  batch 160 loss: 0.5596772910328582\n",
      "  batch 170 loss: 0.2862161117081996\n",
      "  batch 180 loss: 0.6003207172965631\n",
      "  batch 190 loss: 0.8913677281001583\n",
      "LOSS train 0.8913677281001583 valid 0.5686421030326388\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 0.4795191097189672\n",
      "  batch 20 loss: 0.5530242891167291\n",
      "  batch 30 loss: 0.7011306135682389\n",
      "  batch 40 loss: 0.330371056846343\n",
      "  batch 50 loss: 0.5615570716676302\n",
      "  batch 60 loss: 0.7010816213325597\n",
      "  batch 70 loss: 0.564590591005981\n",
      "  batch 80 loss: 0.6788512147264555\n",
      "  batch 90 loss: 0.398811613896396\n",
      "  batch 100 loss: 0.26985439005075024\n",
      "  batch 110 loss: 0.6108339337748475\n",
      "  batch 120 loss: 0.3485651905182749\n",
      "  batch 130 loss: 0.6135527810547501\n",
      "  batch 140 loss: 0.4202197610284202\n",
      "  batch 150 loss: 0.4282211771118455\n",
      "  batch 160 loss: 0.6067984748864547\n",
      "  batch 170 loss: 0.2840124243637547\n",
      "  batch 180 loss: 0.3628932053223252\n",
      "  batch 190 loss: 0.9006078030273784\n",
      "LOSS train 0.9006078030273784 valid 0.4417006580579954\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 0.6643644328927621\n",
      "  batch 20 loss: 0.6010678236256354\n",
      "  batch 30 loss: 0.5527689041482517\n",
      "  batch 40 loss: 0.7062099900096654\n",
      "  batch 50 loss: 0.3811586309806444\n",
      "  batch 60 loss: 0.27894111960195006\n",
      "  batch 70 loss: 0.38164865961298344\n",
      "  batch 80 loss: 0.3118995984084904\n",
      "  batch 90 loss: 0.5879625659552403\n",
      "  batch 100 loss: 0.6103491069748997\n",
      "  batch 110 loss: 0.7940141751780174\n",
      "  batch 120 loss: 0.38576291123754347\n",
      "  batch 130 loss: 0.46183735543163495\n",
      "  batch 140 loss: 0.3967051170126069\n",
      "  batch 150 loss: 0.48854545218637213\n",
      "  batch 160 loss: 0.41250619452039244\n",
      "  batch 170 loss: 0.39087817427353\n",
      "  batch 180 loss: 0.292191067506792\n",
      "  batch 190 loss: 0.5092389818804804\n",
      "LOSS train 0.5092389818804804 valid 3.607827360335833\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 1.255996529449476\n",
      "  batch 20 loss: 0.5822038447950035\n",
      "  batch 30 loss: 0.6002041001105681\n",
      "  batch 40 loss: 0.5336956849816488\n",
      "  batch 50 loss: 0.4236218151869252\n",
      "  batch 60 loss: 0.555403257033322\n",
      "  batch 70 loss: 0.37471582198049874\n",
      "  batch 80 loss: 0.3164656980079599\n",
      "  batch 90 loss: 0.317415990983136\n",
      "  batch 100 loss: 0.5690454356896225\n",
      "  batch 110 loss: 0.34116700837621466\n",
      "  batch 120 loss: 0.48631212502368726\n",
      "  batch 130 loss: 0.45588825825834645\n",
      "  batch 140 loss: 0.8582779629097785\n",
      "  batch 150 loss: 0.39834363216068597\n",
      "  batch 160 loss: 0.4490006886422634\n",
      "  batch 170 loss: 0.5972170736757108\n",
      "  batch 180 loss: 0.6782753095962107\n",
      "  batch 190 loss: 0.4202690444071777\n",
      "LOSS train 0.4202690444071777 valid 0.7082105093316223\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 0.23736862523073796\n",
      "  batch 20 loss: 0.34591080929967577\n",
      "  batch 30 loss: 0.22031355685612652\n",
      "  batch 40 loss: 0.4139411178184673\n",
      "  batch 50 loss: 0.5262334330123849\n",
      "  batch 60 loss: 0.5415087707224302\n",
      "  batch 70 loss: 0.42131220757437404\n",
      "  batch 80 loss: 0.34686184002785014\n",
      "  batch 90 loss: 0.5112227246572729\n",
      "  batch 100 loss: 0.5456472713849507\n",
      "  batch 110 loss: 0.3404599921545014\n",
      "  batch 120 loss: 0.38781036852160466\n",
      "  batch 130 loss: 0.5015646068961359\n",
      "  batch 140 loss: 0.4332933681551367\n",
      "  batch 150 loss: 0.42548486031009813\n",
      "  batch 160 loss: 0.571802545490209\n",
      "  batch 170 loss: 0.7419478126277681\n",
      "  batch 180 loss: 0.5146557612693868\n",
      "  batch 190 loss: 0.8204167665913701\n",
      "LOSS train 0.8204167665913701 valid 0.5127529104028303\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 0.45046831977088003\n",
      "  batch 20 loss: 0.8059414746880066\n",
      "  batch 30 loss: 0.722409664990846\n",
      "  batch 40 loss: 0.3808187946095131\n",
      "  batch 50 loss: 0.4378216111916117\n",
      "  batch 60 loss: 0.4352689293213189\n",
      "  batch 70 loss: 0.5998304138076491\n",
      "  batch 80 loss: 0.4144584949593991\n",
      "  batch 90 loss: 0.40531292115338147\n",
      "  batch 100 loss: 0.6696904248194187\n",
      "  batch 110 loss: 0.4685174590209499\n",
      "  batch 120 loss: 0.7279309977544471\n",
      "  batch 130 loss: 0.5605974430363858\n",
      "  batch 140 loss: 0.2642149798339233\n",
      "  batch 150 loss: 0.4299612337956205\n",
      "  batch 160 loss: 0.42023195696529003\n",
      "  batch 170 loss: 0.36436267546960155\n",
      "  batch 180 loss: 0.5135710824484704\n",
      "  batch 190 loss: 0.3263803934445605\n",
      "LOSS train 0.3263803934445605 valid 1.151024427836931\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 0.4475335832452402\n",
      "  batch 20 loss: 0.5443062736769206\n",
      "  batch 30 loss: 0.3140087076870259\n",
      "  batch 40 loss: 0.501097424517502\n",
      "  batch 50 loss: 0.5709837023285218\n",
      "  batch 60 loss: 0.5522065410710638\n",
      "  batch 70 loss: 0.5081940672374913\n",
      "  batch 80 loss: 0.6001805614389013\n",
      "  batch 90 loss: 0.8503260211378801\n",
      "  batch 100 loss: 0.3750454157416243\n",
      "  batch 110 loss: 0.24309301700704963\n",
      "  batch 120 loss: 0.4412337049085181\n",
      "  batch 130 loss: 0.4628874002257362\n",
      "  batch 140 loss: 0.33879872092220464\n",
      "  batch 150 loss: 0.7062491274758941\n",
      "  batch 160 loss: 0.319309741910547\n",
      "  batch 170 loss: 0.4750542119189049\n",
      "  batch 180 loss: 0.6568581549479859\n",
      "  batch 190 loss: 0.3353623414761387\n",
      "LOSS train 0.3353623414761387 valid 0.36055642026789986\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 0.3221506408212008\n",
      "  batch 20 loss: 0.26017807529715353\n",
      "  batch 30 loss: 0.4687936588190496\n",
      "  batch 40 loss: 0.5789129422744737\n",
      "  batch 50 loss: 0.5813283338211477\n",
      "  batch 60 loss: 0.40155883093830197\n",
      "  batch 70 loss: 0.49333825859357605\n",
      "  batch 80 loss: 0.37036393414600755\n",
      "  batch 90 loss: 0.8163461377727799\n",
      "  batch 100 loss: 0.6654071228927932\n",
      "  batch 110 loss: 0.4523085778113455\n",
      "  batch 120 loss: 0.41448355236789214\n",
      "  batch 130 loss: 0.38961211099522186\n",
      "  batch 140 loss: 0.3675476349540986\n",
      "  batch 150 loss: 0.22793487638700755\n",
      "  batch 160 loss: 0.21450047128601\n",
      "  batch 170 loss: 0.31114819762296975\n",
      "  batch 180 loss: 0.32441632634145207\n",
      "  batch 190 loss: 0.5193956350558437\n",
      "LOSS train 0.5193956350558437 valid 0.5543192961426356\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 0.5896763902564999\n",
      "  batch 20 loss: 0.3110629454633454\n",
      "  batch 30 loss: 0.47934405455889645\n",
      "  batch 40 loss: 0.36955964350781867\n",
      "  batch 50 loss: 0.36538326677982697\n",
      "  batch 60 loss: 0.39682170310406945\n",
      "  batch 70 loss: 0.4970043085922953\n",
      "  batch 80 loss: 0.6486685698386282\n",
      "  batch 90 loss: 0.2675661056098761\n",
      "  batch 100 loss: 0.38782414071029053\n",
      "  batch 110 loss: 0.7926269149640575\n",
      "  batch 120 loss: 0.5727792227524333\n",
      "  batch 130 loss: 0.273734087968478\n",
      "  batch 140 loss: 0.5919349545729347\n",
      "  batch 150 loss: 0.5538628337089904\n",
      "  batch 160 loss: 0.496708924183622\n",
      "  batch 170 loss: 0.5581248169299216\n",
      "  batch 180 loss: 0.3445978255622322\n",
      "  batch 190 loss: 0.5580229199433233\n",
      "LOSS train 0.5580229199433233 valid 0.3677945473755244\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 0.3876781373619451\n",
      "  batch 20 loss: 0.3821871297026519\n",
      "  batch 30 loss: 0.5150895013677654\n",
      "  batch 40 loss: 0.4538547250529518\n",
      "  batch 50 loss: 0.2821284250880126\n",
      "  batch 60 loss: 0.1622079339926131\n",
      "  batch 70 loss: 0.3690404391731136\n",
      "  batch 80 loss: 0.26327811991795896\n",
      "  batch 90 loss: 0.44922914703493005\n",
      "  batch 100 loss: 0.28420247975736856\n",
      "  batch 110 loss: 0.5068470750382403\n",
      "  batch 120 loss: 0.852338209998561\n",
      "  batch 130 loss: 0.3198810876609059\n",
      "  batch 140 loss: 0.2924297806050163\n",
      "  batch 150 loss: 0.41898937995138114\n",
      "  batch 160 loss: 0.4198819046607241\n",
      "  batch 170 loss: 0.36751347070676277\n",
      "  batch 180 loss: 0.33992787150491494\n",
      "  batch 190 loss: 0.8780053419992327\n",
      "LOSS train 0.8780053419992327 valid 2.3014581974671158\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 0.8197058756544721\n",
      "  batch 20 loss: 0.3711023851996288\n",
      "  batch 30 loss: 0.29464524299255573\n",
      "  batch 40 loss: 0.48183349672472103\n",
      "  batch 50 loss: 0.5832297946210019\n",
      "  batch 60 loss: 0.2933525340631604\n",
      "  batch 70 loss: 0.40604833693942055\n",
      "  batch 80 loss: 0.5674773556762375\n",
      "  batch 90 loss: 0.4826381601335015\n",
      "  batch 100 loss: 0.3960640777135268\n",
      "  batch 110 loss: 0.31711929450102616\n",
      "  batch 120 loss: 0.4019216862390749\n",
      "  batch 130 loss: 0.6938077718717978\n",
      "  batch 140 loss: 0.35652869518380614\n",
      "  batch 150 loss: 0.2603596930246567\n",
      "  batch 160 loss: 0.30437537721008995\n",
      "  batch 170 loss: 0.42635903624177446\n",
      "  batch 180 loss: 0.4793467070441693\n",
      "  batch 190 loss: 0.4624757899509859\n",
      "LOSS train 0.4624757899509859 valid 0.36150749653517356\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 0.39640681678138207\n",
      "  batch 20 loss: 0.36604103361605667\n",
      "  batch 30 loss: 0.36316689263185253\n",
      "  batch 40 loss: 0.6473769886724767\n",
      "  batch 50 loss: 0.63975196548854\n",
      "  batch 60 loss: 0.3269285344926175\n",
      "  batch 70 loss: 0.6812602243153378\n",
      "  batch 80 loss: 0.2741242263262393\n",
      "  batch 90 loss: 0.4538247760006925\n",
      "  batch 100 loss: 0.2058463704888709\n",
      "  batch 110 loss: 0.40581564331951087\n",
      "  batch 120 loss: 0.3285068499681074\n",
      "  batch 130 loss: 0.4099930725700688\n",
      "  batch 140 loss: 0.8159017733705696\n",
      "  batch 150 loss: 0.24749811389483511\n",
      "  batch 160 loss: 0.3072844981768867\n",
      "  batch 170 loss: 0.6447231830796227\n",
      "  batch 180 loss: 0.4337732641259208\n",
      "  batch 190 loss: 0.29216849560034464\n",
      "LOSS train 0.29216849560034464 valid 0.4560772936921029\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 0.17277255541994235\n",
      "  batch 20 loss: 0.2535670464101713\n",
      "  batch 30 loss: 0.44198469679104163\n",
      "  batch 40 loss: 0.39438125681481323\n",
      "  batch 50 loss: 0.48017948173801417\n",
      "  batch 60 loss: 0.27998819220229054\n",
      "  batch 70 loss: 0.3713812842965126\n",
      "  batch 80 loss: 0.4936698482459178\n",
      "  batch 90 loss: 0.4680159730429295\n",
      "  batch 100 loss: 0.6544002962880768\n",
      "  batch 110 loss: 0.33616188427840826\n",
      "  batch 120 loss: 0.6429093837708934\n",
      "  batch 130 loss: 0.5455949459341355\n",
      "  batch 140 loss: 0.31895058492082173\n",
      "  batch 150 loss: 0.5372813591151498\n",
      "  batch 160 loss: 0.5331380692892708\n",
      "  batch 170 loss: 0.29325817900244144\n",
      "  batch 180 loss: 0.40351397888734936\n",
      "  batch 190 loss: 0.5049118182127131\n",
      "LOSS train 0.5049118182127131 valid 0.3655351935173176\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 0.31865771038574164\n",
      "  batch 20 loss: 0.27476125562097875\n",
      "  batch 30 loss: 0.4670332865411183\n",
      "  batch 40 loss: 0.5542086023837328\n",
      "  batch 50 loss: 0.40108062797808086\n",
      "  batch 60 loss: 0.3051268767361762\n",
      "  batch 70 loss: 0.23278010886278935\n",
      "  batch 80 loss: 0.4281438181933481\n",
      "  batch 90 loss: 0.3618856185115874\n",
      "  batch 100 loss: 0.5357582438504324\n",
      "  batch 110 loss: 0.3806715853206697\n",
      "  batch 120 loss: 0.6372805914041237\n",
      "  batch 130 loss: 0.2901981360162608\n",
      "  batch 140 loss: 0.3900342186912894\n",
      "  batch 150 loss: 0.7979010470662615\n",
      "  batch 160 loss: 0.4126369205216179\n",
      "  batch 170 loss: 0.4126783710060408\n",
      "  batch 180 loss: 0.5698831380112097\n",
      "  batch 190 loss: 0.6434018958272645\n",
      "LOSS train 0.6434018958272645 valid 0.6682994105522551\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 0.9229440454451833\n",
      "  batch 20 loss: 0.2326546769007109\n",
      "  batch 30 loss: 0.5495608253520914\n",
      "  batch 40 loss: 0.35397515483782627\n",
      "  batch 50 loss: 0.4676570840209024\n",
      "  batch 60 loss: 0.5619188132113777\n",
      "  batch 70 loss: 0.33242885034997016\n",
      "  batch 80 loss: 0.4626135596248787\n",
      "  batch 90 loss: 0.4601452308474109\n",
      "  batch 100 loss: 0.36866427171626126\n",
      "  batch 110 loss: 0.488845537751331\n",
      "  batch 120 loss: 0.17988045034289826\n",
      "  batch 130 loss: 0.28622997577185744\n",
      "  batch 140 loss: 0.454162485874258\n",
      "  batch 150 loss: 0.42912450679577885\n",
      "  batch 160 loss: 0.25115394135937097\n",
      "  batch 170 loss: 0.3872730157105252\n",
      "  batch 180 loss: 0.3498904898762703\n",
      "  batch 190 loss: 0.41420760203618556\n",
      "LOSS train 0.41420760203618556 valid 0.7317018490245503\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 0.6130207629234065\n",
      "  batch 20 loss: 0.6414407377305906\n",
      "  batch 30 loss: 0.24832911396515556\n",
      "  batch 40 loss: 0.6406523275196377\n",
      "  batch 50 loss: 0.4498196311178617\n",
      "  batch 60 loss: 0.5053106758976356\n",
      "  batch 70 loss: 0.3424096080649178\n",
      "  batch 80 loss: 0.34108566667418927\n",
      "  batch 90 loss: 0.40113973833504135\n",
      "  batch 100 loss: 0.34896328612521754\n",
      "  batch 110 loss: 0.48170554338285\n",
      "  batch 120 loss: 0.35132994143059476\n",
      "  batch 130 loss: 0.36860380571451967\n",
      "  batch 140 loss: 0.6551533285761252\n",
      "  batch 150 loss: 0.3314831810683245\n",
      "  batch 160 loss: 0.4532802970497869\n",
      "  batch 170 loss: 0.3027479904878419\n",
      "  batch 180 loss: 0.4115146611002274\n",
      "  batch 190 loss: 0.22188062905333936\n",
      "LOSS train 0.22188062905333936 valid 0.7926629035464896\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 0.5658313551306492\n",
      "  batch 20 loss: 0.2752405489241937\n",
      "  batch 30 loss: 0.20054211911628955\n",
      "  batch 40 loss: 0.4446825522696599\n",
      "  batch 50 loss: 0.5251115135120926\n",
      "  batch 60 loss: 0.4536684684033389\n",
      "  batch 70 loss: 0.31700477020349355\n",
      "  batch 80 loss: 0.43961732695461253\n",
      "  batch 90 loss: 0.47770550469867884\n",
      "  batch 100 loss: 0.3590911177743692\n",
      "  batch 110 loss: 0.4626105460047256\n",
      "  batch 120 loss: 0.2113112648425158\n",
      "  batch 130 loss: 0.41669494218658654\n",
      "  batch 140 loss: 0.3759658438619226\n",
      "  batch 150 loss: 0.44083499669213777\n",
      "  batch 160 loss: 0.7532662195095327\n",
      "  batch 170 loss: 0.36239586332812906\n",
      "  batch 180 loss: 0.40159529403026684\n",
      "  batch 190 loss: 0.46239600575645456\n",
      "LOSS train 0.46239600575645456 valid 0.3979448612312225\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 0.3681281192286406\n",
      "  batch 20 loss: 0.2897461794433184\n",
      "  batch 30 loss: 0.3688739032601006\n",
      "  batch 40 loss: 0.5445747898716945\n",
      "  batch 50 loss: 0.6295369635976386\n",
      "  batch 60 loss: 0.33638559768442067\n",
      "  batch 70 loss: 0.47282646168023346\n",
      "  batch 80 loss: 0.43254760913841894\n",
      "  batch 90 loss: 0.4481265552341938\n",
      "  batch 100 loss: 0.27512245916586836\n",
      "  batch 110 loss: 0.22561656970938201\n",
      "  batch 120 loss: 0.3585855006793281\n",
      "  batch 130 loss: 0.707820512203034\n",
      "  batch 140 loss: 0.3303008723858511\n",
      "  batch 150 loss: 0.43893641906324776\n",
      "  batch 160 loss: 0.23011189224780537\n",
      "  batch 170 loss: 0.48379564366769046\n",
      "  batch 180 loss: 0.46234438093088104\n",
      "  batch 190 loss: 0.4520162589615211\n",
      "LOSS train 0.4520162589615211 valid 0.9584391302084371\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 0.9813197497423971\n",
      "  batch 20 loss: 0.5030380496522412\n",
      "  batch 30 loss: 0.39446542704245074\n",
      "  batch 40 loss: 0.29886834792851\n",
      "  batch 50 loss: 0.42549116226728073\n",
      "  batch 60 loss: 0.4169017644308042\n",
      "  batch 70 loss: 0.3432742638804484\n",
      "  batch 80 loss: 0.19737450498505496\n",
      "  batch 90 loss: 0.4290541905444115\n",
      "  batch 100 loss: 0.5602004055515863\n",
      "  batch 110 loss: 0.4572148142178776\n",
      "  batch 120 loss: 0.33660126156464687\n",
      "  batch 130 loss: 0.3333581446728203\n",
      "  batch 140 loss: 0.4466357071098173\n",
      "  batch 150 loss: 0.3680121634854004\n",
      "  batch 160 loss: 0.7778009196350467\n",
      "  batch 170 loss: 0.3342958862922387\n",
      "  batch 180 loss: 0.4067330391320866\n",
      "  batch 190 loss: 0.2771344986278564\n",
      "LOSS train 0.2771344986278564 valid 0.44532998921409345\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 0.38743816206988413\n",
      "  batch 20 loss: 0.36108794201427374\n",
      "  batch 30 loss: 0.4976779373770114\n",
      "  batch 40 loss: 0.3073631019811728\n",
      "  batch 50 loss: 0.24581597788492218\n",
      "  batch 60 loss: 0.5880035231501097\n",
      "  batch 70 loss: 0.4980427165151923\n",
      "  batch 80 loss: 0.38147783462191\n",
      "  batch 90 loss: 0.6595324681722559\n",
      "  batch 100 loss: 0.4323046944278758\n",
      "  batch 110 loss: 0.4058752886136062\n",
      "  batch 120 loss: 0.21776346414117143\n",
      "  batch 130 loss: 0.35476099922670984\n",
      "  batch 140 loss: 0.5800758387485985\n",
      "  batch 150 loss: 0.38604160910763313\n",
      "  batch 160 loss: 0.26794550019549207\n",
      "  batch 170 loss: 0.7732652137157856\n",
      "  batch 180 loss: 0.4140636455558706\n",
      "  batch 190 loss: 0.4331354912254028\n",
      "LOSS train 0.4331354912254028 valid 0.40300234759361514\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 0.3156528342340607\n",
      "  batch 20 loss: 0.3251530024019303\n",
      "  batch 30 loss: 0.23766278901312035\n",
      "  batch 40 loss: 0.3778600645207916\n",
      "  batch 50 loss: 0.3237501774026896\n",
      "  batch 60 loss: 0.412246773194056\n",
      "  batch 70 loss: 0.44903239082777874\n",
      "  batch 80 loss: 0.5374749501235783\n",
      "  batch 90 loss: 0.3141891061066417\n",
      "  batch 100 loss: 0.4691985260782531\n",
      "  batch 110 loss: 0.6365714769694023\n",
      "  batch 120 loss: 0.3534224566014018\n",
      "  batch 130 loss: 0.35845847147575116\n",
      "  batch 140 loss: 0.2114789706247393\n",
      "  batch 150 loss: 0.3710353087633848\n",
      "  batch 160 loss: 0.6408347888151184\n",
      "  batch 170 loss: 0.3451256941247266\n",
      "  batch 180 loss: 0.29141404255060477\n",
      "  batch 190 loss: 0.3087265450449195\n",
      "LOSS train 0.3087265450449195 valid 0.5523930050832673\n",
      "EPOCH 101:\n",
      "  batch 10 loss: 0.3967391925834818\n",
      "  batch 20 loss: 0.5185236069271923\n",
      "  batch 30 loss: 0.6026993424631655\n",
      "  batch 40 loss: 0.30969803885091096\n",
      "  batch 50 loss: 0.4007943593489472\n",
      "  batch 60 loss: 0.24684423610015074\n",
      "  batch 70 loss: 0.4050284255994484\n",
      "  batch 80 loss: 0.30586599788512103\n",
      "  batch 90 loss: 0.2496506126684835\n",
      "  batch 100 loss: 0.2396444063371746\n",
      "  batch 110 loss: 0.31185513772943524\n",
      "  batch 120 loss: 0.3780757522850763\n",
      "  batch 130 loss: 0.32521900458377784\n",
      "  batch 140 loss: 0.48358597038313744\n",
      "  batch 150 loss: 0.36489482729230077\n",
      "  batch 160 loss: 0.22129503289470448\n",
      "  batch 170 loss: 0.24299429778475315\n",
      "  batch 180 loss: 0.34875275069789496\n",
      "  batch 190 loss: 0.5322362269391305\n",
      "LOSS train 0.5322362269391305 valid 0.4210225452382777\n",
      "EPOCH 102:\n",
      "  batch 10 loss: 0.2215804299368756\n",
      "  batch 20 loss: 0.3050799341348466\n",
      "  batch 30 loss: 0.38462266493879727\n",
      "  batch 40 loss: 0.35460306761669924\n",
      "  batch 50 loss: 0.2988420571942697\n",
      "  batch 60 loss: 0.31911901432904416\n",
      "  batch 70 loss: 0.2855740608734777\n",
      "  batch 80 loss: 0.3349663205241086\n",
      "  batch 90 loss: 0.5039581533696037\n",
      "  batch 100 loss: 0.4549591540824622\n",
      "  batch 110 loss: 0.3752168397302739\n",
      "  batch 120 loss: 0.2822038454862195\n",
      "  batch 130 loss: 0.3715460882871412\n",
      "  batch 140 loss: 0.478441074868897\n",
      "  batch 150 loss: 0.6220921766711399\n",
      "  batch 160 loss: 0.7282755762105808\n",
      "  batch 170 loss: 0.4545115150685888\n",
      "  batch 180 loss: 0.44573957085376603\n",
      "  batch 190 loss: 0.4230519782984629\n",
      "LOSS train 0.4230519782984629 valid 0.334785216431229\n",
      "EPOCH 103:\n",
      "  batch 10 loss: 0.2624956775223836\n",
      "  batch 20 loss: 0.08212322708568535\n",
      "  batch 30 loss: 0.3755062980853836\n",
      "  batch 40 loss: 0.6147880916672875\n",
      "  batch 50 loss: 0.4935153092665132\n",
      "  batch 60 loss: 0.23995431950315832\n",
      "  batch 70 loss: 0.5218988672160776\n",
      "  batch 80 loss: 0.5033810214838013\n",
      "  batch 90 loss: 0.6469164748094045\n",
      "  batch 100 loss: 0.3408132247568574\n",
      "  batch 110 loss: 0.2147398324625101\n",
      "  batch 120 loss: 0.726270668185316\n",
      "  batch 130 loss: 0.3700463912100531\n",
      "  batch 140 loss: 0.47671575040440073\n",
      "  batch 150 loss: 0.4825693941907957\n",
      "  batch 160 loss: 0.26230591036146506\n",
      "  batch 170 loss: 0.45837314740638246\n",
      "  batch 180 loss: 0.5614320731488988\n",
      "  batch 190 loss: 0.36439570592483506\n",
      "LOSS train 0.36439570592483506 valid 2.685630828709318\n",
      "EPOCH 104:\n",
      "  batch 10 loss: 0.8952469441050198\n",
      "  batch 20 loss: 0.3793595106806606\n",
      "  batch 30 loss: 0.1901736080995761\n",
      "  batch 40 loss: 0.6841218864778057\n",
      "  batch 50 loss: 0.5315258666756563\n",
      "  batch 60 loss: 0.25173616014653816\n",
      "  batch 70 loss: 0.3721231662784703\n",
      "  batch 80 loss: 0.3227746996417409\n",
      "  batch 90 loss: 0.32687008382927163\n",
      "  batch 100 loss: 0.23572438196279108\n",
      "  batch 110 loss: 0.3875919871032238\n",
      "  batch 120 loss: 0.5022807538567576\n",
      "  batch 130 loss: 0.29219254683703183\n",
      "  batch 140 loss: 0.4939903800317552\n",
      "  batch 150 loss: 0.30815079001476986\n",
      "  batch 160 loss: 0.2757790180694428\n",
      "  batch 170 loss: 0.3026581543672364\n",
      "  batch 180 loss: 0.26321628951118325\n",
      "  batch 190 loss: 0.44470944883069025\n",
      "LOSS train 0.44470944883069025 valid 0.5486440159992736\n",
      "EPOCH 105:\n",
      "  batch 10 loss: 0.35697856936458267\n",
      "  batch 20 loss: 0.5223957994952798\n",
      "  batch 30 loss: 0.5347055413934868\n",
      "  batch 40 loss: 0.27755442276247777\n",
      "  batch 50 loss: 0.43084097844257485\n",
      "  batch 60 loss: 0.3395401387679158\n",
      "  batch 70 loss: 0.5980519252771046\n",
      "  batch 80 loss: 0.3399367536068894\n",
      "  batch 90 loss: 0.2912691101402743\n",
      "  batch 100 loss: 0.3096765928872628\n",
      "  batch 110 loss: 0.25136537283542565\n",
      "  batch 120 loss: 0.42780880322097803\n",
      "  batch 130 loss: 0.4014740213548066\n",
      "  batch 140 loss: 0.4959296092507429\n",
      "  batch 150 loss: 0.18132932961452752\n",
      "  batch 160 loss: 0.30920102646850867\n",
      "  batch 170 loss: 0.13673892323568\n",
      "  batch 180 loss: 0.29862579597975125\n",
      "  batch 190 loss: 0.7258831229293718\n",
      "LOSS train 0.7258831229293718 valid 0.49776895594223813\n",
      "EPOCH 106:\n",
      "  batch 10 loss: 0.2502197009831434\n",
      "  batch 20 loss: 0.2727494714410568\n",
      "  batch 30 loss: 0.3493490276479861\n",
      "  batch 40 loss: 0.9187455828068778\n",
      "  batch 50 loss: 0.3421812343702186\n",
      "  batch 60 loss: 0.3991046681549051\n",
      "  batch 70 loss: 0.245932280170382\n",
      "  batch 80 loss: 0.5119083606216008\n",
      "  batch 90 loss: 0.41327109464909884\n",
      "  batch 100 loss: 0.20029451630398398\n",
      "  batch 110 loss: 0.3415999190649018\n",
      "  batch 120 loss: 0.36892664031765887\n",
      "  batch 130 loss: 0.35862294970429504\n",
      "  batch 140 loss: 0.35258521277355614\n",
      "  batch 150 loss: 0.5321608214289881\n",
      "  batch 160 loss: 0.41939471140358364\n",
      "  batch 170 loss: 0.4436081607505912\n",
      "  batch 180 loss: 0.45207205619080926\n",
      "  batch 190 loss: 0.536822308396222\n",
      "LOSS train 0.536822308396222 valid 0.46524106406155996\n",
      "EPOCH 107:\n",
      "  batch 10 loss: 0.20734645115444436\n",
      "  batch 20 loss: 0.4254890026131761\n",
      "  batch 30 loss: 0.44401777982711793\n",
      "  batch 40 loss: 0.17432824128336505\n",
      "  batch 50 loss: 0.4116499308947823\n",
      "  batch 60 loss: 0.3698576805851189\n",
      "  batch 70 loss: 0.35664402536203854\n",
      "  batch 80 loss: 0.37387047668453305\n",
      "  batch 90 loss: 0.31259069569350684\n",
      "  batch 100 loss: 0.3884313131158706\n",
      "  batch 110 loss: 0.3009268754685763\n",
      "  batch 120 loss: 0.6913822817936307\n",
      "  batch 130 loss: 0.174433147336822\n",
      "  batch 140 loss: 0.40248624924279286\n",
      "  batch 150 loss: 0.3878327495011035\n",
      "  batch 160 loss: 0.25846637565700803\n",
      "  batch 170 loss: 0.3932394606177695\n",
      "  batch 180 loss: 0.4385391075746156\n",
      "  batch 190 loss: 0.4610269870609045\n",
      "LOSS train 0.4610269870609045 valid 0.44495688940738687\n",
      "EPOCH 108:\n",
      "  batch 10 loss: 0.43165500785107724\n",
      "  batch 20 loss: 0.200375615543453\n",
      "  batch 30 loss: 0.3368668861105107\n",
      "  batch 40 loss: 0.7631647020578385\n",
      "  batch 50 loss: 0.39431400942848993\n",
      "  batch 60 loss: 0.4595222229399951\n",
      "  batch 70 loss: 0.6494550153787714\n",
      "  batch 80 loss: 0.2645172498101601\n",
      "  batch 90 loss: 0.2612564975686837\n",
      "  batch 100 loss: 0.39051828901137925\n",
      "  batch 110 loss: 0.8058513510768535\n",
      "  batch 120 loss: 0.420977534809208\n",
      "  batch 130 loss: 0.33535175497236197\n",
      "  batch 140 loss: 0.17122715772420632\n",
      "  batch 150 loss: 0.2772295890375972\n",
      "  batch 160 loss: 0.08395850697997957\n",
      "  batch 170 loss: 0.244265234580962\n",
      "  batch 180 loss: 0.7007353530265391\n",
      "  batch 190 loss: 0.42564434190280737\n",
      "LOSS train 0.42564434190280737 valid 0.40176845781985693\n",
      "EPOCH 109:\n",
      "  batch 10 loss: 0.23581706966506316\n",
      "  batch 20 loss: 0.6173911352409049\n",
      "  batch 30 loss: 0.12288607087975834\n",
      "  batch 40 loss: 0.28429761255974884\n",
      "  batch 50 loss: 0.18452777233178494\n",
      "  batch 60 loss: 0.33412211284739896\n",
      "  batch 70 loss: 0.2427608980317018\n",
      "  batch 80 loss: 0.27387948922405486\n",
      "  batch 90 loss: 0.3152955676312558\n",
      "  batch 100 loss: 0.30055365252192134\n",
      "  batch 110 loss: 0.3439809791976586\n",
      "  batch 120 loss: 0.6283358512155246\n",
      "  batch 130 loss: 0.4623447954771109\n",
      "  batch 140 loss: 0.5142614435404539\n",
      "  batch 150 loss: 0.76759510274278\n",
      "  batch 160 loss: 0.3420075687114149\n",
      "  batch 170 loss: 0.2980070271762088\n",
      "  batch 180 loss: 0.26701749485218895\n",
      "  batch 190 loss: 0.2696983741567237\n",
      "LOSS train 0.2696983741567237 valid 0.4542117854404914\n",
      "EPOCH 110:\n",
      "  batch 10 loss: 0.2721768524876097\n",
      "  batch 20 loss: 0.6084726464163396\n",
      "  batch 30 loss: 0.4248024442247697\n",
      "  batch 40 loss: 0.6057517797133187\n",
      "  batch 50 loss: 0.4607618003617972\n",
      "  batch 60 loss: 0.5462349478853866\n",
      "  batch 70 loss: 0.2770964762828953\n",
      "  batch 80 loss: 0.31407458561006935\n",
      "  batch 90 loss: 0.29655380703043194\n",
      "  batch 100 loss: 0.3251633988460526\n",
      "  batch 110 loss: 0.27806877363764215\n",
      "  batch 120 loss: 0.1300604686664883\n",
      "  batch 130 loss: 0.2788644783111522\n",
      "  batch 140 loss: 0.19577058459981345\n",
      "  batch 150 loss: 0.15605649341305253\n",
      "  batch 160 loss: 0.26515137317474\n",
      "  batch 170 loss: 0.3215989263917436\n",
      "  batch 180 loss: 0.5634643882178352\n",
      "  batch 190 loss: 0.25487403659208213\n",
      "LOSS train 0.25487403659208213 valid 0.3177824349921325\n",
      "EPOCH 111:\n",
      "  batch 10 loss: 0.345749818457989\n",
      "  batch 20 loss: 0.17068899397854692\n",
      "  batch 30 loss: 0.17127891426498537\n",
      "  batch 40 loss: 0.28030878568824846\n",
      "  batch 50 loss: 0.2829030523134861\n",
      "  batch 60 loss: 0.2949398600918357\n",
      "  batch 70 loss: 0.46870467883418315\n",
      "  batch 80 loss: 0.537468121064012\n",
      "  batch 90 loss: 0.6436272917548195\n",
      "  batch 100 loss: 0.5051510302029782\n",
      "  batch 110 loss: 0.4103014427644666\n",
      "  batch 120 loss: 0.37628168620431096\n",
      "  batch 130 loss: 0.3825328248931328\n",
      "  batch 140 loss: 0.45265371978166513\n",
      "  batch 150 loss: 0.29175351836602204\n",
      "  batch 160 loss: 0.4108650603448041\n",
      "  batch 170 loss: 0.40620597958622967\n",
      "  batch 180 loss: 0.2987205920013366\n",
      "  batch 190 loss: 0.19818625936040918\n",
      "LOSS train 0.19818625936040918 valid 3.573615016585861\n",
      "EPOCH 112:\n",
      "  batch 10 loss: 1.1107315957437094\n",
      "  batch 20 loss: 0.32749846773804164\n",
      "  batch 30 loss: 0.2792164232523646\n",
      "  batch 40 loss: 0.26179354765918106\n",
      "  batch 50 loss: 0.4162317909416743\n",
      "  batch 60 loss: 0.23512800399621483\n",
      "  batch 70 loss: 0.31473856783122756\n",
      "  batch 80 loss: 0.26223328442138155\n",
      "  batch 90 loss: 0.29332638059713645\n",
      "  batch 100 loss: 0.6938303224393166\n",
      "  batch 110 loss: 0.204825387425808\n",
      "  batch 120 loss: 0.24687860216072294\n",
      "  batch 130 loss: 0.3071571149746887\n",
      "  batch 140 loss: 0.34731836544233374\n",
      "  batch 150 loss: 0.4645036955102114\n",
      "  batch 160 loss: 0.5219003390346189\n",
      "  batch 170 loss: 0.34838129819254393\n",
      "  batch 180 loss: 0.3922310646274127\n",
      "  batch 190 loss: 0.4863372512139904\n",
      "LOSS train 0.4863372512139904 valid 2.9145210306874687\n",
      "EPOCH 113:\n",
      "  batch 10 loss: 0.7516826259728987\n",
      "  batch 20 loss: 0.5286432448898267\n",
      "  batch 30 loss: 0.2893105942668626\n",
      "  batch 40 loss: 0.27406504799582765\n",
      "  batch 50 loss: 0.39580989418027457\n",
      "  batch 60 loss: 0.5097080469800858\n",
      "  batch 70 loss: 0.21556262019730638\n",
      "  batch 80 loss: 0.3444079527980648\n",
      "  batch 90 loss: 0.32558173768920823\n",
      "  batch 100 loss: 0.33122634199098683\n",
      "  batch 110 loss: 0.24153913221234688\n",
      "  batch 120 loss: 0.2581639825555612\n",
      "  batch 130 loss: 0.16668553958879784\n",
      "  batch 140 loss: 0.5986248472996522\n",
      "  batch 150 loss: 0.3822352903342107\n",
      "  batch 160 loss: 0.24938773543981368\n",
      "  batch 170 loss: 0.42683932614454534\n",
      "  batch 180 loss: 0.32337323114043104\n",
      "  batch 190 loss: 0.3051896585675422\n",
      "LOSS train 0.3051896585675422 valid 0.3035215438506454\n",
      "EPOCH 114:\n",
      "  batch 10 loss: 0.42145709056640046\n",
      "  batch 20 loss: 0.6390153716667555\n",
      "  batch 30 loss: 0.27070310010312826\n",
      "  batch 40 loss: 0.3919370059622452\n",
      "  batch 50 loss: 0.1839349234185647\n",
      "  batch 60 loss: 0.4959147919296811\n",
      "  batch 70 loss: 0.32614153899339726\n",
      "  batch 80 loss: 0.32959172746413967\n",
      "  batch 90 loss: 0.16313951079500838\n",
      "  batch 100 loss: 0.2358953084447421\n",
      "  batch 110 loss: 0.4842934672895353\n",
      "  batch 120 loss: 0.8964593447482911\n",
      "  batch 130 loss: 0.2851185964711476\n",
      "  batch 140 loss: 0.21544762465637177\n",
      "  batch 150 loss: 0.39833444319374395\n",
      "  batch 160 loss: 0.3653187613614136\n",
      "  batch 170 loss: 0.39207131700823084\n",
      "  batch 180 loss: 0.41711518649244683\n",
      "  batch 190 loss: 0.19356744454271393\n",
      "LOSS train 0.19356744454271393 valid 0.48576454570026656\n",
      "EPOCH 115:\n",
      "  batch 10 loss: 0.2086518215306569\n",
      "  batch 20 loss: 0.37745462196762675\n",
      "  batch 30 loss: 0.23675926856230944\n",
      "  batch 40 loss: 0.5175984796107513\n",
      "  batch 50 loss: 0.3050280066250707\n",
      "  batch 60 loss: 0.4351299705856945\n",
      "  batch 70 loss: 0.521724877867382\n",
      "  batch 80 loss: 0.3209623661416117\n",
      "  batch 90 loss: 0.3122097661238513\n",
      "  batch 100 loss: 0.36507154574792366\n",
      "  batch 110 loss: 0.13693934010370867\n",
      "  batch 120 loss: 0.395721985650016\n",
      "  batch 130 loss: 0.6578962078579934\n",
      "  batch 140 loss: 0.22236757371429122\n",
      "  batch 150 loss: 0.3724339436739683\n",
      "  batch 160 loss: 0.6435398222063669\n",
      "  batch 170 loss: 0.4112133070710115\n",
      "  batch 180 loss: 0.18305663651844953\n",
      "  batch 190 loss: 0.14636803163448348\n",
      "LOSS train 0.14636803163448348 valid 0.3963715757524299\n",
      "EPOCH 116:\n",
      "  batch 10 loss: 0.3452928369762958\n",
      "  batch 20 loss: 0.5449985056184232\n",
      "  batch 30 loss: 0.2034621960541699\n",
      "  batch 40 loss: 0.43979226356022993\n",
      "  batch 50 loss: 0.12808036904316394\n",
      "  batch 60 loss: 0.09951786642195656\n",
      "  batch 70 loss: 0.36028087480808607\n",
      "  batch 80 loss: 0.23064744195435197\n",
      "  batch 90 loss: 0.2222584717295831\n",
      "  batch 100 loss: 0.8296384036308154\n",
      "  batch 110 loss: 0.2810057476177462\n",
      "  batch 120 loss: 0.32816373734676746\n",
      "  batch 130 loss: 0.24601494834932963\n",
      "  batch 140 loss: 0.33751303897588514\n",
      "  batch 150 loss: 0.24735329720570007\n",
      "  batch 160 loss: 0.49780576822231526\n",
      "  batch 170 loss: 0.41953266265336425\n",
      "  batch 180 loss: 0.285291746398434\n",
      "  batch 190 loss: 0.29424892009337783\n",
      "LOSS train 0.29424892009337783 valid 0.8460872796112642\n",
      "EPOCH 117:\n",
      "  batch 10 loss: 0.406156450585695\n",
      "  batch 20 loss: 0.22838873202272225\n",
      "  batch 30 loss: 0.2285423061635811\n",
      "  batch 40 loss: 0.3138828459428623\n",
      "  batch 50 loss: 0.47515156349691096\n",
      "  batch 60 loss: 0.1633033614998567\n",
      "  batch 70 loss: 0.46999065787240396\n",
      "  batch 80 loss: 0.4701978659519227\n",
      "  batch 90 loss: 0.26533185334847076\n",
      "  batch 100 loss: 0.1659645489009563\n",
      "  batch 110 loss: 0.557500069346861\n",
      "  batch 120 loss: 0.4174891201677383\n",
      "  batch 130 loss: 0.5687747440853854\n",
      "  batch 140 loss: 0.360446692557889\n",
      "  batch 150 loss: 0.12455767708597705\n",
      "  batch 160 loss: 0.3170771537901601\n",
      "  batch 170 loss: 0.19589818374661264\n",
      "  batch 180 loss: 0.1611162324756151\n",
      "  batch 190 loss: 0.48058739871485157\n",
      "LOSS train 0.48058739871485157 valid 0.6559743725667477\n",
      "EPOCH 118:\n",
      "  batch 10 loss: 0.3048399401654024\n",
      "  batch 20 loss: 0.19290179923773393\n",
      "  batch 30 loss: 0.1773368321431917\n",
      "  batch 40 loss: 0.3463815853989217\n",
      "  batch 50 loss: 0.3541823234874755\n",
      "  batch 60 loss: 0.2305894740333315\n",
      "  batch 70 loss: 0.43595747149374803\n",
      "  batch 80 loss: 0.14129001575347502\n",
      "  batch 90 loss: 0.2181033702378045\n",
      "  batch 100 loss: 0.47333137823443394\n",
      "  batch 110 loss: 0.1614716556359781\n",
      "  batch 120 loss: 0.49474774713162334\n",
      "  batch 130 loss: 0.3172540992090944\n",
      "  batch 140 loss: 0.3346168971969746\n",
      "  batch 150 loss: 0.33462394743073676\n",
      "  batch 160 loss: 0.23292124405415962\n",
      "  batch 170 loss: 0.6518622939998749\n",
      "  batch 180 loss: 0.2247259903277154\n",
      "  batch 190 loss: 0.39663587127288336\n",
      "LOSS train 0.39663587127288336 valid 0.7078079786267316\n",
      "EPOCH 119:\n",
      "  batch 10 loss: 0.4494187834323384\n",
      "  batch 20 loss: 0.27745746659929865\n",
      "  batch 30 loss: 0.23212037162229535\n",
      "  batch 40 loss: 0.3656355770333903\n",
      "  batch 50 loss: 0.23798572868399787\n",
      "  batch 60 loss: 0.29674043504346626\n",
      "  batch 70 loss: 0.16563894255814376\n",
      "  batch 80 loss: 0.14467716659419239\n",
      "  batch 90 loss: 0.24299443746858743\n",
      "  batch 100 loss: 0.17858456283647683\n",
      "  batch 110 loss: 0.6720629736839328\n",
      "  batch 120 loss: 0.234917668212438\n",
      "  batch 130 loss: 0.3138349357192055\n",
      "  batch 140 loss: 0.3316827506023401\n",
      "  batch 150 loss: 0.36487357578007507\n",
      "  batch 160 loss: 0.24318328015942825\n",
      "  batch 170 loss: 0.4748683661826362\n",
      "  batch 180 loss: 0.3429964710026979\n",
      "  batch 190 loss: 0.1286744536992046\n",
      "LOSS train 0.1286744536992046 valid 0.38945088956853186\n",
      "EPOCH 120:\n",
      "  batch 10 loss: 0.32353303876370776\n",
      "  batch 20 loss: 0.3720102319610305\n",
      "  batch 30 loss: 0.2265880703460425\n",
      "  batch 40 loss: 0.2622533434088837\n",
      "  batch 50 loss: 0.2644762349467783\n",
      "  batch 60 loss: 0.5409350945381448\n",
      "  batch 70 loss: 0.49109219915699215\n",
      "  batch 80 loss: 0.14858807956225065\n",
      "  batch 90 loss: 0.43580840397626164\n",
      "  batch 100 loss: 0.3003055902918277\n",
      "  batch 110 loss: 0.4680067401888664\n",
      "  batch 120 loss: 0.2872268690087367\n",
      "  batch 130 loss: 0.3821123745030491\n",
      "  batch 140 loss: 0.2785037694440689\n",
      "  batch 150 loss: 0.25496679753414353\n",
      "  batch 160 loss: 0.34129775073961355\n",
      "  batch 170 loss: 0.11043199322302825\n",
      "  batch 180 loss: 0.29289451026561436\n",
      "  batch 190 loss: 0.37656973014964024\n",
      "LOSS train 0.37656973014964024 valid 0.28794268655119826\n",
      "EPOCH 121:\n",
      "  batch 10 loss: 0.2674365680257324\n",
      "  batch 20 loss: 0.09578582093017758\n",
      "  batch 30 loss: 0.4245113996934379\n",
      "  batch 40 loss: 0.12362173884903313\n",
      "  batch 50 loss: 0.333169821894262\n",
      "  batch 60 loss: 0.2827332398941508\n",
      "  batch 70 loss: 0.658237049212039\n",
      "  batch 80 loss: 0.40601465372601525\n",
      "  batch 90 loss: 0.40695164357020985\n",
      "  batch 100 loss: 0.19423831292806426\n",
      "  batch 110 loss: 0.16392337439901894\n",
      "  batch 120 loss: 0.19853771390917246\n",
      "  batch 130 loss: 0.5442570185077784\n",
      "  batch 140 loss: 0.22560589140921367\n",
      "  batch 150 loss: 0.23490095732850022\n",
      "  batch 160 loss: 0.26480141688880393\n",
      "  batch 170 loss: 0.5826092086208519\n",
      "  batch 180 loss: 0.3070447752113978\n",
      "  batch 190 loss: 0.1593393932300387\n",
      "LOSS train 0.1593393932300387 valid 0.34889594468926916\n",
      "EPOCH 122:\n",
      "  batch 10 loss: 0.18323917943343987\n",
      "  batch 20 loss: 0.11711290021194146\n",
      "  batch 30 loss: 0.4337944033672102\n",
      "  batch 40 loss: 0.29970827755896606\n",
      "  batch 50 loss: 0.21435356652073095\n",
      "  batch 60 loss: 0.21796439820318483\n",
      "  batch 70 loss: 0.3822610652539879\n",
      "  batch 80 loss: 0.3453083184736897\n",
      "  batch 90 loss: 0.2743419997685123\n",
      "  batch 100 loss: 0.48125130565604196\n",
      "  batch 110 loss: 0.23920599604971357\n",
      "  batch 120 loss: 0.24710727999336085\n",
      "  batch 130 loss: 0.2913588602139498\n",
      "  batch 140 loss: 0.22640147865458857\n",
      "  batch 150 loss: 0.1272458787250798\n",
      "  batch 160 loss: 0.4837389494066883\n",
      "  batch 170 loss: 0.16163844525435705\n",
      "  batch 180 loss: 0.5283359821478371\n",
      "  batch 190 loss: 0.4368441085709492\n",
      "LOSS train 0.4368441085709492 valid 0.31057961206002244\n",
      "EPOCH 123:\n",
      "  batch 10 loss: 0.21400326137663797\n",
      "  batch 20 loss: 0.18671660092513775\n",
      "  batch 30 loss: 0.6243022950715386\n",
      "  batch 40 loss: 0.1290225012116025\n",
      "  batch 50 loss: 0.7117179151111486\n",
      "  batch 60 loss: 0.4912813197268406\n",
      "  batch 70 loss: 0.7924055625808251\n",
      "  batch 80 loss: 0.20386506081122208\n",
      "  batch 90 loss: 0.2435188831179403\n",
      "  batch 100 loss: 0.3506803812757425\n",
      "  batch 110 loss: 0.3345047775146668\n",
      "  batch 120 loss: 0.5503401537018362\n",
      "  batch 130 loss: 0.20173992050113157\n",
      "  batch 140 loss: 0.24407821448985487\n",
      "  batch 150 loss: 0.18864842155599035\n",
      "  batch 160 loss: 0.23463181689148768\n",
      "  batch 170 loss: 0.16453182789991844\n",
      "  batch 180 loss: 0.23615537160949315\n",
      "  batch 190 loss: 0.19237228532147127\n",
      "LOSS train 0.19237228532147127 valid 0.4003960148254872\n",
      "EPOCH 124:\n",
      "  batch 10 loss: 0.4134814526518312\n",
      "  batch 20 loss: 0.5125940155354328\n",
      "  batch 30 loss: 0.3185905815917067\n",
      "  batch 40 loss: 0.3082041328339983\n",
      "  batch 50 loss: 0.4042540385737084\n",
      "  batch 60 loss: 0.2162303413831978\n",
      "  batch 70 loss: 0.32852350242319517\n",
      "  batch 80 loss: 0.2972761698794784\n",
      "  batch 90 loss: 0.2559234857559204\n",
      "  batch 100 loss: 0.31462656076619167\n",
      "  batch 110 loss: 0.20461072787293233\n",
      "  batch 120 loss: 0.2853099842905067\n",
      "  batch 130 loss: 0.13344622468866874\n",
      "  batch 140 loss: 0.15918730016273913\n",
      "  batch 150 loss: 0.16833887960819993\n",
      "  batch 160 loss: 0.14971384085147293\n",
      "  batch 170 loss: 0.6428803708433406\n",
      "  batch 180 loss: 0.44690045009192547\n",
      "  batch 190 loss: 0.28943354077637196\n",
      "LOSS train 0.28943354077637196 valid 0.45970496941654115\n",
      "EPOCH 125:\n",
      "  batch 10 loss: 0.17612439590302528\n",
      "  batch 20 loss: 0.46090192791307344\n",
      "  batch 30 loss: 0.3680678266478935\n",
      "  batch 40 loss: 0.25468012509081744\n",
      "  batch 50 loss: 0.29143889890401625\n",
      "  batch 60 loss: 0.14379269257042324\n",
      "  batch 70 loss: 0.7394685163279064\n",
      "  batch 80 loss: 0.47009311442961915\n",
      "  batch 90 loss: 0.2295918305491796\n",
      "  batch 100 loss: 0.3906843498174567\n",
      "  batch 110 loss: 0.39166928542545065\n",
      "  batch 120 loss: 0.5087584284396144\n",
      "  batch 130 loss: 0.20975366656202823\n",
      "  batch 140 loss: 0.27230929524957903\n",
      "  batch 150 loss: 0.4281434538046597\n",
      "  batch 160 loss: 0.293623888703587\n",
      "  batch 170 loss: 0.24709490532550263\n",
      "  batch 180 loss: 0.6964734014356508\n",
      "  batch 190 loss: 0.13465650751022623\n",
      "LOSS train 0.13465650751022623 valid 0.29850403784728724\n",
      "EPOCH 126:\n",
      "  batch 10 loss: 0.205104259197833\n",
      "  batch 20 loss: 0.4402425779640907\n",
      "  batch 30 loss: 0.41538844504975714\n",
      "  batch 40 loss: 0.19366314305370907\n",
      "  batch 50 loss: 0.2877078598816297\n",
      "  batch 60 loss: 0.18253681801070343\n",
      "  batch 70 loss: 0.4081475363345817\n",
      "  batch 80 loss: 0.28779875098407504\n",
      "  batch 90 loss: 0.27294417835146306\n",
      "  batch 100 loss: 0.2958939860014652\n",
      "  batch 110 loss: 0.2098390129787731\n",
      "  batch 120 loss: 0.39855635083004015\n",
      "  batch 130 loss: 0.13517765634751414\n",
      "  batch 140 loss: 0.31242406081582885\n",
      "  batch 150 loss: 0.22921910126606235\n",
      "  batch 160 loss: 0.20456518380378838\n",
      "  batch 170 loss: 0.6528194344457006\n",
      "  batch 180 loss: 0.33955132563132795\n",
      "  batch 190 loss: 0.23871660639997572\n",
      "LOSS train 0.23871660639997572 valid 0.338790351267119\n",
      "EPOCH 127:\n",
      "  batch 10 loss: 0.23465526554209645\n",
      "  batch 20 loss: 0.332948197589576\n",
      "  batch 30 loss: 0.21445280676489348\n",
      "  batch 40 loss: 0.27086215736926533\n",
      "  batch 50 loss: 0.2632648730446817\n",
      "  batch 60 loss: 0.20268950999889057\n",
      "  batch 70 loss: 0.33990864005609184\n",
      "  batch 80 loss: 0.13504271706187865\n",
      "  batch 90 loss: 0.5095603794910858\n",
      "  batch 100 loss: 0.34174616831442106\n",
      "  batch 110 loss: 0.43057814356434393\n",
      "  batch 120 loss: 0.23899342018630704\n",
      "  batch 130 loss: 0.3773949690978043\n",
      "  batch 140 loss: 0.3173576524015516\n",
      "  batch 150 loss: 0.3195164660573937\n",
      "  batch 160 loss: 0.5295760170185531\n",
      "  batch 170 loss: 0.532494413072709\n",
      "  batch 180 loss: 0.16257677638059248\n",
      "  batch 190 loss: 0.22414355599175906\n",
      "LOSS train 0.22414355599175906 valid 0.4476895864136168\n",
      "EPOCH 128:\n",
      "  batch 10 loss: 0.21908022882707884\n",
      "  batch 20 loss: 0.2132139271212509\n",
      "  batch 30 loss: 0.2467999765271088\n",
      "  batch 40 loss: 0.14699220649927155\n",
      "  batch 50 loss: 0.1813854926789645\n",
      "  batch 60 loss: 0.28695511820260433\n",
      "  batch 70 loss: 0.8038209462538362\n",
      "  batch 80 loss: 0.5661125393453403\n",
      "  batch 90 loss: 0.29119803416178913\n",
      "  batch 100 loss: 0.2319695416081231\n",
      "  batch 110 loss: 0.2161279439671489\n",
      "  batch 120 loss: 0.12079672611289424\n",
      "  batch 130 loss: 0.2672698373498861\n",
      "  batch 140 loss: 0.14620965233043534\n",
      "  batch 150 loss: 0.30283234345824894\n",
      "  batch 160 loss: 0.1881954147524084\n",
      "  batch 170 loss: 0.3479753413696017\n",
      "  batch 180 loss: 0.6765328455920099\n",
      "  batch 190 loss: 0.21212981379503618\n",
      "LOSS train 0.21212981379503618 valid 0.7995119534031964\n",
      "EPOCH 129:\n",
      "  batch 10 loss: 0.3130242646802799\n",
      "  batch 20 loss: 0.08316243169974769\n",
      "  batch 30 loss: 0.12200787301771925\n",
      "  batch 40 loss: 0.17705225276695274\n",
      "  batch 50 loss: 0.26428674595226764\n",
      "  batch 60 loss: 0.16691944686463103\n",
      "  batch 70 loss: 0.31216141438781053\n",
      "  batch 80 loss: 0.5079517812315316\n",
      "  batch 90 loss: 0.30822005495574556\n",
      "  batch 100 loss: 0.47777807328529887\n",
      "  batch 110 loss: 0.2737769537314307\n",
      "  batch 120 loss: 1.487841725780163\n",
      "  batch 130 loss: 0.4605632022197824\n",
      "  batch 140 loss: 0.5072882992608356\n",
      "  batch 150 loss: 0.19522788632602897\n",
      "  batch 160 loss: 0.37296266062621725\n",
      "  batch 170 loss: 0.320802531868685\n",
      "  batch 180 loss: 0.2784018344209471\n",
      "  batch 190 loss: 0.23757622841367265\n",
      "LOSS train 0.23757622841367265 valid 0.37150436226959127\n",
      "EPOCH 130:\n",
      "  batch 10 loss: 0.2779728128880379\n",
      "  batch 20 loss: 0.3822125960607082\n",
      "  batch 30 loss: 0.38670623140133104\n",
      "  batch 40 loss: 0.12042523476775387\n",
      "  batch 50 loss: 1.2907088220352307\n",
      "  batch 60 loss: 0.28287853363581233\n",
      "  batch 70 loss: 0.21752541159803512\n",
      "  batch 80 loss: 0.26186059737810863\n",
      "  batch 90 loss: 0.21075841811980353\n",
      "  batch 100 loss: 0.27308316124253906\n",
      "  batch 110 loss: 0.30052915135456715\n",
      "  batch 120 loss: 0.23888347074389457\n",
      "  batch 130 loss: 0.26702834021998567\n",
      "  batch 140 loss: 0.42755828726803885\n",
      "  batch 150 loss: 0.2804100096836919\n",
      "  batch 160 loss: 0.39832508735344163\n",
      "  batch 170 loss: 0.24857476746401516\n",
      "  batch 180 loss: 0.10914763400360243\n",
      "  batch 190 loss: 0.27357535603514405\n",
      "LOSS train 0.27357535603514405 valid 0.4604888943428635\n",
      "EPOCH 131:\n",
      "  batch 10 loss: 0.12483358255849453\n",
      "  batch 20 loss: 0.16722033637342976\n",
      "  batch 30 loss: 0.3085849121416686\n",
      "  batch 40 loss: 0.11448160915351764\n",
      "  batch 50 loss: 0.34221372344036355\n",
      "  batch 60 loss: 0.22275724323335452\n",
      "  batch 70 loss: 0.34599719800316964\n",
      "  batch 80 loss: 0.18963113813952076\n",
      "  batch 90 loss: 0.3367648665443994\n",
      "  batch 100 loss: 0.587902480750563\n",
      "  batch 110 loss: 0.11106278362913144\n",
      "  batch 120 loss: 0.4129065637402164\n",
      "  batch 130 loss: 0.3401279719841114\n",
      "  batch 140 loss: 0.35873001139843835\n",
      "  batch 150 loss: 0.2640622422346496\n",
      "  batch 160 loss: 0.2790600695429021\n",
      "  batch 170 loss: 0.26078249596976094\n",
      "  batch 180 loss: 0.24921114052995108\n",
      "  batch 190 loss: 0.27733347122120905\n",
      "LOSS train 0.27733347122120905 valid 0.37037951242223827\n",
      "EPOCH 132:\n",
      "  batch 10 loss: 0.15413791015907918\n",
      "  batch 20 loss: 0.10644801289745373\n",
      "  batch 30 loss: 0.5785560697302572\n",
      "  batch 40 loss: 0.18765718672875664\n",
      "  batch 50 loss: 0.22891709355753848\n",
      "  batch 60 loss: 0.3914032082509948\n",
      "  batch 70 loss: 0.1675068623299012\n",
      "  batch 80 loss: 0.2920678510425205\n",
      "  batch 90 loss: 0.10940780977689428\n",
      "  batch 100 loss: 0.17176657264353706\n",
      "  batch 110 loss: 0.2465817496238742\n",
      "  batch 120 loss: 0.2956582747370703\n",
      "  batch 130 loss: 0.1379905565852823\n",
      "  batch 140 loss: 0.5429994798822009\n",
      "  batch 150 loss: 0.46183360076393\n",
      "  batch 160 loss: 0.24456608835462249\n",
      "  batch 170 loss: 0.4451988231954601\n",
      "  batch 180 loss: 0.09670484892994864\n",
      "  batch 190 loss: 0.1196658502871287\n",
      "LOSS train 0.1196658502871287 valid 0.3373716453556065\n",
      "EPOCH 133:\n",
      "  batch 10 loss: 0.4079711900136317\n",
      "  batch 20 loss: 0.23199188796570525\n",
      "  batch 30 loss: 0.23150310509372501\n",
      "  batch 40 loss: 0.19233617361460348\n",
      "  batch 50 loss: 0.39379238814508427\n",
      "  batch 60 loss: 0.2498946806485037\n",
      "  batch 70 loss: 0.6014165456057526\n",
      "  batch 80 loss: 0.38731644348663397\n",
      "  batch 90 loss: 0.272360461383505\n",
      "  batch 100 loss: 0.23564473806764\n",
      "  batch 110 loss: 0.11458250935247634\n",
      "  batch 120 loss: 0.42190193084825295\n",
      "  batch 130 loss: 0.1761742683855118\n",
      "  batch 140 loss: 0.16302412407967495\n",
      "  batch 150 loss: 0.5185745329246856\n",
      "  batch 160 loss: 0.23584468356566504\n",
      "  batch 170 loss: 0.2776623543468304\n",
      "  batch 180 loss: 0.21716382664671982\n",
      "  batch 190 loss: 0.22533353817343596\n",
      "LOSS train 0.22533353817343596 valid 0.3810229541226223\n",
      "EPOCH 134:\n",
      "  batch 10 loss: 0.4566481204528827\n",
      "  batch 20 loss: 0.25719941466923046\n",
      "  batch 30 loss: 0.2823757966834819\n",
      "  batch 40 loss: 0.324366999551421\n",
      "  batch 50 loss: 0.24636313184309983\n",
      "  batch 60 loss: 0.33507082531832566\n",
      "  batch 70 loss: 0.17154423503016006\n",
      "  batch 80 loss: 0.2721099350836084\n",
      "  batch 90 loss: 0.1872667464951519\n",
      "  batch 100 loss: 0.20161701339675347\n",
      "  batch 110 loss: 0.2984502925290144\n",
      "  batch 120 loss: 0.15796266018005553\n",
      "  batch 130 loss: 0.41017344723804855\n",
      "  batch 140 loss: 0.7793445460487419\n",
      "  batch 150 loss: 0.5602596930286381\n",
      "  batch 160 loss: 0.4181113982689567\n",
      "  batch 170 loss: 0.19884325125458419\n",
      "  batch 180 loss: 0.2577399287263688\n",
      "  batch 190 loss: 0.16391935557767284\n",
      "LOSS train 0.16391935557767284 valid 0.29313798144842923\n",
      "EPOCH 135:\n",
      "  batch 10 loss: 0.18753492960095172\n",
      "  batch 20 loss: 0.11874150674848352\n",
      "  batch 30 loss: 0.2683967748642317\n",
      "  batch 40 loss: 0.48899427067954093\n",
      "  batch 50 loss: 0.2154616977852129\n",
      "  batch 60 loss: 0.4170593435890623\n",
      "  batch 70 loss: 0.2858080502715893\n",
      "  batch 80 loss: 0.4150090758004808\n",
      "  batch 90 loss: 0.21160223334736655\n",
      "  batch 100 loss: 0.24250182089599548\n",
      "  batch 110 loss: 0.24049501028493978\n",
      "  batch 120 loss: 0.4225518625778932\n",
      "  batch 130 loss: 0.24712326375156407\n",
      "  batch 140 loss: 0.2934486542508239\n",
      "  batch 150 loss: 0.15348333767906297\n",
      "  batch 160 loss: 0.08131883192108944\n",
      "  batch 170 loss: 0.25353045063675383\n",
      "  batch 180 loss: 0.37811822627845687\n",
      "  batch 190 loss: 0.42759686564822913\n",
      "LOSS train 0.42759686564822913 valid 0.3789559025193366\n",
      "EPOCH 136:\n",
      "  batch 10 loss: 0.35009893133246806\n",
      "  batch 20 loss: 0.16236076180939563\n",
      "  batch 30 loss: 0.2528413624357199\n",
      "  batch 40 loss: 0.4122986033791676\n",
      "  batch 50 loss: 0.6048842549735127\n",
      "  batch 60 loss: 0.13078811518862493\n",
      "  batch 70 loss: 0.08419143169521703\n",
      "  batch 80 loss: 0.2804458279722894\n",
      "  batch 90 loss: 0.14078771413114738\n",
      "  batch 100 loss: 0.25096658345282774\n",
      "  batch 110 loss: 0.3175300925271586\n",
      "  batch 120 loss: 0.2480875236738939\n",
      "  batch 130 loss: 0.2781476620439207\n",
      "  batch 140 loss: 0.2405018986115465\n",
      "  batch 150 loss: 0.28662892714201005\n",
      "  batch 160 loss: 0.3206700384107535\n",
      "  batch 170 loss: 0.16048118777980563\n",
      "  batch 180 loss: 0.2390827377450478\n",
      "  batch 190 loss: 0.12894308138420457\n",
      "LOSS train 0.12894308138420457 valid 0.450388594061037\n",
      "EPOCH 137:\n",
      "  batch 10 loss: 0.18665049802075373\n",
      "  batch 20 loss: 0.11171886151205399\n",
      "  batch 30 loss: 0.08354414617715519\n",
      "  batch 40 loss: 0.18457516183516418\n",
      "  batch 50 loss: 0.1268982367233548\n",
      "  batch 60 loss: 0.3648337533173617\n",
      "  batch 70 loss: 0.1058034801044414\n",
      "  batch 80 loss: 0.15376835437491537\n",
      "  batch 90 loss: 0.18524014364520552\n",
      "  batch 100 loss: 0.23261646241662676\n",
      "  batch 110 loss: 0.29113702328195357\n",
      "  batch 120 loss: 0.47616331602876016\n",
      "  batch 130 loss: 0.3899191869877541\n",
      "  batch 140 loss: 0.28693330159585456\n",
      "  batch 150 loss: 0.6733878428647586\n",
      "  batch 160 loss: 0.20148404415231197\n",
      "  batch 170 loss: 0.2142044320004061\n",
      "  batch 180 loss: 0.3794655091667664\n",
      "  batch 190 loss: 0.1422938624964445\n",
      "LOSS train 0.1422938624964445 valid 2.36596823849369\n",
      "EPOCH 138:\n",
      "  batch 10 loss: 1.1704555474418157\n",
      "  batch 20 loss: 0.281608881214197\n",
      "  batch 30 loss: 0.13979910030611792\n",
      "  batch 40 loss: 0.23406216256553308\n",
      "  batch 50 loss: 0.1669448555854615\n",
      "  batch 60 loss: 0.14715287781436928\n",
      "  batch 70 loss: 0.30893405815331787\n",
      "  batch 80 loss: 0.33744007361237893\n",
      "  batch 90 loss: 0.44171332191981494\n",
      "  batch 100 loss: 0.30305264038397584\n",
      "  batch 110 loss: 0.2459323707356816\n",
      "  batch 120 loss: 0.10025026200019056\n",
      "  batch 130 loss: 0.3738131959486054\n",
      "  batch 140 loss: 0.34200029027924755\n",
      "  batch 150 loss: 0.3287664266332285\n",
      "  batch 160 loss: 0.2755039588082582\n",
      "  batch 170 loss: 0.22050191582529804\n",
      "  batch 180 loss: 0.536198114335275\n",
      "  batch 190 loss: 0.40520904894638077\n",
      "LOSS train 0.40520904894638077 valid 0.26889644146257735\n",
      "EPOCH 139:\n",
      "  batch 10 loss: 0.21004636348880013\n",
      "  batch 20 loss: 0.4711365841263614\n",
      "  batch 30 loss: 0.23519644920743304\n",
      "  batch 40 loss: 0.24923323079237888\n",
      "  batch 50 loss: 0.23383033377394896\n",
      "  batch 60 loss: 0.31642946373322045\n",
      "  batch 70 loss: 0.18045924690377432\n",
      "  batch 80 loss: 0.4390948315005517\n",
      "  batch 90 loss: 0.16087063086743\n",
      "  batch 100 loss: 0.3147255548858084\n",
      "  batch 110 loss: 0.11742096839152509\n",
      "  batch 120 loss: 0.2676467144199705\n",
      "  batch 130 loss: 0.18168768697796622\n",
      "  batch 140 loss: 0.3161193151041516\n",
      "  batch 150 loss: 0.145779599895468\n",
      "  batch 160 loss: 0.18037721915025032\n",
      "  batch 170 loss: 0.22225942004442914\n",
      "  batch 180 loss: 0.4038225320560741\n",
      "  batch 190 loss: 0.3811882365858764\n",
      "LOSS train 0.3811882365858764 valid 0.3426793989056023\n",
      "EPOCH 140:\n",
      "  batch 10 loss: 0.2979131226282334\n",
      "  batch 20 loss: 0.2613015196795459\n",
      "  batch 30 loss: 0.20229692313441774\n",
      "  batch 40 loss: 0.3407736858738645\n",
      "  batch 50 loss: 0.3816895877549541\n",
      "  batch 60 loss: 0.1955775356214872\n",
      "  batch 70 loss: 0.10218390858353814\n",
      "  batch 80 loss: 0.30349928014038596\n",
      "  batch 90 loss: 0.2670698536778218\n",
      "  batch 100 loss: 0.1858419651631266\n",
      "  batch 110 loss: 0.27728312808321787\n",
      "  batch 120 loss: 0.3703152915695682\n",
      "  batch 130 loss: 0.40512350983772194\n",
      "  batch 140 loss: 0.28833832964955947\n",
      "  batch 150 loss: 0.171906546043283\n",
      "  batch 160 loss: 0.14980818898457074\n",
      "  batch 170 loss: 0.5327320303855231\n",
      "  batch 180 loss: 0.09849019762405078\n",
      "  batch 190 loss: 0.05021134662893019\n",
      "LOSS train 0.05021134662893019 valid 0.30045968527422073\n",
      "EPOCH 141:\n",
      "  batch 10 loss: 0.5816333129732811\n",
      "  batch 20 loss: 0.6447070846195857\n",
      "  batch 30 loss: 0.10410985263479233\n",
      "  batch 40 loss: 0.11124773189658299\n",
      "  batch 50 loss: 0.07443342505212058\n",
      "  batch 60 loss: 0.1232654679013649\n",
      "  batch 70 loss: 0.18530608505097917\n",
      "  batch 80 loss: 0.15061250916405697\n",
      "  batch 90 loss: 0.4890831550801522\n",
      "  batch 100 loss: 0.5788368650626581\n",
      "  batch 110 loss: 0.16583802349923643\n",
      "  batch 120 loss: 0.18781278483511415\n",
      "  batch 130 loss: 0.15251467551497627\n",
      "  batch 140 loss: 0.1205255304601451\n",
      "  batch 150 loss: 0.29337474162093713\n",
      "  batch 160 loss: 0.27350558629768784\n",
      "  batch 170 loss: 0.7475197623803979\n",
      "  batch 180 loss: 0.3729478560941061\n",
      "  batch 190 loss: 0.17182209849124774\n",
      "LOSS train 0.17182209849124774 valid 4.646576297745909\n",
      "EPOCH 142:\n",
      "  batch 10 loss: 0.885909934438223\n",
      "  batch 20 loss: 0.38516488573732205\n",
      "  batch 30 loss: 0.2691882441347843\n",
      "  batch 40 loss: 0.3192801987068378\n",
      "  batch 50 loss: 0.28596702636568805\n",
      "  batch 60 loss: 0.27470703510625755\n",
      "  batch 70 loss: 0.2598069822495745\n",
      "  batch 80 loss: 0.06664699967950583\n",
      "  batch 90 loss: 0.1823555271148507\n",
      "  batch 100 loss: 0.11713493723800639\n",
      "  batch 110 loss: 0.2220913282993024\n",
      "  batch 120 loss: 0.24879157597897575\n",
      "  batch 130 loss: 0.4114357369158824\n",
      "  batch 140 loss: 0.10006186486716614\n",
      "  batch 150 loss: 0.36502431244734906\n",
      "  batch 160 loss: 0.12411803492868785\n",
      "  batch 170 loss: 0.30665757339465927\n",
      "  batch 180 loss: 0.10688856964843581\n",
      "  batch 190 loss: 0.1534570804258692\n",
      "LOSS train 0.1534570804258692 valid 0.5230413314920253\n",
      "EPOCH 143:\n",
      "  batch 10 loss: 0.2864909695084862\n",
      "  batch 20 loss: 0.38350600754711195\n",
      "  batch 30 loss: 0.27992143836745526\n",
      "  batch 40 loss: 0.09636453110724688\n",
      "  batch 50 loss: 0.4407953000831185\n",
      "  batch 60 loss: 0.6135075449928991\n",
      "  batch 70 loss: 0.2830095516736037\n",
      "  batch 80 loss: 0.26515351091693445\n",
      "  batch 90 loss: 0.22302480301004834\n",
      "  batch 100 loss: 0.21242857351826389\n",
      "  batch 110 loss: 0.28871568948670756\n",
      "  batch 120 loss: 0.2101930650882423\n",
      "  batch 130 loss: 0.11991340252234295\n",
      "  batch 140 loss: 0.26435847035900223\n",
      "  batch 150 loss: 0.16180891138537845\n",
      "  batch 160 loss: 0.24095679349047713\n",
      "  batch 170 loss: 0.21450982635287802\n",
      "  batch 180 loss: 0.32436649351147934\n",
      "  batch 190 loss: 0.2812807795118715\n",
      "LOSS train 0.2812807795118715 valid 0.32565833856512116\n",
      "EPOCH 144:\n",
      "  batch 10 loss: 0.28456794455923956\n",
      "  batch 20 loss: 0.43684774529319836\n",
      "  batch 30 loss: 0.5093565478046003\n",
      "  batch 40 loss: 0.3435083472766564\n",
      "  batch 50 loss: 0.3255674671661836\n",
      "  batch 60 loss: 0.2588913846557716\n",
      "  batch 70 loss: 0.1607406954630278\n",
      "  batch 80 loss: 0.09639092872967012\n",
      "  batch 90 loss: 0.14180657271499514\n",
      "  batch 100 loss: 0.3540142114696209\n",
      "  batch 110 loss: 0.2472022410482168\n",
      "  batch 120 loss: 0.4381017192903528\n",
      "  batch 130 loss: 0.42530934246315155\n",
      "  batch 140 loss: 0.19116186445298808\n",
      "  batch 150 loss: 0.19178595951016178\n",
      "  batch 160 loss: 0.0844871541041357\n",
      "  batch 170 loss: 0.23894069575617322\n",
      "  batch 180 loss: 0.4350996477587614\n",
      "  batch 190 loss: 0.505729762478586\n",
      "LOSS train 0.505729762478586 valid 0.7107873934327915\n",
      "EPOCH 145:\n",
      "  batch 10 loss: 0.5765390128086437\n",
      "  batch 20 loss: 0.21973438801819611\n",
      "  batch 30 loss: 0.33820914165626165\n",
      "  batch 40 loss: 0.4717734543744882\n",
      "  batch 50 loss: 0.38179007453341\n",
      "  batch 60 loss: 0.10611601143027656\n",
      "  batch 70 loss: 0.12784050636764732\n",
      "  batch 80 loss: 0.20391685493741535\n",
      "  batch 90 loss: 0.2222714240415371\n",
      "  batch 100 loss: 0.1561834054584324\n",
      "  batch 110 loss: 0.11008554464142435\n",
      "  batch 120 loss: 0.46546627890402303\n",
      "  batch 130 loss: 0.3747884523119865\n",
      "  batch 140 loss: 0.2510255912951834\n",
      "  batch 150 loss: 0.3511156473083247\n",
      "  batch 160 loss: 0.07623277897801017\n",
      "  batch 170 loss: 0.30176828168405334\n",
      "  batch 180 loss: 0.12506257152344916\n",
      "  batch 190 loss: 0.14703281612601132\n",
      "LOSS train 0.14703281612601132 valid 0.39604903179665246\n",
      "EPOCH 146:\n",
      "  batch 10 loss: 0.19002702256402698\n",
      "  batch 20 loss: 0.38577760040425346\n",
      "  batch 30 loss: 0.33973243373329753\n",
      "  batch 40 loss: 0.24348464822578536\n",
      "  batch 50 loss: 0.35186260777263667\n",
      "  batch 60 loss: 0.2104632130060054\n",
      "  batch 70 loss: 0.14600132317864337\n",
      "  batch 80 loss: 0.06383426367610809\n",
      "  batch 90 loss: 0.4859275886548858\n",
      "  batch 100 loss: 0.2546615425257187\n",
      "  batch 110 loss: 0.4407355514733354\n",
      "  batch 120 loss: 0.1260406500310637\n",
      "  batch 130 loss: 0.23152818122471217\n",
      "  batch 140 loss: 0.08307659005440655\n",
      "  batch 150 loss: 0.15639097065795796\n",
      "  batch 160 loss: 0.6977090189080627\n",
      "  batch 170 loss: 0.11449519293164485\n",
      "  batch 180 loss: 0.14378850911962218\n",
      "  batch 190 loss: 0.09847662334177584\n",
      "LOSS train 0.09847662334177584 valid 0.3168747798056127\n",
      "EPOCH 147:\n",
      "  batch 10 loss: 0.206998565896356\n",
      "  batch 20 loss: 0.21704275829470135\n",
      "  batch 30 loss: 0.11624493135605007\n",
      "  batch 40 loss: 0.2096510941439192\n",
      "  batch 50 loss: 0.30973745519149815\n",
      "  batch 60 loss: 0.3424656090461212\n",
      "  batch 70 loss: 0.22633342192639247\n",
      "  batch 80 loss: 0.12643198368896263\n",
      "  batch 90 loss: 0.3418071364802017\n",
      "  batch 100 loss: 0.29270537231204796\n",
      "  batch 110 loss: 0.11680247259064344\n",
      "  batch 120 loss: 0.1319897995017527\n",
      "  batch 130 loss: 0.19967807932480355\n",
      "  batch 140 loss: 0.21924477969441797\n",
      "  batch 150 loss: 0.12648103227620594\n",
      "  batch 160 loss: 0.41514587364945327\n",
      "  batch 170 loss: 0.5466761050563946\n",
      "  batch 180 loss: 0.2951318882393025\n",
      "  batch 190 loss: 0.07774459235915857\n",
      "LOSS train 0.07774459235915857 valid 0.3202948394001186\n",
      "EPOCH 148:\n",
      "  batch 10 loss: 0.23429795583488158\n",
      "  batch 20 loss: 0.16901000559009843\n",
      "  batch 30 loss: 0.34893005079211437\n",
      "  batch 40 loss: 0.1607977910869522\n",
      "  batch 50 loss: 0.1998979788433644\n",
      "  batch 60 loss: 0.3045137896308006\n",
      "  batch 70 loss: 0.2614272166392766\n",
      "  batch 80 loss: 0.10523356829799013\n",
      "  batch 90 loss: 0.138390273794721\n",
      "  batch 100 loss: 0.33295498576408133\n",
      "  batch 110 loss: 0.24936543086296298\n",
      "  batch 120 loss: 0.2720899765125068\n",
      "  batch 130 loss: 0.2126987492076296\n",
      "  batch 140 loss: 0.11462782092421549\n",
      "  batch 150 loss: 0.17071288011902652\n",
      "  batch 160 loss: 0.24174333339560689\n",
      "  batch 170 loss: 0.1265406529097163\n",
      "  batch 180 loss: 0.36489869134456965\n",
      "  batch 190 loss: 0.23833725434233202\n",
      "LOSS train 0.23833725434233202 valid 0.2671682643240507\n",
      "EPOCH 149:\n",
      "  batch 10 loss: 0.21240784392575734\n",
      "  batch 20 loss: 0.34817739484424237\n",
      "  batch 30 loss: 0.09498728988182847\n",
      "  batch 40 loss: 0.2826904769571229\n",
      "  batch 50 loss: 0.14066920599216245\n",
      "  batch 60 loss: 0.2728487103446241\n",
      "  batch 70 loss: 0.22718479536197264\n",
      "  batch 80 loss: 0.26102768229247886\n",
      "  batch 90 loss: 0.2627447575396218\n",
      "  batch 100 loss: 0.2305884299756144\n",
      "  batch 110 loss: 0.2742219243067666\n",
      "  batch 120 loss: 1.4192760053900657\n",
      "  batch 130 loss: 0.24388570079390776\n",
      "  batch 140 loss: 0.29336176764627453\n",
      "  batch 150 loss: 0.30437235773570137\n",
      "  batch 160 loss: 0.3484231993192225\n",
      "  batch 170 loss: 0.14747504319457222\n",
      "  batch 180 loss: 0.2568189564277418\n",
      "  batch 190 loss: 0.2754752846842166\n",
      "LOSS train 0.2754752846842166 valid 0.30750258440168227\n",
      "EPOCH 150:\n",
      "  batch 10 loss: 0.07185684181604302\n",
      "  batch 20 loss: 0.29982450715797315\n",
      "  batch 30 loss: 0.07207910953366081\n",
      "  batch 40 loss: 0.1563563341194822\n",
      "  batch 50 loss: 0.22391408534567744\n",
      "  batch 60 loss: 0.17817699839069973\n",
      "  batch 70 loss: 0.15911471326544416\n",
      "  batch 80 loss: 0.2662943361840007\n",
      "  batch 90 loss: 0.6762412011150445\n",
      "  batch 100 loss: 0.2418165295763174\n",
      "  batch 110 loss: 0.21983856891165487\n",
      "  batch 120 loss: 0.14011164947078214\n",
      "  batch 130 loss: 0.43545938454553834\n",
      "  batch 140 loss: 0.2656861115014181\n",
      "  batch 150 loss: 0.41141767288936537\n",
      "  batch 160 loss: 0.19665992438822286\n",
      "  batch 170 loss: 0.23053215381250994\n",
      "  batch 180 loss: 0.30374154117525903\n",
      "  batch 190 loss: 0.2404375198941125\n",
      "LOSS train 0.2404375198941125 valid 0.825941373440937\n",
      "EPOCH 151:\n",
      "  batch 10 loss: 0.3291696012413013\n",
      "  batch 20 loss: 0.10572028092428809\n",
      "  batch 30 loss: 0.18659063000523018\n",
      "  batch 40 loss: 0.16247266945720185\n",
      "  batch 50 loss: 0.201518277139985\n",
      "  batch 60 loss: 0.15452032885987138\n",
      "  batch 70 loss: 0.1827817031298764\n",
      "  batch 80 loss: 0.12684519082213228\n",
      "  batch 90 loss: 0.3792227527937939\n",
      "  batch 100 loss: 0.5002180445444537\n",
      "  batch 110 loss: 0.4450398277716886\n",
      "  batch 120 loss: 0.9211326107375498\n",
      "  batch 130 loss: 0.0943804118924163\n",
      "  batch 140 loss: 0.1638700031897315\n",
      "  batch 150 loss: 0.11416716515523148\n",
      "  batch 160 loss: 0.08562055785005214\n",
      "  batch 170 loss: 0.21107503282346443\n",
      "  batch 180 loss: 0.19938889037493937\n",
      "  batch 190 loss: 0.44018113087440724\n",
      "LOSS train 0.44018113087440724 valid 0.3175198661913386\n",
      "EPOCH 152:\n",
      "  batch 10 loss: 0.19282968635379802\n",
      "  batch 20 loss: 0.08307664172752993\n",
      "  batch 30 loss: 0.18671070938480624\n",
      "  batch 40 loss: 0.22947869122072007\n",
      "  batch 50 loss: 0.08997610601691122\n",
      "  batch 60 loss: 0.23698974135713796\n",
      "  batch 70 loss: 0.2273873027668742\n",
      "  batch 80 loss: 0.171227004593311\n",
      "  batch 90 loss: 0.3612017392035341\n",
      "  batch 100 loss: 0.2818361411467777\n",
      "  batch 110 loss: 0.3187206947906816\n",
      "  batch 120 loss: 0.05681711384477239\n",
      "  batch 130 loss: 0.13373715683701448\n",
      "  batch 140 loss: 0.24088314750515566\n",
      "  batch 150 loss: 0.43769665494182847\n",
      "  batch 160 loss: 0.3438146735843475\n",
      "  batch 170 loss: 0.22376498779922258\n",
      "  batch 180 loss: 0.14688584943432942\n",
      "  batch 190 loss: 0.25674383784462407\n",
      "LOSS train 0.25674383784462407 valid 0.38457485156097904\n",
      "EPOCH 153:\n",
      "  batch 10 loss: 0.17402390655770433\n",
      "  batch 20 loss: 0.09291214100958314\n",
      "  batch 30 loss: 0.13425636147876502\n",
      "  batch 40 loss: 0.4335701780963063\n",
      "  batch 50 loss: 0.1647911965948879\n",
      "  batch 60 loss: 0.24586280193434504\n",
      "  batch 70 loss: 0.20157411376876552\n",
      "  batch 80 loss: 0.37868588835772243\n",
      "  batch 90 loss: 0.36729318408943074\n",
      "  batch 100 loss: 0.0864840765898407\n",
      "  batch 110 loss: 0.1209783981921646\n",
      "  batch 120 loss: 0.10481204497191357\n",
      "  batch 130 loss: 0.16814939763280562\n",
      "  batch 140 loss: 0.35919378203216185\n",
      "  batch 150 loss: 0.14580589382676407\n",
      "  batch 160 loss: 0.28875080776560935\n",
      "  batch 170 loss: 0.32111972981820147\n",
      "  batch 180 loss: 0.26093247784883716\n",
      "  batch 190 loss: 0.2724591432521265\n",
      "LOSS train 0.2724591432521265 valid 0.2432437194953915\n",
      "EPOCH 154:\n",
      "  batch 10 loss: 0.11191585990600288\n",
      "  batch 20 loss: 0.12674592603761994\n",
      "  batch 30 loss: 0.10840014706773218\n",
      "  batch 40 loss: 0.17120491876794403\n",
      "  batch 50 loss: 0.1793550510341447\n",
      "  batch 60 loss: 0.14461279775268848\n",
      "  batch 70 loss: 0.31969581188786833\n",
      "  batch 80 loss: 0.41227515861792197\n",
      "  batch 90 loss: 0.07806055149485473\n",
      "  batch 100 loss: 0.13755034100177\n",
      "  batch 110 loss: 0.32972404100473796\n",
      "  batch 120 loss: 0.4802343387807923\n",
      "  batch 130 loss: 0.11841285912159946\n",
      "  batch 140 loss: 0.3665818126635713\n",
      "  batch 150 loss: 0.19303868851329753\n",
      "  batch 160 loss: 0.2539276852927287\n",
      "  batch 170 loss: 0.16614426592823292\n",
      "  batch 180 loss: 0.25280921592384403\n",
      "  batch 190 loss: 0.11083418749985867\n",
      "LOSS train 0.11083418749985867 valid 0.47383962276571534\n",
      "EPOCH 155:\n",
      "  batch 10 loss: 0.1775969926489779\n",
      "  batch 20 loss: 0.22638776831026916\n",
      "  batch 30 loss: 0.21008394173259148\n",
      "  batch 40 loss: 0.10687041455857979\n",
      "  batch 50 loss: 0.1427710211948579\n",
      "  batch 60 loss: 0.2794999939724221\n",
      "  batch 70 loss: 0.08006181230794027\n",
      "  batch 80 loss: 0.37406702512234913\n",
      "  batch 90 loss: 0.19216014506600915\n",
      "  batch 100 loss: 0.16697465360957722\n",
      "  batch 110 loss: 0.1982239899254637\n",
      "  batch 120 loss: 0.22240897334595502\n",
      "  batch 130 loss: 0.07363742075795017\n",
      "  batch 140 loss: 0.26747773602692176\n",
      "  batch 150 loss: 0.27892770140606443\n",
      "  batch 160 loss: 0.19841337200668932\n",
      "  batch 170 loss: 0.2921117889583911\n",
      "  batch 180 loss: 0.22572002456563495\n",
      "  batch 190 loss: 0.3013432444233331\n",
      "LOSS train 0.3013432444233331 valid 0.41579783622530203\n",
      "EPOCH 156:\n",
      "  batch 10 loss: 0.2253626102423368\n",
      "  batch 20 loss: 0.13450124084265552\n",
      "  batch 30 loss: 0.2271387703396613\n",
      "  batch 40 loss: 0.22831815294975968\n",
      "  batch 50 loss: 0.5152410868817242\n",
      "  batch 60 loss: 0.3033930543257156\n",
      "  batch 70 loss: 0.2011026930427761\n",
      "  batch 80 loss: 0.057641525143480976\n",
      "  batch 90 loss: 0.6138993475440657\n",
      "  batch 100 loss: 0.25813549569647876\n",
      "  batch 110 loss: 0.2308575613402354\n",
      "  batch 120 loss: 0.24835708667560538\n",
      "  batch 130 loss: 0.20320744065647886\n",
      "  batch 140 loss: 0.061710380414660905\n",
      "  batch 150 loss: 0.1269405067578191\n",
      "  batch 160 loss: 0.170559505464189\n",
      "  batch 170 loss: 0.1368728774039482\n",
      "  batch 180 loss: 0.26004887218878137\n",
      "  batch 190 loss: 0.18967656755212375\n",
      "LOSS train 0.18967656755212375 valid 0.41384429670180023\n",
      "EPOCH 157:\n",
      "  batch 10 loss: 0.34701938829020945\n",
      "  batch 20 loss: 0.2181757451930025\n",
      "  batch 30 loss: 0.10194583669017447\n",
      "  batch 40 loss: 0.2567249376119435\n",
      "  batch 50 loss: 0.18206726825374062\n",
      "  batch 60 loss: 0.06946193220937857\n",
      "  batch 70 loss: 0.13591921698921397\n",
      "  batch 80 loss: 0.2928910708868216\n",
      "  batch 90 loss: 0.3408169637303217\n",
      "  batch 100 loss: 0.12242888037508237\n",
      "  batch 110 loss: 0.08341053232506965\n",
      "  batch 120 loss: 0.29634182414556565\n",
      "  batch 130 loss: 0.12442011397215538\n",
      "  batch 140 loss: 0.49538151455490154\n",
      "  batch 150 loss: 0.24316009858048346\n",
      "  batch 160 loss: 0.3047683120825241\n",
      "  batch 170 loss: 0.07191958581315702\n",
      "  batch 180 loss: 0.09650755795300939\n",
      "  batch 190 loss: 0.2362813240950345\n",
      "LOSS train 0.2362813240950345 valid 2.071791648330616\n",
      "EPOCH 158:\n",
      "  batch 10 loss: 0.8261784817277658\n",
      "  batch 20 loss: 0.2130746847275077\n",
      "  batch 30 loss: 0.1620412359799957\n",
      "  batch 40 loss: 0.08851634276652476\n",
      "  batch 50 loss: 0.1393188697460573\n",
      "  batch 60 loss: 0.21415066182889858\n",
      "  batch 70 loss: 0.3665407541731838\n",
      "  batch 80 loss: 0.3425103646623029\n",
      "  batch 90 loss: 0.109546426378256\n",
      "  batch 100 loss: 0.20085685459926025\n",
      "  batch 110 loss: 0.2757527216620474\n",
      "  batch 120 loss: 0.2979599088372197\n",
      "  batch 130 loss: 0.15491847327211872\n",
      "  batch 140 loss: 0.23321346975353663\n",
      "  batch 150 loss: 0.30358235852800136\n",
      "  batch 160 loss: 0.20581610747576634\n",
      "  batch 170 loss: 0.15676793999082292\n",
      "  batch 180 loss: 0.2550710933857772\n",
      "  batch 190 loss: 0.13410202530558307\n",
      "LOSS train 0.13410202530558307 valid 0.293077416324055\n",
      "EPOCH 159:\n",
      "  batch 10 loss: 0.054157752371247626\n",
      "  batch 20 loss: 0.10323367701184907\n",
      "  batch 30 loss: 0.09561379851766105\n",
      "  batch 40 loss: 0.13698303336568643\n",
      "  batch 50 loss: 0.14340279819552962\n",
      "  batch 60 loss: 0.1377199686918175\n",
      "  batch 70 loss: 0.39460596182907465\n",
      "  batch 80 loss: 0.1223142048889713\n",
      "  batch 90 loss: 0.383807413040995\n",
      "  batch 100 loss: 0.33845865400653563\n",
      "  batch 110 loss: 0.47628483551452516\n",
      "  batch 120 loss: 0.07655670492868012\n",
      "  batch 130 loss: 0.2781242080689935\n",
      "  batch 140 loss: 0.2802176922768922\n",
      "  batch 150 loss: 0.2687175479762118\n",
      "  batch 160 loss: 0.23303919177124044\n",
      "  batch 170 loss: 0.21404423848143778\n",
      "  batch 180 loss: 0.31419845144081365\n",
      "  batch 190 loss: 0.09294182412286318\n",
      "LOSS train 0.09294182412286318 valid 0.32249480706128125\n",
      "EPOCH 160:\n",
      "  batch 10 loss: 0.17170600143890624\n",
      "  batch 20 loss: 0.21709141886276484\n",
      "  batch 30 loss: 0.24702490661211413\n",
      "  batch 40 loss: 0.1018261883447849\n",
      "  batch 50 loss: 0.3286752648205038\n",
      "  batch 60 loss: 0.15335417294181752\n",
      "  batch 70 loss: 0.455037142828769\n",
      "  batch 80 loss: 0.16767063757906725\n",
      "  batch 90 loss: 0.12019047361291087\n",
      "  batch 100 loss: 0.3199726129816554\n",
      "  batch 110 loss: 0.08091695720941061\n",
      "  batch 120 loss: 0.5158835984075267\n",
      "  batch 130 loss: 0.16965584525169106\n",
      "  batch 140 loss: 0.1318768540098972\n",
      "  batch 150 loss: 0.04499469249749381\n",
      "  batch 160 loss: 0.21978190238951356\n",
      "  batch 170 loss: 0.15758913624267734\n",
      "  batch 180 loss: 0.15977114901324968\n",
      "  batch 190 loss: 0.2180482936269982\n",
      "LOSS train 0.2180482936269982 valid 7.670092103271505\n",
      "EPOCH 161:\n",
      "  batch 10 loss: 2.0994881256230657\n",
      "  batch 20 loss: 0.2776992997665729\n",
      "  batch 30 loss: 0.1605340570145927\n",
      "  batch 40 loss: 0.1760208959065494\n",
      "  batch 50 loss: 0.23515113712346647\n",
      "  batch 60 loss: 0.17266159260361746\n",
      "  batch 70 loss: 0.18431611608102685\n",
      "  batch 80 loss: 0.1402832755133204\n",
      "  batch 90 loss: 0.11539406465890352\n",
      "  batch 100 loss: 0.2855679310116102\n",
      "  batch 110 loss: 0.25284028342866804\n",
      "  batch 120 loss: 0.06260887959360843\n",
      "  batch 130 loss: 0.6956431789745693\n",
      "  batch 140 loss: 0.08724912011239212\n",
      "  batch 150 loss: 0.2329690360915265\n",
      "  batch 160 loss: 0.3401654403831344\n",
      "  batch 170 loss: 0.5134338486021989\n",
      "  batch 180 loss: 0.42383902920773836\n",
      "  batch 190 loss: 0.5643104265978763\n",
      "LOSS train 0.5643104265978763 valid 0.3209785189673988\n",
      "EPOCH 162:\n",
      "  batch 10 loss: 0.21071177608537256\n",
      "  batch 20 loss: 0.32177515220482744\n",
      "  batch 30 loss: 0.32501084285722753\n",
      "  batch 40 loss: 0.35530442717299593\n",
      "  batch 50 loss: 0.19748394992784596\n",
      "  batch 60 loss: 0.15955976705436115\n",
      "  batch 70 loss: 0.15539133656202467\n",
      "  batch 80 loss: 0.15232490746857366\n",
      "  batch 90 loss: 0.1334651916666189\n",
      "  batch 100 loss: 0.4937387430516537\n",
      "  batch 110 loss: 0.19857532791647828\n",
      "  batch 120 loss: 0.12586575394743704\n",
      "  batch 130 loss: 0.09892558287465363\n",
      "  batch 140 loss: 0.28514852429107124\n",
      "  batch 150 loss: 0.15734329370534397\n",
      "  batch 160 loss: 0.12618634407408535\n",
      "  batch 170 loss: 0.2247130457086314\n",
      "  batch 180 loss: 0.1870339924353175\n",
      "  batch 190 loss: 0.19442915177833128\n",
      "LOSS train 0.19442915177833128 valid 0.3992399671762355\n",
      "EPOCH 163:\n",
      "  batch 10 loss: 0.300587532352074\n",
      "  batch 20 loss: 0.1953674373911781\n",
      "  batch 30 loss: 0.3279356164258843\n",
      "  batch 40 loss: 0.26668114968560985\n",
      "  batch 50 loss: 0.19390986797880033\n",
      "  batch 60 loss: 0.1962079016955613\n",
      "  batch 70 loss: 0.17107043946289197\n",
      "  batch 80 loss: 0.14651930938853183\n",
      "  batch 90 loss: 0.38662330248698706\n",
      "  batch 100 loss: 0.08675244006080902\n",
      "  batch 110 loss: 0.195706691985788\n",
      "  batch 120 loss: 0.3553526686315308\n",
      "  batch 130 loss: 0.1586556977199507\n",
      "  batch 140 loss: 0.19669918964791577\n",
      "  batch 150 loss: 0.14597062562970678\n",
      "  batch 160 loss: 0.14664867391838926\n",
      "  batch 170 loss: 0.12064350233968071\n",
      "  batch 180 loss: 0.14551730709936236\n",
      "  batch 190 loss: 0.1035327277966644\n",
      "LOSS train 0.1035327277966644 valid 1.1864668487422443\n",
      "EPOCH 164:\n",
      "  batch 10 loss: 0.6170794629109878\n",
      "  batch 20 loss: 0.2891010659521271\n",
      "  batch 30 loss: 0.4471612145360268\n",
      "  batch 40 loss: 0.1421465530496789\n",
      "  batch 50 loss: 0.15997279284783872\n",
      "  batch 60 loss: 0.3254448680767382\n",
      "  batch 70 loss: 0.1556525925250753\n",
      "  batch 80 loss: 0.18195806970979903\n",
      "  batch 90 loss: 0.13967121051391587\n",
      "  batch 100 loss: 0.1771652942043147\n",
      "  batch 110 loss: 0.24633132426315568\n",
      "  batch 120 loss: 0.06488011389210442\n",
      "  batch 130 loss: 0.16563270586702855\n",
      "  batch 140 loss: 0.14158129108327558\n",
      "  batch 150 loss: 0.42693334901196067\n",
      "  batch 160 loss: 0.17617934959162085\n",
      "  batch 170 loss: 0.09940708196891138\n",
      "  batch 180 loss: 0.2245870198647026\n",
      "  batch 190 loss: 0.14467289551830617\n",
      "LOSS train 0.14467289551830617 valid 0.3060219052299661\n",
      "EPOCH 165:\n",
      "  batch 10 loss: 0.11824790637911065\n",
      "  batch 20 loss: 0.08961984147827025\n",
      "  batch 30 loss: 0.12497526039678633\n",
      "  batch 40 loss: 0.15884432262391784\n",
      "  batch 50 loss: 0.3802579775951472\n",
      "  batch 60 loss: 0.08085534798519803\n",
      "  batch 70 loss: 0.21270765248991666\n",
      "  batch 80 loss: 0.08013671658263774\n",
      "  batch 90 loss: 0.06659764687683492\n",
      "  batch 100 loss: 0.26385103971224455\n",
      "  batch 110 loss: 0.31498160875107717\n",
      "  batch 120 loss: 0.13477640311648428\n",
      "  batch 130 loss: 0.8594985387495399\n",
      "  batch 140 loss: 0.21777261983752397\n",
      "  batch 150 loss: 0.11740458490676246\n",
      "  batch 160 loss: 0.3613803174710483\n",
      "  batch 170 loss: 0.15793450256533106\n",
      "  batch 180 loss: 0.4699330160507088\n",
      "  batch 190 loss: 0.1294140230771518\n",
      "LOSS train 0.1294140230771518 valid 0.3111308318032873\n",
      "EPOCH 166:\n",
      "  batch 10 loss: 0.20372952009838627\n",
      "  batch 20 loss: 0.34039628723694476\n",
      "  batch 30 loss: 0.2378971026496856\n",
      "  batch 40 loss: 0.371854222661932\n",
      "  batch 50 loss: 0.2498784944462386\n",
      "  batch 60 loss: 0.20927655015311758\n",
      "  batch 70 loss: 0.19970751085820665\n",
      "  batch 80 loss: 0.191510731051676\n",
      "  batch 90 loss: 0.04086258304232615\n",
      "  batch 100 loss: 0.2387722261773888\n",
      "  batch 110 loss: 0.2927214691830159\n",
      "  batch 120 loss: 0.244072436690476\n",
      "  batch 130 loss: 0.13217721284454456\n",
      "  batch 140 loss: 0.17780376065711606\n",
      "  batch 150 loss: 0.20158649781260465\n",
      "  batch 160 loss: 0.23541299429052742\n",
      "  batch 170 loss: 0.10669784300625906\n",
      "  batch 180 loss: 0.2555330119175778\n",
      "  batch 190 loss: 0.12863788509475854\n",
      "LOSS train 0.12863788509475854 valid 0.3283990967364587\n",
      "EPOCH 167:\n",
      "  batch 10 loss: 0.281145938525151\n",
      "  batch 20 loss: 0.23157502545509487\n",
      "  batch 30 loss: 0.2023458274336008\n",
      "  batch 40 loss: 0.15984857506409753\n",
      "  batch 50 loss: 0.14354909575631608\n",
      "  batch 60 loss: 0.09039782049949281\n",
      "  batch 70 loss: 0.21476930116987206\n",
      "  batch 80 loss: 0.08712670718377921\n",
      "  batch 90 loss: 0.12826823037503346\n",
      "  batch 100 loss: 0.1610538032298791\n",
      "  batch 110 loss: 0.19015936214309476\n",
      "  batch 120 loss: 0.23498406315984538\n",
      "  batch 130 loss: 0.1092053891334217\n",
      "  batch 140 loss: 0.14946816873925856\n",
      "  batch 150 loss: 0.19330106161069124\n",
      "  batch 160 loss: 0.11962614732183283\n",
      "  batch 170 loss: 0.16554039272728005\n",
      "  batch 180 loss: 0.11134445341940591\n",
      "  batch 190 loss: 0.11633086108777206\n",
      "LOSS train 0.11633086108777206 valid 0.32044194934560905\n",
      "EPOCH 168:\n",
      "  batch 10 loss: 0.06673087393010065\n",
      "  batch 20 loss: 0.30191440739326936\n",
      "  batch 30 loss: 0.1772584461254155\n",
      "  batch 40 loss: 0.1255047897753684\n",
      "  batch 50 loss: 0.4825944987416733\n",
      "  batch 60 loss: 0.28261694172215357\n",
      "  batch 70 loss: 0.25450052212763696\n",
      "  batch 80 loss: 0.23600080866381176\n",
      "  batch 90 loss: 0.45860924365201755\n",
      "  batch 100 loss: 0.342101484064915\n",
      "  batch 110 loss: 0.09505362449408494\n",
      "  batch 120 loss: 0.33814288444627894\n",
      "  batch 130 loss: 0.17322781551902153\n",
      "  batch 140 loss: 0.22737785751523915\n",
      "  batch 150 loss: 0.09389012242900208\n",
      "  batch 160 loss: 0.10289395605941536\n",
      "  batch 170 loss: 0.07003742307588254\n",
      "  batch 180 loss: 0.05808967067860067\n",
      "  batch 190 loss: 0.22748706818447317\n",
      "LOSS train 0.22748706818447317 valid 0.2891991488046561\n",
      "EPOCH 169:\n",
      "  batch 10 loss: 0.22231030154762266\n",
      "  batch 20 loss: 0.22560377575155144\n",
      "  batch 30 loss: 0.1671548874729524\n",
      "  batch 40 loss: 0.09730995381187313\n",
      "  batch 50 loss: 0.055971796621633985\n",
      "  batch 60 loss: 0.24393838523083106\n",
      "  batch 70 loss: 0.12974828709920985\n",
      "  batch 80 loss: 0.08623105700025917\n",
      "  batch 90 loss: 0.12346480188898568\n",
      "  batch 100 loss: 0.10629796590492332\n",
      "  batch 110 loss: 0.1909497565990023\n",
      "  batch 120 loss: 0.5270796160977624\n",
      "  batch 130 loss: 0.1371224233362227\n",
      "  batch 140 loss: 0.10369658479248756\n",
      "  batch 150 loss: 0.4471990161902795\n",
      "  batch 160 loss: 0.1530412032698223\n",
      "  batch 170 loss: 0.14549603256964475\n",
      "  batch 180 loss: 0.15191061758087016\n",
      "  batch 190 loss: 0.27777023737098716\n",
      "LOSS train 0.27777023737098716 valid 0.5108827969844032\n",
      "EPOCH 170:\n",
      "  batch 10 loss: 0.13214592597942101\n",
      "  batch 20 loss: 0.2521264656854328\n",
      "  batch 30 loss: 0.24524620717693324\n",
      "  batch 40 loss: 0.2513861318271665\n",
      "  batch 50 loss: 0.3099102895397664\n",
      "  batch 60 loss: 0.09544151210520795\n",
      "  batch 70 loss: 0.07671259827984614\n",
      "  batch 80 loss: 0.12782493481208804\n",
      "  batch 90 loss: 0.16012367189323412\n",
      "  batch 100 loss: 0.35289031586080455\n",
      "  batch 110 loss: 0.28152057589268226\n",
      "  batch 120 loss: 0.14969183138891823\n",
      "  batch 130 loss: 0.31972820599730767\n",
      "  batch 140 loss: 0.10463961373097845\n",
      "  batch 150 loss: 0.1339406341641734\n",
      "  batch 160 loss: 0.12611713917412998\n",
      "  batch 170 loss: 0.11675983965906198\n",
      "  batch 180 loss: 0.15841087362314282\n",
      "  batch 190 loss: 0.8274213535032005\n",
      "LOSS train 0.8274213535032005 valid 0.39446578652301373\n",
      "EPOCH 171:\n",
      "  batch 10 loss: 0.1529460928808021\n",
      "  batch 20 loss: 0.3370798549327446\n",
      "  batch 30 loss: 0.35318352277545273\n",
      "  batch 40 loss: 0.06497513034119037\n",
      "  batch 50 loss: 0.06902509000237841\n",
      "  batch 60 loss: 0.07720351862444659\n",
      "  batch 70 loss: 0.13347972146293613\n",
      "  batch 80 loss: 0.11793663737589669\n",
      "  batch 90 loss: 0.10816711927091091\n",
      "  batch 100 loss: 0.09043868679000297\n",
      "  batch 110 loss: 0.03380865609142347\n",
      "  batch 120 loss: 0.06778059852531441\n",
      "  batch 130 loss: 0.20855912656475084\n",
      "  batch 140 loss: 0.1502097500699165\n",
      "  batch 150 loss: 0.2456899825569053\n",
      "  batch 160 loss: 0.22980848369625165\n",
      "  batch 170 loss: 0.28351744554402103\n",
      "  batch 180 loss: 0.2420031547633698\n",
      "  batch 190 loss: 0.8382159003765992\n",
      "LOSS train 0.8382159003765992 valid 0.3164929286985614\n",
      "EPOCH 172:\n",
      "  batch 10 loss: 0.0838156270016043\n",
      "  batch 20 loss: 0.15363144814509724\n",
      "  batch 30 loss: 0.2533346748536133\n",
      "  batch 40 loss: 0.2315783719812316\n",
      "  batch 50 loss: 0.1135678318067221\n",
      "  batch 60 loss: 0.16354915573610923\n",
      "  batch 70 loss: 0.1206762609857833\n",
      "  batch 80 loss: 0.2583059483979014\n",
      "  batch 90 loss: 0.19178952248039421\n",
      "  batch 100 loss: 0.10229265435464185\n",
      "  batch 110 loss: 0.14536151480406262\n",
      "  batch 120 loss: 0.10157939771379461\n",
      "  batch 130 loss: 0.4268057622143715\n",
      "  batch 140 loss: 0.24208128094105633\n",
      "  batch 150 loss: 0.3550533591101157\n",
      "  batch 160 loss: 0.20639023414598795\n",
      "  batch 170 loss: 0.19159554324614875\n",
      "  batch 180 loss: 0.1755560695341046\n",
      "  batch 190 loss: 0.10719278005599335\n",
      "LOSS train 0.10719278005599335 valid 0.35263700098962897\n",
      "EPOCH 173:\n",
      "  batch 10 loss: 0.21277968399140262\n",
      "  batch 20 loss: 0.07274246744291304\n",
      "  batch 30 loss: 0.16771743069305103\n",
      "  batch 40 loss: 0.13832213219138795\n",
      "  batch 50 loss: 0.15110795118634995\n",
      "  batch 60 loss: 0.4834513462265022\n",
      "  batch 70 loss: 0.15409347374370555\n",
      "  batch 80 loss: 0.10156840884901612\n",
      "  batch 90 loss: 0.3670836665703973\n",
      "  batch 100 loss: 0.1050233250038218\n",
      "  batch 110 loss: 0.16332521045587783\n",
      "  batch 120 loss: 0.15124127368326298\n",
      "  batch 130 loss: 0.20410497806005878\n",
      "  batch 140 loss: 0.2380854214628016\n",
      "  batch 150 loss: 0.20291863564307278\n",
      "  batch 160 loss: 0.1384400411741126\n",
      "  batch 170 loss: 0.326717298598669\n",
      "  batch 180 loss: 0.10940086514783616\n",
      "  batch 190 loss: 0.07105110380261977\n",
      "LOSS train 0.07105110380261977 valid 0.41536663344534946\n",
      "EPOCH 174:\n",
      "  batch 10 loss: 0.14207750620262233\n",
      "  batch 20 loss: 0.17244022742133894\n",
      "  batch 30 loss: 0.11835851756586635\n",
      "  batch 40 loss: 0.3057955927861258\n",
      "  batch 50 loss: 0.14369568475704\n",
      "  batch 60 loss: 0.15199014739075664\n",
      "  batch 70 loss: 0.24070040321057604\n",
      "  batch 80 loss: 0.2112986943793203\n",
      "  batch 90 loss: 0.23082697917707265\n",
      "  batch 100 loss: 0.19480601336344988\n",
      "  batch 110 loss: 0.14103491025616677\n",
      "  batch 120 loss: 0.11108869370655157\n",
      "  batch 130 loss: 0.161656956825027\n",
      "  batch 140 loss: 0.1704895230155671\n",
      "  batch 150 loss: 0.2868799756193766\n",
      "  batch 160 loss: 0.5459553563862756\n",
      "  batch 170 loss: 0.1701564013731968\n",
      "  batch 180 loss: 0.05540665122188102\n",
      "  batch 190 loss: 0.06865363606111714\n",
      "LOSS train 0.06865363606111714 valid 3.178316386510804\n",
      "EPOCH 175:\n",
      "  batch 10 loss: 1.6004328020802632\n",
      "  batch 20 loss: 0.421525748744898\n",
      "  batch 30 loss: 0.5743570250984703\n",
      "  batch 40 loss: 0.15708544601438917\n",
      "  batch 50 loss: 0.16507804983539245\n",
      "  batch 60 loss: 0.28195273657720465\n",
      "  batch 70 loss: 0.12290615249748953\n",
      "  batch 80 loss: 0.2159186749942819\n",
      "  batch 90 loss: 0.07334807499501039\n",
      "  batch 100 loss: 0.4687599898836197\n",
      "  batch 110 loss: 0.09827232459774678\n",
      "  batch 120 loss: 0.21041315080328787\n",
      "  batch 130 loss: 0.23204633227833255\n",
      "  batch 140 loss: 0.12861755073245149\n",
      "  batch 150 loss: 0.10967737676401157\n",
      "  batch 160 loss: 0.15289801578510379\n",
      "  batch 170 loss: 0.12229056594842405\n",
      "  batch 180 loss: 0.20748394483030097\n",
      "  batch 190 loss: 0.06390849840099691\n",
      "LOSS train 0.06390849840099691 valid 0.31194302300019233\n",
      "EPOCH 176:\n",
      "  batch 10 loss: 0.10814465953299077\n",
      "  batch 20 loss: 0.6764827041908574\n",
      "  batch 30 loss: 0.3185092907584476\n",
      "  batch 40 loss: 0.3953356510441154\n",
      "  batch 50 loss: 0.20134493414952886\n",
      "  batch 60 loss: 0.0858859818327801\n",
      "  batch 70 loss: 0.2898896657527075\n",
      "  batch 80 loss: 0.13179471331004605\n",
      "  batch 90 loss: 0.38810681102013406\n",
      "  batch 100 loss: 0.12022962993250985\n",
      "  batch 110 loss: 0.13692311308586796\n",
      "  batch 120 loss: 0.24933362078882054\n",
      "  batch 130 loss: 0.18335156717130302\n",
      "  batch 140 loss: 0.05146423926726129\n",
      "  batch 150 loss: 0.19421499026939273\n",
      "  batch 160 loss: 0.05134351335655083\n",
      "  batch 170 loss: 0.3408849943152745\n",
      "  batch 180 loss: 0.2010343168483814\n",
      "  batch 190 loss: 0.16165118843819073\n",
      "LOSS train 0.16165118843819073 valid 0.34338344144426186\n",
      "EPOCH 177:\n",
      "  batch 10 loss: 0.15054130563657964\n",
      "  batch 20 loss: 0.10100314704395714\n",
      "  batch 30 loss: 0.2592937801793596\n",
      "  batch 40 loss: 0.15003558415119186\n",
      "  batch 50 loss: 0.18870078048730649\n",
      "  batch 60 loss: 0.2178694873065979\n",
      "  batch 70 loss: 0.33876890297960927\n",
      "  batch 80 loss: 0.19795355413480137\n",
      "  batch 90 loss: 0.1393948420885863\n",
      "  batch 100 loss: 0.03544650134863332\n",
      "  batch 110 loss: 0.3326524838277692\n",
      "  batch 120 loss: 0.39210314321462647\n",
      "  batch 130 loss: 0.22353057125146733\n",
      "  batch 140 loss: 0.16075123808896025\n",
      "  batch 150 loss: 0.06726605447192924\n",
      "  batch 160 loss: 0.23270762107549672\n",
      "  batch 170 loss: 0.12019614796481619\n",
      "  batch 180 loss: 0.3173108133779351\n",
      "  batch 190 loss: 0.16207896700602759\n",
      "LOSS train 0.16207896700602759 valid 0.3958493192709657\n",
      "EPOCH 178:\n",
      "  batch 10 loss: 0.4533248518378059\n",
      "  batch 20 loss: 0.20633431122514595\n",
      "  batch 30 loss: 0.12299478099789667\n",
      "  batch 40 loss: 0.15560001806297805\n",
      "  batch 50 loss: 0.10192910243276856\n",
      "  batch 60 loss: 0.23879401056910865\n",
      "  batch 70 loss: 0.1562066370071989\n",
      "  batch 80 loss: 0.17495479697099653\n",
      "  batch 90 loss: 0.19339365288597038\n",
      "  batch 100 loss: 0.10657672341985744\n",
      "  batch 110 loss: 0.07930571602773853\n",
      "  batch 120 loss: 0.1506436067341383\n",
      "  batch 130 loss: 0.15949048900502022\n",
      "  batch 140 loss: 0.14853502646874403\n",
      "  batch 150 loss: 0.10628668035601549\n",
      "  batch 160 loss: 0.12455246387253283\n",
      "  batch 170 loss: 0.15300784601040504\n",
      "  batch 180 loss: 0.3371341175892667\n",
      "  batch 190 loss: 0.03387825835816329\n",
      "LOSS train 0.03387825835816329 valid 0.6940817880319616\n",
      "EPOCH 179:\n",
      "  batch 10 loss: 0.4077684648105787\n",
      "  batch 20 loss: 0.4607886210244942\n",
      "  batch 30 loss: 0.1548635046241543\n",
      "  batch 40 loss: 0.10671405234854774\n",
      "  batch 50 loss: 0.278876969411067\n",
      "  batch 60 loss: 0.23218980342971918\n",
      "  batch 70 loss: 0.3915175747260037\n",
      "  batch 80 loss: 0.17743667861027462\n",
      "  batch 90 loss: 0.27434781700503663\n",
      "  batch 100 loss: 0.08318467516510282\n",
      "  batch 110 loss: 0.10201402605889598\n",
      "  batch 120 loss: 0.07731430582716711\n",
      "  batch 130 loss: 0.08313540702920363\n",
      "  batch 140 loss: 0.19650797856138524\n",
      "  batch 150 loss: 0.08412563123629298\n",
      "  batch 160 loss: 0.0801721130599617\n",
      "  batch 170 loss: 0.06758991180577141\n",
      "  batch 180 loss: 0.05847146985715881\n",
      "  batch 190 loss: 0.15860295976763156\n",
      "LOSS train 0.15860295976763156 valid 0.29722749983876057\n",
      "EPOCH 180:\n",
      "  batch 10 loss: 0.1784480653808714\n",
      "  batch 20 loss: 0.0750119859933875\n",
      "  batch 30 loss: 0.10793675148779584\n",
      "  batch 40 loss: 0.14417732804768094\n",
      "  batch 50 loss: 0.17392686730527201\n",
      "  batch 60 loss: 0.10242879131283189\n",
      "  batch 70 loss: 0.02367879430312314\n",
      "  batch 80 loss: 0.20361414074795903\n",
      "  batch 90 loss: 0.5226554478737853\n",
      "  batch 100 loss: 0.20241395329558146\n",
      "  batch 110 loss: 0.1835051245900104\n",
      "  batch 120 loss: 0.04530117102258373\n",
      "  batch 130 loss: 0.15362315893871709\n",
      "  batch 140 loss: 0.16752041562522207\n",
      "  batch 150 loss: 0.1849519929717644\n",
      "  batch 160 loss: 0.03745703342510751\n",
      "  batch 170 loss: 0.10668031006953242\n",
      "  batch 180 loss: 0.37308574995913657\n",
      "  batch 190 loss: 0.27097119594327523\n",
      "LOSS train 0.27097119594327523 valid 0.38198802504781526\n",
      "EPOCH 181:\n",
      "  batch 10 loss: 0.14794259792981848\n",
      "  batch 20 loss: 0.11236852807342075\n",
      "  batch 30 loss: 0.08933010564971937\n",
      "  batch 40 loss: 0.12174707730082446\n",
      "  batch 50 loss: 0.05738761255868212\n",
      "  batch 60 loss: 0.12601413267948375\n",
      "  batch 70 loss: 0.1772799598344136\n",
      "  batch 80 loss: 0.11164320584916823\n",
      "  batch 90 loss: 0.0763770273886621\n",
      "  batch 100 loss: 0.12030663963043935\n",
      "  batch 110 loss: 0.029942261136238813\n",
      "  batch 120 loss: 0.10428914559388432\n",
      "  batch 130 loss: 0.08727003292055997\n",
      "  batch 140 loss: 0.1945945008619674\n",
      "  batch 150 loss: 0.1990229828525571\n",
      "  batch 160 loss: 0.09295675897160435\n",
      "  batch 170 loss: 0.13996997640733752\n",
      "  batch 180 loss: 0.07735396997813951\n",
      "  batch 190 loss: 0.28638266504835885\n",
      "LOSS train 0.28638266504835885 valid 0.33241938182948455\n",
      "EPOCH 182:\n",
      "  batch 10 loss: 0.1235817456186851\n",
      "  batch 20 loss: 0.12212955424856772\n",
      "  batch 30 loss: 0.17791693269682582\n",
      "  batch 40 loss: 0.1353674721707648\n",
      "  batch 50 loss: 0.13376012437333884\n",
      "  batch 60 loss: 0.5381289216575169\n",
      "  batch 70 loss: 0.2671186359384592\n",
      "  batch 80 loss: 0.08538252070502494\n",
      "  batch 90 loss: 0.13521776273737487\n",
      "  batch 100 loss: 0.32222606020732203\n",
      "  batch 110 loss: 0.4180378744385962\n",
      "  batch 120 loss: 0.17549384502017346\n",
      "  batch 130 loss: 0.4350005382761083\n",
      "  batch 140 loss: 0.04140865703811869\n",
      "  batch 150 loss: 0.44368044676230056\n",
      "  batch 160 loss: 0.1331822172149259\n",
      "  batch 170 loss: 0.13852549148787147\n",
      "  batch 180 loss: 0.15254345873595412\n",
      "  batch 190 loss: 0.22660326250036178\n",
      "LOSS train 0.22660326250036178 valid 0.30845511860775915\n",
      "EPOCH 183:\n",
      "  batch 10 loss: 0.27601856315959594\n",
      "  batch 20 loss: 0.1463852177024819\n",
      "  batch 30 loss: 0.15344771494383166\n",
      "  batch 40 loss: 0.28525297079795564\n",
      "  batch 50 loss: 0.10012632828784263\n",
      "  batch 60 loss: 0.1108479190500475\n",
      "  batch 70 loss: 0.49588482885737906\n",
      "  batch 80 loss: 0.07704412562179641\n",
      "  batch 90 loss: 0.126787022544886\n",
      "  batch 100 loss: 0.08769221667243983\n",
      "  batch 110 loss: 0.13687276099917653\n",
      "  batch 120 loss: 0.1631422502990972\n",
      "  batch 130 loss: 0.040017923115578925\n",
      "  batch 140 loss: 0.13365487328187556\n",
      "  batch 150 loss: 0.07751201888731885\n",
      "  batch 160 loss: 0.26762616901714864\n",
      "  batch 170 loss: 0.17101643380310633\n",
      "  batch 180 loss: 0.10881177336577821\n",
      "  batch 190 loss: 0.08786218682325853\n",
      "LOSS train 0.08786218682325853 valid 0.3033414333609611\n",
      "EPOCH 184:\n",
      "  batch 10 loss: 0.168272330560103\n",
      "  batch 20 loss: 0.2042202523186461\n",
      "  batch 30 loss: 0.09050624673764104\n",
      "  batch 40 loss: 0.07712571947031392\n",
      "  batch 50 loss: 0.14307182490956621\n",
      "  batch 60 loss: 0.1828364176404648\n",
      "  batch 70 loss: 0.8418732286731029\n",
      "  batch 80 loss: 0.4591406100658787\n",
      "  batch 90 loss: 0.4867486503854934\n",
      "  batch 100 loss: 0.10001327232275799\n",
      "  batch 110 loss: 0.28850222632463557\n",
      "  batch 120 loss: 0.11058529033152809\n",
      "  batch 130 loss: 0.08221696529622022\n",
      "  batch 140 loss: 0.17408933796759812\n",
      "  batch 150 loss: 0.20433510565871985\n",
      "  batch 160 loss: 0.10138719107599173\n",
      "  batch 170 loss: 0.07847962677751638\n",
      "  batch 180 loss: 0.10430581519813131\n",
      "  batch 190 loss: 0.3630888320646591\n",
      "LOSS train 0.3630888320646591 valid 0.3089167236749185\n",
      "EPOCH 185:\n",
      "  batch 10 loss: 0.2511847360897036\n",
      "  batch 20 loss: 0.4663175030538696\n",
      "  batch 30 loss: 0.1539688646869763\n",
      "  batch 40 loss: 0.11674817976098666\n",
      "  batch 50 loss: 0.10366700928534556\n",
      "  batch 60 loss: 0.02335841940166574\n",
      "  batch 70 loss: 0.19224329228927672\n",
      "  batch 80 loss: 0.3071873164984936\n",
      "  batch 90 loss: 0.12851877137718476\n",
      "  batch 100 loss: 0.10597054881291115\n",
      "  batch 110 loss: 0.044000698531363014\n",
      "  batch 120 loss: 0.29161014160454213\n",
      "  batch 130 loss: 0.06662070287143251\n",
      "  batch 140 loss: 0.08516754537286034\n",
      "  batch 150 loss: 0.3346018734831887\n",
      "  batch 160 loss: 0.665557046165759\n",
      "  batch 170 loss: 0.10556668121716939\n",
      "  batch 180 loss: 0.2755474134068209\n",
      "  batch 190 loss: 0.32437037312447503\n",
      "LOSS train 0.32437037312447503 valid 0.29260763840045456\n",
      "EPOCH 186:\n",
      "  batch 10 loss: 0.12142506350683106\n",
      "  batch 20 loss: 0.12822639673177036\n",
      "  batch 30 loss: 0.08976085787580815\n",
      "  batch 40 loss: 0.2545852828576244\n",
      "  batch 50 loss: 0.2961739297314125\n",
      "  batch 60 loss: 0.13226667208018625\n",
      "  batch 70 loss: 0.23762685024612437\n",
      "  batch 80 loss: 0.14351817173583187\n",
      "  batch 90 loss: 0.22534085059141945\n",
      "  batch 100 loss: 0.27544119377998866\n",
      "  batch 110 loss: 0.14358293756240528\n",
      "  batch 120 loss: 0.08308089795937121\n",
      "  batch 130 loss: 0.2307351585737706\n",
      "  batch 140 loss: 0.14123005095441385\n",
      "  batch 150 loss: 0.02693761780838031\n",
      "  batch 160 loss: 0.016139838990056887\n",
      "  batch 170 loss: 0.1596850657941104\n",
      "  batch 180 loss: 0.23110015537931758\n",
      "  batch 190 loss: 0.1379840907481366\n",
      "LOSS train 0.1379840907481366 valid 0.2980991924494167\n",
      "EPOCH 187:\n",
      "  batch 10 loss: 0.15368727000695798\n",
      "  batch 20 loss: 0.1464732426451519\n",
      "  batch 30 loss: 0.2525864061768516\n",
      "  batch 40 loss: 0.2078241156580134\n",
      "  batch 50 loss: 0.1957137129061266\n",
      "  batch 60 loss: 0.2923184221886913\n",
      "  batch 70 loss: 0.09816389495563271\n",
      "  batch 80 loss: 0.05886279240658041\n",
      "  batch 90 loss: 0.09998592331380678\n",
      "  batch 100 loss: 0.14207289200676315\n",
      "  batch 110 loss: 0.10951782435777205\n",
      "  batch 120 loss: 0.18863503778356971\n",
      "  batch 130 loss: 0.06974852294624725\n",
      "  batch 140 loss: 0.09137319113615376\n",
      "  batch 150 loss: 0.08692634408907907\n",
      "  batch 160 loss: 0.13077622295104446\n",
      "  batch 170 loss: 0.2200268174019584\n",
      "  batch 180 loss: 0.22720921505897423\n",
      "  batch 190 loss: 0.11015271301712345\n",
      "LOSS train 0.11015271301712345 valid 0.29616644353702726\n",
      "EPOCH 188:\n",
      "  batch 10 loss: 0.058149048364793995\n",
      "  batch 20 loss: 0.09358576962767984\n",
      "  batch 30 loss: 0.23637893027189422\n",
      "  batch 40 loss: 0.5016300144172419\n",
      "  batch 50 loss: 0.2936600671633641\n",
      "  batch 60 loss: 0.2098382483129626\n",
      "  batch 70 loss: 0.13867418200243264\n",
      "  batch 80 loss: 0.27117538028114724\n",
      "  batch 90 loss: 0.08469917009788333\n",
      "  batch 100 loss: 0.12063341682314785\n",
      "  batch 110 loss: 0.05680576736615421\n",
      "  batch 120 loss: 0.05592546138270791\n",
      "  batch 130 loss: 0.1512393397577398\n",
      "  batch 140 loss: 0.09175707757876808\n",
      "  batch 150 loss: 0.04625378401324269\n",
      "  batch 160 loss: 0.04536900217426591\n",
      "  batch 170 loss: 0.04323828964334098\n",
      "  batch 180 loss: 0.5038434882822912\n",
      "  batch 190 loss: 0.525731620651186\n",
      "LOSS train 0.525731620651186 valid 0.3715269122944147\n",
      "EPOCH 189:\n",
      "  batch 10 loss: 0.271056485810368\n",
      "  batch 20 loss: 0.12107415752352609\n",
      "  batch 30 loss: 0.14103230620494286\n",
      "  batch 40 loss: 0.20394956521090535\n",
      "  batch 50 loss: 0.13967789018224722\n",
      "  batch 60 loss: 0.05740031048567289\n",
      "  batch 70 loss: 0.2285393303178239\n",
      "  batch 80 loss: 0.16916903370329237\n",
      "  batch 90 loss: 0.07463432607000868\n",
      "  batch 100 loss: 0.10961328736298129\n",
      "  batch 110 loss: 0.11433934176748153\n",
      "  batch 120 loss: 0.1761409632188588\n",
      "  batch 130 loss: 0.25311989957044717\n",
      "  batch 140 loss: 0.07049050253554015\n",
      "  batch 150 loss: 0.0679426450496976\n",
      "  batch 160 loss: 0.20898865727594967\n",
      "  batch 170 loss: 0.11541972657933001\n",
      "  batch 180 loss: 0.04191533527732645\n",
      "  batch 190 loss: 0.04350294453706738\n",
      "LOSS train 0.04350294453706738 valid 0.28540043282663274\n",
      "EPOCH 190:\n",
      "  batch 10 loss: 0.18163132661223927\n",
      "  batch 20 loss: 0.16431632224903298\n",
      "  batch 30 loss: 0.11821044301550501\n",
      "  batch 40 loss: 0.1155708753575709\n",
      "  batch 50 loss: 0.3182230617131154\n",
      "  batch 60 loss: 0.10674629304794507\n",
      "  batch 70 loss: 0.18232599041548384\n",
      "  batch 80 loss: 0.2514527971197822\n",
      "  batch 90 loss: 0.2295976610170328\n",
      "  batch 100 loss: 0.31123570691179053\n",
      "  batch 110 loss: 0.17277739425144317\n",
      "  batch 120 loss: 0.067654144822518\n",
      "  batch 130 loss: 0.23092301650967784\n",
      "  batch 140 loss: 0.11325514313029998\n",
      "  batch 150 loss: 0.20105082005366057\n",
      "  batch 160 loss: 0.10583583227225972\n",
      "  batch 170 loss: 0.07867949987303291\n",
      "  batch 180 loss: 0.17921057467792706\n",
      "  batch 190 loss: 0.03866675800090889\n",
      "LOSS train 0.03866675800090889 valid 0.26770910205498066\n",
      "EPOCH 191:\n",
      "  batch 10 loss: 0.047429249889182755\n",
      "  batch 20 loss: 0.3191515602702566\n",
      "  batch 30 loss: 0.32772075254779337\n",
      "  batch 40 loss: 0.22414901050140087\n",
      "  batch 50 loss: 0.16057693114744326\n",
      "  batch 60 loss: 0.07594972121842147\n",
      "  batch 70 loss: 0.06915625581332278\n",
      "  batch 80 loss: 0.08434903860288614\n",
      "  batch 90 loss: 0.16777505902582562\n",
      "  batch 100 loss: 0.23401033729960546\n",
      "  batch 110 loss: 0.1617539939123162\n",
      "  batch 120 loss: 0.1398883515757916\n",
      "  batch 130 loss: 0.15579586270450818\n",
      "  batch 140 loss: 0.05221447253120459\n",
      "  batch 150 loss: 0.20731331343189935\n",
      "  batch 160 loss: 0.2528586625294338\n",
      "  batch 170 loss: 0.08732407979514392\n",
      "  batch 180 loss: 0.1204574008606869\n",
      "  batch 190 loss: 0.1448257999035377\n",
      "LOSS train 0.1448257999035377 valid 0.265946914621089\n",
      "EPOCH 192:\n",
      "  batch 10 loss: 0.10042753442048706\n",
      "  batch 20 loss: 0.2569802721554879\n",
      "  batch 30 loss: 0.0743423800686287\n",
      "  batch 40 loss: 0.23090648891561613\n",
      "  batch 50 loss: 0.07291293869147922\n",
      "  batch 60 loss: 0.09881055607183953\n",
      "  batch 70 loss: 0.18423385556286007\n",
      "  batch 80 loss: 0.08826264372619334\n",
      "  batch 90 loss: 0.2696714810545927\n",
      "  batch 100 loss: 0.13486082680310574\n",
      "  batch 110 loss: 0.14010313833355212\n",
      "  batch 120 loss: 0.048846235282576346\n",
      "  batch 130 loss: 0.05422100950290769\n",
      "  batch 140 loss: 0.044750462552292446\n",
      "  batch 150 loss: 0.11409730026916805\n",
      "  batch 160 loss: 0.19283541544771196\n",
      "  batch 170 loss: 0.20077385844026593\n",
      "  batch 180 loss: 0.3579429030549363\n",
      "  batch 190 loss: 0.24230129642933207\n",
      "LOSS train 0.24230129642933207 valid 0.32554072095522546\n",
      "EPOCH 193:\n",
      "  batch 10 loss: 0.08202025719691335\n",
      "  batch 20 loss: 0.32024107966863086\n",
      "  batch 30 loss: 0.19470049576466408\n",
      "  batch 40 loss: 0.12933587277457265\n",
      "  batch 50 loss: 0.056988378811092845\n",
      "  batch 60 loss: 0.05960528594932839\n",
      "  batch 70 loss: 0.0939831855205739\n",
      "  batch 80 loss: 0.13775598747470214\n",
      "  batch 90 loss: 0.03553622156723577\n",
      "  batch 100 loss: 0.249687157168637\n",
      "  batch 110 loss: 0.16228659420075645\n",
      "  batch 120 loss: 0.13325317908065698\n",
      "  batch 130 loss: 0.022099331193749093\n",
      "  batch 140 loss: 0.28968707242174785\n",
      "  batch 150 loss: 0.28353995064326226\n",
      "  batch 160 loss: 0.25239601835260145\n",
      "  batch 170 loss: 0.07265452833444215\n",
      "  batch 180 loss: 0.1204044742971746\n",
      "  batch 190 loss: 0.12061061641761625\n",
      "LOSS train 0.12061061641761625 valid 0.4138587350348243\n",
      "EPOCH 194:\n",
      "  batch 10 loss: 0.34567503716261855\n",
      "  batch 20 loss: 0.03383711496435353\n",
      "  batch 30 loss: 0.31030418051595915\n",
      "  batch 40 loss: 0.10759620893898614\n",
      "  batch 50 loss: 0.08431215990713099\n",
      "  batch 60 loss: 0.05456898574429943\n",
      "  batch 70 loss: 0.07297747543096307\n",
      "  batch 80 loss: 0.13618395343137307\n",
      "  batch 90 loss: 0.13586503498781893\n",
      "  batch 100 loss: 0.3825071614031003\n",
      "  batch 110 loss: 0.14486997289716327\n",
      "  batch 120 loss: 0.07691900398031067\n",
      "  batch 130 loss: 0.43854325603024336\n",
      "  batch 140 loss: 0.24317998369060662\n",
      "  batch 150 loss: 0.22759939620889327\n",
      "  batch 160 loss: 0.18842196638697714\n",
      "  batch 170 loss: 0.3060160546153384\n",
      "  batch 180 loss: 0.1638133245429799\n",
      "  batch 190 loss: 0.27838657532010985\n",
      "LOSS train 0.27838657532010985 valid 0.4537244490224438\n",
      "EPOCH 195:\n",
      "  batch 10 loss: 0.30811954993616836\n",
      "  batch 20 loss: 0.26927970037377236\n",
      "  batch 30 loss: 0.0818859348099977\n",
      "  batch 40 loss: 0.07158589602840948\n",
      "  batch 50 loss: 0.12173103679160704\n",
      "  batch 60 loss: 0.022042196072015942\n",
      "  batch 70 loss: 0.09143817069257239\n",
      "  batch 80 loss: 0.319025363882929\n",
      "  batch 90 loss: 0.37083601964504853\n",
      "  batch 100 loss: 0.45822428444171237\n",
      "  batch 110 loss: 0.26221149195826\n",
      "  batch 120 loss: 0.12020229223508068\n",
      "  batch 130 loss: 0.04359165156420204\n",
      "  batch 140 loss: 0.05623454981723626\n",
      "  batch 150 loss: 0.09917700939831775\n",
      "  batch 160 loss: 0.19793562479935645\n",
      "  batch 170 loss: 0.09272902589955265\n",
      "  batch 180 loss: 0.24320674290847819\n",
      "  batch 190 loss: 0.055312013780167035\n",
      "LOSS train 0.055312013780167035 valid 0.272823027978878\n",
      "EPOCH 196:\n",
      "  batch 10 loss: 0.04800355772913463\n",
      "  batch 20 loss: 0.06793827959218106\n",
      "  batch 30 loss: 0.08306552600577959\n",
      "  batch 40 loss: 0.055771596846079775\n",
      "  batch 50 loss: 0.06121652002429982\n",
      "  batch 60 loss: 0.1257675141652726\n",
      "  batch 70 loss: 0.03228698452084018\n",
      "  batch 80 loss: 0.09132869349164138\n",
      "  batch 90 loss: 0.24241458204123773\n",
      "  batch 100 loss: 0.18954242034083107\n",
      "  batch 110 loss: 0.13896298086231126\n",
      "  batch 120 loss: 0.20280994737231595\n",
      "  batch 130 loss: 0.8802279129577073\n",
      "  batch 140 loss: 0.06073273850088299\n",
      "  batch 150 loss: 0.4096336182568848\n",
      "  batch 160 loss: 0.33499134947880976\n",
      "  batch 170 loss: 0.24354613890950533\n",
      "  batch 180 loss: 0.3218552018786795\n",
      "  batch 190 loss: 0.31391586769518653\n",
      "LOSS train 0.31391586769518653 valid 0.35389787993425414\n",
      "EPOCH 197:\n",
      "  batch 10 loss: 0.23208672949587025\n",
      "  batch 20 loss: 0.0961177521301579\n",
      "  batch 30 loss: 0.058214043924635916\n",
      "  batch 40 loss: 0.2508382951115891\n",
      "  batch 50 loss: 0.07213847934963269\n",
      "  batch 60 loss: 0.027686049070234732\n",
      "  batch 70 loss: 0.07342665168062013\n",
      "  batch 80 loss: 0.053722483357387316\n",
      "  batch 90 loss: 0.1287613473528836\n",
      "  batch 100 loss: 0.09821315928634249\n",
      "  batch 110 loss: 0.17727853620272072\n",
      "  batch 120 loss: 0.11917046916696564\n",
      "  batch 130 loss: 0.07189042598961351\n",
      "  batch 140 loss: 1.0069556116970488\n",
      "  batch 150 loss: 0.17601288574348928\n",
      "  batch 160 loss: 0.1420512000770941\n",
      "  batch 170 loss: 0.17883086189322056\n",
      "  batch 180 loss: 0.15243511348307948\n",
      "  batch 190 loss: 0.35807241101101683\n",
      "LOSS train 0.35807241101101683 valid 0.28729682174459925\n",
      "EPOCH 198:\n",
      "  batch 10 loss: 0.2120851613337436\n",
      "  batch 20 loss: 0.22152975662320387\n",
      "  batch 30 loss: 0.20059759546056738\n",
      "  batch 40 loss: 0.1387013344858133\n",
      "  batch 50 loss: 0.35387747156692057\n",
      "  batch 60 loss: 0.3507548768263291\n",
      "  batch 70 loss: 0.1610349306723947\n",
      "  batch 80 loss: 0.15572035869899992\n",
      "  batch 90 loss: 0.4491814852048265\n",
      "  batch 100 loss: 0.20663757491238358\n",
      "  batch 110 loss: 0.05547990459016035\n",
      "  batch 120 loss: 0.4894490817282417\n",
      "  batch 130 loss: 0.14427226949428587\n",
      "  batch 140 loss: 0.06670460909967915\n",
      "  batch 150 loss: 0.09701659601778374\n",
      "  batch 160 loss: 0.06447344474518105\n",
      "  batch 170 loss: 0.2004609462170265\n",
      "  batch 180 loss: 0.22119096197857288\n",
      "  batch 190 loss: 0.07414758984878063\n",
      "LOSS train 0.07414758984878063 valid 7.118876919084624\n",
      "EPOCH 199:\n",
      "  batch 10 loss: 1.4765787853068786\n",
      "  batch 20 loss: 0.14427756751379092\n",
      "  batch 30 loss: 0.08966928010650008\n",
      "  batch 40 loss: 0.25184759506482807\n",
      "  batch 50 loss: 0.2552154376593535\n",
      "  batch 60 loss: 0.122237605164446\n",
      "  batch 70 loss: 0.3345066530855547\n",
      "  batch 80 loss: 0.520359934935368\n",
      "  batch 90 loss: 0.1807264707995273\n",
      "  batch 100 loss: 0.191349855585986\n",
      "  batch 110 loss: 0.11941959466957996\n",
      "  batch 120 loss: 0.2754688792887464\n",
      "  batch 130 loss: 0.057904032700753304\n",
      "  batch 140 loss: 0.18461180192534812\n",
      "  batch 150 loss: 0.11670643114848644\n",
      "  batch 160 loss: 0.10545674775121369\n",
      "  batch 170 loss: 0.09015253175457474\n",
      "  batch 180 loss: 0.09590313637372674\n",
      "  batch 190 loss: 0.12427297335261756\n",
      "LOSS train 0.12427297335261756 valid 0.2994758873882499\n",
      "EPOCH 200:\n",
      "  batch 10 loss: 0.1744795733428873\n",
      "  batch 20 loss: 0.06834613454298051\n",
      "  batch 30 loss: 0.14753303230709208\n",
      "  batch 40 loss: 0.050931878839969615\n",
      "  batch 50 loss: 0.1242417896232837\n",
      "  batch 60 loss: 0.08512708478501736\n",
      "  batch 70 loss: 0.07172535184672597\n",
      "  batch 80 loss: 0.10569101835117181\n",
      "  batch 90 loss: 0.08742590431593271\n",
      "  batch 100 loss: 0.13965718694817042\n",
      "  batch 110 loss: 0.08106994269955976\n",
      "  batch 120 loss: 0.14507364469968706\n",
      "  batch 130 loss: 0.16025694720751743\n",
      "  batch 140 loss: 0.05989814657623356\n",
      "  batch 150 loss: 0.1666420874225416\n",
      "  batch 160 loss: 0.16042272392880932\n",
      "  batch 170 loss: 0.13002096520633585\n",
      "  batch 180 loss: 0.21808847707416135\n",
      "  batch 190 loss: 0.3415873918131524\n",
      "LOSS train 0.3415873918131524 valid 0.3412635764273091\n",
      "EPOCH 201:\n",
      "  batch 10 loss: 0.11860337013056324\n",
      "  batch 20 loss: 0.2268472976611065\n",
      "  batch 30 loss: 0.21755628455116494\n",
      "  batch 40 loss: 0.06237459231920184\n",
      "  batch 50 loss: 0.07342633078292238\n",
      "  batch 60 loss: 0.10810373173860625\n",
      "  batch 70 loss: 0.3917602739459426\n",
      "  batch 80 loss: 0.06684283631693688\n",
      "  batch 90 loss: 0.09699454464598603\n",
      "  batch 100 loss: 0.13839695760466383\n",
      "  batch 110 loss: 0.36891674628469673\n",
      "  batch 120 loss: 0.14743745319810841\n",
      "  batch 130 loss: 0.12649650361163367\n",
      "  batch 140 loss: 0.24149623080338642\n",
      "  batch 150 loss: 0.14435788468289842\n",
      "  batch 160 loss: 0.17525907020472004\n",
      "  batch 170 loss: 0.03822400823501084\n",
      "  batch 180 loss: 0.2844977343493156\n",
      "  batch 190 loss: 0.08424828184233775\n",
      "LOSS train 0.08424828184233775 valid 0.3305256756898328\n",
      "EPOCH 202:\n",
      "  batch 10 loss: 0.036469546336456916\n",
      "  batch 20 loss: 0.09624104935828655\n",
      "  batch 30 loss: 0.22446065630501835\n",
      "  batch 40 loss: 0.03129499012356973\n",
      "  batch 50 loss: 0.2238330231083694\n",
      "  batch 60 loss: 0.26379213909822286\n",
      "  batch 70 loss: 0.1521451903484376\n",
      "  batch 80 loss: 0.1266859039744304\n",
      "  batch 90 loss: 0.05714250311393698\n",
      "  batch 100 loss: 0.10371279219116332\n",
      "  batch 110 loss: 0.07502051506671706\n",
      "  batch 120 loss: 0.02542671616959069\n",
      "  batch 130 loss: 0.07130503634516572\n",
      "  batch 140 loss: 0.033960665627182605\n",
      "  batch 150 loss: 0.0732167946651316\n",
      "  batch 160 loss: 0.028784543992401268\n",
      "  batch 170 loss: 0.09997437900942714\n",
      "  batch 180 loss: 0.15625799849030955\n",
      "  batch 190 loss: 0.06510245114786813\n",
      "LOSS train 0.06510245114786813 valid 0.34810157193451463\n",
      "EPOCH 203:\n",
      "  batch 10 loss: 0.14536520646452117\n",
      "  batch 20 loss: 0.9620487651611939\n",
      "  batch 30 loss: 0.3656823899731535\n",
      "  batch 40 loss: 0.07853280639392324\n",
      "  batch 50 loss: 0.19975047468306\n",
      "  batch 60 loss: 0.15437871435099168\n",
      "  batch 70 loss: 0.13707138956672224\n",
      "  batch 80 loss: 0.2153897440601213\n",
      "  batch 90 loss: 0.050965642946513866\n",
      "  batch 100 loss: 0.09168638779001412\n",
      "  batch 110 loss: 0.1305571497126948\n",
      "  batch 120 loss: 0.13933615675068722\n",
      "  batch 130 loss: 0.1123655145333032\n",
      "  batch 140 loss: 0.08743976231849046\n",
      "  batch 150 loss: 0.14258905544011213\n",
      "  batch 160 loss: 0.17419182505400385\n",
      "  batch 170 loss: 0.07239209528779611\n",
      "  batch 180 loss: 0.06793750239203292\n",
      "  batch 190 loss: 0.058423245814719846\n",
      "LOSS train 0.058423245814719846 valid 0.3418983037392641\n",
      "EPOCH 204:\n",
      "  batch 10 loss: 0.4030491352319586\n",
      "  batch 20 loss: 0.19487566535981388\n",
      "  batch 30 loss: 0.07065757467971708\n",
      "  batch 40 loss: 0.16594513897480284\n",
      "  batch 50 loss: 0.12318400954609388\n",
      "  batch 60 loss: 0.19013029796314954\n",
      "  batch 70 loss: 0.206560061145251\n",
      "  batch 80 loss: 0.2809016660708039\n",
      "  batch 90 loss: 0.8320127924257121\n",
      "  batch 100 loss: 0.4068427489003625\n",
      "  batch 110 loss: 0.2514071632814641\n",
      "  batch 120 loss: 0.13505944306253354\n",
      "  batch 130 loss: 0.38712448850742476\n",
      "  batch 140 loss: 0.3978631829377264\n",
      "  batch 150 loss: 0.14949767159444036\n",
      "  batch 160 loss: 0.1934690563724871\n",
      "  batch 170 loss: 0.15084153936877556\n",
      "  batch 180 loss: 0.11104275398413392\n",
      "  batch 190 loss: 0.047920982762116184\n",
      "LOSS train 0.047920982762116184 valid 0.28165345409970893\n",
      "EPOCH 205:\n",
      "  batch 10 loss: 0.2697318399382084\n",
      "  batch 20 loss: 0.06689646281904515\n",
      "  batch 30 loss: 0.05991616995092954\n",
      "  batch 40 loss: 0.05170777225403071\n",
      "  batch 50 loss: 0.1089588547508356\n",
      "  batch 60 loss: 0.14477712788554697\n",
      "  batch 70 loss: 0.20871724245295412\n",
      "  batch 80 loss: 0.2738159745689245\n",
      "  batch 90 loss: 0.0699238178393898\n",
      "  batch 100 loss: 0.18919512863435556\n",
      "  batch 110 loss: 0.320623651650385\n",
      "  batch 120 loss: 0.20061476749187931\n",
      "  batch 130 loss: 0.0821074370624956\n",
      "  batch 140 loss: 0.12510211957242062\n",
      "  batch 150 loss: 0.0820209966228731\n",
      "  batch 160 loss: 0.11431130711607693\n",
      "  batch 170 loss: 0.16641529520165932\n",
      "  batch 180 loss: 0.09851105275101872\n",
      "  batch 190 loss: 0.18420517961458244\n",
      "LOSS train 0.18420517961458244 valid 0.26226525963974356\n",
      "EPOCH 206:\n",
      "  batch 10 loss: 0.03911533126095605\n",
      "  batch 20 loss: 0.08676442840014716\n",
      "  batch 30 loss: 0.20659951395537063\n",
      "  batch 40 loss: 0.11551586683117421\n",
      "  batch 50 loss: 0.10631146580944914\n",
      "  batch 60 loss: 0.13457467411353718\n",
      "  batch 70 loss: 0.25631412440798157\n",
      "  batch 80 loss: 0.05305558707800628\n",
      "  batch 90 loss: 0.20231826527115118\n",
      "  batch 100 loss: 0.08923338534220875\n",
      "  batch 110 loss: 0.06186728450543342\n",
      "  batch 120 loss: 0.10547536680742269\n",
      "  batch 130 loss: 0.1112634714942942\n",
      "  batch 140 loss: 0.23388714648717723\n",
      "  batch 150 loss: 0.15896744448800745\n",
      "  batch 160 loss: 0.22254195022433124\n",
      "  batch 170 loss: 0.028764894109372108\n",
      "  batch 180 loss: 0.10670480141234293\n",
      "  batch 190 loss: 0.10860099800675016\n",
      "LOSS train 0.10860099800675016 valid 0.2901507451117071\n",
      "EPOCH 207:\n",
      "  batch 10 loss: 0.045462760339341914\n",
      "  batch 20 loss: 0.23312849672274752\n",
      "  batch 30 loss: 0.13365983165040235\n",
      "  batch 40 loss: 0.05894034059203932\n",
      "  batch 50 loss: 0.06934363472219048\n",
      "  batch 60 loss: 0.16058889141115743\n",
      "  batch 70 loss: 0.12287535804407526\n",
      "  batch 80 loss: 0.27066081380962714\n",
      "  batch 90 loss: 0.04862276540611674\n",
      "  batch 100 loss: 0.0577131917434599\n",
      "  batch 110 loss: 0.1236256382495867\n",
      "  batch 120 loss: 0.3035848060262708\n",
      "  batch 130 loss: 0.1399506056539394\n",
      "  batch 140 loss: 0.037940553696330426\n",
      "  batch 150 loss: 0.23899441544485853\n",
      "  batch 160 loss: 0.26502467388900186\n",
      "  batch 170 loss: 0.6520225484330467\n",
      "  batch 180 loss: 0.11528113818741303\n",
      "  batch 190 loss: 0.0995016152049871\n",
      "LOSS train 0.0995016152049871 valid 0.2836150618698551\n",
      "EPOCH 208:\n",
      "  batch 10 loss: 0.09086258261907006\n",
      "  batch 20 loss: 0.2955302707243391\n",
      "  batch 30 loss: 0.18679470708627832\n",
      "  batch 40 loss: 0.22579909551582772\n",
      "  batch 50 loss: 0.21527779386433393\n",
      "  batch 60 loss: 0.037801791493802736\n",
      "  batch 70 loss: 0.134481834847702\n",
      "  batch 80 loss: 0.0824153098570605\n",
      "  batch 90 loss: 0.0324033113708083\n",
      "  batch 100 loss: 0.04554941155895449\n",
      "  batch 110 loss: 0.1597031079447163\n",
      "  batch 120 loss: 0.11263638135533256\n",
      "  batch 130 loss: 0.26520095315072467\n",
      "  batch 140 loss: 0.40288714505722967\n",
      "  batch 150 loss: 0.21360417721402883\n",
      "  batch 160 loss: 0.34261146396056574\n",
      "  batch 170 loss: 0.06599677512631388\n",
      "  batch 180 loss: 0.15791494450261326\n",
      "  batch 190 loss: 0.04171153197480635\n",
      "LOSS train 0.04171153197480635 valid 0.2893901445397893\n",
      "EPOCH 209:\n",
      "  batch 10 loss: 0.04220305750470743\n",
      "  batch 20 loss: 0.15556391430613986\n",
      "  batch 30 loss: 0.06709424766986558\n",
      "  batch 40 loss: 0.062184910200176094\n",
      "  batch 50 loss: 0.13383407159162744\n",
      "  batch 60 loss: 0.33230651893600227\n",
      "  batch 70 loss: 0.41244772914719763\n",
      "  batch 80 loss: 0.3284926604268549\n",
      "  batch 90 loss: 0.5740429222763851\n",
      "  batch 100 loss: 0.05428603808250045\n",
      "  batch 110 loss: 0.09802886130969454\n",
      "  batch 120 loss: 0.08585232700996812\n",
      "  batch 130 loss: 0.2235900935325617\n",
      "  batch 140 loss: 0.1418579621099525\n",
      "  batch 150 loss: 0.06792120117788727\n",
      "  batch 160 loss: 0.05438994067230851\n",
      "  batch 170 loss: 0.047283288069331776\n",
      "  batch 180 loss: 0.06389865080304843\n",
      "  batch 190 loss: 0.14078037749354735\n",
      "LOSS train 0.14078037749354735 valid 0.2875768310990073\n",
      "EPOCH 210:\n",
      "  batch 10 loss: 0.050238856084081365\n",
      "  batch 20 loss: 0.10057178924841992\n",
      "  batch 30 loss: 0.13963724103668937\n",
      "  batch 40 loss: 0.043172449760572815\n",
      "  batch 50 loss: 0.028363894900937225\n",
      "  batch 60 loss: 0.26769782329688496\n",
      "  batch 70 loss: 0.04134259807560738\n",
      "  batch 80 loss: 0.10354695232076665\n",
      "  batch 90 loss: 0.07597218432711088\n",
      "  batch 100 loss: 0.047635186299066844\n",
      "  batch 110 loss: 0.06015714515724539\n",
      "  batch 120 loss: 0.05310796499470598\n",
      "  batch 130 loss: 0.07772342810330883\n",
      "  batch 140 loss: 0.14113949058373693\n",
      "  batch 150 loss: 0.03965612423271523\n",
      "  batch 160 loss: 0.138846083936005\n",
      "  batch 170 loss: 0.1582892306433223\n",
      "  batch 180 loss: 0.2159810834193195\n",
      "  batch 190 loss: 0.12269220451335058\n",
      "LOSS train 0.12269220451335058 valid 0.30549869137854996\n",
      "EPOCH 211:\n",
      "  batch 10 loss: 0.04316409658163138\n",
      "  batch 20 loss: 0.04445975060555156\n",
      "  batch 30 loss: 0.06232369748013298\n",
      "  batch 40 loss: 0.2400058823474865\n",
      "  batch 50 loss: 0.16453126430133125\n",
      "  batch 60 loss: 0.11709473189657729\n",
      "  batch 70 loss: 0.021917157936104558\n",
      "  batch 80 loss: 0.04983038986100326\n",
      "  batch 90 loss: 0.061874053342489785\n",
      "  batch 100 loss: 0.13348344945302415\n",
      "  batch 110 loss: 0.20997170856971917\n",
      "  batch 120 loss: 0.06790237088757749\n",
      "  batch 130 loss: 0.2030127819727568\n",
      "  batch 140 loss: 0.07861723335995521\n",
      "  batch 150 loss: 0.06220241875698775\n",
      "  batch 160 loss: 0.12350008888761295\n",
      "  batch 170 loss: 0.1089558472979661\n",
      "  batch 180 loss: 0.10343936865760953\n",
      "  batch 190 loss: 0.09267586255941751\n",
      "LOSS train 0.09267586255941751 valid 0.38309943235284055\n",
      "EPOCH 212:\n",
      "  batch 10 loss: 0.02308259048677428\n",
      "  batch 20 loss: 0.04753664886986826\n",
      "  batch 30 loss: 0.09233341998146898\n",
      "  batch 40 loss: 0.018489534807304153\n",
      "  batch 50 loss: 0.22254512554281974\n",
      "  batch 60 loss: 0.3179301079294419\n",
      "  batch 70 loss: 0.3287019192383468\n",
      "  batch 80 loss: 0.04641927549455431\n",
      "  batch 90 loss: 0.15311666607140068\n",
      "  batch 100 loss: 0.2860766223868268\n",
      "  batch 110 loss: 0.119262307171811\n",
      "  batch 120 loss: 0.12600651771135746\n",
      "  batch 130 loss: 0.02902299411216518\n",
      "  batch 140 loss: 0.11214751943215333\n",
      "  batch 150 loss: 0.26748753210113135\n",
      "  batch 160 loss: 0.03273685320618824\n",
      "  batch 170 loss: 0.16605187670647864\n",
      "  batch 180 loss: 0.1611133797072398\n",
      "  batch 190 loss: 0.16372417301632822\n",
      "LOSS train 0.16372417301632822 valid 0.2518720881682664\n",
      "EPOCH 213:\n",
      "  batch 10 loss: 0.12473176388039064\n",
      "  batch 20 loss: 0.1455228504470142\n",
      "  batch 30 loss: 0.15439671701574298\n",
      "  batch 40 loss: 0.09890077346415183\n",
      "  batch 50 loss: 0.04864056587302912\n",
      "  batch 60 loss: 0.06721985979074815\n",
      "  batch 70 loss: 0.06941101542017805\n",
      "  batch 80 loss: 0.0951843173974332\n",
      "  batch 90 loss: 0.21197556806982476\n",
      "  batch 100 loss: 0.2628261220639615\n",
      "  batch 110 loss: 0.09888166013586215\n",
      "  batch 120 loss: 0.04131315663998976\n",
      "  batch 130 loss: 0.20589453625702844\n",
      "  batch 140 loss: 0.29091919849997794\n",
      "  batch 150 loss: 0.15924998704522295\n",
      "  batch 160 loss: 0.10887011321908631\n",
      "  batch 170 loss: 0.033295079800382155\n",
      "  batch 180 loss: 0.16210322617735073\n",
      "  batch 190 loss: 0.05903517194324195\n",
      "LOSS train 0.05903517194324195 valid 0.32322661132085034\n",
      "EPOCH 214:\n",
      "  batch 10 loss: 0.153087417453969\n",
      "  batch 20 loss: 0.027685643947825155\n",
      "  batch 30 loss: 0.05185486415084597\n",
      "  batch 40 loss: 0.08656702122443675\n",
      "  batch 50 loss: 0.27016754397072873\n",
      "  batch 60 loss: 0.08193970050770076\n",
      "  batch 70 loss: 0.008753242214152124\n",
      "  batch 80 loss: 0.14976932474828572\n",
      "  batch 90 loss: 0.050317819319113256\n",
      "  batch 100 loss: 0.03221873751331259\n",
      "  batch 110 loss: 0.013500159625709784\n",
      "  batch 120 loss: 0.04647576436454983\n",
      "  batch 130 loss: 0.07536278231518737\n",
      "  batch 140 loss: 0.07869406195511601\n",
      "  batch 150 loss: 0.0636753640401821\n",
      "  batch 160 loss: 0.0248865299404315\n",
      "  batch 170 loss: 0.23151975601299454\n",
      "  batch 180 loss: 0.21566380451100714\n",
      "  batch 190 loss: 0.12717630129509344\n",
      "LOSS train 0.12717630129509344 valid 1.5232658232379526\n",
      "EPOCH 215:\n",
      "  batch 10 loss: 0.37574523959251566\n",
      "  batch 20 loss: 0.02468267397061936\n",
      "  batch 30 loss: 0.1027063520057709\n",
      "  batch 40 loss: 0.09213248265817811\n",
      "  batch 50 loss: 0.25484788440990086\n",
      "  batch 60 loss: 0.12916890409687767\n",
      "  batch 70 loss: 0.03437963350143036\n",
      "  batch 80 loss: 0.12140909718818875\n",
      "  batch 90 loss: 0.14065767344764027\n",
      "  batch 100 loss: 0.04091703019889792\n",
      "  batch 110 loss: 0.1180314304525382\n",
      "  batch 120 loss: 0.3382626098925357\n",
      "  batch 130 loss: 0.16160714782417926\n",
      "  batch 140 loss: 0.09619400811939158\n",
      "  batch 150 loss: 0.05712126816736145\n",
      "  batch 160 loss: 0.11714767745700101\n",
      "  batch 170 loss: 0.40620446880693634\n",
      "  batch 180 loss: 0.19184382928351623\n",
      "  batch 190 loss: 0.35710374402999606\n",
      "LOSS train 0.35710374402999606 valid 0.4113437069002937\n",
      "EPOCH 216:\n",
      "  batch 10 loss: 0.24085973466571603\n",
      "  batch 20 loss: 0.5491749090811936\n",
      "  batch 30 loss: 0.38815240321637245\n",
      "  batch 40 loss: 0.18465323518989862\n",
      "  batch 50 loss: 0.2807249249679444\n",
      "  batch 60 loss: 0.2884253221075596\n",
      "  batch 70 loss: 0.1931676670386196\n",
      "  batch 80 loss: 0.1812722368834329\n",
      "  batch 90 loss: 0.46792840325915674\n",
      "  batch 100 loss: 0.37924569402816816\n",
      "  batch 110 loss: 0.07111857838026339\n",
      "  batch 120 loss: 0.05644263325350494\n",
      "  batch 130 loss: 0.060529261126612256\n",
      "  batch 140 loss: 0.13441966984200918\n",
      "  batch 150 loss: 0.06590367873509421\n",
      "  batch 160 loss: 0.17035409034197055\n",
      "  batch 170 loss: 0.04610399869284265\n",
      "  batch 180 loss: 0.044654596405598566\n",
      "  batch 190 loss: 0.08926949227025034\n",
      "LOSS train 0.08926949227025034 valid 0.27841328165308055\n",
      "EPOCH 217:\n",
      "  batch 10 loss: 0.056380585886199694\n",
      "  batch 20 loss: 0.08599920771812322\n",
      "  batch 30 loss: 0.0627935311795909\n",
      "  batch 40 loss: 0.055758400200011236\n",
      "  batch 50 loss: 0.0834066666675426\n",
      "  batch 60 loss: 0.061363921380666396\n",
      "  batch 70 loss: 0.0553535748752438\n",
      "  batch 80 loss: 0.15663846199545334\n",
      "  batch 90 loss: 0.09818596097884438\n",
      "  batch 100 loss: 0.024591951331353813\n",
      "  batch 110 loss: 0.12269825228258924\n",
      "  batch 120 loss: 0.07493960970732587\n",
      "  batch 130 loss: 0.12983915679031952\n",
      "  batch 140 loss: 0.4581093774116198\n",
      "  batch 150 loss: 0.17287565804485894\n",
      "  batch 160 loss: 0.2084793638321571\n",
      "  batch 170 loss: 0.05051586919516922\n",
      "  batch 180 loss: 0.14065976917131592\n",
      "  batch 190 loss: 0.1027509795098922\n",
      "LOSS train 0.1027509795098922 valid 0.4360202899551251\n",
      "EPOCH 218:\n",
      "  batch 10 loss: 0.08993317717713581\n",
      "  batch 20 loss: 0.16505401207687082\n",
      "  batch 30 loss: 0.09066192486207\n",
      "  batch 40 loss: 0.09184359833084273\n",
      "  batch 50 loss: 0.02769772176452534\n",
      "  batch 60 loss: 0.17930573048465703\n",
      "  batch 70 loss: 0.08804167643847904\n",
      "  batch 80 loss: 0.033758877334867064\n",
      "  batch 90 loss: 0.08883178684791346\n",
      "  batch 100 loss: 0.08183527995024616\n",
      "  batch 110 loss: 0.15835051246751847\n",
      "  batch 120 loss: 0.03743642395588722\n",
      "  batch 130 loss: 0.08233638482543029\n",
      "  batch 140 loss: 0.04570691561384592\n",
      "  batch 150 loss: 0.03814865882432059\n",
      "  batch 160 loss: 0.12521191059386183\n",
      "  batch 170 loss: 0.12540117601085968\n",
      "  batch 180 loss: 0.19136850813000592\n",
      "  batch 190 loss: 0.1138914479698542\n",
      "LOSS train 0.1138914479698542 valid 0.3406495776424845\n",
      "EPOCH 219:\n",
      "  batch 10 loss: 0.08279507992193658\n",
      "  batch 20 loss: 0.10161635663145603\n",
      "  batch 30 loss: 0.21437322816057075\n",
      "  batch 40 loss: 0.3896151090558305\n",
      "  batch 50 loss: 0.14572753970123814\n",
      "  batch 60 loss: 0.0938689030799651\n",
      "  batch 70 loss: 0.127564349370914\n",
      "  batch 80 loss: 0.34665468670175414\n",
      "  batch 90 loss: 0.24757355680767432\n",
      "  batch 100 loss: 0.8889243406803871\n",
      "  batch 110 loss: 0.20039238857107194\n",
      "  batch 120 loss: 0.08248088590808038\n",
      "  batch 130 loss: 0.13903517300477689\n",
      "  batch 140 loss: 0.06527439295969088\n",
      "  batch 150 loss: 0.058673164550236835\n",
      "  batch 160 loss: 0.13116018437221993\n",
      "  batch 170 loss: 0.12974727929467916\n",
      "  batch 180 loss: 0.057654909048278566\n",
      "  batch 190 loss: 0.12417942651177327\n",
      "LOSS train 0.12417942651177327 valid 0.3179462320505833\n",
      "EPOCH 220:\n",
      "  batch 10 loss: 0.03529087312867887\n",
      "  batch 20 loss: 0.19283149487191623\n",
      "  batch 30 loss: 0.11597606710229229\n",
      "  batch 40 loss: 0.07862298524373727\n",
      "  batch 50 loss: 0.05119492067539113\n",
      "  batch 60 loss: 0.04058276179976019\n",
      "  batch 70 loss: 0.05639370309600054\n",
      "  batch 80 loss: 0.22961188459585175\n",
      "  batch 90 loss: 0.08528718884999761\n",
      "  batch 100 loss: 0.18839459675527906\n",
      "  batch 110 loss: 0.018797291949158534\n",
      "  batch 120 loss: 0.07258675215834956\n",
      "  batch 130 loss: 0.16099190856280074\n",
      "  batch 140 loss: 0.21914904703330224\n",
      "  batch 150 loss: 0.28002458377186484\n",
      "  batch 160 loss: 0.2734478147817754\n",
      "  batch 170 loss: 0.2204014122794419\n",
      "  batch 180 loss: 0.1321200340781729\n",
      "  batch 190 loss: 0.06185972161670179\n",
      "LOSS train 0.06185972161670179 valid 0.7027325760606701\n",
      "EPOCH 221:\n",
      "  batch 10 loss: 0.14067320902822758\n",
      "  batch 20 loss: 0.03458271179588337\n",
      "  batch 30 loss: 0.024210788229720493\n",
      "  batch 40 loss: 0.15285667307489348\n",
      "  batch 50 loss: 0.18072515827225288\n",
      "  batch 60 loss: 0.055074068487738256\n",
      "  batch 70 loss: 0.038084750854091\n",
      "  batch 80 loss: 0.08556098010285496\n",
      "  batch 90 loss: 0.11578337133496461\n",
      "  batch 100 loss: 0.07803987497434264\n",
      "  batch 110 loss: 0.10645830909425058\n",
      "  batch 120 loss: 0.1543048030085629\n",
      "  batch 130 loss: 0.18973275521043415\n",
      "  batch 140 loss: 0.10123545870305861\n",
      "  batch 150 loss: 0.04453664584118542\n",
      "  batch 160 loss: 0.07707405835917598\n",
      "  batch 170 loss: 0.22531056815205375\n",
      "  batch 180 loss: 0.2084271685567046\n",
      "  batch 190 loss: 0.21413369355104805\n",
      "LOSS train 0.21413369355104805 valid 0.27417972493236675\n",
      "EPOCH 222:\n",
      "  batch 10 loss: 0.07910675056273249\n",
      "  batch 20 loss: 0.09955783656018866\n",
      "  batch 30 loss: 0.061500462643516586\n",
      "  batch 40 loss: 0.1481130586049403\n",
      "  batch 50 loss: 0.12499983960278768\n",
      "  batch 60 loss: 0.5512514577217871\n",
      "  batch 70 loss: 0.07467495894788953\n",
      "  batch 80 loss: 0.06915244697484013\n",
      "  batch 90 loss: 0.0825937022592825\n",
      "  batch 100 loss: 0.17301981508962855\n",
      "  batch 110 loss: 0.39256659553957435\n",
      "  batch 120 loss: 0.08753983615333709\n",
      "  batch 130 loss: 0.027035646251169963\n",
      "  batch 140 loss: 0.10529324312572044\n",
      "  batch 150 loss: 0.060577071223065104\n",
      "  batch 160 loss: 0.0881592683074814\n",
      "  batch 170 loss: 0.1458778656593495\n",
      "  batch 180 loss: 0.13737882540430918\n",
      "  batch 190 loss: 0.10767650555226282\n",
      "LOSS train 0.10767650555226282 valid 0.286159623067883\n",
      "EPOCH 223:\n",
      "  batch 10 loss: 0.04618529453073279\n",
      "  batch 20 loss: 0.021560807431524154\n",
      "  batch 30 loss: 0.10775716530647514\n",
      "  batch 40 loss: 0.09072744699351461\n",
      "  batch 50 loss: 0.17646188048295244\n",
      "  batch 60 loss: 0.0507540364199599\n",
      "  batch 70 loss: 0.029841545786155166\n",
      "  batch 80 loss: 0.045474786999511706\n",
      "  batch 90 loss: 0.19597608113595016\n",
      "  batch 100 loss: 0.09409533193677362\n",
      "  batch 110 loss: 0.08422963825741618\n",
      "  batch 120 loss: 0.08575716672720546\n",
      "  batch 130 loss: 0.07798339625660447\n",
      "  batch 140 loss: 0.16284654030055207\n",
      "  batch 150 loss: 0.06554131743268954\n",
      "  batch 160 loss: 0.12055316592351346\n",
      "  batch 170 loss: 0.09589439242404296\n",
      "  batch 180 loss: 0.15524400871672697\n",
      "  batch 190 loss: 0.06310486780225802\n",
      "LOSS train 0.06310486780225802 valid 0.7993614545858353\n",
      "EPOCH 224:\n",
      "  batch 10 loss: 0.6629802956547792\n",
      "  batch 20 loss: 0.11808469668858379\n",
      "  batch 30 loss: 0.03042071397235304\n",
      "  batch 40 loss: 0.029000821666500087\n",
      "  batch 50 loss: 0.10118257656927199\n",
      "  batch 60 loss: 0.17255454497571918\n",
      "  batch 70 loss: 0.30883112409619573\n",
      "  batch 80 loss: 0.06745380229258444\n",
      "  batch 90 loss: 0.3535111893716021\n",
      "  batch 100 loss: 0.11957632346184255\n",
      "  batch 110 loss: 0.04868810154457322\n",
      "  batch 120 loss: 0.08401946302519718\n",
      "  batch 130 loss: 0.08286525390967654\n",
      "  batch 140 loss: 0.04422678188293503\n",
      "  batch 150 loss: 0.07598428236858581\n",
      "  batch 160 loss: 0.1059739016085132\n",
      "  batch 170 loss: 0.04821787364890042\n",
      "  batch 180 loss: 0.033204852688413666\n",
      "  batch 190 loss: 0.022663421435208164\n",
      "LOSS train 0.022663421435208164 valid 4.306433466591557\n",
      "EPOCH 225:\n",
      "  batch 10 loss: 1.4078809325256543\n",
      "  batch 20 loss: 0.25139237177409085\n",
      "  batch 30 loss: 0.06401063338362292\n",
      "  batch 40 loss: 0.13401605872732034\n",
      "  batch 50 loss: 0.22636048124904845\n",
      "  batch 60 loss: 0.2507227145921206\n",
      "  batch 70 loss: 0.14284987490013917\n",
      "  batch 80 loss: 0.047623337243112476\n",
      "  batch 90 loss: 0.12799325799155667\n",
      "  batch 100 loss: 0.06898070146166901\n",
      "  batch 110 loss: 0.06768928934984615\n",
      "  batch 120 loss: 0.02403838633954365\n",
      "  batch 130 loss: 0.037682568104901296\n",
      "  batch 140 loss: 0.07158281760930549\n",
      "  batch 150 loss: 0.26028332976184176\n",
      "  batch 160 loss: 0.28749623471067026\n",
      "  batch 170 loss: 0.16436948319606018\n",
      "  batch 180 loss: 0.21782229933178315\n",
      "  batch 190 loss: 0.060366979008311714\n",
      "LOSS train 0.060366979008311714 valid 0.3104448306591405\n",
      "EPOCH 226:\n",
      "  batch 10 loss: 0.09808516380027185\n",
      "  batch 20 loss: 0.08403447566361137\n",
      "  batch 30 loss: 0.04574511590330985\n",
      "  batch 40 loss: 0.12381004190155806\n",
      "  batch 50 loss: 0.03671326541950748\n",
      "  batch 60 loss: 0.02245922687775419\n",
      "  batch 70 loss: 0.07252249005630347\n",
      "  batch 80 loss: 0.03240220064433288\n",
      "  batch 90 loss: 0.06504606964699633\n",
      "  batch 100 loss: 0.11730172921306803\n",
      "  batch 110 loss: 0.36326916513198737\n",
      "  batch 120 loss: 0.21394067519850068\n",
      "  batch 130 loss: 0.15947423220416113\n",
      "  batch 140 loss: 0.07115387450176058\n",
      "  batch 150 loss: 0.07380545349715248\n",
      "  batch 160 loss: 0.04358716793049098\n",
      "  batch 170 loss: 0.03368222502685967\n",
      "  batch 180 loss: 0.1582698703707024\n",
      "  batch 190 loss: 0.24608746001813414\n",
      "LOSS train 0.24608746001813414 valid 0.3804046837542289\n",
      "EPOCH 227:\n",
      "  batch 10 loss: 0.10160198568692067\n",
      "  batch 20 loss: 0.11640011530531638\n",
      "  batch 30 loss: 0.075346766299117\n",
      "  batch 40 loss: 0.15192205618281918\n",
      "  batch 50 loss: 0.05703360867373704\n",
      "  batch 60 loss: 0.07938035724946531\n",
      "  batch 70 loss: 0.07751909244897774\n",
      "  batch 80 loss: 0.14760206413453716\n",
      "  batch 90 loss: 0.05145712406551865\n",
      "  batch 100 loss: 0.16184459452224473\n",
      "  batch 110 loss: 0.044128969173198127\n",
      "  batch 120 loss: 0.04434925241130259\n",
      "  batch 130 loss: 0.14207726848403582\n",
      "  batch 140 loss: 0.06920278849896704\n",
      "  batch 150 loss: 0.05847239090635412\n",
      "  batch 160 loss: 0.1020816782154725\n",
      "  batch 170 loss: 0.09360393485173972\n",
      "  batch 180 loss: 0.05255174654566872\n",
      "  batch 190 loss: 0.13258455106663405\n",
      "LOSS train 0.13258455106663405 valid 0.3332579220058409\n",
      "EPOCH 228:\n",
      "  batch 10 loss: 0.20043781397666863\n",
      "  batch 20 loss: 0.04244398781147538\n",
      "  batch 30 loss: 0.07970502242542352\n",
      "  batch 40 loss: 0.042332259123713814\n",
      "  batch 50 loss: 0.23017386200617623\n",
      "  batch 60 loss: 0.0395201901309747\n",
      "  batch 70 loss: 0.0805376397974669\n",
      "  batch 80 loss: 0.39548850027051685\n",
      "  batch 90 loss: 0.2659460489737171\n",
      "  batch 100 loss: 0.07174306826941575\n",
      "  batch 110 loss: 0.1192232215887655\n",
      "  batch 120 loss: 0.13148921189899737\n",
      "  batch 130 loss: 0.033262923384609164\n",
      "  batch 140 loss: 0.15533250368612245\n",
      "  batch 150 loss: 0.1579507563067068\n",
      "  batch 160 loss: 0.03211669283699621\n",
      "  batch 170 loss: 0.046505046751690315\n",
      "  batch 180 loss: 0.04796817422725894\n",
      "  batch 190 loss: 0.08742034918286663\n",
      "LOSS train 0.08742034918286663 valid 1.683634385784428\n",
      "EPOCH 229:\n",
      "  batch 10 loss: 0.8312388849051786\n",
      "  batch 20 loss: 0.1999198770952944\n",
      "  batch 30 loss: 0.17034531498429714\n",
      "  batch 40 loss: 0.622816413687542\n",
      "  batch 50 loss: 0.06885267102697981\n",
      "  batch 60 loss: 0.06026165750029122\n",
      "  batch 70 loss: 0.03397512068326023\n",
      "  batch 80 loss: 0.20840771679950193\n",
      "  batch 90 loss: 0.19853759295728537\n",
      "  batch 100 loss: 0.1469563835158624\n",
      "  batch 110 loss: 0.10105394415131741\n",
      "  batch 120 loss: 0.15516852711925821\n",
      "  batch 130 loss: 0.13161330649269304\n",
      "  batch 140 loss: 0.07889637536445662\n",
      "  batch 150 loss: 0.03591666331349188\n",
      "  batch 160 loss: 0.26513025496336695\n",
      "  batch 170 loss: 0.2503184283241353\n",
      "  batch 180 loss: 0.15740457161041377\n",
      "  batch 190 loss: 0.15466416073035133\n",
      "LOSS train 0.15466416073035133 valid 0.548662853418218\n",
      "EPOCH 230:\n",
      "  batch 10 loss: 0.31265259287789604\n",
      "  batch 20 loss: 0.18410966052900904\n",
      "  batch 30 loss: 0.1333280759378795\n",
      "  batch 40 loss: 0.0432064877859375\n",
      "  batch 50 loss: 0.051808071188321494\n",
      "  batch 60 loss: 0.06980773839432004\n",
      "  batch 70 loss: 0.06547180176821712\n",
      "  batch 80 loss: 0.04795470182298232\n",
      "  batch 90 loss: 0.07942318160203285\n",
      "  batch 100 loss: 0.07366430125198349\n",
      "  batch 110 loss: 0.15694320988802132\n",
      "  batch 120 loss: 0.11018508438396565\n",
      "  batch 130 loss: 0.10598080966979069\n",
      "  batch 140 loss: 0.14554538692777896\n",
      "  batch 150 loss: 0.0485687152402761\n",
      "  batch 160 loss: 0.14940467938813526\n",
      "  batch 170 loss: 0.1544690846928461\n",
      "  batch 180 loss: 0.11303530162012976\n",
      "  batch 190 loss: 0.07194608388962251\n",
      "LOSS train 0.07194608388962251 valid 0.2649149694090202\n",
      "EPOCH 231:\n",
      "  batch 10 loss: 0.026889781326917727\n",
      "  batch 20 loss: 0.016825985833384037\n",
      "  batch 30 loss: 0.5873304946254848\n",
      "  batch 40 loss: 0.641472419924412\n",
      "  batch 50 loss: 0.17106845467478707\n",
      "  batch 60 loss: 0.05819054748317285\n",
      "  batch 70 loss: 0.07056628251839356\n",
      "  batch 80 loss: 0.18117278878498838\n",
      "  batch 90 loss: 0.03448164316191651\n",
      "  batch 100 loss: 0.06777235669380843\n",
      "  batch 110 loss: 0.022767684737891612\n",
      "  batch 120 loss: 0.07648689883789075\n",
      "  batch 130 loss: 0.011239945457418798\n",
      "  batch 140 loss: 0.02917464024558285\n",
      "  batch 150 loss: 0.1297338606541075\n",
      "  batch 160 loss: 0.06824705962917506\n",
      "  batch 170 loss: 0.08757052939872664\n",
      "  batch 180 loss: 0.04845922556651203\n",
      "  batch 190 loss: 0.11056138679996366\n",
      "LOSS train 0.11056138679996366 valid 0.3283788313077783\n",
      "EPOCH 232:\n",
      "  batch 10 loss: 0.019634748623820995\n",
      "  batch 20 loss: 0.1661243822150027\n",
      "  batch 30 loss: 0.08736946830058515\n",
      "  batch 40 loss: 0.23807609969932172\n",
      "  batch 50 loss: 0.09484063363597244\n",
      "  batch 60 loss: 0.2184944667025775\n",
      "  batch 70 loss: 0.20394342722670444\n",
      "  batch 80 loss: 0.06291999174436569\n",
      "  batch 90 loss: 0.2034208493038136\n",
      "  batch 100 loss: 0.0773052626245601\n",
      "  batch 110 loss: 0.057831551040283105\n",
      "  batch 120 loss: 0.06604451413495553\n",
      "  batch 130 loss: 0.06242673943866066\n",
      "  batch 140 loss: 0.05189378770091935\n",
      "  batch 150 loss: 0.04009231664467734\n",
      "  batch 160 loss: 0.03669763992625121\n",
      "  batch 170 loss: 0.05040222558891401\n",
      "  batch 180 loss: 0.10550284241858207\n",
      "  batch 190 loss: 0.4581264771593766\n",
      "LOSS train 0.4581264771593766 valid 7.177275974402342\n",
      "EPOCH 233:\n",
      "  batch 10 loss: 3.113650387234293\n",
      "  batch 20 loss: 0.32435159868164193\n",
      "  batch 30 loss: 0.4188433151008212\n",
      "  batch 40 loss: 0.25326868281308634\n",
      "  batch 50 loss: 0.2665056218526843\n",
      "  batch 60 loss: 0.2012752229742091\n",
      "  batch 70 loss: 0.11530220677523176\n",
      "  batch 80 loss: 0.07997900917544029\n",
      "  batch 90 loss: 0.07198675962963534\n",
      "  batch 100 loss: 0.1918492485461684\n",
      "  batch 110 loss: 0.14821210198206244\n",
      "  batch 120 loss: 0.20027825472816402\n",
      "  batch 130 loss: 0.14602997707088433\n",
      "  batch 140 loss: 0.1315476250472784\n",
      "  batch 150 loss: 0.12330374729363029\n",
      "  batch 160 loss: 0.13681397897889838\n",
      "  batch 170 loss: 0.19748355928604724\n",
      "  batch 180 loss: 0.055654346910068854\n",
      "  batch 190 loss: 0.11429429171948868\n",
      "LOSS train 0.11429429171948868 valid 0.3162040802093228\n",
      "EPOCH 234:\n",
      "  batch 10 loss: 0.05232500559532127\n",
      "  batch 20 loss: 0.09679720287567761\n",
      "  batch 30 loss: 0.04984545613756382\n",
      "  batch 40 loss: 0.0634310500092397\n",
      "  batch 50 loss: 0.03659074450606568\n",
      "  batch 60 loss: 0.2130027377098031\n",
      "  batch 70 loss: 0.05526809599250555\n",
      "  batch 80 loss: 0.10837134311614136\n",
      "  batch 90 loss: 0.061174304947826386\n",
      "  batch 100 loss: 0.06827678504055257\n",
      "  batch 110 loss: 0.08367051025088586\n",
      "  batch 120 loss: 0.05907101506354593\n",
      "  batch 130 loss: 0.1665316701390111\n",
      "  batch 140 loss: 0.11842253478807833\n",
      "  batch 150 loss: 0.24020282525293624\n",
      "  batch 160 loss: 0.13170583164138633\n",
      "  batch 170 loss: 0.07647797023264502\n",
      "  batch 180 loss: 0.0508126521796612\n",
      "  batch 190 loss: 0.05586414992368418\n",
      "LOSS train 0.05586414992368418 valid 0.36682029948771383\n",
      "EPOCH 235:\n",
      "  batch 10 loss: 0.16329618515903804\n",
      "  batch 20 loss: 0.051506656949084115\n",
      "  batch 30 loss: 0.0616597477491041\n",
      "  batch 40 loss: 0.08398149141276008\n",
      "  batch 50 loss: 0.036143574014306526\n",
      "  batch 60 loss: 0.060423050071767646\n",
      "  batch 70 loss: 0.01799977256996499\n",
      "  batch 80 loss: 0.21470659507226628\n",
      "  batch 90 loss: 0.09074759125949186\n",
      "  batch 100 loss: 0.09912468496627298\n",
      "  batch 110 loss: 0.04590761690853924\n",
      "  batch 120 loss: 0.07591796983097084\n",
      "  batch 130 loss: 0.12098790009698632\n",
      "  batch 140 loss: 0.06606414617733662\n",
      "  batch 150 loss: 0.16231388191921498\n",
      "  batch 160 loss: 0.14496654539113935\n",
      "  batch 170 loss: 0.1054959921826594\n",
      "  batch 180 loss: 0.04620014796321357\n",
      "  batch 190 loss: 0.04970557755973459\n",
      "LOSS train 0.04970557755973459 valid 0.24847793590475492\n",
      "EPOCH 236:\n",
      "  batch 10 loss: 0.08017349430620016\n",
      "  batch 20 loss: 0.05544790474511672\n",
      "  batch 30 loss: 0.04110768376594933\n",
      "  batch 40 loss: 0.09516836794351775\n",
      "  batch 50 loss: 0.03469976820716738\n",
      "  batch 60 loss: 0.11970949194268314\n",
      "  batch 70 loss: 0.13315994864856293\n",
      "  batch 80 loss: 0.026019788427629465\n",
      "  batch 90 loss: 0.18751939035184934\n",
      "  batch 100 loss: 0.1583344284537816\n",
      "  batch 110 loss: 0.06123211695194186\n",
      "  batch 120 loss: 0.03570289125764248\n",
      "  batch 130 loss: 0.4068833215519817\n",
      "  batch 140 loss: 0.2990449640422865\n",
      "  batch 150 loss: 0.05162084579460498\n",
      "  batch 160 loss: 0.5623206689708695\n",
      "  batch 170 loss: 0.049075010136903074\n",
      "  batch 180 loss: 0.07846648524407555\n",
      "  batch 190 loss: 0.041144820287672704\n",
      "LOSS train 0.041144820287672704 valid 0.2744975897027218\n",
      "EPOCH 237:\n",
      "  batch 10 loss: 0.03794204758742126\n",
      "  batch 20 loss: 0.03412306023565179\n",
      "  batch 30 loss: 0.04232677986684621\n",
      "  batch 40 loss: 0.03513005878435251\n",
      "  batch 50 loss: 0.049166436844211604\n",
      "  batch 60 loss: 0.12022961233128626\n",
      "  batch 70 loss: 0.06341753637229885\n",
      "  batch 80 loss: 0.03609090684940384\n",
      "  batch 90 loss: 0.0675929351221157\n",
      "  batch 100 loss: 0.08570343013414856\n",
      "  batch 110 loss: 0.18935057917125278\n",
      "  batch 120 loss: 0.1987559187596986\n",
      "  batch 130 loss: 0.0927010675994552\n",
      "  batch 140 loss: 0.26779674286408406\n",
      "  batch 150 loss: 0.1047939550458068\n",
      "  batch 160 loss: 0.04563090849906075\n",
      "  batch 170 loss: 0.02778934183970705\n",
      "  batch 180 loss: 0.03155288960861071\n",
      "  batch 190 loss: 0.20008337891673023\n",
      "LOSS train 0.20008337891673023 valid 0.3025195964067393\n",
      "EPOCH 238:\n",
      "  batch 10 loss: 0.06423503009466458\n",
      "  batch 20 loss: 0.08853405636969\n",
      "  batch 30 loss: 0.12353118942688752\n",
      "  batch 40 loss: 0.058629757256403535\n",
      "  batch 50 loss: 0.025367175377368767\n",
      "  batch 60 loss: 0.0629630851651882\n",
      "  batch 70 loss: 0.05208632878454722\n",
      "  batch 80 loss: 0.020866209815812907\n",
      "  batch 90 loss: 0.07234282992694716\n",
      "  batch 100 loss: 0.27472588385587643\n",
      "  batch 110 loss: 1.4972703834989205\n",
      "  batch 120 loss: 0.34182960044665833\n",
      "  batch 130 loss: 0.15835257282167275\n",
      "  batch 140 loss: 0.04959825079246514\n",
      "  batch 150 loss: 0.16490354646630295\n",
      "  batch 160 loss: 0.04323235263163951\n",
      "  batch 170 loss: 0.1491296289007721\n",
      "  batch 180 loss: 0.05126023759112286\n",
      "  batch 190 loss: 0.04195389293331573\n",
      "LOSS train 0.04195389293331573 valid 0.2925586560040989\n",
      "EPOCH 239:\n",
      "  batch 10 loss: 0.12396425572514999\n",
      "  batch 20 loss: 0.09999267941760195\n",
      "  batch 30 loss: 0.1529164157967216\n",
      "  batch 40 loss: 0.06114754766203987\n",
      "  batch 50 loss: 0.1148757990822105\n",
      "  batch 60 loss: 0.12232816162022572\n",
      "  batch 70 loss: 0.13652138262491462\n",
      "  batch 80 loss: 0.21977678208359067\n",
      "  batch 90 loss: 0.04897055765977711\n",
      "  batch 100 loss: 0.09209456492224036\n",
      "  batch 110 loss: 0.07017235174016605\n",
      "  batch 120 loss: 0.12943031289996726\n",
      "  batch 130 loss: 0.08928496955795709\n",
      "  batch 140 loss: 0.04302429087820201\n",
      "  batch 150 loss: 0.06120584016571229\n",
      "  batch 160 loss: 0.0340205478485359\n",
      "  batch 170 loss: 0.02739433965621103\n",
      "  batch 180 loss: 0.06856031404349779\n",
      "  batch 190 loss: 0.08365903857365993\n",
      "LOSS train 0.08365903857365993 valid 0.29764316512478217\n",
      "EPOCH 240:\n",
      "  batch 10 loss: 0.0746072650608312\n",
      "  batch 20 loss: 0.10963628016551183\n",
      "  batch 30 loss: 0.11556440315616782\n",
      "  batch 40 loss: 0.03308481812064201\n",
      "  batch 50 loss: 0.055072846155121626\n",
      "  batch 60 loss: 0.08721563555027387\n",
      "  batch 70 loss: 0.11736171504510366\n",
      "  batch 80 loss: 0.01600550280990092\n",
      "  batch 90 loss: 0.06867915589627885\n",
      "  batch 100 loss: 0.08721013780113936\n",
      "  batch 110 loss: 0.061763882369268684\n",
      "  batch 120 loss: 0.022690637386585878\n",
      "  batch 130 loss: 0.051260894848610405\n",
      "  batch 140 loss: 0.0501428896989637\n",
      "  batch 150 loss: 0.053562388194313824\n",
      "  batch 160 loss: 0.12284345266491528\n",
      "  batch 170 loss: 0.07801790765629449\n",
      "  batch 180 loss: 0.10955560454756323\n",
      "  batch 190 loss: 0.3191666454062215\n",
      "LOSS train 0.3191666454062215 valid 0.25559686539237886\n",
      "EPOCH 241:\n",
      "  batch 10 loss: 0.15520388625620427\n",
      "  batch 20 loss: 0.08534307281477141\n",
      "  batch 30 loss: 0.027036522889102344\n",
      "  batch 40 loss: 0.05438272121236878\n",
      "  batch 50 loss: 0.2607989322292156\n",
      "  batch 60 loss: 0.12073112884154398\n",
      "  batch 70 loss: 0.04966809192101209\n",
      "  batch 80 loss: 0.032567145281518606\n",
      "  batch 90 loss: 0.15810528657730175\n",
      "  batch 100 loss: 0.20213413725516602\n",
      "  batch 110 loss: 0.1019703653702777\n",
      "  batch 120 loss: 0.06829098475900537\n",
      "  batch 130 loss: 0.0667605636108874\n",
      "  batch 140 loss: 0.03042949587070325\n",
      "  batch 150 loss: 0.0671886033629164\n",
      "  batch 160 loss: 0.12298737753990281\n",
      "  batch 170 loss: 0.09336748512410509\n",
      "  batch 180 loss: 0.3104706654713937\n",
      "  batch 190 loss: 0.12464575481330939\n",
      "LOSS train 0.12464575481330939 valid 0.3023903382909732\n",
      "EPOCH 242:\n",
      "  batch 10 loss: 0.02668750616849138\n",
      "  batch 20 loss: 0.014178697275360719\n",
      "  batch 30 loss: 0.07371776049476467\n",
      "  batch 40 loss: 0.1031328071899111\n",
      "  batch 50 loss: 0.0987437690543004\n",
      "  batch 60 loss: 0.039496672253517315\n",
      "  batch 70 loss: 0.1788911813252696\n",
      "  batch 80 loss: 0.17898248333578123\n",
      "  batch 90 loss: 0.14002199107356433\n",
      "  batch 100 loss: 0.10120928254647196\n",
      "  batch 110 loss: 0.12054350416153739\n",
      "  batch 120 loss: 0.04534078493477267\n",
      "  batch 130 loss: 0.06761554485201486\n",
      "  batch 140 loss: 0.07851056254912692\n",
      "  batch 150 loss: 0.26694925210708786\n",
      "  batch 160 loss: 0.036462028566393204\n",
      "  batch 170 loss: 0.06749967966648\n",
      "  batch 180 loss: 0.14096110934478928\n",
      "  batch 190 loss: 0.0715532244066793\n",
      "LOSS train 0.0715532244066793 valid 0.39637755111744116\n",
      "EPOCH 243:\n",
      "  batch 10 loss: 0.10997372646584154\n",
      "  batch 20 loss: 0.16702648375862736\n",
      "  batch 30 loss: 0.16341017686811482\n",
      "  batch 40 loss: 0.027590492236186037\n",
      "  batch 50 loss: 0.12111082236060611\n",
      "  batch 60 loss: 0.02829830801706521\n",
      "  batch 70 loss: 0.023437693516370928\n",
      "  batch 80 loss: 0.11969409860945461\n",
      "  batch 90 loss: 0.23274945871621638\n",
      "  batch 100 loss: 0.12009778501578694\n",
      "  batch 110 loss: 0.09365767270127208\n",
      "  batch 120 loss: 0.05905876883834651\n",
      "  batch 130 loss: 0.08767759963265007\n",
      "  batch 140 loss: 0.16980743711792456\n",
      "  batch 150 loss: 0.7108106346712247\n",
      "  batch 160 loss: 0.07462973682177107\n",
      "  batch 170 loss: 0.04202127402286351\n",
      "  batch 180 loss: 0.07706274568736263\n",
      "  batch 190 loss: 0.0729414782674894\n",
      "LOSS train 0.0729414782674894 valid 0.3283896634588767\n",
      "EPOCH 244:\n",
      "  batch 10 loss: 0.05741964519670546\n",
      "  batch 20 loss: 0.14591397216490803\n",
      "  batch 30 loss: 0.1031831890909416\n",
      "  batch 40 loss: 0.14725971497464344\n",
      "  batch 50 loss: 0.045882516495021265\n",
      "  batch 60 loss: 0.07268776195301144\n",
      "  batch 70 loss: 0.0400700852994305\n",
      "  batch 80 loss: 0.019019575892571083\n",
      "  batch 90 loss: 0.025571483346129752\n",
      "  batch 100 loss: 0.09939080022130611\n",
      "  batch 110 loss: 0.039426850580886706\n",
      "  batch 120 loss: 0.10543518704625968\n",
      "  batch 130 loss: 0.031197073480416292\n",
      "  batch 140 loss: 0.05726411379348519\n",
      "  batch 150 loss: 0.020480624944684677\n",
      "  batch 160 loss: 0.03352307787465634\n",
      "  batch 170 loss: 0.051810438529810196\n",
      "  batch 180 loss: 0.18294868427359462\n",
      "  batch 190 loss: 0.14902252482365838\n",
      "LOSS train 0.14902252482365838 valid 9.038452093805148\n",
      "EPOCH 245:\n",
      "  batch 10 loss: 2.089717818700187\n",
      "  batch 20 loss: 0.3366182844139985\n",
      "  batch 30 loss: 0.2471145394791165\n",
      "  batch 40 loss: 0.09795477262132408\n",
      "  batch 50 loss: 0.5119123369138834\n",
      "  batch 60 loss: 0.048788389123092205\n",
      "  batch 70 loss: 0.3044109234922871\n",
      "  batch 80 loss: 0.33206629084497763\n",
      "  batch 90 loss: 0.08560278995601038\n",
      "  batch 100 loss: 0.19963671594243806\n",
      "  batch 110 loss: 0.35534380003431354\n",
      "  batch 120 loss: 0.0920668284787098\n",
      "  batch 130 loss: 0.11752683035301743\n",
      "  batch 140 loss: 0.04188168521118314\n",
      "  batch 150 loss: 0.10056779572369123\n",
      "  batch 160 loss: 0.18817750430762317\n",
      "  batch 170 loss: 0.04424120466203476\n",
      "  batch 180 loss: 0.09751888381251775\n",
      "  batch 190 loss: 0.07974299700836127\n",
      "LOSS train 0.07974299700836127 valid 0.286384433565576\n",
      "EPOCH 246:\n",
      "  batch 10 loss: 0.11910019725246457\n",
      "  batch 20 loss: 0.08918666826539265\n",
      "  batch 30 loss: 0.11048603956250673\n",
      "  batch 40 loss: 0.03602690745914287\n",
      "  batch 50 loss: 0.17857866709937298\n",
      "  batch 60 loss: 0.03809720519582242\n",
      "  batch 70 loss: 0.29120182775009196\n",
      "  batch 80 loss: 0.16847623321764332\n",
      "  batch 90 loss: 0.32535703386411113\n",
      "  batch 100 loss: 0.1794162105145915\n",
      "  batch 110 loss: 0.1225070506297925\n",
      "  batch 120 loss: 0.038346045977596076\n",
      "  batch 130 loss: 0.07883980296974187\n",
      "  batch 140 loss: 0.24334546363897971\n",
      "  batch 150 loss: 0.05988937983190681\n",
      "  batch 160 loss: 0.01326963158315948\n",
      "  batch 170 loss: 0.08456618842451462\n",
      "  batch 180 loss: 0.2821085985931404\n",
      "  batch 190 loss: 0.13721290407843298\n",
      "LOSS train 0.13721290407843298 valid 0.32094063409953993\n",
      "EPOCH 247:\n",
      "  batch 10 loss: 0.1242614970321938\n",
      "  batch 20 loss: 0.030221906725682857\n",
      "  batch 30 loss: 0.03841075942051475\n",
      "  batch 40 loss: 0.04489133952833981\n",
      "  batch 50 loss: 0.02809019250482834\n",
      "  batch 60 loss: 0.02189925187038284\n",
      "  batch 70 loss: 0.06706775711570571\n",
      "  batch 80 loss: 0.04236097887145718\n",
      "  batch 90 loss: 0.03749039888828065\n",
      "  batch 100 loss: 0.0610431075419001\n",
      "  batch 110 loss: 0.12067907331702372\n",
      "  batch 120 loss: 0.033523973082765224\n",
      "  batch 130 loss: 0.05354974810168187\n",
      "  batch 140 loss: 0.027717951826286936\n",
      "  batch 150 loss: 0.06669611836570084\n",
      "  batch 160 loss: 0.14312611180253043\n",
      "  batch 170 loss: 0.14317991123589308\n",
      "  batch 180 loss: 0.26221783792000225\n",
      "  batch 190 loss: 0.05172407781772108\n",
      "LOSS train 0.05172407781772108 valid 0.4064907727166087\n",
      "EPOCH 248:\n",
      "  batch 10 loss: 0.08855906554399554\n",
      "  batch 20 loss: 0.17199508490905374\n",
      "  batch 30 loss: 0.0705336080865436\n",
      "  batch 40 loss: 0.1119753117007349\n",
      "  batch 50 loss: 0.1590299825377059\n",
      "  batch 60 loss: 0.06639471597045485\n",
      "  batch 70 loss: 0.05306958781442859\n",
      "  batch 80 loss: 0.04189820887713722\n",
      "  batch 90 loss: 0.14089460040586346\n",
      "  batch 100 loss: 0.0395433860917251\n",
      "  batch 110 loss: 0.20271708609457165\n",
      "  batch 120 loss: 0.08643173761583967\n",
      "  batch 130 loss: 0.06753508880960908\n",
      "  batch 140 loss: 0.10188915544777047\n",
      "  batch 150 loss: 0.03161472574799973\n",
      "  batch 160 loss: 0.08100774620918401\n",
      "  batch 170 loss: 0.056086890689545046\n",
      "  batch 180 loss: 0.05730479850600432\n",
      "  batch 190 loss: 0.043844921468189565\n",
      "LOSS train 0.043844921468189565 valid 2.758011812719128\n",
      "EPOCH 249:\n",
      "  batch 10 loss: 1.521073477701293\n",
      "  batch 20 loss: 0.15446220388130313\n",
      "  batch 30 loss: 0.1187252162594632\n",
      "  batch 40 loss: 0.13626015345580528\n",
      "  batch 50 loss: 0.3444782054791176\n",
      "  batch 60 loss: 0.05999961472205086\n",
      "  batch 70 loss: 0.042807096298997746\n",
      "  batch 80 loss: 0.4599695698163032\n",
      "  batch 90 loss: 0.23441543361213918\n",
      "  batch 100 loss: 0.7591432668506165\n",
      "  batch 110 loss: 0.3426818692039205\n",
      "  batch 120 loss: 0.07466208490586723\n",
      "  batch 130 loss: 0.070875018859806\n",
      "  batch 140 loss: 0.11717193009944822\n",
      "  batch 150 loss: 0.13611281177581985\n",
      "  batch 160 loss: 0.038193553461587725\n",
      "  batch 170 loss: 0.07701066814503861\n",
      "  batch 180 loss: 0.1484736596313269\n",
      "  batch 190 loss: 0.0665729095628194\n",
      "LOSS train 0.0665729095628194 valid 0.27368939675754944\n",
      "EPOCH 250:\n",
      "  batch 10 loss: 0.10795987454694114\n",
      "  batch 20 loss: 0.024537937030618195\n",
      "  batch 30 loss: 0.050206513976536374\n",
      "  batch 40 loss: 0.0507024147203083\n",
      "  batch 50 loss: 0.1539400405643221\n",
      "  batch 60 loss: 0.1268922934330476\n",
      "  batch 70 loss: 0.07813546187759642\n",
      "  batch 80 loss: 0.11365454007082007\n",
      "  batch 90 loss: 0.04653835301674007\n",
      "  batch 100 loss: 0.027717198308528167\n",
      "  batch 110 loss: 0.05262086247912521\n",
      "  batch 120 loss: 0.09487994924252234\n",
      "  batch 130 loss: 0.10279642345544744\n",
      "  batch 140 loss: 0.048057360879647605\n",
      "  batch 150 loss: 0.033935389313774066\n",
      "  batch 160 loss: 0.09537285170091518\n",
      "  batch 170 loss: 0.07052848926496154\n",
      "  batch 180 loss: 0.055275850597627144\n",
      "  batch 190 loss: 0.04581319272845121\n",
      "LOSS train 0.04581319272845121 valid 0.2692485329707954\n",
      "EPOCH 251:\n",
      "  batch 10 loss: 0.0118482076831242\n",
      "  batch 20 loss: 0.02658066362582758\n",
      "  batch 30 loss: 0.07773493579143179\n",
      "  batch 40 loss: 0.03201789450679371\n",
      "  batch 50 loss: 0.051280127020663716\n",
      "  batch 60 loss: 0.05740751871053362\n",
      "  batch 70 loss: 0.12436459414736803\n",
      "  batch 80 loss: 0.12758439847253272\n",
      "  batch 90 loss: 0.015679931725082953\n",
      "  batch 100 loss: 0.054309390512332814\n",
      "  batch 110 loss: 0.030139497372056213\n",
      "  batch 120 loss: 0.02183519403442915\n",
      "  batch 130 loss: 0.10001200548171027\n",
      "  batch 140 loss: 0.1184448425898779\n",
      "  batch 150 loss: 0.10071097845076338\n",
      "  batch 160 loss: 0.08762383886689804\n",
      "  batch 170 loss: 0.16527450625708867\n",
      "  batch 180 loss: 0.07073923918107994\n",
      "  batch 190 loss: 0.110914690061324\n",
      "LOSS train 0.110914690061324 valid 0.42672503951652846\n",
      "EPOCH 252:\n",
      "  batch 10 loss: 0.05493217816429023\n",
      "  batch 20 loss: 0.02014470233530119\n",
      "  batch 30 loss: 0.023170345868811636\n",
      "  batch 40 loss: 0.03325170386041236\n",
      "  batch 50 loss: 0.4372427983301577\n",
      "  batch 60 loss: 0.3160801123802912\n",
      "  batch 70 loss: 0.0744636744444506\n",
      "  batch 80 loss: 0.04826148267136432\n",
      "  batch 90 loss: 0.10042716818770714\n",
      "  batch 100 loss: 0.1459433415543117\n",
      "  batch 110 loss: 0.19549931688134164\n",
      "  batch 120 loss: 0.19514636773092206\n",
      "  batch 130 loss: 0.036018106889969204\n",
      "  batch 140 loss: 0.016058881561366433\n",
      "  batch 150 loss: 0.04647700753175741\n",
      "  batch 160 loss: 0.016838956829951714\n",
      "  batch 170 loss: 0.02018469802860636\n",
      "  batch 180 loss: 0.029228478423488014\n",
      "  batch 190 loss: 0.038676313561290955\n",
      "LOSS train 0.038676313561290955 valid 0.28683252967972406\n",
      "EPOCH 253:\n",
      "  batch 10 loss: 0.01980074544069339\n",
      "  batch 20 loss: 0.015450757753364996\n",
      "  batch 30 loss: 0.0791660772294108\n",
      "  batch 40 loss: 0.03695981080600177\n",
      "  batch 50 loss: 0.0762481661462516\n",
      "  batch 60 loss: 0.1042425299257161\n",
      "  batch 70 loss: 0.07216770882623677\n",
      "  batch 80 loss: 0.05448564806122249\n",
      "  batch 90 loss: 0.08982574553722315\n",
      "  batch 100 loss: 0.07041098214220938\n",
      "  batch 110 loss: 0.022874644118240896\n",
      "  batch 120 loss: 0.04983949202427311\n",
      "  batch 130 loss: 0.038562902791636586\n",
      "  batch 140 loss: 0.06513049978149184\n",
      "  batch 150 loss: 0.02082954815922733\n",
      "  batch 160 loss: 0.02866320150392294\n",
      "  batch 170 loss: 0.14523979194314052\n",
      "  batch 180 loss: 0.05154842054549249\n",
      "  batch 190 loss: 0.11527550256228096\n",
      "LOSS train 0.11527550256228096 valid 0.37386117936968294\n",
      "EPOCH 254:\n",
      "  batch 10 loss: 0.021611323613427658\n",
      "  batch 20 loss: 0.012586107933361745\n",
      "  batch 30 loss: 0.009023483612600103\n",
      "  batch 40 loss: 0.1365583032426912\n",
      "  batch 50 loss: 0.12796954670227478\n",
      "  batch 60 loss: 0.2275329584003231\n",
      "  batch 70 loss: 0.2154976899174244\n",
      "  batch 80 loss: 0.1388750859868651\n",
      "  batch 90 loss: 0.032976972425913686\n",
      "  batch 100 loss: 0.0746401838140173\n",
      "  batch 110 loss: 0.09296457764210117\n",
      "  batch 120 loss: 0.04414461209917135\n",
      "  batch 130 loss: 0.04735011405314253\n",
      "  batch 140 loss: 0.08779009983190918\n",
      "  batch 150 loss: 0.5898274771128598\n",
      "  batch 160 loss: 0.2257954873606593\n",
      "  batch 170 loss: 0.02348689112457123\n",
      "  batch 180 loss: 0.11001312069479355\n",
      "  batch 190 loss: 0.023252798404064377\n",
      "LOSS train 0.023252798404064377 valid 0.6419779697520825\n",
      "EPOCH 255:\n",
      "  batch 10 loss: 0.08936657312966645\n",
      "  batch 20 loss: 0.02998755398209596\n",
      "  batch 30 loss: 0.08223831712313086\n",
      "  batch 40 loss: 0.12951499136879646\n",
      "  batch 50 loss: 0.04039650618924497\n",
      "  batch 60 loss: 0.025776961429369295\n",
      "  batch 70 loss: 0.16249013456788292\n",
      "  batch 80 loss: 0.0536854435917121\n",
      "  batch 90 loss: 0.0367624194531345\n",
      "  batch 100 loss: 0.028227357306241173\n",
      "  batch 110 loss: 0.04519547828901978\n",
      "  batch 120 loss: 0.04747653154576028\n",
      "  batch 130 loss: 0.07379415138773311\n",
      "  batch 140 loss: 0.05334194599245166\n",
      "  batch 150 loss: 0.0364822893064229\n",
      "  batch 160 loss: 0.07189089882240154\n",
      "  batch 170 loss: 0.059881241708863796\n",
      "  batch 180 loss: 0.030557255952669494\n",
      "  batch 190 loss: 0.024824628069291067\n",
      "LOSS train 0.024824628069291067 valid 0.32972732089643514\n",
      "EPOCH 256:\n",
      "  batch 10 loss: 0.03972071224319507\n",
      "  batch 20 loss: 0.050139595294808714\n",
      "  batch 30 loss: 0.02880291057775821\n",
      "  batch 40 loss: 0.027910119466992\n",
      "  batch 50 loss: 0.036762787541192665\n",
      "  batch 60 loss: 0.06471985625602769\n",
      "  batch 70 loss: 0.017835627203771763\n",
      "  batch 80 loss: 0.1258924094892791\n",
      "  batch 90 loss: 0.09191040542461906\n",
      "  batch 100 loss: 0.20403867426673514\n",
      "  batch 110 loss: 0.07278668049468706\n",
      "  batch 120 loss: 0.07631446266712771\n",
      "  batch 130 loss: 0.024273863327482558\n",
      "  batch 140 loss: 0.032553421723946485\n",
      "  batch 150 loss: 0.054681528296077885\n",
      "  batch 160 loss: 0.07564968034571393\n",
      "  batch 170 loss: 0.07167126424442341\n",
      "  batch 180 loss: 0.0262829540530106\n",
      "  batch 190 loss: 0.13791839620939755\n",
      "LOSS train 0.13791839620939755 valid 0.29328934255081096\n",
      "EPOCH 257:\n",
      "  batch 10 loss: 0.03924430402554435\n",
      "  batch 20 loss: 0.01737781790952795\n",
      "  batch 30 loss: 0.027762146738950833\n",
      "  batch 40 loss: 0.050395051699862844\n",
      "  batch 50 loss: 0.03880376475186722\n",
      "  batch 60 loss: 0.037881067868283935\n",
      "  batch 70 loss: 0.01673392519104482\n",
      "  batch 80 loss: 0.07288171971117663\n",
      "  batch 90 loss: 0.03132113417559594\n",
      "  batch 100 loss: 0.014577540909431263\n",
      "  batch 110 loss: 0.022584926435376927\n",
      "  batch 120 loss: 0.030516386367389715\n",
      "  batch 130 loss: 0.0526947795636147\n",
      "  batch 140 loss: 0.009571578218566402\n",
      "  batch 150 loss: 0.1104602572979502\n",
      "  batch 160 loss: 0.049935205786459845\n",
      "  batch 170 loss: 0.1592299663340782\n",
      "  batch 180 loss: 0.359291008531099\n",
      "  batch 190 loss: 0.22606291770246684\n",
      "LOSS train 0.22606291770246684 valid 0.2824842949266645\n",
      "EPOCH 258:\n",
      "  batch 10 loss: 0.02823956254954396\n",
      "  batch 20 loss: 0.3168361117231825\n",
      "  batch 30 loss: 0.23710433061969524\n",
      "  batch 40 loss: 0.1192766762456813\n",
      "  batch 50 loss: 0.12379985190548837\n",
      "  batch 60 loss: 0.0637226674374686\n",
      "  batch 70 loss: 0.04620770723196301\n",
      "  batch 80 loss: 0.03372738188899689\n",
      "  batch 90 loss: 0.15919152075487092\n",
      "  batch 100 loss: 0.091123424392174\n",
      "  batch 110 loss: 0.04219962951920024\n",
      "  batch 120 loss: 0.11330498780980633\n",
      "  batch 130 loss: 0.2548190964199307\n",
      "  batch 140 loss: 0.1920062770815548\n",
      "  batch 150 loss: 0.037504157115165526\n",
      "  batch 160 loss: 0.08291671318074804\n",
      "  batch 170 loss: 0.007887795559918231\n",
      "  batch 180 loss: 0.10203478386704319\n",
      "  batch 190 loss: 0.09413218456520536\n",
      "LOSS train 0.09413218456520536 valid 1.140191793643872\n",
      "EPOCH 259:\n",
      "  batch 10 loss: 0.05941659543173046\n",
      "  batch 20 loss: 0.03879197312371616\n",
      "  batch 30 loss: 0.04204363451235622\n",
      "  batch 40 loss: 0.12888026339290945\n",
      "  batch 50 loss: 0.06871459606660153\n",
      "  batch 60 loss: 0.04149399459189453\n",
      "  batch 70 loss: 0.05408893039343621\n",
      "  batch 80 loss: 0.021300549809222958\n",
      "  batch 90 loss: 0.018412719799539446\n",
      "  batch 100 loss: 0.08529813811884424\n",
      "  batch 110 loss: 0.09666527155650328\n",
      "  batch 120 loss: 0.14100225949811146\n",
      "  batch 130 loss: 0.17267313763481978\n",
      "  batch 140 loss: 0.041261172418694515\n",
      "  batch 150 loss: 0.08206316559790139\n",
      "  batch 160 loss: 0.05513751653430106\n",
      "  batch 170 loss: 0.04465830945300695\n",
      "  batch 180 loss: 0.019005903248876167\n",
      "  batch 190 loss: 0.0603213621131772\n",
      "LOSS train 0.0603213621131772 valid 0.27365640978302636\n",
      "EPOCH 260:\n",
      "  batch 10 loss: 0.09926518227307497\n",
      "  batch 20 loss: 0.21275655949900737\n",
      "  batch 30 loss: 0.12221180515061861\n",
      "  batch 40 loss: 0.04520406779970472\n",
      "  batch 50 loss: 0.05701252493795437\n",
      "  batch 60 loss: 0.0519822052039558\n",
      "  batch 70 loss: 0.025171468446387735\n",
      "  batch 80 loss: 0.06629507305005902\n",
      "  batch 90 loss: 0.024285431432917903\n",
      "  batch 100 loss: 0.06202889343035167\n",
      "  batch 110 loss: 0.04426633348626865\n",
      "  batch 120 loss: 0.02868053572981335\n",
      "  batch 130 loss: 0.09189596793985402\n",
      "  batch 140 loss: 0.031791256501287536\n",
      "  batch 150 loss: 0.053842584685435214\n",
      "  batch 160 loss: 0.09214137484730145\n",
      "  batch 170 loss: 0.07031085310467233\n",
      "  batch 180 loss: 0.015403408859060618\n",
      "  batch 190 loss: 0.3929754979506015\n",
      "LOSS train 0.3929754979506015 valid 0.6320395903855587\n",
      "EPOCH 261:\n",
      "  batch 10 loss: 0.05506963574416659\n",
      "  batch 20 loss: 0.06219122620163944\n",
      "  batch 30 loss: 0.039556691204131765\n",
      "  batch 40 loss: 0.03390556172886931\n",
      "  batch 50 loss: 0.07181058704245799\n",
      "  batch 60 loss: 0.03787336773007155\n",
      "  batch 70 loss: 0.11204020962259165\n",
      "  batch 80 loss: 0.08305614500818166\n",
      "  batch 90 loss: 0.03678148058146462\n",
      "  batch 100 loss: 0.07201817462112103\n",
      "  batch 110 loss: 0.03486045769379853\n",
      "  batch 120 loss: 0.03375393321330193\n",
      "  batch 130 loss: 0.03147839218540867\n",
      "  batch 140 loss: 0.04223879889814271\n",
      "  batch 150 loss: 0.11199575030994993\n",
      "  batch 160 loss: 0.14797726782360315\n",
      "  batch 170 loss: 0.055736290255637755\n",
      "  batch 180 loss: 0.028542760073878525\n",
      "  batch 190 loss: 0.09501943079292233\n",
      "LOSS train 0.09501943079292233 valid 0.3390373637654362\n",
      "EPOCH 262:\n",
      "  batch 10 loss: 0.020512912875256008\n",
      "  batch 20 loss: 0.022159289417481888\n",
      "  batch 30 loss: 0.10690648420495563\n",
      "  batch 40 loss: 0.024939142385119163\n",
      "  batch 50 loss: 0.04272260237805199\n",
      "  batch 60 loss: 0.030903975606304358\n",
      "  batch 70 loss: 0.05241763930488759\n",
      "  batch 80 loss: 0.0771063624699309\n",
      "  batch 90 loss: 0.03759946313412428\n",
      "  batch 100 loss: 0.04066978902183109\n",
      "  batch 110 loss: 0.022723448702163297\n",
      "  batch 120 loss: 0.02556468193470778\n",
      "  batch 130 loss: 0.011423795859377606\n",
      "  batch 140 loss: 0.05457207744929775\n",
      "  batch 150 loss: 0.015724352142456155\n",
      "  batch 160 loss: 0.2191605884099431\n",
      "  batch 170 loss: 0.03130000246044347\n",
      "  batch 180 loss: 0.024481562816970382\n",
      "  batch 190 loss: 0.01419883305923122\n",
      "LOSS train 0.01419883305923122 valid 0.28594015783213694\n",
      "EPOCH 263:\n",
      "  batch 10 loss: 0.08211767015866371\n",
      "  batch 20 loss: 0.06634217300821206\n",
      "  batch 30 loss: 0.048061838336559505\n",
      "  batch 40 loss: 0.013433464964373342\n",
      "  batch 50 loss: 0.062192280998363006\n",
      "  batch 60 loss: 0.0313736254144942\n",
      "  batch 70 loss: 0.03042224427513247\n",
      "  batch 80 loss: 0.015410099004816403\n",
      "  batch 90 loss: 0.044518576311867265\n",
      "  batch 100 loss: 0.054569453184564055\n",
      "  batch 110 loss: 0.04272729689218977\n",
      "  batch 120 loss: 0.08090654678954365\n",
      "  batch 130 loss: 0.026215398865224414\n",
      "  batch 140 loss: 0.027228342635351056\n",
      "  batch 150 loss: 0.1665857896468566\n",
      "  batch 160 loss: 0.22197874582739133\n",
      "  batch 170 loss: 0.028788229740666794\n",
      "  batch 180 loss: 0.022723559260680305\n",
      "  batch 190 loss: 0.054795317737801955\n",
      "LOSS train 0.054795317737801955 valid 0.295224469948612\n",
      "EPOCH 264:\n",
      "  batch 10 loss: 0.012785883953347366\n",
      "  batch 20 loss: 0.01124997211685468\n",
      "  batch 30 loss: 0.07119393398019155\n",
      "  batch 40 loss: 0.045973462672338886\n",
      "  batch 50 loss: 0.14245119311360668\n",
      "  batch 60 loss: 0.10444253370025365\n",
      "  batch 70 loss: 0.0695288090950271\n",
      "  batch 80 loss: 0.04196912406941067\n",
      "  batch 90 loss: 0.02004209195411022\n",
      "  batch 100 loss: 0.01223139171333969\n",
      "  batch 110 loss: 0.20477270087636726\n",
      "  batch 120 loss: 0.15475294839848175\n",
      "  batch 130 loss: 0.17776179743286774\n",
      "  batch 140 loss: 0.02518836196754819\n",
      "  batch 150 loss: 0.13482833667158048\n",
      "  batch 160 loss: 0.2927172272298435\n",
      "  batch 170 loss: 0.44327534127257306\n",
      "  batch 180 loss: 0.15048070670304697\n",
      "  batch 190 loss: 0.04603969428063124\n",
      "LOSS train 0.04603969428063124 valid 0.6175534630299149\n",
      "EPOCH 265:\n",
      "  batch 10 loss: 0.32390100031288965\n",
      "  batch 20 loss: 0.2791850469373685\n",
      "  batch 30 loss: 0.09443629311947462\n",
      "  batch 40 loss: 0.34004931934476873\n",
      "  batch 50 loss: 0.36354083363931977\n",
      "  batch 60 loss: 0.14969477181894036\n",
      "  batch 70 loss: 0.033286728095208676\n",
      "  batch 80 loss: 0.017800767448295573\n",
      "  batch 90 loss: 0.0649884005637631\n",
      "  batch 100 loss: 0.01864737197856812\n",
      "  batch 110 loss: 0.0468946449606392\n",
      "  batch 120 loss: 0.036500237846394154\n",
      "  batch 130 loss: 0.025853722800275137\n",
      "  batch 140 loss: 0.03464046328417662\n",
      "  batch 150 loss: 0.02302529618900735\n",
      "  batch 160 loss: 0.11216717994636838\n",
      "  batch 170 loss: 0.026853427938021923\n",
      "  batch 180 loss: 0.16303276127808886\n",
      "  batch 190 loss: 0.20256667062087672\n",
      "LOSS train 0.20256667062087672 valid 0.32529584171820186\n",
      "EPOCH 266:\n",
      "  batch 10 loss: 0.08137068975518673\n",
      "  batch 20 loss: 0.09593240654845658\n",
      "  batch 30 loss: 0.05134518186509922\n",
      "  batch 40 loss: 0.023534896398683714\n",
      "  batch 50 loss: 0.016628004989161126\n",
      "  batch 60 loss: 0.008444563019747876\n",
      "  batch 70 loss: 0.014507330217622894\n",
      "  batch 80 loss: 0.0075008680664495845\n",
      "  batch 90 loss: 0.05073097772469737\n",
      "  batch 100 loss: 0.04182933968922953\n",
      "  batch 110 loss: 0.03398175835110351\n",
      "  batch 120 loss: 0.010459992673781926\n",
      "  batch 130 loss: 0.025048083681440403\n",
      "  batch 140 loss: 0.1280908188059243\n",
      "  batch 150 loss: 0.03174711806424853\n",
      "  batch 160 loss: 0.027812347347747846\n",
      "  batch 170 loss: 0.7294181654091461\n",
      "  batch 180 loss: 0.13916009360857515\n",
      "  batch 190 loss: 0.06366729942492384\n",
      "LOSS train 0.06366729942492384 valid 0.3013919856254472\n",
      "EPOCH 267:\n",
      "  batch 10 loss: 0.10057407471376792\n",
      "  batch 20 loss: 0.02250612218213064\n",
      "  batch 30 loss: 0.02844894691529589\n",
      "  batch 40 loss: 0.02879145990765437\n",
      "  batch 50 loss: 0.013626526616053524\n",
      "  batch 60 loss: 0.0066876373555942335\n",
      "  batch 70 loss: 0.06913690704697614\n",
      "  batch 80 loss: 0.04394994581954279\n",
      "  batch 90 loss: 0.011630323503390905\n",
      "  batch 100 loss: 0.04096512314475831\n",
      "  batch 110 loss: 0.04941776227236687\n",
      "  batch 120 loss: 0.06231000493449965\n",
      "  batch 130 loss: 0.01464438788843836\n",
      "  batch 140 loss: 0.027227668816703954\n",
      "  batch 150 loss: 0.1389838365650121\n",
      "  batch 160 loss: 0.6661730268135443\n",
      "  batch 170 loss: 0.02824143296445527\n",
      "  batch 180 loss: 0.07449976806868222\n",
      "  batch 190 loss: 0.055877561244551545\n",
      "LOSS train 0.055877561244551545 valid 0.308166122392138\n",
      "EPOCH 268:\n",
      "  batch 10 loss: 0.009020617707108158\n",
      "  batch 20 loss: 0.050910926186702454\n",
      "  batch 30 loss: 0.030324651581122453\n",
      "  batch 40 loss: 0.30211010801540394\n",
      "  batch 50 loss: 0.08910065348190414\n",
      "  batch 60 loss: 0.011430781433688253\n",
      "  batch 70 loss: 0.043987336286079426\n",
      "  batch 80 loss: 0.036513067222426795\n",
      "  batch 90 loss: 0.7181905513815992\n",
      "  batch 100 loss: 0.360227450421462\n",
      "  batch 110 loss: 0.42235962363183716\n",
      "  batch 120 loss: 0.029219192996151833\n",
      "  batch 130 loss: 0.1857759869147456\n",
      "  batch 140 loss: 0.01896965865701077\n",
      "  batch 150 loss: 0.24084012933675467\n",
      "  batch 160 loss: 0.035761558490725064\n",
      "  batch 170 loss: 0.032599185532399134\n",
      "  batch 180 loss: 0.0962307996069967\n",
      "  batch 190 loss: 0.09910411708218021\n",
      "LOSS train 0.09910411708218021 valid 0.3496388770485616\n",
      "EPOCH 269:\n",
      "  batch 10 loss: 0.1255740559176955\n",
      "  batch 20 loss: 0.038602087462351165\n",
      "  batch 30 loss: 0.03545728622034403\n",
      "  batch 40 loss: 0.022040466631415255\n",
      "  batch 50 loss: 0.04371604309646955\n",
      "  batch 60 loss: 0.03330119379056669\n",
      "  batch 70 loss: 0.045041231991967835\n",
      "  batch 80 loss: 0.10143531049844796\n",
      "  batch 90 loss: 0.03553066962076627\n",
      "  batch 100 loss: 0.040730953733026355\n",
      "  batch 110 loss: 0.049072421636674336\n",
      "  batch 120 loss: 0.02610537323018889\n",
      "  batch 130 loss: 0.02351204078819933\n",
      "  batch 140 loss: 0.072158618979131\n",
      "  batch 150 loss: 0.24778557991485287\n",
      "  batch 160 loss: 0.09053836616017179\n",
      "  batch 170 loss: 0.07208795088583883\n",
      "  batch 180 loss: 0.04609500557022557\n",
      "  batch 190 loss: 0.07844171447814005\n",
      "LOSS train 0.07844171447814005 valid 0.349933935734839\n",
      "EPOCH 270:\n",
      "  batch 10 loss: 0.04612866600371035\n",
      "  batch 20 loss: 0.012487160090813632\n",
      "  batch 30 loss: 0.009395034365763878\n",
      "  batch 40 loss: 0.015096331196866685\n",
      "  batch 50 loss: 0.0692231317794267\n",
      "  batch 60 loss: 0.34582126702011917\n",
      "  batch 70 loss: 0.11978672985553658\n",
      "  batch 80 loss: 0.1391586229722975\n",
      "  batch 90 loss: 0.072118481787669\n",
      "  batch 100 loss: 0.05399330558529982\n",
      "  batch 110 loss: 0.10730002192685788\n",
      "  batch 120 loss: 0.05362962115841583\n",
      "  batch 130 loss: 0.040841013296176246\n",
      "  batch 140 loss: 0.09672315107818577\n",
      "  batch 150 loss: 0.04682552060219223\n",
      "  batch 160 loss: 0.024726823450566825\n",
      "  batch 170 loss: 0.08313654665616924\n",
      "  batch 180 loss: 0.10693585727666459\n",
      "  batch 190 loss: 0.05604985319209845\n",
      "LOSS train 0.05604985319209845 valid 0.30130552291779095\n",
      "EPOCH 271:\n",
      "  batch 10 loss: 0.02735166073093733\n",
      "  batch 20 loss: 0.0437748068623506\n",
      "  batch 30 loss: 0.04022269043384767\n",
      "  batch 40 loss: 0.0191837220058801\n",
      "  batch 50 loss: 0.0244113981644432\n",
      "  batch 60 loss: 0.014382100219773975\n",
      "  batch 70 loss: 0.17315783163390108\n",
      "  batch 80 loss: 0.16986592774971995\n",
      "  batch 90 loss: 0.015584137739369907\n",
      "  batch 100 loss: 0.14730417278396998\n",
      "  batch 110 loss: 0.03352785372748031\n",
      "  batch 120 loss: 0.06746304758450208\n",
      "  batch 130 loss: 0.1406858185592455\n",
      "  batch 140 loss: 0.24307629579437845\n",
      "  batch 150 loss: 0.07166067143339205\n",
      "  batch 160 loss: 0.0428655506147976\n",
      "  batch 170 loss: 0.019627797544399073\n",
      "  batch 180 loss: 0.03228808419867164\n",
      "  batch 190 loss: 0.053787516197735384\n",
      "LOSS train 0.053787516197735384 valid 0.2671760860210081\n",
      "EPOCH 272:\n",
      "  batch 10 loss: 0.026826785893535997\n",
      "  batch 20 loss: 0.024946446926026055\n",
      "  batch 30 loss: 0.014282950405572592\n",
      "  batch 40 loss: 0.0182376134732948\n",
      "  batch 50 loss: 0.009152063769602137\n",
      "  batch 60 loss: 0.024460786379063394\n",
      "  batch 70 loss: 0.011689444868648025\n",
      "  batch 80 loss: 0.04168102081620191\n",
      "  batch 90 loss: 0.06876737163368034\n",
      "  batch 100 loss: 0.1397642189807641\n",
      "  batch 110 loss: 0.04242802328163862\n",
      "  batch 120 loss: 0.05098222251962738\n",
      "  batch 130 loss: 0.016089330819045244\n",
      "  batch 140 loss: 0.019682858545729688\n",
      "  batch 150 loss: 0.04283762559097113\n",
      "  batch 160 loss: 0.007108442163735162\n",
      "  batch 170 loss: 0.08550066391559596\n",
      "  batch 180 loss: 0.28271893844634804\n",
      "  batch 190 loss: 0.09913119326402012\n",
      "LOSS train 0.09913119326402012 valid 0.30122761027195\n",
      "EPOCH 273:\n",
      "  batch 10 loss: 0.15584656034321825\n",
      "  batch 20 loss: 0.06621153189335019\n",
      "  batch 30 loss: 0.029678885730132266\n",
      "  batch 40 loss: 0.04310760571740957\n",
      "  batch 50 loss: 0.027101027282697032\n",
      "  batch 60 loss: 0.06254214562025026\n",
      "  batch 70 loss: 0.024884167451955364\n",
      "  batch 80 loss: 0.01654569224579063\n",
      "  batch 90 loss: 0.018513851793613867\n",
      "  batch 100 loss: 0.009326916326097034\n",
      "  batch 110 loss: 0.04898215572284244\n",
      "  batch 120 loss: 0.021898317830880386\n",
      "  batch 130 loss: 0.07083983545629735\n",
      "  batch 140 loss: 0.2606177899518059\n",
      "  batch 150 loss: 0.045078003875846664\n",
      "  batch 160 loss: 0.01957359098479401\n",
      "  batch 170 loss: 0.05321194790427057\n",
      "  batch 180 loss: 0.013233534129028612\n",
      "  batch 190 loss: 0.049585137065173514\n",
      "LOSS train 0.049585137065173514 valid 0.33999266379762705\n",
      "EPOCH 274:\n",
      "  batch 10 loss: 0.12068237741395933\n",
      "  batch 20 loss: 0.06774280651233085\n",
      "  batch 30 loss: 0.030883086378156576\n",
      "  batch 40 loss: 0.038490013648626586\n",
      "  batch 50 loss: 0.016282409345797076\n",
      "  batch 60 loss: 0.04558291105273042\n",
      "  batch 70 loss: 0.03973669404131215\n",
      "  batch 80 loss: 0.033931769183993765\n",
      "  batch 90 loss: 0.0044431906906766015\n",
      "  batch 100 loss: 0.029522380966432137\n",
      "  batch 110 loss: 0.042849120297466924\n",
      "  batch 120 loss: 0.00683171956602564\n",
      "  batch 130 loss: 0.1383168629464734\n",
      "  batch 140 loss: 0.02226843232106148\n",
      "  batch 150 loss: 0.03309068827245483\n",
      "  batch 160 loss: 0.042248114233052546\n",
      "  batch 170 loss: 0.03349032306070967\n",
      "  batch 180 loss: 0.02819192282544236\n",
      "  batch 190 loss: 0.02050461790495319\n",
      "LOSS train 0.02050461790495319 valid 0.29789119691239235\n",
      "EPOCH 275:\n",
      "  batch 10 loss: 0.02403038393820438\n",
      "  batch 20 loss: 0.043615096399980756\n",
      "  batch 30 loss: 0.046473975230363604\n",
      "  batch 40 loss: 0.14155142340869134\n",
      "  batch 50 loss: 0.03231816135428005\n",
      "  batch 60 loss: 0.057376076846033186\n",
      "  batch 70 loss: 0.04312411625176367\n",
      "  batch 80 loss: 0.01650281751718552\n",
      "  batch 90 loss: 0.04693796871561062\n",
      "  batch 100 loss: 0.12396251517742485\n",
      "  batch 110 loss: 0.05256202283686662\n",
      "  batch 120 loss: 0.9290124964308462\n",
      "  batch 130 loss: 0.07646931800808261\n",
      "  batch 140 loss: 0.286599299727348\n",
      "  batch 150 loss: 0.0940568538660557\n",
      "  batch 160 loss: 0.022924680476688764\n",
      "  batch 170 loss: 0.06527265965584092\n",
      "  batch 180 loss: 0.03369142464298278\n",
      "  batch 190 loss: 0.055704537082532826\n",
      "LOSS train 0.055704537082532826 valid 0.35960598616705763\n",
      "EPOCH 276:\n",
      "  batch 10 loss: 0.021875099575618152\n",
      "  batch 20 loss: 0.013298653892843505\n",
      "  batch 30 loss: 0.02812822462552731\n",
      "  batch 40 loss: 0.03994336968249854\n",
      "  batch 50 loss: 0.017923221494747565\n",
      "  batch 60 loss: 0.04135444592646422\n",
      "  batch 70 loss: 0.04210439452517676\n",
      "  batch 80 loss: 0.03870930266718915\n",
      "  batch 90 loss: 0.04045705976961358\n",
      "  batch 100 loss: 0.07258497450381754\n",
      "  batch 110 loss: 0.1195511031479782\n",
      "  batch 120 loss: 0.114809611188781\n",
      "  batch 130 loss: 0.0544263118393701\n",
      "  batch 140 loss: 0.06285581949792914\n",
      "  batch 150 loss: 0.09476268300768424\n",
      "  batch 160 loss: 0.08593869975841244\n",
      "  batch 170 loss: 0.19121120027427452\n",
      "  batch 180 loss: 0.3322079001258992\n",
      "  batch 190 loss: 0.23054615051736108\n",
      "LOSS train 0.23054615051736108 valid 1.6141191114332947\n",
      "EPOCH 277:\n",
      "  batch 10 loss: 0.7515329163277243\n",
      "  batch 20 loss: 0.13907708084238662\n",
      "  batch 30 loss: 0.030666260488482068\n",
      "  batch 40 loss: 0.011620400498679828\n",
      "  batch 50 loss: 0.08087540284267333\n",
      "  batch 60 loss: 0.09919817125590953\n",
      "  batch 70 loss: 0.3570981978454256\n",
      "  batch 80 loss: 0.03922242350652141\n",
      "  batch 90 loss: 0.06696417260280327\n",
      "  batch 100 loss: 0.05079473398095615\n",
      "  batch 110 loss: 0.04952492596081015\n",
      "  batch 120 loss: 0.017495035469920595\n",
      "  batch 130 loss: 0.02117184293047103\n",
      "  batch 140 loss: 0.009746036850446416\n",
      "  batch 150 loss: 0.016720464634983046\n",
      "  batch 160 loss: 0.032512313190454736\n",
      "  batch 170 loss: 0.019327041262033617\n",
      "  batch 180 loss: 0.17434795555913069\n",
      "  batch 190 loss: 0.046206832004531864\n",
      "LOSS train 0.046206832004531864 valid 0.29688982519419466\n",
      "EPOCH 278:\n",
      "  batch 10 loss: 0.009347748717073046\n",
      "  batch 20 loss: 0.025386969435044194\n",
      "  batch 30 loss: 0.04644659083616602\n",
      "  batch 40 loss: 0.06783293625546775\n",
      "  batch 50 loss: 0.02263669652634235\n",
      "  batch 60 loss: 0.028835783595127396\n",
      "  batch 70 loss: 0.013018115794881168\n",
      "  batch 80 loss: 0.044347103251914176\n",
      "  batch 90 loss: 0.029975771801690598\n",
      "  batch 100 loss: 0.07177633461193392\n",
      "  batch 110 loss: 0.10485039639077058\n",
      "  batch 120 loss: 0.030192807703429025\n",
      "  batch 130 loss: 0.05204649941583721\n",
      "  batch 140 loss: 0.1286289920700483\n",
      "  batch 150 loss: 0.02525275596822212\n",
      "  batch 160 loss: 0.043500526282014104\n",
      "  batch 170 loss: 0.05610610645521774\n",
      "  batch 180 loss: 0.1400951537377523\n",
      "  batch 190 loss: 0.04137803058981717\n",
      "LOSS train 0.04137803058981717 valid 0.31772559441384773\n",
      "EPOCH 279:\n",
      "  batch 10 loss: 0.04919840085458418\n",
      "  batch 20 loss: 0.012081453080719484\n",
      "  batch 30 loss: 0.054775281301135695\n",
      "  batch 40 loss: 0.10408136036753887\n",
      "  batch 50 loss: 0.03826896155259192\n",
      "  batch 60 loss: 0.011314243990011264\n",
      "  batch 70 loss: 0.023664074038777017\n",
      "  batch 80 loss: 0.03820341340992854\n",
      "  batch 90 loss: 0.09532701347520742\n",
      "  batch 100 loss: 0.03671909560810036\n",
      "  batch 110 loss: 0.020579993070830936\n",
      "  batch 120 loss: 0.037428392138349406\n",
      "  batch 130 loss: 0.02198355808386907\n",
      "  batch 140 loss: 0.05421582188971001\n",
      "  batch 150 loss: 0.011524766541418785\n",
      "  batch 160 loss: 0.042837424163394644\n",
      "  batch 170 loss: 0.01566032473015184\n",
      "  batch 180 loss: 0.05948689627384738\n",
      "  batch 190 loss: 0.017084224108026548\n",
      "LOSS train 0.017084224108026548 valid 0.29030711481479043\n",
      "EPOCH 280:\n",
      "  batch 10 loss: 0.01876204203758789\n",
      "  batch 20 loss: 0.009377940623789983\n",
      "  batch 30 loss: 0.02610416671034841\n",
      "  batch 40 loss: 0.029382915639487805\n",
      "  batch 50 loss: 0.05882417747882869\n",
      "  batch 60 loss: 0.07034806903084245\n",
      "  batch 70 loss: 0.04325177706441537\n",
      "  batch 80 loss: 0.02218630626528295\n",
      "  batch 90 loss: 0.012772707903968695\n",
      "  batch 100 loss: 0.021860447684673545\n",
      "  batch 110 loss: 0.04403170629019826\n",
      "  batch 120 loss: 0.0063062844583555485\n",
      "  batch 130 loss: 0.03376333526081225\n",
      "  batch 140 loss: 0.01725059294747098\n",
      "  batch 150 loss: 0.05292156341173211\n",
      "  batch 160 loss: 0.09200457578201622\n",
      "  batch 170 loss: 0.12582051033211297\n",
      "  batch 180 loss: 0.022115788582732422\n",
      "  batch 190 loss: 0.058955065471323564\n",
      "LOSS train 0.058955065471323564 valid 0.30553975945900436\n",
      "EPOCH 281:\n",
      "  batch 10 loss: 0.02940526007249673\n",
      "  batch 20 loss: 0.06285887724596932\n",
      "  batch 30 loss: 0.02922592781521871\n",
      "  batch 40 loss: 0.04052772156882156\n",
      "  batch 50 loss: 0.040586051536261894\n",
      "  batch 60 loss: 0.03252519795416333\n",
      "  batch 70 loss: 0.021762895948025118\n",
      "  batch 80 loss: 0.05026705362784014\n",
      "  batch 90 loss: 0.02616830008289526\n",
      "  batch 100 loss: 0.20237956184064387\n",
      "  batch 110 loss: 0.18001562154425912\n",
      "  batch 120 loss: 0.18611449201349578\n",
      "  batch 130 loss: 0.0680794616298229\n",
      "  batch 140 loss: 0.10541104631138012\n",
      "  batch 150 loss: 0.019429478279076307\n",
      "  batch 160 loss: 0.08293404118539911\n",
      "  batch 170 loss: 0.0470013749564032\n",
      "  batch 180 loss: 0.04636034859463507\n",
      "  batch 190 loss: 0.1993963852322352\n",
      "LOSS train 0.1993963852322352 valid 0.329565672116869\n",
      "EPOCH 282:\n",
      "  batch 10 loss: 0.02603789737305533\n",
      "  batch 20 loss: 0.028445328761529255\n",
      "  batch 30 loss: 0.11375142681667967\n",
      "  batch 40 loss: 0.07392960305619667\n",
      "  batch 50 loss: 0.004841034581590975\n",
      "  batch 60 loss: 0.14245937585526464\n",
      "  batch 70 loss: 0.14758533188562506\n",
      "  batch 80 loss: 0.09293996079055589\n",
      "  batch 90 loss: 0.011612920966535966\n",
      "  batch 100 loss: 0.049865428009434255\n",
      "  batch 110 loss: 0.08078308843170134\n",
      "  batch 120 loss: 0.0437347760079291\n",
      "  batch 130 loss: 0.09922153935953873\n",
      "  batch 140 loss: 0.2412403769187222\n",
      "  batch 150 loss: 0.05187010681396487\n",
      "  batch 160 loss: 0.08318352315262416\n",
      "  batch 170 loss: 0.0595348039523401\n",
      "  batch 180 loss: 0.2107265447956479\n",
      "  batch 190 loss: 0.2301792665912444\n",
      "LOSS train 0.2301792665912444 valid 0.3222754634901271\n",
      "EPOCH 283:\n",
      "  batch 10 loss: 0.03923145017591878\n",
      "  batch 20 loss: 0.10173397950782145\n",
      "  batch 30 loss: 0.030029629075102093\n",
      "  batch 40 loss: 0.019800262971000394\n",
      "  batch 50 loss: 0.029735329108621044\n",
      "  batch 60 loss: 0.0068693101198050496\n",
      "  batch 70 loss: 0.013291309722458778\n",
      "  batch 80 loss: 0.1364981192385585\n",
      "  batch 90 loss: 0.0793258738691236\n",
      "  batch 100 loss: 0.03362291640780768\n",
      "  batch 110 loss: 0.059260465694160304\n",
      "  batch 120 loss: 0.2157476619383658\n",
      "  batch 130 loss: 0.20406059586436812\n",
      "  batch 140 loss: 0.05501532947686485\n",
      "  batch 150 loss: 0.08975741523659053\n",
      "  batch 160 loss: 0.11815267516174402\n",
      "  batch 170 loss: 0.005726290431525172\n",
      "  batch 180 loss: 0.1169984947532896\n",
      "  batch 190 loss: 0.12323113340168561\n",
      "LOSS train 0.12323113340168561 valid 0.35253349768782816\n",
      "EPOCH 284:\n",
      "  batch 10 loss: 0.03198589192638792\n",
      "  batch 20 loss: 0.042937937342401256\n",
      "  batch 30 loss: 0.03525118642764369\n",
      "  batch 40 loss: 0.07874840138381387\n",
      "  batch 50 loss: 0.24124125676048608\n",
      "  batch 60 loss: 0.531798375271056\n",
      "  batch 70 loss: 0.3431436791777941\n",
      "  batch 80 loss: 0.047311641999135644\n",
      "  batch 90 loss: 0.023303787229701813\n",
      "  batch 100 loss: 0.21046765024564137\n",
      "  batch 110 loss: 0.18227266268581274\n",
      "  batch 120 loss: 0.3800007678050207\n",
      "  batch 130 loss: 0.1401427207083316\n",
      "  batch 140 loss: 0.028491086404147836\n",
      "  batch 150 loss: 0.40065446979563113\n",
      "  batch 160 loss: 0.9443524118267306\n",
      "  batch 170 loss: 0.3405031338959589\n",
      "  batch 180 loss: 0.23483365070685522\n",
      "  batch 190 loss: 0.56961911852801\n",
      "LOSS train 0.56961911852801 valid 0.3429559630278895\n",
      "EPOCH 285:\n",
      "  batch 10 loss: 0.18024559947435675\n",
      "  batch 20 loss: 0.38028354816965476\n",
      "  batch 30 loss: 0.15253113660849635\n",
      "  batch 40 loss: 0.24984458202615656\n",
      "  batch 50 loss: 0.033028884410441606\n",
      "  batch 60 loss: 0.03281794993914104\n",
      "  batch 70 loss: 0.05702516529534023\n",
      "  batch 80 loss: 0.07138348964635952\n",
      "  batch 90 loss: 0.0649772954272521\n",
      "  batch 100 loss: 0.05450140883483527\n",
      "  batch 110 loss: 0.10567575319792014\n",
      "  batch 120 loss: 0.03143689932612688\n",
      "  batch 130 loss: 0.02322278950857708\n",
      "  batch 140 loss: 0.008221834521111759\n",
      "  batch 150 loss: 0.011519045641210823\n",
      "  batch 160 loss: 0.0713581035921493\n",
      "  batch 170 loss: 0.023802713128020514\n",
      "  batch 180 loss: 0.019798976587190965\n",
      "  batch 190 loss: 0.24728353346818022\n",
      "LOSS train 0.24728353346818022 valid 0.3445114843112235\n",
      "EPOCH 286:\n",
      "  batch 10 loss: 0.3219640722096585\n",
      "  batch 20 loss: 0.03403558406444045\n",
      "  batch 30 loss: 0.07346215346754832\n",
      "  batch 40 loss: 0.06765447255504568\n",
      "  batch 50 loss: 0.05280008018705758\n",
      "  batch 60 loss: 0.07183241200266366\n",
      "  batch 70 loss: 0.030205420227122203\n",
      "  batch 80 loss: 0.08360424267326608\n",
      "  batch 90 loss: 0.051595545213785954\n",
      "  batch 100 loss: 0.04591470561262554\n",
      "  batch 110 loss: 0.0378139209033165\n",
      "  batch 120 loss: 0.04674412376047883\n",
      "  batch 130 loss: 0.043652272244335676\n",
      "  batch 140 loss: 0.018807586251409703\n",
      "  batch 150 loss: 0.034392436235611966\n",
      "  batch 160 loss: 0.25641118794092677\n",
      "  batch 170 loss: 0.045234017497011794\n",
      "  batch 180 loss: 0.02105537995505813\n",
      "  batch 190 loss: 0.024962468591513697\n",
      "LOSS train 0.024962468591513697 valid 0.31865250971931186\n",
      "EPOCH 287:\n",
      "  batch 10 loss: 0.027579283140653388\n",
      "  batch 20 loss: 0.019212452355600363\n",
      "  batch 30 loss: 0.019674776927774928\n",
      "  batch 40 loss: 0.018622955126943452\n",
      "  batch 50 loss: 0.061880811778337375\n",
      "  batch 60 loss: 0.024978113924862554\n",
      "  batch 70 loss: 0.013824314469786714\n",
      "  batch 80 loss: 0.009220075399170469\n",
      "  batch 90 loss: 0.013481791731473436\n",
      "  batch 100 loss: 0.035518961623756694\n",
      "  batch 110 loss: 0.04170980846761552\n",
      "  batch 120 loss: 0.3135572889759629\n",
      "  batch 130 loss: 0.09445608506493955\n",
      "  batch 140 loss: 0.027751583685943614\n",
      "  batch 150 loss: 0.08270647083267732\n",
      "  batch 160 loss: 0.08055617939103285\n",
      "  batch 170 loss: 0.016583948880167298\n",
      "  batch 180 loss: 0.014360798207743386\n",
      "  batch 190 loss: 0.00752494239663406\n",
      "LOSS train 0.00752494239663406 valid 0.3225881023334249\n",
      "EPOCH 288:\n",
      "  batch 10 loss: 0.013809104160179686\n",
      "  batch 20 loss: 0.021976107874070294\n",
      "  batch 30 loss: 0.02207272020949631\n",
      "  batch 40 loss: 0.017660073216350724\n",
      "  batch 50 loss: 0.05239799315061191\n",
      "  batch 60 loss: 0.03221516571032908\n",
      "  batch 70 loss: 0.030775792827209613\n",
      "  batch 80 loss: 0.04414998718897891\n",
      "  batch 90 loss: 0.03251006884848948\n",
      "  batch 100 loss: 0.03820938864057553\n",
      "  batch 110 loss: 0.022833026346086173\n",
      "  batch 120 loss: 0.01574317887228318\n",
      "  batch 130 loss: 0.03504607587387909\n",
      "  batch 140 loss: 0.04819182493641279\n",
      "  batch 150 loss: 0.036440106892936794\n",
      "  batch 160 loss: 0.01223924693145335\n",
      "  batch 170 loss: 0.04825058230732111\n",
      "  batch 180 loss: 0.02008275718310699\n",
      "  batch 190 loss: 0.01311263059411658\n",
      "LOSS train 0.01311263059411658 valid 0.2834357662987459\n",
      "EPOCH 289:\n",
      "  batch 10 loss: 0.029542615946002115\n",
      "  batch 20 loss: 0.0165871857909309\n",
      "  batch 30 loss: 0.018133178246927173\n",
      "  batch 40 loss: 0.004936324761411015\n",
      "  batch 50 loss: 0.029555941645276106\n",
      "  batch 60 loss: 0.03083182705077121\n",
      "  batch 70 loss: 0.015093417545324428\n",
      "  batch 80 loss: 0.03780481554511539\n",
      "  batch 90 loss: 0.02294242822747492\n",
      "  batch 100 loss: 0.012134761816997752\n",
      "  batch 110 loss: 0.008437679558565492\n",
      "  batch 120 loss: 0.007621920337584243\n",
      "  batch 130 loss: 0.013036681214407509\n",
      "  batch 140 loss: 0.01439518567487994\n",
      "  batch 150 loss: 0.026940901660437076\n",
      "  batch 160 loss: 0.05880148947102981\n",
      "  batch 170 loss: 0.07123492409335767\n",
      "  batch 180 loss: 0.05968210327848596\n",
      "  batch 190 loss: 0.10895708993113526\n",
      "LOSS train 0.10895708993113526 valid 0.31102768727209523\n",
      "EPOCH 290:\n",
      "  batch 10 loss: 0.02157360165830937\n",
      "  batch 20 loss: 0.023998869434463187\n",
      "  batch 30 loss: 0.03726204277192267\n",
      "  batch 40 loss: 0.023809810435489\n",
      "  batch 50 loss: 0.003914399752136433\n",
      "  batch 60 loss: 0.056546247270443925\n",
      "  batch 70 loss: 0.027852073938981904\n",
      "  batch 80 loss: 0.008045661068331355\n",
      "  batch 90 loss: 0.01712164318645364\n",
      "  batch 100 loss: 0.019726105226584422\n",
      "  batch 110 loss: 0.03640633147249446\n",
      "  batch 120 loss: 0.06460808694183698\n",
      "  batch 130 loss: 1.2014493434649012\n",
      "  batch 140 loss: 0.18473446961782541\n",
      "  batch 150 loss: 0.10044765674821007\n",
      "  batch 160 loss: 0.027163883351011008\n",
      "  batch 170 loss: 0.012918562710592597\n",
      "  batch 180 loss: 0.08803833794336242\n",
      "  batch 190 loss: 0.0753434121606574\n",
      "LOSS train 0.0753434121606574 valid 0.31063630136474\n",
      "EPOCH 291:\n",
      "  batch 10 loss: 0.020382499661423026\n",
      "  batch 20 loss: 0.008611577787087299\n",
      "  batch 30 loss: 0.08276907589035147\n",
      "  batch 40 loss: 0.044157535041483696\n",
      "  batch 50 loss: 0.04492477853648893\n",
      "  batch 60 loss: 0.013693221005928536\n",
      "  batch 70 loss: 0.005518305709247784\n",
      "  batch 80 loss: 0.006994915174186644\n",
      "  batch 90 loss: 0.0769077788877496\n",
      "  batch 100 loss: 0.015271196360231443\n",
      "  batch 110 loss: 0.02176882154024611\n",
      "  batch 120 loss: 0.02911431804598692\n",
      "  batch 130 loss: 0.016030128175373193\n",
      "  batch 140 loss: 0.006769470308961445\n",
      "  batch 150 loss: 0.047287650202980556\n",
      "  batch 160 loss: 0.306302742237051\n",
      "  batch 170 loss: 0.3429265095837309\n",
      "  batch 180 loss: 0.15316715997130928\n",
      "  batch 190 loss: 0.07420442711506894\n",
      "LOSS train 0.07420442711506894 valid 0.3231372316588659\n",
      "EPOCH 292:\n",
      "  batch 10 loss: 0.024748102556168307\n",
      "  batch 20 loss: 0.05585403845526571\n",
      "  batch 30 loss: 0.018767740381446175\n",
      "  batch 40 loss: 0.03894236117405399\n",
      "  batch 50 loss: 0.08244215738666298\n",
      "  batch 60 loss: 0.03558868232541954\n",
      "  batch 70 loss: 0.025903485464516506\n",
      "  batch 80 loss: 0.1171410283215664\n",
      "  batch 90 loss: 0.061584902939375755\n",
      "  batch 100 loss: 0.02753551794926352\n",
      "  batch 110 loss: 0.06602823124419359\n",
      "  batch 120 loss: 0.4585787601852047\n",
      "  batch 130 loss: 0.09238964133697873\n",
      "  batch 140 loss: 0.027064803445055928\n",
      "  batch 150 loss: 0.030289362473217805\n",
      "  batch 160 loss: 0.07443909085005203\n",
      "  batch 170 loss: 0.03765005804038424\n",
      "  batch 180 loss: 0.030208864412020375\n",
      "  batch 190 loss: 0.012882267931215097\n",
      "LOSS train 0.012882267931215097 valid 0.320247789325673\n",
      "EPOCH 293:\n",
      "  batch 10 loss: 0.1656238852311162\n",
      "  batch 20 loss: 0.19353136308029661\n",
      "  batch 30 loss: 0.015583328012894526\n",
      "  batch 40 loss: 0.03073581347974823\n",
      "  batch 50 loss: 0.01484045422707254\n",
      "  batch 60 loss: 0.13713845485102638\n",
      "  batch 70 loss: 0.24673372747908892\n",
      "  batch 80 loss: 0.18720312462342575\n",
      "  batch 90 loss: 0.01102187755632258\n",
      "  batch 100 loss: 0.018149192806527027\n",
      "  batch 110 loss: 0.05432124911824303\n",
      "  batch 120 loss: 0.03058499094831859\n",
      "  batch 130 loss: 0.020101443310613653\n",
      "  batch 140 loss: 0.017565449777293907\n",
      "  batch 150 loss: 0.031782731464710424\n",
      "  batch 160 loss: 0.0275543806317728\n",
      "  batch 170 loss: 0.12009678165936748\n",
      "  batch 180 loss: 0.030071599882180066\n",
      "  batch 190 loss: 0.019255408160060484\n",
      "LOSS train 0.019255408160060484 valid 0.30501600500471643\n",
      "EPOCH 294:\n",
      "  batch 10 loss: 0.01746232990728913\n",
      "  batch 20 loss: 0.014451141370147979\n",
      "  batch 30 loss: 0.08350401735859805\n",
      "  batch 40 loss: 0.06270598489824267\n",
      "  batch 50 loss: 0.049491393341389765\n",
      "  batch 60 loss: 0.020875809576881465\n",
      "  batch 70 loss: 0.12666691191002427\n",
      "  batch 80 loss: 0.022561796319041604\n",
      "  batch 90 loss: 0.015472631234479195\n",
      "  batch 100 loss: 0.00512168241459392\n",
      "  batch 110 loss: 0.004467574581485678\n",
      "  batch 120 loss: 0.009520488419911821\n",
      "  batch 130 loss: 0.025866109824139016\n",
      "  batch 140 loss: 0.021071515444236865\n",
      "  batch 150 loss: 0.032629833371493076\n",
      "  batch 160 loss: 0.009650212870093355\n",
      "  batch 170 loss: 0.05648779052476414\n",
      "  batch 180 loss: 0.016573413659291703\n",
      "  batch 190 loss: 0.015867777441275165\n",
      "LOSS train 0.015867777441275165 valid 0.2884496989270363\n",
      "EPOCH 295:\n",
      "  batch 10 loss: 0.02103260335215964\n",
      "  batch 20 loss: 0.019281907603485138\n",
      "  batch 30 loss: 0.017189113211497898\n",
      "  batch 40 loss: 0.028369502458690476\n",
      "  batch 50 loss: 0.017591521320937886\n",
      "  batch 60 loss: 0.020514872338299028\n",
      "  batch 70 loss: 0.024226229154055545\n",
      "  batch 80 loss: 0.014929898033420841\n",
      "  batch 90 loss: 0.02489770898600909\n",
      "  batch 100 loss: 0.02375222672228574\n",
      "  batch 110 loss: 0.011753889119120231\n",
      "  batch 120 loss: 0.023348976991263724\n",
      "  batch 130 loss: 0.011260964541332718\n",
      "  batch 140 loss: 0.018161939075912414\n",
      "  batch 150 loss: 0.008023049184379261\n",
      "  batch 160 loss: 0.03861669464476449\n",
      "  batch 170 loss: 0.021234884043667534\n",
      "  batch 180 loss: 0.013941905227963503\n",
      "  batch 190 loss: 0.016177099428040263\n",
      "LOSS train 0.016177099428040263 valid 0.29402492775654476\n",
      "EPOCH 296:\n",
      "  batch 10 loss: 0.011258543931057829\n",
      "  batch 20 loss: 0.016196554246687356\n",
      "  batch 30 loss: 0.0059562351463455345\n",
      "  batch 40 loss: 0.005025862663497094\n",
      "  batch 50 loss: 0.2912695158015623\n",
      "  batch 60 loss: 0.12506333429574265\n",
      "  batch 70 loss: 0.08403210871271086\n",
      "  batch 80 loss: 0.055645242493592664\n",
      "  batch 90 loss: 0.042459395917023635\n",
      "  batch 100 loss: 0.03747443106603896\n",
      "  batch 110 loss: 0.02054994858331156\n",
      "  batch 120 loss: 0.016447835115832275\n",
      "  batch 130 loss: 0.01748785523989227\n",
      "  batch 140 loss: 0.024563029456339792\n",
      "  batch 150 loss: 0.009533051426453198\n",
      "  batch 160 loss: 0.0642315951486637\n",
      "  batch 170 loss: 0.03354267991703068\n",
      "  batch 180 loss: 0.017545170813173173\n",
      "  batch 190 loss: 0.012973602861211474\n",
      "LOSS train 0.012973602861211474 valid 0.28689573480779035\n",
      "EPOCH 297:\n",
      "  batch 10 loss: 0.012913656199245338\n",
      "  batch 20 loss: 0.033516514180814735\n",
      "  batch 30 loss: 0.012116846335578657\n",
      "  batch 40 loss: 0.01059166894966097\n",
      "  batch 50 loss: 0.001980255892468108\n",
      "  batch 60 loss: 0.010539322329100287\n",
      "  batch 70 loss: 0.009467837151635194\n",
      "  batch 80 loss: 0.026989926827764067\n",
      "  batch 90 loss: 0.10583740845291914\n",
      "  batch 100 loss: 0.2262700281949151\n",
      "  batch 110 loss: 0.32509826416975895\n",
      "  batch 120 loss: 0.09392083949654761\n",
      "  batch 130 loss: 0.031230284334000658\n",
      "  batch 140 loss: 0.014591822713995839\n",
      "  batch 150 loss: 0.01850397560438637\n",
      "  batch 160 loss: 0.03983233333574958\n",
      "  batch 170 loss: 0.08810003144250658\n",
      "  batch 180 loss: 0.05425129159627318\n",
      "  batch 190 loss: 0.07571217292813799\n",
      "LOSS train 0.07571217292813799 valid 0.2918479603479872\n",
      "EPOCH 298:\n",
      "  batch 10 loss: 0.025145793007010298\n",
      "  batch 20 loss: 0.017016714557871637\n",
      "  batch 30 loss: 0.03077523445351673\n",
      "  batch 40 loss: 0.061985240386491114\n",
      "  batch 50 loss: 0.02074898279384172\n",
      "  batch 60 loss: 0.015501217237944332\n",
      "  batch 70 loss: 0.01825046182775907\n",
      "  batch 80 loss: 0.01483947357010038\n",
      "  batch 90 loss: 0.018928595221564138\n",
      "  batch 100 loss: 0.04421068302319142\n",
      "  batch 110 loss: 0.015816378087311022\n",
      "  batch 120 loss: 0.02653318001671323\n",
      "  batch 130 loss: 0.01920575768008348\n",
      "  batch 140 loss: 0.054833851107804324\n",
      "  batch 150 loss: 0.015323866729636393\n",
      "  batch 160 loss: 0.007772231790684714\n",
      "  batch 170 loss: 0.010524974701479551\n",
      "  batch 180 loss: 0.006829504621532578\n",
      "  batch 190 loss: 0.02140689418704369\n",
      "LOSS train 0.02140689418704369 valid 0.28976361477741147\n",
      "EPOCH 299:\n",
      "  batch 10 loss: 0.010433226065197232\n",
      "  batch 20 loss: 0.013511460608151537\n",
      "  batch 30 loss: 0.02329152577920297\n",
      "  batch 40 loss: 0.015971535450864847\n",
      "  batch 50 loss: 0.03136212827861869\n",
      "  batch 60 loss: 0.03285691617863904\n",
      "  batch 70 loss: 0.011945901597243846\n",
      "  batch 80 loss: 0.013602343792456395\n",
      "  batch 90 loss: 0.017668914762924714\n",
      "  batch 100 loss: 0.011749165469313993\n",
      "  batch 110 loss: 0.013929485793278218\n",
      "  batch 120 loss: 0.014833370139410818\n",
      "  batch 130 loss: 0.0218853552090863\n",
      "  batch 140 loss: 0.00988550429828976\n",
      "  batch 150 loss: 0.026299393342688403\n",
      "  batch 160 loss: 0.010493485809661252\n",
      "  batch 170 loss: 0.016523756306679616\n",
      "  batch 180 loss: 0.013744246092801404\n",
      "  batch 190 loss: 0.012257978308434758\n",
      "LOSS train 0.012257978308434758 valid 0.3418010050326326\n",
      "EPOCH 300:\n",
      "  batch 10 loss: 0.08483454488154507\n",
      "  batch 20 loss: 0.09322215927675756\n",
      "  batch 30 loss: 0.0373597938469743\n",
      "  batch 40 loss: 0.009317866883452552\n",
      "  batch 50 loss: 0.019606818175651596\n",
      "  batch 60 loss: 0.021912059639029736\n",
      "  batch 70 loss: 0.02820261351741351\n",
      "  batch 80 loss: 0.02833496321333655\n",
      "  batch 90 loss: 0.035412679602198696\n",
      "  batch 100 loss: 0.004861265293635597\n",
      "  batch 110 loss: 0.007507481810475269\n",
      "  batch 120 loss: 0.0573479537261619\n",
      "  batch 130 loss: 0.025147413099603\n",
      "  batch 140 loss: 0.021883487346508267\n",
      "  batch 150 loss: 0.1431730000126521\n",
      "  batch 160 loss: 0.0863032987369479\n",
      "  batch 170 loss: 0.03959793482018483\n",
      "  batch 180 loss: 0.010145537875260402\n",
      "  batch 190 loss: 0.028439774978266285\n",
      "LOSS train 0.028439774978266285 valid 0.3828932028871637\n",
      "EPOCH 301:\n",
      "  batch 10 loss: 0.0698326498531003\n",
      "  batch 20 loss: 0.01857829012384684\n",
      "  batch 30 loss: 0.016963695546058942\n",
      "  batch 40 loss: 0.01104146851803307\n",
      "  batch 50 loss: 0.008546252045221081\n",
      "  batch 60 loss: 0.026390196687776778\n",
      "  batch 70 loss: 0.012313482370328189\n",
      "  batch 80 loss: 0.008475118234536972\n",
      "  batch 90 loss: 0.02014459083899851\n",
      "  batch 100 loss: 0.0448034608168598\n",
      "  batch 110 loss: 0.056991716555535275\n",
      "  batch 120 loss: 0.08229453550006838\n",
      "  batch 130 loss: 0.21367629399971957\n",
      "  batch 140 loss: 0.33431384024077443\n",
      "  batch 150 loss: 0.04072782685521901\n",
      "  batch 160 loss: 0.0248222073790771\n",
      "  batch 170 loss: 0.12522085520201925\n",
      "  batch 180 loss: 0.06383217364907949\n",
      "  batch 190 loss: 0.0227979953215538\n",
      "LOSS train 0.0227979953215538 valid 0.31552471770133306\n",
      "EPOCH 302:\n",
      "  batch 10 loss: 0.021667088804656308\n",
      "  batch 20 loss: 0.014182415518411063\n",
      "  batch 30 loss: 0.025955875536806162\n",
      "  batch 40 loss: 0.010668335937799612\n",
      "  batch 50 loss: 0.02339831204532743\n",
      "  batch 60 loss: 0.009502510680610499\n",
      "  batch 70 loss: 0.02589605721585144\n",
      "  batch 80 loss: 0.02952018759206112\n",
      "  batch 90 loss: 0.011700243768960662\n",
      "  batch 100 loss: 0.06227858451904922\n",
      "  batch 110 loss: 0.019739653741545737\n",
      "  batch 120 loss: 0.009811340240040068\n",
      "  batch 130 loss: 0.02169487402803725\n",
      "  batch 140 loss: 0.0052921560915194735\n",
      "  batch 150 loss: 0.016464461278428644\n",
      "  batch 160 loss: 0.20733956715321825\n",
      "  batch 170 loss: 0.020202245188427524\n",
      "  batch 180 loss: 0.010991629639045186\n",
      "  batch 190 loss: 0.05333811926942929\n",
      "LOSS train 0.05333811926942929 valid 0.5566736781453088\n",
      "EPOCH 303:\n",
      "  batch 10 loss: 0.028532876847663147\n",
      "  batch 20 loss: 0.058134872879273304\n",
      "  batch 30 loss: 0.09612588261973087\n",
      "  batch 40 loss: 0.012521682013988312\n",
      "  batch 50 loss: 0.018700603466334086\n",
      "  batch 60 loss: 0.07636934840450635\n",
      "  batch 70 loss: 0.023211849777067074\n",
      "  batch 80 loss: 0.05059559779916469\n",
      "  batch 90 loss: 0.2591955771259677\n",
      "  batch 100 loss: 0.0617338465735088\n",
      "  batch 110 loss: 0.012583078747871923\n",
      "  batch 120 loss: 0.01189861097585645\n",
      "  batch 130 loss: 0.006792388574967845\n",
      "  batch 140 loss: 0.17491972041869985\n",
      "  batch 150 loss: 0.04313619440775653\n",
      "  batch 160 loss: 0.037218454841337054\n",
      "  batch 170 loss: 0.15766120721115584\n",
      "  batch 180 loss: 0.15591305610500045\n",
      "  batch 190 loss: 0.12157634403962092\n",
      "LOSS train 0.12157634403962092 valid 0.29558769534867185\n",
      "EPOCH 304:\n",
      "  batch 10 loss: 0.013514488493126465\n",
      "  batch 20 loss: 0.007493255609929861\n",
      "  batch 30 loss: 0.012703302187691001\n",
      "  batch 40 loss: 0.06628159921798424\n",
      "  batch 50 loss: 0.22564860805405546\n",
      "  batch 60 loss: 0.0656217974777519\n",
      "  batch 70 loss: 0.02455505689109714\n",
      "  batch 80 loss: 0.031084341143952088\n",
      "  batch 90 loss: 0.016205692878270384\n",
      "  batch 100 loss: 0.09210442754391579\n",
      "  batch 110 loss: 0.06264145420029195\n",
      "  batch 120 loss: 0.049016515660198934\n",
      "  batch 130 loss: 0.017054195635526525\n",
      "  batch 140 loss: 0.020868414139357583\n",
      "  batch 150 loss: 0.027946030179586502\n",
      "  batch 160 loss: 0.08919463786322694\n",
      "  batch 170 loss: 0.07159770152014175\n",
      "  batch 180 loss: 0.043305549092542604\n",
      "  batch 190 loss: 0.010734994602879055\n",
      "LOSS train 0.010734994602879055 valid 0.2934824536137586\n",
      "EPOCH 305:\n",
      "  batch 10 loss: 0.027714014088859075\n",
      "  batch 20 loss: 0.010399180678071218\n",
      "  batch 30 loss: 0.005323036443988372\n",
      "  batch 40 loss: 0.013704142465167024\n",
      "  batch 50 loss: 0.03652209386787035\n",
      "  batch 60 loss: 0.019596020710002902\n",
      "  batch 70 loss: 0.010552554564111461\n",
      "  batch 80 loss: 0.011896515448586342\n",
      "  batch 90 loss: 0.017831042382158557\n",
      "  batch 100 loss: 0.060816208131222994\n",
      "  batch 110 loss: 0.024892407749035606\n",
      "  batch 120 loss: 0.062121611864716896\n",
      "  batch 130 loss: 0.07431809328006693\n",
      "  batch 140 loss: 0.010131707841946992\n",
      "  batch 150 loss: 0.018856866684541274\n",
      "  batch 160 loss: 0.023976780055033942\n",
      "  batch 170 loss: 0.021763605568139433\n",
      "  batch 180 loss: 0.006985964144865875\n",
      "  batch 190 loss: 0.00790500467627293\n",
      "LOSS train 0.00790500467627293 valid 0.29900745727360434\n",
      "EPOCH 306:\n",
      "  batch 10 loss: 0.008584850233614817\n",
      "  batch 20 loss: 0.009229701631070952\n",
      "  batch 30 loss: 0.009942526511051143\n",
      "  batch 40 loss: 0.010426546265750857\n",
      "  batch 50 loss: 0.004144777581802117\n",
      "  batch 60 loss: 0.035419163073535744\n",
      "  batch 70 loss: 0.008650462211414833\n",
      "  batch 80 loss: 0.00576161507493822\n",
      "  batch 90 loss: 0.07708455502229299\n",
      "  batch 100 loss: 0.009981800420857213\n",
      "  batch 110 loss: 0.010793807218976603\n",
      "  batch 120 loss: 0.010487133264064141\n",
      "  batch 130 loss: 0.0045857832709344844\n",
      "  batch 140 loss: 0.01599668353558741\n",
      "  batch 150 loss: 0.013397749321831043\n",
      "  batch 160 loss: 0.013098077237722805\n",
      "  batch 170 loss: 0.005325463144072273\n",
      "  batch 180 loss: 0.03484796199161337\n",
      "  batch 190 loss: 0.05945472661536542\n",
      "LOSS train 0.05945472661536542 valid 0.287084281510368\n",
      "EPOCH 307:\n",
      "  batch 10 loss: 0.016227387859497355\n",
      "  batch 20 loss: 0.03510331376618794\n",
      "  batch 30 loss: 0.10079871341903868\n",
      "  batch 40 loss: 0.030433859787359553\n",
      "  batch 50 loss: 0.02100794415264886\n",
      "  batch 60 loss: 0.049955350751815786\n",
      "  batch 70 loss: 0.010131831061439556\n",
      "  batch 80 loss: 0.012280072316298175\n",
      "  batch 90 loss: 0.005729975786221075\n",
      "  batch 100 loss: 0.005237136662651664\n",
      "  batch 110 loss: 0.018760142790560506\n",
      "  batch 120 loss: 0.014769942482206488\n",
      "  batch 130 loss: 0.024080171243042337\n",
      "  batch 140 loss: 0.0053370312987510715\n",
      "  batch 150 loss: 0.017558826515693227\n",
      "  batch 160 loss: 0.008779549637989704\n",
      "  batch 170 loss: 0.006446907757000986\n",
      "  batch 180 loss: 0.010900646112899893\n",
      "  batch 190 loss: 0.024226583530858648\n",
      "LOSS train 0.024226583530858648 valid 0.2837720071039898\n",
      "EPOCH 308:\n",
      "  batch 10 loss: 0.00800013965320261\n",
      "  batch 20 loss: 0.012231538890404181\n",
      "  batch 30 loss: 0.023356866685816158\n",
      "  batch 40 loss: 0.005979973329056065\n",
      "  batch 50 loss: 0.0037444817953144137\n",
      "  batch 60 loss: 0.010419053876350403\n",
      "  batch 70 loss: 0.011037351330401002\n",
      "  batch 80 loss: 0.004909199471939019\n",
      "  batch 90 loss: 0.02460447218979809\n",
      "  batch 100 loss: 0.011866861602353396\n",
      "  batch 110 loss: 0.012222991226695257\n",
      "  batch 120 loss: 0.021241366201314803\n",
      "  batch 130 loss: 0.01995040768529748\n",
      "  batch 140 loss: 0.013114660050806037\n",
      "  batch 150 loss: 0.006668389742442571\n",
      "  batch 160 loss: 0.015081480275965476\n",
      "  batch 170 loss: 0.0032646532058663523\n",
      "  batch 180 loss: 0.011364489066677664\n",
      "  batch 190 loss: 0.017510559004460902\n",
      "LOSS train 0.017510559004460902 valid 0.30507834863545014\n",
      "EPOCH 309:\n",
      "  batch 10 loss: 0.00797446111574196\n",
      "  batch 20 loss: 0.006324564297665347\n",
      "  batch 30 loss: 0.01453892158981418\n",
      "  batch 40 loss: 0.02860570093070578\n",
      "  batch 50 loss: 0.012910510795995834\n",
      "  batch 60 loss: 0.03313864576711865\n",
      "  batch 70 loss: 0.10090446550130139\n",
      "  batch 80 loss: 0.013959413567094714\n",
      "  batch 90 loss: 0.007964303032701991\n",
      "  batch 100 loss: 0.013548420737959078\n",
      "  batch 110 loss: 0.012924226343631062\n",
      "  batch 120 loss: 0.006943559317460313\n",
      "  batch 130 loss: 0.018576331882668738\n",
      "  batch 140 loss: 0.09025150793199828\n",
      "  batch 150 loss: 0.014978394148621987\n",
      "  batch 160 loss: 0.011955502340438785\n",
      "  batch 170 loss: 0.061210438033242554\n",
      "  batch 180 loss: 0.03011599812033694\n",
      "  batch 190 loss: 0.03912563457837433\n",
      "LOSS train 0.03912563457837433 valid 0.3136717006762216\n",
      "EPOCH 310:\n",
      "  batch 10 loss: 0.008667825728400658\n",
      "  batch 20 loss: 0.03715681024390562\n",
      "  batch 30 loss: 0.008209271398334295\n",
      "  batch 40 loss: 0.02053945764776586\n",
      "  batch 50 loss: 0.011258411372769927\n",
      "  batch 60 loss: 0.006771694321747646\n",
      "  batch 70 loss: 0.00627925145342072\n",
      "  batch 80 loss: 0.04380287153297502\n",
      "  batch 90 loss: 0.008496145288174262\n",
      "  batch 100 loss: 0.006642348475531889\n",
      "  batch 110 loss: 0.06336616366461953\n",
      "  batch 120 loss: 0.9945635294043427\n",
      "  batch 130 loss: 0.755575097864787\n",
      "  batch 140 loss: 0.12151621330738464\n",
      "  batch 150 loss: 0.05536318004182874\n",
      "  batch 160 loss: 0.057436782125676974\n",
      "  batch 170 loss: 0.129746453794837\n",
      "  batch 180 loss: 0.11304389262281234\n",
      "  batch 190 loss: 0.05537976579446422\n",
      "LOSS train 0.05537976579446422 valid 0.680647614154264\n",
      "EPOCH 311:\n",
      "  batch 10 loss: 0.350624753733109\n",
      "  batch 20 loss: 0.012655806969144123\n",
      "  batch 30 loss: 0.08956021222772961\n",
      "  batch 40 loss: 0.022284220050278237\n",
      "  batch 50 loss: 0.017713845848180653\n",
      "  batch 60 loss: 0.7497937398856889\n",
      "  batch 70 loss: 1.118158157754624\n",
      "  batch 80 loss: 0.3419455018488719\n",
      "  batch 90 loss: 0.09343755109389348\n",
      "  batch 100 loss: 0.1560807906138903\n",
      "  batch 110 loss: 0.39784714086457595\n",
      "  batch 120 loss: 0.04135775933253214\n",
      "  batch 130 loss: 0.056070748625222676\n",
      "  batch 140 loss: 0.2332680134092925\n",
      "  batch 150 loss: 0.10024499888801301\n",
      "  batch 160 loss: 0.10194147474257989\n",
      "  batch 170 loss: 0.04884448808475099\n",
      "  batch 180 loss: 0.025545849464333515\n",
      "  batch 190 loss: 0.05116960778434532\n",
      "LOSS train 0.05116960778434532 valid 5.036444362102089\n",
      "EPOCH 312:\n",
      "  batch 10 loss: 1.9599834040669522\n",
      "  batch 20 loss: 0.5244838873842526\n",
      "  batch 30 loss: 0.1310515464401533\n",
      "  batch 40 loss: 0.051892425954554254\n",
      "  batch 50 loss: 0.08905964102159487\n",
      "  batch 60 loss: 0.12824184624741974\n",
      "  batch 70 loss: 0.0629180923747981\n",
      "  batch 80 loss: 0.19134445249866588\n",
      "  batch 90 loss: 0.03733628824893458\n",
      "  batch 100 loss: 0.0935687091577165\n",
      "  batch 110 loss: 0.028866650939539794\n",
      "  batch 120 loss: 0.04358010308939129\n",
      "  batch 130 loss: 0.05202646649172475\n",
      "  batch 140 loss: 0.09029571863346746\n",
      "  batch 150 loss: 0.02510027685227243\n",
      "  batch 160 loss: 0.0782472318090754\n",
      "  batch 170 loss: 0.023553942111459492\n",
      "  batch 180 loss: 0.08705072742159814\n",
      "  batch 190 loss: 0.1116486218188129\n",
      "LOSS train 0.1116486218188129 valid 0.30179254255422633\n",
      "EPOCH 313:\n",
      "  batch 10 loss: 0.06431941345326778\n",
      "  batch 20 loss: 0.027316671697860784\n",
      "  batch 30 loss: 0.07680859060283182\n",
      "  batch 40 loss: 0.06551532922919705\n",
      "  batch 50 loss: 0.027402514374080054\n",
      "  batch 60 loss: 0.03845690244470461\n",
      "  batch 70 loss: 0.03165551784982199\n",
      "  batch 80 loss: 0.11549921705800728\n",
      "  batch 90 loss: 0.14559183737189868\n",
      "  batch 100 loss: 0.1266337121110837\n",
      "  batch 110 loss: 0.05490759597642807\n",
      "  batch 120 loss: 0.016822958422715573\n",
      "  batch 130 loss: 0.014107512164173386\n",
      "  batch 140 loss: 0.013176105558125073\n",
      "  batch 150 loss: 0.021802599751413253\n",
      "  batch 160 loss: 0.017786611306655688\n",
      "  batch 170 loss: 0.018538470465989575\n",
      "  batch 180 loss: 0.01149553995789745\n",
      "  batch 190 loss: 0.01633113526008856\n",
      "LOSS train 0.01633113526008856 valid 0.3031764225268785\n",
      "EPOCH 314:\n",
      "  batch 10 loss: 0.019686429819819294\n",
      "  batch 20 loss: 0.021445423048713507\n",
      "  batch 30 loss: 0.01886833010157716\n",
      "  batch 40 loss: 0.011457425835169488\n",
      "  batch 50 loss: 0.0070102963090562295\n",
      "  batch 60 loss: 0.008431688708748197\n",
      "  batch 70 loss: 0.02858814392345721\n",
      "  batch 80 loss: 0.013503845998775433\n",
      "  batch 90 loss: 0.026531425698919975\n",
      "  batch 100 loss: 0.043856465737417236\n",
      "  batch 110 loss: 0.05210715402192534\n",
      "  batch 120 loss: 0.010543586880470458\n",
      "  batch 130 loss: 0.02100485501374578\n",
      "  batch 140 loss: 0.008669083235224662\n",
      "  batch 150 loss: 0.015184008079131672\n",
      "  batch 160 loss: 0.005127654465195519\n",
      "  batch 170 loss: 0.06574433323562498\n",
      "  batch 180 loss: 0.14941759779891867\n",
      "  batch 190 loss: 0.24194091485198613\n",
      "LOSS train 0.24194091485198613 valid 0.3756034603857494\n",
      "EPOCH 315:\n",
      "  batch 10 loss: 0.058113584759774996\n",
      "  batch 20 loss: 0.05691310739577489\n",
      "  batch 30 loss: 0.015002637916137474\n",
      "  batch 40 loss: 0.0407264404708144\n",
      "  batch 50 loss: 0.02115638328443765\n",
      "  batch 60 loss: 0.012707272531559965\n",
      "  batch 70 loss: 0.026439968871648033\n",
      "  batch 80 loss: 0.30695341549535443\n",
      "  batch 90 loss: 0.4296382341882726\n",
      "  batch 100 loss: 0.13192750045415097\n",
      "  batch 110 loss: 0.11682197456082122\n",
      "  batch 120 loss: 0.06733431697039123\n",
      "  batch 130 loss: 0.09897541016616743\n",
      "  batch 140 loss: 0.07815801253921109\n",
      "  batch 150 loss: 0.056447480349879696\n",
      "  batch 160 loss: 0.015069916543184547\n",
      "  batch 170 loss: 0.022519629378302142\n",
      "  batch 180 loss: 0.0089143557902716\n",
      "  batch 190 loss: 0.037295433064241476\n",
      "LOSS train 0.037295433064241476 valid 0.38365866156931017\n",
      "EPOCH 316:\n",
      "  batch 10 loss: 0.16638242109189605\n",
      "  batch 20 loss: 0.024613973991858985\n",
      "  batch 30 loss: 0.03764891237967163\n",
      "  batch 40 loss: 0.04246730981281246\n",
      "  batch 50 loss: 0.020151950007573305\n",
      "  batch 60 loss: 0.016322947384167997\n",
      "  batch 70 loss: 0.02106493971544694\n",
      "  batch 80 loss: 0.019001449898536294\n",
      "  batch 90 loss: 0.03869129841735912\n",
      "  batch 100 loss: 0.015045510924275618\n",
      "  batch 110 loss: 0.016065415371178914\n",
      "  batch 120 loss: 0.049552694843544035\n",
      "  batch 130 loss: 0.07124706786244132\n",
      "  batch 140 loss: 0.019728320760304997\n",
      "  batch 150 loss: 0.01365622577850445\n",
      "  batch 160 loss: 0.05199529585970595\n",
      "  batch 170 loss: 0.048445749241636804\n",
      "  batch 180 loss: 0.03071189121704947\n",
      "  batch 190 loss: 0.016751990896665347\n",
      "LOSS train 0.016751990896665347 valid 0.32537884422405683\n",
      "EPOCH 317:\n",
      "  batch 10 loss: 0.023064835027116714\n",
      "  batch 20 loss: 0.012536716741919918\n",
      "  batch 30 loss: 0.01870560948806883\n",
      "  batch 40 loss: 0.05205309440443671\n",
      "  batch 50 loss: 0.0314654012777396\n",
      "  batch 60 loss: 0.026325550218524542\n",
      "  batch 70 loss: 0.022743527166397826\n",
      "  batch 80 loss: 0.01094355822805113\n",
      "  batch 90 loss: 0.00790837981762138\n",
      "  batch 100 loss: 0.01709612104040161\n",
      "  batch 110 loss: 0.009946823106128022\n",
      "  batch 120 loss: 0.017808554173875814\n",
      "  batch 130 loss: 0.021255257085897483\n",
      "  batch 140 loss: 0.012197011367038612\n",
      "  batch 150 loss: 0.02522167392191932\n",
      "  batch 160 loss: 0.03770282224093648\n",
      "  batch 170 loss: 0.06707725457407605\n",
      "  batch 180 loss: 0.0386856619773198\n",
      "  batch 190 loss: 0.011002945077439107\n",
      "LOSS train 0.011002945077439107 valid 0.29276037194488386\n",
      "EPOCH 318:\n",
      "  batch 10 loss: 0.013920543280796949\n",
      "  batch 20 loss: 0.00938710203708979\n",
      "  batch 30 loss: 0.009850735246214982\n",
      "  batch 40 loss: 0.00685161531181393\n",
      "  batch 50 loss: 0.006379271752621207\n",
      "  batch 60 loss: 0.009662994362963672\n",
      "  batch 70 loss: 0.021152640662734257\n",
      "  batch 80 loss: 0.008099754134673277\n",
      "  batch 90 loss: 0.009151836010880742\n",
      "  batch 100 loss: 0.023331952379004407\n",
      "  batch 110 loss: 0.02304646941000783\n",
      "  batch 120 loss: 0.06511995345988453\n",
      "  batch 130 loss: 0.0900364035356084\n",
      "  batch 140 loss: 0.26776100480497556\n",
      "  batch 150 loss: 0.05139187713959217\n",
      "  batch 160 loss: 0.365205471182685\n",
      "  batch 170 loss: 0.12510072451488555\n",
      "  batch 180 loss: 0.035697582031664864\n",
      "  batch 190 loss: 0.05842232600313082\n",
      "LOSS train 0.05842232600313082 valid 0.2992465182927059\n",
      "EPOCH 319:\n",
      "  batch 10 loss: 0.179838893892088\n",
      "  batch 20 loss: 0.07348668776966179\n",
      "  batch 30 loss: 0.20654533624224386\n",
      "  batch 40 loss: 0.1343392327925585\n",
      "  batch 50 loss: 0.24527795126645485\n",
      "  batch 60 loss: 0.33011744062134196\n",
      "  batch 70 loss: 0.24179673591351047\n",
      "  batch 80 loss: 0.0756063899664241\n",
      "  batch 90 loss: 0.5904414312093593\n",
      "  batch 100 loss: 0.14318594512865132\n",
      "  batch 110 loss: 0.15958230601979723\n",
      "  batch 120 loss: 0.5738749576416652\n",
      "  batch 130 loss: 0.28852873569221626\n",
      "  batch 140 loss: 0.45584276597328427\n",
      "  batch 150 loss: 0.17082225070309676\n",
      "  batch 160 loss: 0.015206377409930383\n",
      "  batch 170 loss: 0.06893213384300907\n",
      "  batch 180 loss: 0.13350893413011136\n",
      "  batch 190 loss: 0.046285986801649415\n",
      "LOSS train 0.046285986801649415 valid 0.29591260929473645\n",
      "EPOCH 320:\n",
      "  batch 10 loss: 0.040132246968710206\n",
      "  batch 20 loss: 0.00726580149317897\n",
      "  batch 30 loss: 0.044303302749193564\n",
      "  batch 40 loss: 0.01655468148373984\n",
      "  batch 50 loss: 0.0159480960543533\n",
      "  batch 60 loss: 0.03261881291530244\n",
      "  batch 70 loss: 0.05369278330252882\n",
      "  batch 80 loss: 0.03624001441448854\n",
      "  batch 90 loss: 0.013952436019496871\n",
      "  batch 100 loss: 0.012792429882847501\n",
      "  batch 110 loss: 0.007831546872552053\n",
      "  batch 120 loss: 0.07961655633408497\n",
      "  batch 130 loss: 0.15776793777837667\n",
      "  batch 140 loss: 0.49783180087425694\n",
      "  batch 150 loss: 0.24671570555681796\n",
      "  batch 160 loss: 0.06761886310822263\n",
      "  batch 170 loss: 0.08453831486631884\n",
      "  batch 180 loss: 0.07639019623556464\n",
      "  batch 190 loss: 0.20794199379504902\n",
      "LOSS train 0.20794199379504902 valid 0.3984279819928983\n",
      "EPOCH 321:\n",
      "  batch 10 loss: 0.0651518092942723\n",
      "  batch 20 loss: 0.06059387315235654\n",
      "  batch 30 loss: 0.03655760792290721\n",
      "  batch 40 loss: 0.021052365415734187\n",
      "  batch 50 loss: 0.08145233160416013\n",
      "  batch 60 loss: 0.09025707420848902\n",
      "  batch 70 loss: 0.017384204381565384\n",
      "  batch 80 loss: 0.06567722355609504\n",
      "  batch 90 loss: 0.1252409338393136\n",
      "  batch 100 loss: 0.042239274536422046\n",
      "  batch 110 loss: 0.021874850154247838\n",
      "  batch 120 loss: 0.015556884231955337\n",
      "  batch 130 loss: 0.020557073556628325\n",
      "  batch 140 loss: 0.018058899845163977\n",
      "  batch 150 loss: 0.04215057592978155\n",
      "  batch 160 loss: 0.020532330205656278\n",
      "  batch 170 loss: 0.0048518293799020284\n",
      "  batch 180 loss: 0.01504496701872995\n",
      "  batch 190 loss: 0.018183191701569967\n",
      "LOSS train 0.018183191701569967 valid 0.29655662404178973\n",
      "EPOCH 322:\n",
      "  batch 10 loss: 0.010470053851679494\n",
      "  batch 20 loss: 0.019185421958428605\n",
      "  batch 30 loss: 0.008005389945742358\n",
      "  batch 40 loss: 0.01134869377509915\n",
      "  batch 50 loss: 0.003336586826384291\n",
      "  batch 60 loss: 0.007569102866580124\n",
      "  batch 70 loss: 0.041077980289225026\n",
      "  batch 80 loss: 0.06859499718479896\n",
      "  batch 90 loss: 0.01419362160578146\n",
      "  batch 100 loss: 0.02000376555230332\n",
      "  batch 110 loss: 0.05140030145533956\n",
      "  batch 120 loss: 0.03585379177934698\n",
      "  batch 130 loss: 0.2222730214754634\n",
      "  batch 140 loss: 0.11041118268447861\n",
      "  batch 150 loss: 0.21833056076393403\n",
      "  batch 160 loss: 0.141548774414062\n",
      "  batch 170 loss: 0.02550154292032971\n",
      "  batch 180 loss: 0.006386894006146804\n",
      "  batch 190 loss: 0.10192653594382364\n",
      "LOSS train 0.10192653594382364 valid 0.34390662923090914\n",
      "EPOCH 323:\n",
      "  batch 10 loss: 0.022518561975830666\n",
      "  batch 20 loss: 0.011639310205647745\n",
      "  batch 30 loss: 0.009778864570684932\n",
      "  batch 40 loss: 0.01678323024293036\n",
      "  batch 50 loss: 0.06595554221264166\n",
      "  batch 60 loss: 0.04098627255606857\n",
      "  batch 70 loss: 0.038467973223811215\n",
      "  batch 80 loss: 0.0684027843019976\n",
      "  batch 90 loss: 0.011681118571567594\n",
      "  batch 100 loss: 0.0657129048926393\n",
      "  batch 110 loss: 0.049495893214191254\n",
      "  batch 120 loss: 0.03972926221472743\n",
      "  batch 130 loss: 0.007790163677759665\n",
      "  batch 140 loss: 0.012353325873027643\n",
      "  batch 150 loss: 0.014509546498806003\n",
      "  batch 160 loss: 0.014689583406459405\n",
      "  batch 170 loss: 0.006184678514966712\n",
      "  batch 180 loss: 0.003985318036393437\n",
      "  batch 190 loss: 0.007628484026281512\n",
      "LOSS train 0.007628484026281512 valid 0.6874123233237669\n",
      "EPOCH 324:\n",
      "  batch 10 loss: 0.07443653929235552\n",
      "  batch 20 loss: 0.023511310134563247\n",
      "  batch 30 loss: 0.009966501654218973\n",
      "  batch 40 loss: 0.022114411690137815\n",
      "  batch 50 loss: 0.00840266445547968\n",
      "  batch 60 loss: 0.026389573400388146\n",
      "  batch 70 loss: 0.012193667296924105\n",
      "  batch 80 loss: 0.03752185341933227\n",
      "  batch 90 loss: 0.024677326225429397\n",
      "  batch 100 loss: 0.03532718488756927\n",
      "  batch 110 loss: 0.013825908885290516\n",
      "  batch 120 loss: 0.020123968487038724\n",
      "  batch 130 loss: 0.008118588238033908\n",
      "  batch 140 loss: 0.030928209163862165\n",
      "  batch 150 loss: 0.005405175016855423\n",
      "  batch 160 loss: 0.026892749920841473\n",
      "  batch 170 loss: 0.008338339715316324\n",
      "  batch 180 loss: 0.023768347960891844\n",
      "  batch 190 loss: 0.01672268866594919\n",
      "LOSS train 0.01672268866594919 valid 0.29019734290912624\n",
      "EPOCH 325:\n",
      "  batch 10 loss: 0.026427416280250072\n",
      "  batch 20 loss: 0.005107644675626943\n",
      "  batch 30 loss: 0.02084667772332125\n",
      "  batch 40 loss: 0.008247577361498771\n",
      "  batch 50 loss: 0.01348162282356995\n",
      "  batch 60 loss: 0.00294621610116792\n",
      "  batch 70 loss: 0.014899814107948828\n",
      "  batch 80 loss: 0.005792786804994421\n",
      "  batch 90 loss: 0.007327518614800965\n",
      "  batch 100 loss: 0.00578991793198611\n",
      "  batch 110 loss: 0.012018533843934165\n",
      "  batch 120 loss: 0.03341027425799581\n",
      "  batch 130 loss: 0.016341437295875495\n",
      "  batch 140 loss: 0.03300235350499179\n",
      "  batch 150 loss: 0.023515925537799375\n",
      "  batch 160 loss: 0.013882749139170869\n",
      "  batch 170 loss: 0.03600698716774531\n",
      "  batch 180 loss: 0.007373715656206059\n",
      "  batch 190 loss: 0.017185367550376716\n",
      "LOSS train 0.017185367550376716 valid 0.32676715435427556\n",
      "EPOCH 326:\n",
      "  batch 10 loss: 0.005240273342535318\n",
      "  batch 20 loss: 0.02091598095056497\n",
      "  batch 30 loss: 0.01044933758190325\n",
      "  batch 40 loss: 0.011616483539256706\n",
      "  batch 50 loss: 0.06314418341372345\n",
      "  batch 60 loss: 0.1107963785142175\n",
      "  batch 70 loss: 0.30382432084927513\n",
      "  batch 80 loss: 0.08079961113197101\n",
      "  batch 90 loss: 0.1922234004352859\n",
      "  batch 100 loss: 0.01662587857869067\n",
      "  batch 110 loss: 0.02960390824640626\n",
      "  batch 120 loss: 0.7555811398122018\n",
      "  batch 130 loss: 0.013698172117926788\n",
      "  batch 140 loss: 0.03762388020796834\n",
      "  batch 150 loss: 0.014825834544774352\n",
      "  batch 160 loss: 0.24399114402397687\n",
      "  batch 170 loss: 0.011397181829320858\n",
      "  batch 180 loss: 0.03409987849411209\n",
      "  batch 190 loss: 0.0483979271346243\n",
      "LOSS train 0.0483979271346243 valid 0.28006704749745165\n",
      "EPOCH 327:\n",
      "  batch 10 loss: 0.009279736972621322\n",
      "  batch 20 loss: 0.01559345423263494\n",
      "  batch 30 loss: 0.006033795716194845\n",
      "  batch 40 loss: 0.017701954018320976\n",
      "  batch 50 loss: 0.01895773081208745\n",
      "  batch 60 loss: 0.0167275566466742\n",
      "  batch 70 loss: 0.018995257786232855\n",
      "  batch 80 loss: 0.029655516353846424\n",
      "  batch 90 loss: 0.007797926760314056\n",
      "  batch 100 loss: 0.07644318087204453\n",
      "  batch 110 loss: 0.011956528464543226\n",
      "  batch 120 loss: 0.015916249143754158\n",
      "  batch 130 loss: 0.010596199860469824\n",
      "  batch 140 loss: 0.03370802837151814\n",
      "  batch 150 loss: 0.006011768787016081\n",
      "  batch 160 loss: 0.03829427117741915\n",
      "  batch 170 loss: 0.006755828253142226\n",
      "  batch 180 loss: 0.021194090095883668\n",
      "  batch 190 loss: 0.010237528564189802\n",
      "LOSS train 0.010237528564189802 valid 0.33074847046701794\n",
      "EPOCH 328:\n",
      "  batch 10 loss: 0.006909105434007756\n",
      "  batch 20 loss: 0.07381273585859276\n",
      "  batch 30 loss: 0.010864996368940182\n",
      "  batch 40 loss: 0.15680850150157538\n",
      "  batch 50 loss: 0.9064608969022757\n",
      "  batch 60 loss: 0.6139389033281013\n",
      "  batch 70 loss: 0.18537693020712975\n",
      "  batch 80 loss: 0.03260451135706717\n",
      "  batch 90 loss: 0.022633681137790518\n",
      "  batch 100 loss: 0.23876630172125174\n",
      "  batch 110 loss: 0.21153062077411278\n",
      "  batch 120 loss: 0.1025675755265297\n",
      "  batch 130 loss: 0.14580146354903717\n",
      "  batch 140 loss: 0.009470809573527106\n",
      "  batch 150 loss: 0.029363668338225856\n",
      "  batch 160 loss: 0.053608543278238587\n",
      "  batch 170 loss: 0.056784919320977956\n",
      "  batch 180 loss: 0.012437492758951407\n",
      "  batch 190 loss: 0.0169076172069083\n",
      "LOSS train 0.0169076172069083 valid 0.6894153264114303\n",
      "EPOCH 329:\n",
      "  batch 10 loss: 0.13563150185142375\n",
      "  batch 20 loss: 0.06301200115931352\n",
      "  batch 30 loss: 0.2175353230954812\n",
      "  batch 40 loss: 0.07213188659447382\n",
      "  batch 50 loss: 0.007666867484806517\n",
      "  batch 60 loss: 0.07391436666498237\n",
      "  batch 70 loss: 0.033962339333356614\n",
      "  batch 80 loss: 0.020223517753461807\n",
      "  batch 90 loss: 0.01477522528608972\n",
      "  batch 100 loss: 0.014912372899232196\n",
      "  batch 110 loss: 0.00400280576905061\n",
      "  batch 120 loss: 0.010886459997908559\n",
      "  batch 130 loss: 0.007814341835825012\n",
      "  batch 140 loss: 0.011286892079572652\n",
      "  batch 150 loss: 0.014017855136597746\n",
      "  batch 160 loss: 0.022995933542654258\n",
      "  batch 170 loss: 0.058436845754295065\n",
      "  batch 180 loss: 0.1410147302526724\n",
      "  batch 190 loss: 0.020136402713697522\n",
      "LOSS train 0.020136402713697522 valid 0.3701325881835377\n",
      "EPOCH 330:\n",
      "  batch 10 loss: 0.016472044561572828\n",
      "  batch 20 loss: 0.00699114953951323\n",
      "  batch 30 loss: 0.01934076160128484\n",
      "  batch 40 loss: 0.024018580170138647\n",
      "  batch 50 loss: 0.020894246164823473\n",
      "  batch 60 loss: 0.027906854387461343\n",
      "  batch 70 loss: 0.20844446677228773\n",
      "  batch 80 loss: 0.020693060448081724\n",
      "  batch 90 loss: 0.006783615647998431\n",
      "  batch 100 loss: 0.016008642054541156\n",
      "  batch 110 loss: 0.007449114717223893\n",
      "  batch 120 loss: 0.018363452454340567\n",
      "  batch 130 loss: 0.01790387094868038\n",
      "  batch 140 loss: 0.028563981837510254\n",
      "  batch 150 loss: 0.02380533813389434\n",
      "  batch 160 loss: 0.017177798714340043\n",
      "  batch 170 loss: 0.018486958967250187\n",
      "  batch 180 loss: 0.015359217336049369\n",
      "  batch 190 loss: 0.020292932187032875\n",
      "LOSS train 0.020292932187032875 valid 0.29907330629338674\n",
      "EPOCH 331:\n",
      "  batch 10 loss: 0.01313269517629152\n",
      "  batch 20 loss: 0.008849974029180885\n",
      "  batch 30 loss: 0.013591615960172021\n",
      "  batch 40 loss: 0.007965869819906856\n",
      "  batch 50 loss: 0.012608875614728277\n",
      "  batch 60 loss: 0.005616896431783402\n",
      "  batch 70 loss: 0.018927655387363983\n",
      "  batch 80 loss: 0.02315031788170785\n",
      "  batch 90 loss: 0.015109861106083144\n",
      "  batch 100 loss: 0.03141852295144058\n",
      "  batch 110 loss: 0.003631600769244869\n",
      "  batch 120 loss: 0.012907374887925016\n",
      "  batch 130 loss: 0.013292084748343314\n",
      "  batch 140 loss: 0.015675611446170024\n",
      "  batch 150 loss: 0.019558454033966655\n",
      "  batch 160 loss: 0.005269929646996729\n",
      "  batch 170 loss: 0.0071305188978129765\n",
      "  batch 180 loss: 0.0071841180500968\n",
      "  batch 190 loss: 0.02719720933988583\n",
      "LOSS train 0.02719720933988583 valid 0.31974245595528944\n",
      "EPOCH 332:\n",
      "  batch 10 loss: 0.01921738376766342\n",
      "  batch 20 loss: 0.002259864705894188\n",
      "  batch 30 loss: 0.02084161019915598\n",
      "  batch 40 loss: 0.013345459105812552\n",
      "  batch 50 loss: 0.015510354975833706\n",
      "  batch 60 loss: 0.003914484693552644\n",
      "  batch 70 loss: 0.024142469876625227\n",
      "  batch 80 loss: 0.03588352808889965\n",
      "  batch 90 loss: 0.021972040006721726\n",
      "  batch 100 loss: 0.0025192613282783325\n",
      "  batch 110 loss: 0.0032239169594120653\n",
      "  batch 120 loss: 0.00676083918558561\n",
      "  batch 130 loss: 0.012720997090590913\n",
      "  batch 140 loss: 0.014324608249035009\n",
      "  batch 150 loss: 0.023427097227832407\n",
      "  batch 160 loss: 0.01194900827121046\n",
      "  batch 170 loss: 0.004602364115748969\n",
      "  batch 180 loss: 0.0026862726831126337\n",
      "  batch 190 loss: 0.0131644006550232\n",
      "LOSS train 0.0131644006550232 valid 0.30279985357189504\n",
      "EPOCH 333:\n",
      "  batch 10 loss: 0.007876252526858706\n",
      "  batch 20 loss: 0.01330245458873378\n",
      "  batch 30 loss: 0.013376227124209095\n",
      "  batch 40 loss: 0.007933678606127615\n",
      "  batch 50 loss: 0.0072143643089930265\n",
      "  batch 60 loss: 0.006072710698410333\n",
      "  batch 70 loss: 0.009643302864100178\n",
      "  batch 80 loss: 0.018942972934712542\n",
      "  batch 90 loss: 0.011569724472689558\n",
      "  batch 100 loss: 0.013717073949464975\n",
      "  batch 110 loss: 0.006440257180855724\n",
      "  batch 120 loss: 0.0227383772563158\n",
      "  batch 130 loss: 0.00809368058255302\n",
      "  batch 140 loss: 0.016756348191054826\n",
      "  batch 150 loss: 0.007598991382390352\n",
      "  batch 160 loss: 0.00684839407437039\n",
      "  batch 170 loss: 0.008474323014092988\n",
      "  batch 180 loss: 0.004506785381181544\n",
      "  batch 190 loss: 0.009833752969132092\n",
      "LOSS train 0.009833752969132092 valid 0.30234906254094923\n",
      "EPOCH 334:\n",
      "  batch 10 loss: 0.0029473839225801156\n",
      "  batch 20 loss: 0.017712658665072923\n",
      "  batch 30 loss: 0.006272874306540643\n",
      "  batch 40 loss: 0.008201970066480158\n",
      "  batch 50 loss: 0.005198507871793367\n",
      "  batch 60 loss: 0.005073044662506731\n",
      "  batch 70 loss: 0.017673759898727326\n",
      "  batch 80 loss: 0.00556733930574751\n",
      "  batch 90 loss: 0.07019132745470653\n",
      "  batch 100 loss: 0.18804452757766796\n",
      "  batch 110 loss: 0.026178800276062297\n",
      "  batch 120 loss: 0.011815890405955543\n",
      "  batch 130 loss: 0.005574871797131209\n",
      "  batch 140 loss: 0.03433109848708664\n",
      "  batch 150 loss: 0.09012602778115379\n",
      "  batch 160 loss: 0.05068329689771787\n",
      "  batch 170 loss: 0.037640972551479024\n",
      "  batch 180 loss: 0.027038325308876666\n",
      "  batch 190 loss: 0.023230557433484476\n",
      "LOSS train 0.023230557433484476 valid 0.3053607727825556\n",
      "EPOCH 335:\n",
      "  batch 10 loss: 0.03083820861687627\n",
      "  batch 20 loss: 0.007808784234838129\n",
      "  batch 30 loss: 0.5154026755275423\n",
      "  batch 40 loss: 0.27137621898449565\n",
      "  batch 50 loss: 0.04485225908787242\n",
      "  batch 60 loss: 0.0028203237310037823\n",
      "  batch 70 loss: 0.02888539510840644\n",
      "  batch 80 loss: 0.062288611648205004\n",
      "  batch 90 loss: 0.030090053252489923\n",
      "  batch 100 loss: 0.259508662750943\n",
      "  batch 110 loss: 0.15221736937758124\n",
      "  batch 120 loss: 0.31912380029294224\n",
      "  batch 130 loss: 0.09260827653631623\n",
      "  batch 140 loss: 0.029199429134905586\n",
      "  batch 150 loss: 0.0044292332878683284\n",
      "  batch 160 loss: 0.04552110422152964\n",
      "  batch 170 loss: 0.05120610198741815\n",
      "  batch 180 loss: 0.006855835428456203\n",
      "  batch 190 loss: 0.006830347882646493\n",
      "LOSS train 0.006830347882646493 valid 0.30270055097075155\n",
      "EPOCH 336:\n",
      "  batch 10 loss: 0.011241643130972534\n",
      "  batch 20 loss: 0.014519850394592027\n",
      "  batch 30 loss: 0.01044225579629483\n",
      "  batch 40 loss: 0.004331364913178248\n",
      "  batch 50 loss: 0.00926021098455294\n",
      "  batch 60 loss: 0.03871068319573112\n",
      "  batch 70 loss: 0.0049687166190508945\n",
      "  batch 80 loss: 0.03445544869910009\n",
      "  batch 90 loss: 0.013179436179723326\n",
      "  batch 100 loss: 0.04090360013353802\n",
      "  batch 110 loss: 0.004342559883878039\n",
      "  batch 120 loss: 0.034461486861457044\n",
      "  batch 130 loss: 0.01878075789477407\n",
      "  batch 140 loss: 0.010323015167432458\n",
      "  batch 150 loss: 0.004437136515849716\n",
      "  batch 160 loss: 0.037717759762744406\n",
      "  batch 170 loss: 0.009647937118052142\n",
      "  batch 180 loss: 0.01996845032979593\n",
      "  batch 190 loss: 0.023350580102936648\n",
      "LOSS train 0.023350580102936648 valid 0.34605590233493605\n",
      "EPOCH 337:\n",
      "  batch 10 loss: 0.016934518659557796\n",
      "  batch 20 loss: 0.008723396574217012\n",
      "  batch 30 loss: 0.01406845709919935\n",
      "  batch 40 loss: 0.007688064964114005\n",
      "  batch 50 loss: 0.005536780362518812\n",
      "  batch 60 loss: 0.022943345824842254\n",
      "  batch 70 loss: 0.016645999523643696\n",
      "  batch 80 loss: 0.03393068234643977\n",
      "  batch 90 loss: 0.007707879700819831\n",
      "  batch 100 loss: 0.03057865098153343\n",
      "  batch 110 loss: 0.010241231316524591\n",
      "  batch 120 loss: 0.014554352526732827\n",
      "  batch 130 loss: 0.01849864246423749\n",
      "  batch 140 loss: 0.006791653001813813\n",
      "  batch 150 loss: 0.014759933327570706\n",
      "  batch 160 loss: 0.0066632680723159865\n",
      "  batch 170 loss: 0.008403739025948199\n",
      "  batch 180 loss: 0.008748935499716027\n",
      "  batch 190 loss: 0.00947500148160998\n",
      "LOSS train 0.00947500148160998 valid 0.31237244263477904\n",
      "EPOCH 338:\n",
      "  batch 10 loss: 0.01211800595227146\n",
      "  batch 20 loss: 0.04002924447046326\n",
      "  batch 30 loss: 0.01832950345495856\n",
      "  batch 40 loss: 0.013113243941574381\n",
      "  batch 50 loss: 0.0186712711905777\n",
      "  batch 60 loss: 0.007671918654705223\n",
      "  batch 70 loss: 0.005965104582355707\n",
      "  batch 80 loss: 0.009071975414374656\n",
      "  batch 90 loss: 0.008856471340777716\n",
      "  batch 100 loss: 0.0056199390232876565\n",
      "  batch 110 loss: 0.008814661777842047\n",
      "  batch 120 loss: 0.009466206418204592\n",
      "  batch 130 loss: 0.01036158927654469\n",
      "  batch 140 loss: 0.008808592502867895\n",
      "  batch 150 loss: 0.005098956659297471\n",
      "  batch 160 loss: 0.00944751175481997\n",
      "  batch 170 loss: 0.007988121396547854\n",
      "  batch 180 loss: 0.008605976228204781\n",
      "  batch 190 loss: 0.003606991007714555\n",
      "LOSS train 0.003606991007714555 valid 0.2965184965265563\n",
      "EPOCH 339:\n",
      "  batch 10 loss: 0.003181473938434465\n",
      "  batch 20 loss: 0.00694924971633668\n",
      "  batch 30 loss: 0.006723088933247823\n",
      "  batch 40 loss: 0.004131298734257882\n",
      "  batch 50 loss: 0.011230352280226442\n",
      "  batch 60 loss: 0.007726667346037175\n",
      "  batch 70 loss: 0.010514190399442213\n",
      "  batch 80 loss: 0.006201574853474767\n",
      "  batch 90 loss: 0.009114617007944048\n",
      "  batch 100 loss: 0.01245864619754684\n",
      "  batch 110 loss: 0.01428172783597006\n",
      "  batch 120 loss: 0.008116042740869033\n",
      "  batch 130 loss: 0.003402088631554534\n",
      "  batch 140 loss: 0.008754958419851988\n",
      "  batch 150 loss: 0.007028129228115176\n",
      "  batch 160 loss: 0.00642941622811577\n",
      "  batch 170 loss: 0.00656114684854856\n",
      "  batch 180 loss: 0.009002701468988761\n",
      "  batch 190 loss: 0.0404812019418614\n",
      "LOSS train 0.0404812019418614 valid 0.3025013718877331\n",
      "EPOCH 340:\n",
      "  batch 10 loss: 0.007751441984720486\n",
      "  batch 20 loss: 0.012331344706956316\n",
      "  batch 30 loss: 0.0121013589315055\n",
      "  batch 40 loss: 0.004051555698828047\n",
      "  batch 50 loss: 0.00859598254364471\n",
      "  batch 60 loss: 0.018775528420906795\n",
      "  batch 70 loss: 0.007103300326443218\n",
      "  batch 80 loss: 0.008014790787612469\n",
      "  batch 90 loss: 0.010728099418173365\n",
      "  batch 100 loss: 0.014264480032353788\n",
      "  batch 110 loss: 0.009027358869365542\n",
      "  batch 120 loss: 0.002758071120754835\n",
      "  batch 130 loss: 0.005454634871944109\n",
      "  batch 140 loss: 0.008942949438352343\n",
      "  batch 150 loss: 0.012278531537049275\n",
      "  batch 160 loss: 0.004428917873755722\n",
      "  batch 170 loss: 0.007118070774527041\n",
      "  batch 180 loss: 0.007059834189004732\n",
      "  batch 190 loss: 0.008751257835410798\n",
      "LOSS train 0.008751257835410798 valid 0.3028895507957797\n",
      "EPOCH 341:\n",
      "  batch 10 loss: 0.015700335628307017\n",
      "  batch 20 loss: 0.006030861901966489\n",
      "  batch 30 loss: 0.011649062839216384\n",
      "  batch 40 loss: 0.007875754483382024\n",
      "  batch 50 loss: 0.006184223400845035\n",
      "  batch 60 loss: 0.006758976920487214\n",
      "  batch 70 loss: 0.005699505503272917\n",
      "  batch 80 loss: 0.004450892727857081\n",
      "  batch 90 loss: 0.005452596790917141\n",
      "  batch 100 loss: 0.0019890057057821765\n",
      "  batch 110 loss: 0.006978534154370664\n",
      "  batch 120 loss: 0.008571921353279777\n",
      "  batch 130 loss: 0.009421937537217673\n",
      "  batch 140 loss: 0.011583819127287143\n",
      "  batch 150 loss: 0.01684378327871059\n",
      "  batch 160 loss: 0.0054341504217717326\n",
      "  batch 170 loss: 0.007926789074627827\n",
      "  batch 180 loss: 0.005255322459574785\n",
      "  batch 190 loss: 0.007289159317585359\n",
      "LOSS train 0.007289159317585359 valid 0.3072744158844371\n",
      "EPOCH 342:\n",
      "  batch 10 loss: 0.005156187306783977\n",
      "  batch 20 loss: 0.006199728137949024\n",
      "  batch 30 loss: 0.005428516915507942\n",
      "  batch 40 loss: 0.0027507532209057215\n",
      "  batch 50 loss: 0.006416547886647095\n",
      "  batch 60 loss: 0.005068308587757997\n",
      "  batch 70 loss: 0.003288614770837306\n",
      "  batch 80 loss: 0.015452259563206639\n",
      "  batch 90 loss: 0.002205058068619792\n",
      "  batch 100 loss: 0.0017885763355081964\n",
      "  batch 110 loss: 0.0076178454981999265\n",
      "  batch 120 loss: 0.013789323684153486\n",
      "  batch 130 loss: 0.015186272798422351\n",
      "  batch 140 loss: 0.007220060313340326\n",
      "  batch 150 loss: 0.006738060864696927\n",
      "  batch 160 loss: 0.004045400184311631\n",
      "  batch 170 loss: 0.0035497918531689266\n",
      "  batch 180 loss: 0.011355076502478311\n",
      "  batch 190 loss: 0.0069909347675924495\n",
      "LOSS train 0.0069909347675924495 valid 0.30949938693930307\n",
      "EPOCH 343:\n",
      "  batch 10 loss: 0.007814898570364904\n",
      "  batch 20 loss: 0.003730330438230567\n",
      "  batch 30 loss: 0.0027696384604594115\n",
      "  batch 40 loss: 0.0030005722995767314\n",
      "  batch 50 loss: 0.005139170838648966\n",
      "  batch 60 loss: 0.0026817131846598842\n",
      "  batch 70 loss: 0.015180019510167142\n",
      "  batch 80 loss: 0.00869250563632118\n",
      "  batch 90 loss: 0.004525640388004603\n",
      "  batch 100 loss: 0.00760841562778225\n",
      "  batch 110 loss: 0.008081549336762351\n",
      "  batch 120 loss: 0.0080124809812105\n",
      "  batch 130 loss: 0.005445852323302169\n",
      "  batch 140 loss: 0.0038184685409916597\n",
      "  batch 150 loss: 0.0208942058447505\n",
      "  batch 160 loss: 0.026415352990841256\n",
      "  batch 170 loss: 0.004369711325249881\n",
      "  batch 180 loss: 0.010653208187576979\n",
      "  batch 190 loss: 0.015477695459667018\n",
      "LOSS train 0.015477695459667018 valid 0.3009686988340516\n",
      "EPOCH 344:\n",
      "  batch 10 loss: 0.010349628187930194\n",
      "  batch 20 loss: 0.00587749945838425\n",
      "  batch 30 loss: 0.014146707996078334\n",
      "  batch 40 loss: 0.009695308942241177\n",
      "  batch 50 loss: 0.0029239985648331413\n",
      "  batch 60 loss: 0.009838674063706776\n",
      "  batch 70 loss: 0.00622243949582355\n",
      "  batch 80 loss: 0.007323043290405451\n",
      "  batch 90 loss: 0.0038823350012748394\n",
      "  batch 100 loss: 0.007279240997143432\n",
      "  batch 110 loss: 0.0056202873560136975\n",
      "  batch 120 loss: 0.0042117826434775905\n",
      "  batch 130 loss: 0.02691201957695242\n",
      "  batch 140 loss: 0.009511686916935247\n",
      "  batch 150 loss: 0.003835083730923827\n",
      "  batch 160 loss: 0.006570056797960433\n",
      "  batch 170 loss: 0.007142154005315149\n",
      "  batch 180 loss: 0.00335264625284708\n",
      "  batch 190 loss: 0.005840648369718338\n",
      "LOSS train 0.005840648369718338 valid 0.3179161359622208\n",
      "EPOCH 345:\n",
      "  batch 10 loss: 0.011314316862102203\n",
      "  batch 20 loss: 0.003343988164455425\n",
      "  batch 30 loss: 0.0024258091018337425\n",
      "  batch 40 loss: 0.0035911458253158556\n",
      "  batch 50 loss: 0.014338444248345183\n",
      "  batch 60 loss: 0.0027486617137867596\n",
      "  batch 70 loss: 0.0033123460415936277\n",
      "  batch 80 loss: 0.006464723693459717\n",
      "  batch 90 loss: 0.006878384248604164\n",
      "  batch 100 loss: 0.010882746334129756\n",
      "  batch 110 loss: 0.0042254572622610455\n",
      "  batch 120 loss: 0.006478256948184935\n",
      "  batch 130 loss: 0.011083838310702277\n",
      "  batch 140 loss: 0.00789433357721947\n",
      "  batch 150 loss: 0.022834743644838083\n",
      "  batch 160 loss: 0.011732592262252695\n",
      "  batch 170 loss: 0.014877143532075365\n",
      "  batch 180 loss: 0.00629578722951436\n",
      "  batch 190 loss: 0.008076997917963524\n",
      "LOSS train 0.008076997917963524 valid 0.30219296940750257\n",
      "EPOCH 346:\n",
      "  batch 10 loss: 0.005612731248152159\n",
      "  batch 20 loss: 0.009609187599368418\n",
      "  batch 30 loss: 0.003786352317584374\n",
      "  batch 40 loss: 0.008044913893803596\n",
      "  batch 50 loss: 0.008538695207448654\n",
      "  batch 60 loss: 0.00333421977751982\n",
      "  batch 70 loss: 0.005488713440486492\n",
      "  batch 80 loss: 0.004829516835448544\n",
      "  batch 90 loss: 0.006415420365982527\n",
      "  batch 100 loss: 0.005907407371608997\n",
      "  batch 110 loss: 0.01132595511211303\n",
      "  batch 120 loss: 0.006833700465884362\n",
      "  batch 130 loss: 0.003298562726465093\n",
      "  batch 140 loss: 0.005785444609179536\n",
      "  batch 150 loss: 0.007165064465559113\n",
      "  batch 160 loss: 0.0057979136035269715\n",
      "  batch 170 loss: 0.004673403432857981\n",
      "  batch 180 loss: 0.01545552898259075\n",
      "  batch 190 loss: 0.01380580338748132\n",
      "LOSS train 0.01380580338748132 valid 0.3090618812509881\n",
      "EPOCH 347:\n",
      "  batch 10 loss: 0.00431580563775853\n",
      "  batch 20 loss: 0.010473252129019884\n",
      "  batch 30 loss: 0.005684925019853892\n",
      "  batch 40 loss: 0.008195509163091686\n",
      "  batch 50 loss: 0.005399773471279445\n",
      "  batch 60 loss: 0.004619051062792323\n",
      "  batch 70 loss: 0.0028959314662863277\n",
      "  batch 80 loss: 0.004864580477078384\n",
      "  batch 90 loss: 0.009255693253118124\n",
      "  batch 100 loss: 0.006591694777137036\n",
      "  batch 110 loss: 0.0038617788497276708\n",
      "  batch 120 loss: 0.00420367600455549\n",
      "  batch 130 loss: 0.009809264001830797\n",
      "  batch 140 loss: 0.003047930781242769\n",
      "  batch 150 loss: 0.004498299054912991\n",
      "  batch 160 loss: 0.005229650049406587\n",
      "  batch 170 loss: 0.012855142961876708\n",
      "  batch 180 loss: 0.006383614217469358\n",
      "  batch 190 loss: 0.004426351564676167\n",
      "LOSS train 0.004426351564676167 valid 0.3018108070100895\n",
      "EPOCH 348:\n",
      "  batch 10 loss: 0.006154989423777124\n",
      "  batch 20 loss: 0.009537803670049527\n",
      "  batch 30 loss: 0.0050617345599775375\n",
      "  batch 40 loss: 0.008536925030864317\n",
      "  batch 50 loss: 0.0039052089458550656\n",
      "  batch 60 loss: 0.008528909985875544\n",
      "  batch 70 loss: 0.008879746400256749\n",
      "  batch 80 loss: 0.003752004902244721\n",
      "  batch 90 loss: 0.007136572587655187\n",
      "  batch 100 loss: 0.006870651131336558\n",
      "  batch 110 loss: 0.0024487402677626944\n",
      "  batch 120 loss: 0.003025795278348653\n",
      "  batch 130 loss: 0.0032716171809553884\n",
      "  batch 140 loss: 0.007712437200154909\n",
      "  batch 150 loss: 0.00916758666833104\n",
      "  batch 160 loss: 0.003960507795157752\n",
      "  batch 170 loss: 0.0036071066141111887\n",
      "  batch 180 loss: 0.0067396078627055545\n",
      "  batch 190 loss: 0.010273811925469545\n",
      "LOSS train 0.010273811925469545 valid 0.3067070100735184\n",
      "EPOCH 349:\n",
      "  batch 10 loss: 0.007345978753676263\n",
      "  batch 20 loss: 0.010958319985350328\n",
      "  batch 30 loss: 0.003473239585738952\n",
      "  batch 40 loss: 0.004176095784395528\n",
      "  batch 50 loss: 0.006609071341847539\n",
      "  batch 60 loss: 0.008300452781872992\n",
      "  batch 70 loss: 0.004683593512743301\n",
      "  batch 80 loss: 0.006019447213299145\n",
      "  batch 90 loss: 0.01648385309744622\n",
      "  batch 100 loss: 0.007703894248611931\n",
      "  batch 110 loss: 0.021713656305660578\n",
      "  batch 120 loss: 0.002270829577807376\n",
      "  batch 130 loss: 0.005854810053878623\n",
      "  batch 140 loss: 0.016061715674408818\n",
      "  batch 150 loss: 0.0062871843370615466\n",
      "  batch 160 loss: 0.004149048924988108\n",
      "  batch 170 loss: 0.0029275237558437084\n",
      "  batch 180 loss: 0.0019598058418777596\n",
      "  batch 190 loss: 0.01109891097489708\n",
      "LOSS train 0.01109891097489708 valid 0.3067173163012089\n",
      "EPOCH 350:\n",
      "  batch 10 loss: 0.008962902829167518\n",
      "  batch 20 loss: 0.00699264493081202\n",
      "  batch 30 loss: 0.006701184727269549\n",
      "  batch 40 loss: 0.005926488007600028\n",
      "  batch 50 loss: 0.0036198099236656846\n",
      "  batch 60 loss: 0.005154447907841586\n",
      "  batch 70 loss: 0.007234265589279687\n",
      "  batch 80 loss: 0.007723701834095209\n",
      "  batch 90 loss: 0.005252351001742994\n",
      "  batch 100 loss: 0.0031181104947080483\n",
      "  batch 110 loss: 0.004146655773897123\n",
      "  batch 120 loss: 0.005779630971414917\n",
      "  batch 130 loss: 0.011717461688299836\n",
      "  batch 140 loss: 0.012101272238967908\n",
      "  batch 150 loss: 0.0060497673516010765\n",
      "  batch 160 loss: 0.011869894197484143\n",
      "  batch 170 loss: 0.004823130919361596\n",
      "  batch 180 loss: 0.00389858357154651\n",
      "  batch 190 loss: 0.009563770197740773\n",
      "LOSS train 0.009563770197740773 valid 0.32094619815536346\n",
      "EPOCH 351:\n",
      "  batch 10 loss: 0.0025513621028139255\n",
      "  batch 20 loss: 0.0034724437553748542\n",
      "  batch 30 loss: 0.005880832963964622\n",
      "  batch 40 loss: 0.007725006709458882\n",
      "  batch 50 loss: 0.010580950472955663\n",
      "  batch 60 loss: 0.004960368693451756\n",
      "  batch 70 loss: 0.007091743334098055\n",
      "  batch 80 loss: 0.0028002423471860994\n",
      "  batch 90 loss: 0.009568523545389774\n",
      "  batch 100 loss: 0.004988872112559761\n",
      "  batch 110 loss: 0.0017544961345180355\n",
      "  batch 120 loss: 0.012908012753050003\n",
      "  batch 130 loss: 0.0033997596587553858\n",
      "  batch 140 loss: 0.015385295916425434\n",
      "  batch 150 loss: 0.015029031329375008\n",
      "  batch 160 loss: 0.005777909880781884\n",
      "  batch 170 loss: 0.005734296062580846\n",
      "  batch 180 loss: 0.004796672524456369\n",
      "  batch 190 loss: 0.005169747898989385\n",
      "LOSS train 0.005169747898989385 valid 0.3133421665563496\n",
      "EPOCH 352:\n",
      "  batch 10 loss: 0.0029696373847158952\n",
      "  batch 20 loss: 0.005193144805773997\n",
      "  batch 30 loss: 0.0029171377999347215\n",
      "  batch 40 loss: 0.001492870166560145\n",
      "  batch 50 loss: 0.008462208914736636\n",
      "  batch 60 loss: 0.003411600618714061\n",
      "  batch 70 loss: 0.008282733197629\n",
      "  batch 80 loss: 0.0060901204865615455\n",
      "  batch 90 loss: 0.009671132836007245\n",
      "  batch 100 loss: 0.005116945321854161\n",
      "  batch 110 loss: 0.0045374946980002665\n",
      "  batch 120 loss: 0.009023684946120625\n",
      "  batch 130 loss: 0.007654278581082785\n",
      "  batch 140 loss: 0.007300305081943748\n",
      "  batch 150 loss: 0.003897277969417701\n",
      "  batch 160 loss: 0.006153038925329213\n",
      "  batch 170 loss: 0.006450456901869473\n",
      "  batch 180 loss: 0.003761838917596805\n",
      "  batch 190 loss: 0.0028137884593547823\n",
      "LOSS train 0.0028137884593547823 valid 0.35926238991546866\n",
      "EPOCH 353:\n",
      "  batch 10 loss: 0.0039525028868451885\n",
      "  batch 20 loss: 0.03215704112830551\n",
      "  batch 30 loss: 0.003941122645056794\n",
      "  batch 40 loss: 0.01818215708416062\n",
      "  batch 50 loss: 0.011197065347516678\n",
      "  batch 60 loss: 0.005515234420573734\n",
      "  batch 70 loss: 0.0025636834552429377\n",
      "  batch 80 loss: 0.007894085341035861\n",
      "  batch 90 loss: 0.0035750096873186977\n",
      "  batch 100 loss: 0.0034988271546289697\n",
      "  batch 110 loss: 0.008057295581167523\n",
      "  batch 120 loss: 0.008868706792947023\n",
      "  batch 130 loss: 0.010249332389361853\n",
      "  batch 140 loss: 0.002194409163499067\n",
      "  batch 150 loss: 0.005219221475021918\n",
      "  batch 160 loss: 0.0037823181377545367\n",
      "  batch 170 loss: 0.002610209017160514\n",
      "  batch 180 loss: 0.005446236265231619\n",
      "  batch 190 loss: 0.011791615306221814\n",
      "LOSS train 0.011791615306221814 valid 0.30043703478489503\n",
      "EPOCH 354:\n",
      "  batch 10 loss: 0.0035787692313446586\n",
      "  batch 20 loss: 0.007900234678452555\n",
      "  batch 30 loss: 0.005471625913389744\n",
      "  batch 40 loss: 0.0036086070746023323\n",
      "  batch 50 loss: 0.00377287118321874\n",
      "  batch 60 loss: 0.010421678754880759\n",
      "  batch 70 loss: 0.004447583380924413\n",
      "  batch 80 loss: 0.0057279148746118835\n",
      "  batch 90 loss: 0.0029650688379177837\n",
      "  batch 100 loss: 0.002565976228149225\n",
      "  batch 110 loss: 0.010714528561202741\n",
      "  batch 120 loss: 0.022677695369873165\n",
      "  batch 130 loss: 0.019180189554185746\n",
      "  batch 140 loss: 0.009760427905916736\n",
      "  batch 150 loss: 0.010991448232707058\n",
      "  batch 160 loss: 0.006940709947502022\n",
      "  batch 170 loss: 0.011550863806743195\n",
      "  batch 180 loss: 0.0033231244741109833\n",
      "  batch 190 loss: 0.0038008138824608293\n",
      "LOSS train 0.0038008138824608293 valid 0.3166348887238205\n",
      "EPOCH 355:\n",
      "  batch 10 loss: 0.004080469466674508\n",
      "  batch 20 loss: 0.004867575952090419\n",
      "  batch 30 loss: 0.0026300091601326515\n",
      "  batch 40 loss: 0.007206877114094112\n",
      "  batch 50 loss: 0.004525293216300952\n",
      "  batch 60 loss: 0.0048296095700393945\n",
      "  batch 70 loss: 0.0033938480152100325\n",
      "  batch 80 loss: 0.009276351245267555\n",
      "  batch 90 loss: 0.011961884023936876\n",
      "  batch 100 loss: 0.0042449696700074925\n",
      "  batch 110 loss: 0.0030169364043089787\n",
      "  batch 120 loss: 0.003013508914624019\n",
      "  batch 130 loss: 0.006943531188307617\n",
      "  batch 140 loss: 0.003156100914155502\n",
      "  batch 150 loss: 0.005468010249745703\n",
      "  batch 160 loss: 0.01352023822748034\n",
      "  batch 170 loss: 0.020007894135474304\n",
      "  batch 180 loss: 0.006227381368486817\n",
      "  batch 190 loss: 0.004171414671259299\n",
      "LOSS train 0.004171414671259299 valid 0.31339097023501405\n",
      "EPOCH 356:\n",
      "  batch 10 loss: 0.010544886074383442\n",
      "  batch 20 loss: 0.010050243762643163\n",
      "  batch 30 loss: 0.00248155185228196\n",
      "  batch 40 loss: 0.003223039628993263\n",
      "  batch 50 loss: 0.0026414860915535245\n",
      "  batch 60 loss: 0.004292240314291007\n",
      "  batch 70 loss: 0.0062380466192735184\n",
      "  batch 80 loss: 0.005222968407824169\n",
      "  batch 90 loss: 0.004675582477551643\n",
      "  batch 100 loss: 0.006173018234551364\n",
      "  batch 110 loss: 0.002356949843533407\n",
      "  batch 120 loss: 0.01022359806930293\n",
      "  batch 130 loss: 0.006500710880789029\n",
      "  batch 140 loss: 0.00863384014045394\n",
      "  batch 150 loss: 0.01073615156958141\n",
      "  batch 160 loss: 0.003067007962621204\n",
      "  batch 170 loss: 0.006411622868637323\n",
      "  batch 180 loss: 0.005853504500637996\n",
      "  batch 190 loss: 0.0017712679408598219\n",
      "LOSS train 0.0017712679408598219 valid 0.30991489701771074\n",
      "EPOCH 357:\n",
      "  batch 10 loss: 0.003959286693222453\n",
      "  batch 20 loss: 0.007548047228571697\n",
      "  batch 30 loss: 0.0038825165628892933\n",
      "  batch 40 loss: 0.003947573685468342\n",
      "  batch 50 loss: 0.007329614340823553\n",
      "  batch 60 loss: 0.004318836750121591\n",
      "  batch 70 loss: 0.002512183109683974\n",
      "  batch 80 loss: 0.003145954056267186\n",
      "  batch 90 loss: 0.00404286142771113\n",
      "  batch 100 loss: 0.006667099992617409\n",
      "  batch 110 loss: 0.004983652823128182\n",
      "  batch 120 loss: 0.001900270485281652\n",
      "  batch 130 loss: 0.006330187374557994\n",
      "  batch 140 loss: 0.005647157057003227\n",
      "  batch 150 loss: 0.007591107576581635\n",
      "  batch 160 loss: 0.009131964361773725\n",
      "  batch 170 loss: 0.012225954093632651\n",
      "  batch 180 loss: 0.005066922903392701\n",
      "  batch 190 loss: 0.004811095390077291\n",
      "LOSS train 0.004811095390077291 valid 0.31086092290214024\n",
      "EPOCH 358:\n",
      "  batch 10 loss: 0.005388004606214736\n",
      "  batch 20 loss: 0.012248282048648206\n",
      "  batch 30 loss: 0.007118655205317026\n",
      "  batch 40 loss: 0.0031294419501023186\n",
      "  batch 50 loss: 0.0025492730520760174\n",
      "  batch 60 loss: 0.006357389837472738\n",
      "  batch 70 loss: 0.003618392455379471\n",
      "  batch 80 loss: 0.006388675274811817\n",
      "  batch 90 loss: 0.003158363494995342\n",
      "  batch 100 loss: 0.005512372163403967\n",
      "  batch 110 loss: 0.004553496892503972\n",
      "  batch 120 loss: 0.004891705643400713\n",
      "  batch 130 loss: 0.020304836148988415\n",
      "  batch 140 loss: 0.009175171655326154\n",
      "  batch 150 loss: 0.0035305490392318005\n",
      "  batch 160 loss: 0.008906690505453518\n",
      "  batch 170 loss: 0.005016738734671833\n",
      "  batch 180 loss: 0.011247787499377182\n",
      "  batch 190 loss: 0.004753733764824908\n",
      "LOSS train 0.004753733764824908 valid 0.32874382873108143\n",
      "EPOCH 359:\n",
      "  batch 10 loss: 0.005101561242418029\n",
      "  batch 20 loss: 0.0048465096558373945\n",
      "  batch 30 loss: 0.007732686175067727\n",
      "  batch 40 loss: 0.009182216986528147\n",
      "  batch 50 loss: 0.005759948276187287\n",
      "  batch 60 loss: 0.002478653361163019\n",
      "  batch 70 loss: 0.005213119287265045\n",
      "  batch 80 loss: 0.005446640983873863\n",
      "  batch 90 loss: 0.0027630567337453725\n",
      "  batch 100 loss: 0.0012359902439085246\n",
      "  batch 110 loss: 0.0032001109703998056\n",
      "  batch 120 loss: 0.005414834508415645\n",
      "  batch 130 loss: 0.011929642757692704\n",
      "  batch 140 loss: 0.0057390233495233645\n",
      "  batch 150 loss: 0.005616909716592034\n",
      "  batch 160 loss: 0.007035843839281597\n",
      "  batch 170 loss: 0.005201454488498136\n",
      "  batch 180 loss: 0.002627509343261636\n",
      "  batch 190 loss: 0.007052666057353285\n",
      "LOSS train 0.007052666057353285 valid 0.3208312378662947\n",
      "EPOCH 360:\n",
      "  batch 10 loss: 0.003363734240005556\n",
      "  batch 20 loss: 0.005056861256069567\n",
      "  batch 30 loss: 0.004669689551951706\n",
      "  batch 40 loss: 0.0016717271083095397\n",
      "  batch 50 loss: 0.006468843957773629\n",
      "  batch 60 loss: 0.01226200839562921\n",
      "  batch 70 loss: 0.002473888290646187\n",
      "  batch 80 loss: 0.011472291752377827\n",
      "  batch 90 loss: 0.001785365631774738\n",
      "  batch 100 loss: 0.004307558416621582\n",
      "  batch 110 loss: 0.0027222782676062708\n",
      "  batch 120 loss: 0.004786488191527383\n",
      "  batch 130 loss: 0.0027211978264281454\n",
      "  batch 140 loss: 0.00628209003432616\n",
      "  batch 150 loss: 0.00404271829733176\n",
      "  batch 160 loss: 0.003392051255757167\n",
      "  batch 170 loss: 0.004490981507995961\n",
      "  batch 180 loss: 0.014943093639587346\n",
      "  batch 190 loss: 0.007084240881007986\n",
      "LOSS train 0.007084240881007986 valid 0.31892704068133115\n",
      "EPOCH 361:\n",
      "  batch 10 loss: 0.003977992042547207\n",
      "  batch 20 loss: 0.0021610999219710434\n",
      "  batch 30 loss: 0.005438665446525448\n",
      "  batch 40 loss: 0.0024875974208271147\n",
      "  batch 50 loss: 0.0014084199892295147\n",
      "  batch 60 loss: 0.0036720963164666998\n",
      "  batch 70 loss: 0.011342934586970443\n",
      "  batch 80 loss: 0.0035033199967697826\n",
      "  batch 90 loss: 0.0049538407936353225\n",
      "  batch 100 loss: 0.004984751569733703\n",
      "  batch 110 loss: 0.007500716882132963\n",
      "  batch 120 loss: 0.003861897623912114\n",
      "  batch 130 loss: 0.008100554323615939\n",
      "  batch 140 loss: 0.008793392833941028\n",
      "  batch 150 loss: 0.005617987885303677\n",
      "  batch 160 loss: 0.005806834575695774\n",
      "  batch 170 loss: 0.004024167772436549\n",
      "  batch 180 loss: 0.00707065623141716\n",
      "  batch 190 loss: 0.004018312736671703\n",
      "LOSS train 0.004018312736671703 valid 0.31820934072735807\n",
      "EPOCH 362:\n",
      "  batch 10 loss: 0.0031772315870785663\n",
      "  batch 20 loss: 0.005982827744431063\n",
      "  batch 30 loss: 0.0020590989347894608\n",
      "  batch 40 loss: 0.0076109098955695845\n",
      "  batch 50 loss: 0.0067713818348286384\n",
      "  batch 60 loss: 0.005125578299845302\n",
      "  batch 70 loss: 0.0034119635814302994\n",
      "  batch 80 loss: 0.00423766303203763\n",
      "  batch 90 loss: 0.004970725522197484\n",
      "  batch 100 loss: 0.0044892530410022505\n",
      "  batch 110 loss: 0.0021195622900334497\n",
      "  batch 120 loss: 0.0062662394173626625\n",
      "  batch 130 loss: 0.004846145323563178\n",
      "  batch 140 loss: 0.0034373155826976642\n",
      "  batch 150 loss: 0.004720005290542418\n",
      "  batch 160 loss: 0.002046378138120275\n",
      "  batch 170 loss: 0.0024510452441731446\n",
      "  batch 180 loss: 0.008808949375005959\n",
      "  batch 190 loss: 0.003297987065919017\n",
      "LOSS train 0.003297987065919017 valid 0.32868997597359767\n",
      "EPOCH 363:\n",
      "  batch 10 loss: 0.005473999901467863\n",
      "  batch 20 loss: 0.010737805868350847\n",
      "  batch 30 loss: 0.006707070681653704\n",
      "  batch 40 loss: 0.002737157029977411\n",
      "  batch 50 loss: 0.0034347048705420493\n",
      "  batch 60 loss: 0.0021175088368124762\n",
      "  batch 70 loss: 0.009119798230144837\n",
      "  batch 80 loss: 0.006208700812202039\n",
      "  batch 90 loss: 0.00876043736890324\n",
      "  batch 100 loss: 0.004041964658201636\n",
      "  batch 110 loss: 0.0013413634855055534\n",
      "  batch 120 loss: 0.009591733051848728\n",
      "  batch 130 loss: 0.005098313569634172\n",
      "  batch 140 loss: 0.003996295741449707\n",
      "  batch 150 loss: 0.002686683831545622\n",
      "  batch 160 loss: 0.001916642188319173\n",
      "  batch 170 loss: 0.003709636342511402\n",
      "  batch 180 loss: 0.00468753590831934\n",
      "  batch 190 loss: 0.003611498095040133\n",
      "LOSS train 0.003611498095040133 valid 0.32283806483934624\n",
      "EPOCH 364:\n",
      "  batch 10 loss: 0.004365505934089064\n",
      "  batch 20 loss: 0.009294904352627498\n",
      "  batch 30 loss: 0.006362895208716424\n",
      "  batch 40 loss: 0.005722177888242186\n",
      "  batch 50 loss: 0.006042191519377837\n",
      "  batch 60 loss: 0.0029467712100029074\n",
      "  batch 70 loss: 0.0016425742961210688\n",
      "  batch 80 loss: 0.006102359815468894\n",
      "  batch 90 loss: 0.0012635092246171098\n",
      "  batch 100 loss: 0.005335030330675572\n",
      "  batch 110 loss: 0.003014077577496721\n",
      "  batch 120 loss: 0.010532591862003926\n",
      "  batch 130 loss: 0.0033188765973305224\n",
      "  batch 140 loss: 0.0035483489973344717\n",
      "  batch 150 loss: 0.0034082414117504456\n",
      "  batch 160 loss: 0.004135103128969498\n",
      "  batch 170 loss: 0.005187033699985477\n",
      "  batch 180 loss: 0.0010968275903671554\n",
      "  batch 190 loss: 0.004181561365532893\n",
      "LOSS train 0.004181561365532893 valid 0.3120768318630264\n",
      "EPOCH 365:\n",
      "  batch 10 loss: 0.0015687827315240099\n",
      "  batch 20 loss: 0.006723845543277207\n",
      "  batch 30 loss: 0.0014508402249346019\n",
      "  batch 40 loss: 0.002776122469193609\n",
      "  batch 50 loss: 0.0029684784772115334\n",
      "  batch 60 loss: 0.002141589482341999\n",
      "  batch 70 loss: 0.002071752286707351\n",
      "  batch 80 loss: 0.011483889493622534\n",
      "  batch 90 loss: 0.0026606245950871353\n",
      "  batch 100 loss: 0.006040508757075536\n",
      "  batch 110 loss: 0.012689399464596818\n",
      "  batch 120 loss: 0.001378206392116965\n",
      "  batch 130 loss: 0.001892595007694453\n",
      "  batch 140 loss: 0.005618420283792424\n",
      "  batch 150 loss: 0.0017818267895734151\n",
      "  batch 160 loss: 0.004979409173214577\n",
      "  batch 170 loss: 0.01178146187757818\n",
      "  batch 180 loss: 0.013100537473907537\n",
      "  batch 190 loss: 0.0046910038336914536\n",
      "LOSS train 0.0046910038336914536 valid 0.31344032116024817\n",
      "EPOCH 366:\n",
      "  batch 10 loss: 0.0031797512452042655\n",
      "  batch 20 loss: 0.0014749514345899328\n",
      "  batch 30 loss: 0.002444645593512007\n",
      "  batch 40 loss: 0.009118460329732158\n",
      "  batch 50 loss: 0.002563586735547574\n",
      "  batch 60 loss: 0.003423774466311258\n",
      "  batch 70 loss: 0.0063559067884567\n",
      "  batch 80 loss: 0.002057170682712517\n",
      "  batch 90 loss: 0.009723130257285106\n",
      "  batch 100 loss: 0.0016238657783759435\n",
      "  batch 110 loss: 0.004726405631328134\n",
      "  batch 120 loss: 0.004450755029250786\n",
      "  batch 130 loss: 0.002566799850828083\n",
      "  batch 140 loss: 0.00427344167006094\n",
      "  batch 150 loss: 0.004175122371500884\n",
      "  batch 160 loss: 0.003221080707814927\n",
      "  batch 170 loss: 0.006713672337740206\n",
      "  batch 180 loss: 0.003917156037522318\n",
      "  batch 190 loss: 0.005950344246360828\n",
      "LOSS train 0.005950344246360828 valid 0.3146488625193288\n",
      "EPOCH 367:\n",
      "  batch 10 loss: 0.004314937680754127\n",
      "  batch 20 loss: 0.004069324021494935\n",
      "  batch 30 loss: 0.0019359611206381188\n",
      "  batch 40 loss: 0.004417007240775206\n",
      "  batch 50 loss: 0.007647807329321488\n",
      "  batch 60 loss: 0.0017345043615177858\n",
      "  batch 70 loss: 0.0012868158336271306\n",
      "  batch 80 loss: 0.0044302530737091676\n",
      "  batch 90 loss: 0.002756356007792249\n",
      "  batch 100 loss: 0.0032957435204563623\n",
      "  batch 110 loss: 0.004526333241275715\n",
      "  batch 120 loss: 0.004984447915834522\n",
      "  batch 130 loss: 0.0019961103670283363\n",
      "  batch 140 loss: 0.007434784265046801\n",
      "  batch 150 loss: 0.004439162839912569\n",
      "  batch 160 loss: 0.005021419807695793\n",
      "  batch 170 loss: 0.01830703893255503\n",
      "  batch 180 loss: 0.18800292118722126\n",
      "  batch 190 loss: 1.1763051122361543\n",
      "LOSS train 1.1763051122361543 valid 0.6096327635859339\n",
      "EPOCH 368:\n",
      "  batch 10 loss: 0.20331932216781468\n",
      "  batch 20 loss: 0.5791068204578209\n",
      "  batch 30 loss: 0.16633976434339673\n",
      "  batch 40 loss: 0.15859060180041737\n",
      "  batch 50 loss: 0.31212382735095046\n",
      "  batch 60 loss: 0.042304252075986426\n",
      "  batch 70 loss: 0.019797411390771912\n",
      "  batch 80 loss: 0.037527924483758565\n",
      "  batch 90 loss: 0.02353006311371928\n",
      "  batch 100 loss: 0.011379461154257342\n",
      "  batch 110 loss: 0.013885519551604376\n",
      "  batch 120 loss: 0.33524517922436325\n",
      "  batch 130 loss: 0.0956379030955901\n",
      "  batch 140 loss: 0.057264430112970645\n",
      "  batch 150 loss: 0.05805089597781414\n",
      "  batch 160 loss: 0.009611204398493101\n",
      "  batch 170 loss: 0.0036220483736144615\n",
      "  batch 180 loss: 0.008334482937020483\n",
      "  batch 190 loss: 0.11788058684001541\n",
      "LOSS train 0.11788058684001541 valid 0.45245904544647797\n",
      "EPOCH 369:\n",
      "  batch 10 loss: 0.18068069747870225\n",
      "  batch 20 loss: 0.012556718241887487\n",
      "  batch 30 loss: 0.02573001319929773\n",
      "  batch 40 loss: 0.043894213193962625\n",
      "  batch 50 loss: 0.016895728511209997\n",
      "  batch 60 loss: 0.017022817158314753\n",
      "  batch 70 loss: 0.08416209190653205\n",
      "  batch 80 loss: 0.017041176442701554\n",
      "  batch 90 loss: 0.01775452605972987\n",
      "  batch 100 loss: 0.05696861032719482\n",
      "  batch 110 loss: 0.029601483037441766\n",
      "  batch 120 loss: 0.041213381131325375\n",
      "  batch 130 loss: 0.12850555716324266\n",
      "  batch 140 loss: 0.7229597870888824\n",
      "  batch 150 loss: 0.11762400404664959\n",
      "  batch 160 loss: 0.11229190276150121\n",
      "  batch 170 loss: 0.14091327490898492\n",
      "  batch 180 loss: 0.011143451935399895\n",
      "  batch 190 loss: 0.014798434059945543\n",
      "LOSS train 0.014798434059945543 valid 0.3050991546204133\n",
      "EPOCH 370:\n",
      "  batch 10 loss: 0.01358963492574503\n",
      "  batch 20 loss: 0.027240731060631164\n",
      "  batch 30 loss: 0.024171908960619957\n",
      "  batch 40 loss: 0.1994087179419836\n",
      "  batch 50 loss: 0.030297558768631917\n",
      "  batch 60 loss: 0.013104674695296126\n",
      "  batch 70 loss: 0.32131505488960527\n",
      "  batch 80 loss: 0.06710744012898413\n",
      "  batch 90 loss: 0.060027548693081204\n",
      "  batch 100 loss: 0.08956974340349574\n",
      "  batch 110 loss: 0.40278590052807883\n",
      "  batch 120 loss: 0.30974830056422603\n",
      "  batch 130 loss: 0.4692467056849294\n",
      "  batch 140 loss: 0.131892659019735\n",
      "  batch 150 loss: 0.2134352224564509\n",
      "  batch 160 loss: 0.6875809074187357\n",
      "  batch 170 loss: 0.0635880439194402\n",
      "  batch 180 loss: 0.13218773296066483\n",
      "  batch 190 loss: 0.49974905142253745\n",
      "LOSS train 0.49974905142253745 valid 0.6264948637189147\n",
      "EPOCH 371:\n",
      "  batch 10 loss: 0.19334821873292185\n",
      "  batch 20 loss: 0.01192805502810188\n",
      "  batch 30 loss: 0.061999931186710454\n",
      "  batch 40 loss: 0.03137674265439596\n",
      "  batch 50 loss: 0.04749359729526077\n",
      "  batch 60 loss: 0.05684107628067068\n",
      "  batch 70 loss: 0.02496897080235385\n",
      "  batch 80 loss: 0.00975772323135402\n",
      "  batch 90 loss: 0.044763649197261654\n",
      "  batch 100 loss: 0.04873788624611279\n",
      "  batch 110 loss: 0.019526032744619216\n",
      "  batch 120 loss: 0.02676931518622041\n",
      "  batch 130 loss: 0.029319313668361248\n",
      "  batch 140 loss: 0.013811933984925417\n",
      "  batch 150 loss: 0.25503781767652073\n",
      "  batch 160 loss: 0.010621303014134753\n",
      "  batch 170 loss: 0.015897353856550465\n",
      "  batch 180 loss: 0.06826601283739801\n",
      "  batch 190 loss: 0.06648590050373145\n",
      "LOSS train 0.06648590050373145 valid 0.26221530026103324\n",
      "EPOCH 372:\n",
      "  batch 10 loss: 0.006036639945443767\n",
      "  batch 20 loss: 0.012676330820386283\n",
      "  batch 30 loss: 0.011566588038449765\n",
      "  batch 40 loss: 0.02462109037892333\n",
      "  batch 50 loss: 0.005284949612791934\n",
      "  batch 60 loss: 0.021539430450876296\n",
      "  batch 70 loss: 0.0036438022255651957\n",
      "  batch 80 loss: 0.005773732224869832\n",
      "  batch 90 loss: 0.0162209751317846\n",
      "  batch 100 loss: 0.01815018097360053\n",
      "  batch 110 loss: 0.025266868986113876\n",
      "  batch 120 loss: 0.01972430769588982\n",
      "  batch 130 loss: 0.020200588626755688\n",
      "  batch 140 loss: 0.005121407316948989\n",
      "  batch 150 loss: 0.004904264046086837\n",
      "  batch 160 loss: 0.01938265698224484\n",
      "  batch 170 loss: 0.007325914187305216\n",
      "  batch 180 loss: 0.15452163370784433\n",
      "  batch 190 loss: 0.2176022806417734\n",
      "LOSS train 0.2176022806417734 valid 0.3134931076385173\n",
      "EPOCH 373:\n",
      "  batch 10 loss: 0.01613934816871847\n",
      "  batch 20 loss: 0.009132382247850046\n",
      "  batch 30 loss: 0.0069808142328952275\n",
      "  batch 40 loss: 0.019049483061820638\n",
      "  batch 50 loss: 0.08589006914919253\n",
      "  batch 60 loss: 0.0470668298647066\n",
      "  batch 70 loss: 0.010934730270525782\n",
      "  batch 80 loss: 0.021426504835149275\n",
      "  batch 90 loss: 0.05807976689654595\n",
      "  batch 100 loss: 0.04070949043766632\n",
      "  batch 110 loss: 0.012267689008331218\n",
      "  batch 120 loss: 0.1358582869394354\n",
      "  batch 130 loss: 0.03460216144047763\n",
      "  batch 140 loss: 0.009983102573536939\n",
      "  batch 150 loss: 0.04044520116596573\n",
      "  batch 160 loss: 0.012316561505402035\n",
      "  batch 170 loss: 0.011043167488338668\n",
      "  batch 180 loss: 0.016467507842685337\n",
      "  batch 190 loss: 0.004538637000541712\n",
      "LOSS train 0.004538637000541712 valid 0.2986880480130577\n",
      "EPOCH 374:\n",
      "  batch 10 loss: 0.004387136029913563\n",
      "  batch 20 loss: 0.0075405250577148305\n",
      "  batch 30 loss: 0.01133619725233075\n",
      "  batch 40 loss: 0.015088189673747365\n",
      "  batch 50 loss: 0.00781726462569452\n",
      "  batch 60 loss: 0.007599252332011019\n",
      "  batch 70 loss: 0.006555366041962429\n",
      "  batch 80 loss: 0.0083849214470888\n",
      "  batch 90 loss: 0.02888609944975542\n",
      "  batch 100 loss: 0.009041415897507754\n",
      "  batch 110 loss: 0.011037656687059893\n",
      "  batch 120 loss: 0.016109316248491722\n",
      "  batch 130 loss: 0.011613681272751818\n",
      "  batch 140 loss: 0.010027206313543502\n",
      "  batch 150 loss: 0.015902719810992495\n",
      "  batch 160 loss: 0.008044945743517929\n",
      "  batch 170 loss: 0.006816318493263651\n",
      "  batch 180 loss: 0.004976916237123419\n",
      "  batch 190 loss: 0.008272588296478034\n",
      "LOSS train 0.008272588296478034 valid 0.2871516502455577\n",
      "EPOCH 375:\n",
      "  batch 10 loss: 0.009134327558950873\n",
      "  batch 20 loss: 0.007253828909631466\n",
      "  batch 30 loss: 0.00947920330963541\n",
      "  batch 40 loss: 0.006439565406441261\n",
      "  batch 50 loss: 0.009172814703742205\n",
      "  batch 60 loss: 0.004591958176092703\n",
      "  batch 70 loss: 0.006286628143493544\n",
      "  batch 80 loss: 0.005104839943766137\n",
      "  batch 90 loss: 0.0069253115426249675\n",
      "  batch 100 loss: 0.012715758343452422\n",
      "  batch 110 loss: 0.007436094823503936\n",
      "  batch 120 loss: 0.004731655913710142\n",
      "  batch 130 loss: 0.005666262792325227\n",
      "  batch 140 loss: 0.013304141692242411\n",
      "  batch 150 loss: 0.010533853522389336\n",
      "  batch 160 loss: 0.0054890858194873715\n",
      "  batch 170 loss: 0.005968420823717224\n",
      "  batch 180 loss: 0.0019047980167215429\n",
      "  batch 190 loss: 0.002181200102288017\n",
      "LOSS train 0.002181200102288017 valid 0.2890521816972563\n",
      "EPOCH 376:\n",
      "  batch 10 loss: 0.003978595210824665\n",
      "  batch 20 loss: 0.007535531411130591\n",
      "  batch 30 loss: 0.008040654974070093\n",
      "  batch 40 loss: 0.0021078223280539985\n",
      "  batch 50 loss: 0.007704197266119195\n",
      "  batch 60 loss: 0.0099017182728403\n",
      "  batch 70 loss: 0.003942762087147856\n",
      "  batch 80 loss: 0.00396543304436534\n",
      "  batch 90 loss: 0.008949764179101294\n",
      "  batch 100 loss: 0.009605440641892926\n",
      "  batch 110 loss: 0.001941460831164221\n",
      "  batch 120 loss: 0.006902114617059851\n",
      "  batch 130 loss: 0.006751620838696226\n",
      "  batch 140 loss: 0.01200973946869155\n",
      "  batch 150 loss: 0.0025396970873998725\n",
      "  batch 160 loss: 0.00958653207843554\n",
      "  batch 170 loss: 0.0064134621008804515\n",
      "  batch 180 loss: 0.011103166429330713\n",
      "  batch 190 loss: 0.007045534641437712\n",
      "LOSS train 0.007045534641437712 valid 0.27968681342034674\n",
      "EPOCH 377:\n",
      "  batch 10 loss: 0.006672818345582953\n",
      "  batch 20 loss: 0.006036726440295581\n",
      "  batch 30 loss: 0.011159536315022933\n",
      "  batch 40 loss: 0.008531395008192532\n",
      "  batch 50 loss: 0.0052420328340033965\n",
      "  batch 60 loss: 0.004363860056488989\n",
      "  batch 70 loss: 0.00475799345605239\n",
      "  batch 80 loss: 0.00802903137698081\n",
      "  batch 90 loss: 0.007944444730595811\n",
      "  batch 100 loss: 0.003901842672117084\n",
      "  batch 110 loss: 0.0066712228748542655\n",
      "  batch 120 loss: 0.002932823869355161\n",
      "  batch 130 loss: 0.005109385396809785\n",
      "  batch 140 loss: 0.007515668449330803\n",
      "  batch 150 loss: 0.010659083025132077\n",
      "  batch 160 loss: 0.010973367786289146\n",
      "  batch 170 loss: 0.00791761288283226\n",
      "  batch 180 loss: 0.0027574010889864554\n",
      "  batch 190 loss: 0.007299342837796985\n",
      "LOSS train 0.007299342837796985 valid 0.29641585715610436\n",
      "EPOCH 378:\n",
      "  batch 10 loss: 0.0019452135814560733\n",
      "  batch 20 loss: 0.007806299718372145\n",
      "  batch 30 loss: 0.004837248128466598\n",
      "  batch 40 loss: 0.004889330679459647\n",
      "  batch 50 loss: 0.005573600719000637\n",
      "  batch 60 loss: 0.0010215209849206985\n",
      "  batch 70 loss: 0.00897681315740897\n",
      "  batch 80 loss: 0.008113764326276396\n",
      "  batch 90 loss: 0.00701368946142793\n",
      "  batch 100 loss: 0.017442057365863663\n",
      "  batch 110 loss: 0.006549578356207064\n",
      "  batch 120 loss: 0.001762759886249654\n",
      "  batch 130 loss: 0.002400342859095872\n",
      "  batch 140 loss: 0.004480485517980526\n",
      "  batch 150 loss: 0.005035390296947639\n",
      "  batch 160 loss: 0.006911210874338103\n",
      "  batch 170 loss: 0.003895198046677706\n",
      "  batch 180 loss: 0.010274822359929204\n",
      "  batch 190 loss: 0.019367882550567827\n",
      "LOSS train 0.019367882550567827 valid 0.2984167058374848\n",
      "EPOCH 379:\n",
      "  batch 10 loss: 0.008524758633268448\n",
      "  batch 20 loss: 0.005862600566626952\n",
      "  batch 30 loss: 0.0036079654304060683\n",
      "  batch 40 loss: 0.009705631227419787\n",
      "  batch 50 loss: 0.009851360534824317\n",
      "  batch 60 loss: 0.010815204187887239\n",
      "  batch 70 loss: 0.002165955295083677\n",
      "  batch 80 loss: 0.005576732169356546\n",
      "  batch 90 loss: 0.002487992670958583\n",
      "  batch 100 loss: 0.002344208486442767\n",
      "  batch 110 loss: 0.004052992442764492\n",
      "  batch 120 loss: 0.004398231014737064\n",
      "  batch 130 loss: 0.004300675032759926\n",
      "  batch 140 loss: 0.006968953432360791\n",
      "  batch 150 loss: 0.006546614994107358\n",
      "  batch 160 loss: 0.007902082188027747\n",
      "  batch 170 loss: 0.0029928561460565105\n",
      "  batch 180 loss: 0.012173584676622795\n",
      "  batch 190 loss: 0.001693404147590627\n",
      "LOSS train 0.001693404147590627 valid 0.291559141527277\n",
      "EPOCH 380:\n",
      "  batch 10 loss: 0.00652178714097289\n",
      "  batch 20 loss: 0.0070285970432678365\n",
      "  batch 30 loss: 0.008158257434808292\n",
      "  batch 40 loss: 0.008823294946758153\n",
      "  batch 50 loss: 0.0015945117027314382\n",
      "  batch 60 loss: 0.003487603039724263\n",
      "  batch 70 loss: 0.001890181049127193\n",
      "  batch 80 loss: 0.002134059238397157\n",
      "  batch 90 loss: 0.0019504089910775235\n",
      "  batch 100 loss: 0.010449914895136202\n",
      "  batch 110 loss: 0.0029191414172256456\n",
      "  batch 120 loss: 0.01068159941121749\n",
      "  batch 130 loss: 0.003975519058968757\n",
      "  batch 140 loss: 0.006113160465869782\n",
      "  batch 150 loss: 0.006104492803231665\n",
      "  batch 160 loss: 0.0030504230309787773\n",
      "  batch 170 loss: 0.003814569498968012\n",
      "  batch 180 loss: 0.005748106055091284\n",
      "  batch 190 loss: 0.006900380758002989\n",
      "LOSS train 0.006900380758002989 valid 0.29406016645301\n",
      "EPOCH 381:\n",
      "  batch 10 loss: 0.0036011498030680046\n",
      "  batch 20 loss: 0.004013740358860929\n",
      "  batch 30 loss: 0.005575169187756046\n",
      "  batch 40 loss: 0.004383158860360936\n",
      "  batch 50 loss: 0.007467464981847627\n",
      "  batch 60 loss: 0.011021879991790228\n",
      "  batch 70 loss: 0.0040336099614762585\n",
      "  batch 80 loss: 0.005764164293506724\n",
      "  batch 90 loss: 0.005303295392721452\n",
      "  batch 100 loss: 0.0029229795991838614\n",
      "  batch 110 loss: 0.003361875906176692\n",
      "  batch 120 loss: 0.0027868767529930947\n",
      "  batch 130 loss: 0.007663564316297311\n",
      "  batch 140 loss: 0.004651985277752147\n",
      "  batch 150 loss: 0.008019965261161133\n",
      "  batch 160 loss: 0.004371275941886488\n",
      "  batch 170 loss: 0.002540499532574181\n",
      "  batch 180 loss: 0.0036622666083786725\n",
      "  batch 190 loss: 0.0027613802015405754\n",
      "LOSS train 0.0027613802015405754 valid 0.30275785488610535\n",
      "EPOCH 382:\n",
      "  batch 10 loss: 0.0028772998550749664\n",
      "  batch 20 loss: 0.008197469605471496\n",
      "  batch 30 loss: 0.010362149370833152\n",
      "  batch 40 loss: 0.008932353856624076\n",
      "  batch 50 loss: 0.0018974633734352153\n",
      "  batch 60 loss: 0.004246475033022534\n",
      "  batch 70 loss: 0.004953722780010139\n",
      "  batch 80 loss: 0.0055497475730305725\n",
      "  batch 90 loss: 0.004199297841388727\n",
      "  batch 100 loss: 0.0015601915382518427\n",
      "  batch 110 loss: 0.005843025969005567\n",
      "  batch 120 loss: 0.0024009544636951928\n",
      "  batch 130 loss: 0.008131782813146592\n",
      "  batch 140 loss: 0.003374884818975943\n",
      "  batch 150 loss: 0.013381725483352902\n",
      "  batch 160 loss: 0.006675313569711072\n",
      "  batch 170 loss: 0.003526805312945669\n",
      "  batch 180 loss: 0.002156534284085687\n",
      "  batch 190 loss: 0.004867042915105912\n",
      "LOSS train 0.004867042915105912 valid 0.2924297817059733\n",
      "EPOCH 383:\n",
      "  batch 10 loss: 0.004567291887196489\n",
      "  batch 20 loss: 0.0058670783223703895\n",
      "  batch 30 loss: 0.007507260978880481\n",
      "  batch 40 loss: 0.0016514312053907076\n",
      "  batch 50 loss: 0.005034966967339472\n",
      "  batch 60 loss: 0.003889063095866163\n",
      "  batch 70 loss: 0.004692100794255793\n",
      "  batch 80 loss: 0.0015395608580831776\n",
      "  batch 90 loss: 0.007294561710179437\n",
      "  batch 100 loss: 0.006586437680115864\n",
      "  batch 110 loss: 0.0066652145052643165\n",
      "  batch 120 loss: 0.005078351018873661\n",
      "  batch 130 loss: 0.011803862295857748\n",
      "  batch 140 loss: 0.0012737381938867642\n",
      "  batch 150 loss: 0.0014114653755811445\n",
      "  batch 160 loss: 0.005167926183431604\n",
      "  batch 170 loss: 0.0044146747722287214\n",
      "  batch 180 loss: 0.004508326736197432\n",
      "  batch 190 loss: 0.0026325221388210805\n",
      "LOSS train 0.0026325221388210805 valid 0.28658628557488397\n",
      "EPOCH 384:\n",
      "  batch 10 loss: 0.001576005864101937\n",
      "  batch 20 loss: 0.005158886022938703\n",
      "  batch 30 loss: 0.0039766556932335105\n",
      "  batch 40 loss: 0.0014716099923276714\n",
      "  batch 50 loss: 0.006167997512831392\n",
      "  batch 60 loss: 0.00865032345260488\n",
      "  batch 70 loss: 0.0033195392864598717\n",
      "  batch 80 loss: 0.001940759579727569\n",
      "  batch 90 loss: 0.004746314654310168\n",
      "  batch 100 loss: 0.003658354413175857\n",
      "  batch 110 loss: 0.001185531668465778\n",
      "  batch 120 loss: 0.0006964811125385495\n",
      "  batch 130 loss: 0.0041766625446937765\n",
      "  batch 140 loss: 0.011178982696528372\n",
      "  batch 150 loss: 0.0038639023133960394\n",
      "  batch 160 loss: 0.01368530088535067\n",
      "  batch 170 loss: 0.009978665912805695\n",
      "  batch 180 loss: 0.007245249325849556\n",
      "  batch 190 loss: 0.0035261615754903276\n",
      "LOSS train 0.0035261615754903276 valid 0.2935751167802704\n",
      "EPOCH 385:\n",
      "  batch 10 loss: 0.004782622957391425\n",
      "  batch 20 loss: 0.003251614929322777\n",
      "  batch 30 loss: 0.004554053211648856\n",
      "  batch 40 loss: 0.007794842970096738\n",
      "  batch 50 loss: 0.0059376681162007115\n",
      "  batch 60 loss: 0.002317520223868286\n",
      "  batch 70 loss: 0.005719814243830968\n",
      "  batch 80 loss: 0.004537279307986886\n",
      "  batch 90 loss: 0.00472144893513402\n",
      "  batch 100 loss: 0.0014328323027086754\n",
      "  batch 110 loss: 0.004031813834620657\n",
      "  batch 120 loss: 0.0038144873805606495\n",
      "  batch 130 loss: 0.0035274951406847775\n",
      "  batch 140 loss: 0.0008865329037774927\n",
      "  batch 150 loss: 0.008154487203624683\n",
      "  batch 160 loss: 0.002963021353505546\n",
      "  batch 170 loss: 0.0034463474633014356\n",
      "  batch 180 loss: 0.004346011401497663\n",
      "  batch 190 loss: 0.0052603184956431905\n",
      "LOSS train 0.0052603184956431905 valid 0.3080818242164249\n",
      "EPOCH 386:\n",
      "  batch 10 loss: 0.0035936410575684976\n",
      "  batch 20 loss: 0.0055572986589425\n",
      "  batch 30 loss: 0.0024529916284597776\n",
      "  batch 40 loss: 0.0030680738836196043\n",
      "  batch 50 loss: 0.006393071604088618\n",
      "  batch 60 loss: 0.003598278356599849\n",
      "  batch 70 loss: 0.0017530102005579805\n",
      "  batch 80 loss: 0.0020877772551598285\n",
      "  batch 90 loss: 0.00796937057422653\n",
      "  batch 100 loss: 0.002248688615454597\n",
      "  batch 110 loss: 0.001963848457243245\n",
      "  batch 120 loss: 0.0037808232669000576\n",
      "  batch 130 loss: 0.008886993894242324\n",
      "  batch 140 loss: 0.005704720301812927\n",
      "  batch 150 loss: 0.005544575413776442\n",
      "  batch 160 loss: 0.005880177000056186\n",
      "  batch 170 loss: 0.006093526628182388\n",
      "  batch 180 loss: 0.0053118247028962175\n",
      "  batch 190 loss: 0.003395099001351376\n",
      "LOSS train 0.003395099001351376 valid 0.3047668378742071\n",
      "EPOCH 387:\n",
      "  batch 10 loss: 0.0017616036671455504\n",
      "  batch 20 loss: 0.0012723034181448157\n",
      "  batch 30 loss: 0.0030083082699418283\n",
      "  batch 40 loss: 0.008937858259400855\n",
      "  batch 50 loss: 0.009663171346567445\n",
      "  batch 60 loss: 0.003921890520803118\n",
      "  batch 70 loss: 0.0069182273413616715\n",
      "  batch 80 loss: 0.0021486053982798124\n",
      "  batch 90 loss: 0.005557033097041142\n",
      "  batch 100 loss: 0.007077824013300216\n",
      "  batch 110 loss: 0.007740354829153517\n",
      "  batch 120 loss: 0.0031586398973708187\n",
      "  batch 130 loss: 0.005733980605040756\n",
      "  batch 140 loss: 0.0026083487780354632\n",
      "  batch 150 loss: 0.0050053028798053845\n",
      "  batch 160 loss: 0.0032546923689833916\n",
      "  batch 170 loss: 0.001350972304451936\n",
      "  batch 180 loss: 0.00447484615554572\n",
      "  batch 190 loss: 0.005465848065990109\n",
      "LOSS train 0.005465848065990109 valid 0.295887573590615\n",
      "EPOCH 388:\n",
      "  batch 10 loss: 0.0029575076826487746\n",
      "  batch 20 loss: 0.006608576871581384\n",
      "  batch 30 loss: 0.00574090962529965\n",
      "  batch 40 loss: 0.0027217728653367245\n",
      "  batch 50 loss: 0.007448161750010484\n",
      "  batch 60 loss: 0.004606713618100855\n",
      "  batch 70 loss: 0.00308423688857431\n",
      "  batch 80 loss: 0.003421813826031439\n",
      "  batch 90 loss: 0.0025531408474243735\n",
      "  batch 100 loss: 0.003649524387657088\n",
      "  batch 110 loss: 0.005155439825628427\n",
      "  batch 120 loss: 0.0016432418269188176\n",
      "  batch 130 loss: 0.003232220771489125\n",
      "  batch 140 loss: 0.007656156969696149\n",
      "  batch 150 loss: 0.003763177912267679\n",
      "  batch 160 loss: 0.00415464545871842\n",
      "  batch 170 loss: 0.006862897087392206\n",
      "  batch 180 loss: 0.005081854939375319\n",
      "  batch 190 loss: 0.01662952605710615\n",
      "LOSS train 0.01662952605710615 valid 0.3183727281433654\n",
      "EPOCH 389:\n",
      "  batch 10 loss: 0.0023924600946628517\n",
      "  batch 20 loss: 0.013516633979734927\n",
      "  batch 30 loss: 0.12240741233011647\n",
      "  batch 40 loss: 0.08183127355093163\n",
      "  batch 50 loss: 0.023042189279519044\n",
      "  batch 60 loss: 0.005099758336701399\n",
      "  batch 70 loss: 0.00481764449822748\n",
      "  batch 80 loss: 0.002228741440372062\n",
      "  batch 90 loss: 0.00362319960762818\n",
      "  batch 100 loss: 0.0069298740861086115\n",
      "  batch 110 loss: 0.00324363202301754\n",
      "  batch 120 loss: 0.00289291913034031\n",
      "  batch 130 loss: 0.019830262591633917\n",
      "  batch 140 loss: 0.0027454611372974115\n",
      "  batch 150 loss: 0.006042137374799772\n",
      "  batch 160 loss: 0.011405245316487367\n",
      "  batch 170 loss: 0.004269411522966493\n",
      "  batch 180 loss: 0.0031744615278142875\n",
      "  batch 190 loss: 0.0007601037770674779\n",
      "LOSS train 0.0007601037770674779 valid 0.29513219684825526\n",
      "EPOCH 390:\n",
      "  batch 10 loss: 0.0031014954669537076\n",
      "  batch 20 loss: 0.0044272655169422135\n",
      "  batch 30 loss: 0.002392898252406894\n",
      "  batch 40 loss: 0.0031077402914725383\n",
      "  batch 50 loss: 0.00458678483826116\n",
      "  batch 60 loss: 0.002589484455062063\n",
      "  batch 70 loss: 0.0026767830108468615\n",
      "  batch 80 loss: 0.0065773726126918675\n",
      "  batch 90 loss: 0.0037456248687924186\n",
      "  batch 100 loss: 0.005661485184575099\n",
      "  batch 110 loss: 0.009582655780296535\n",
      "  batch 120 loss: 0.0031337711349740615\n",
      "  batch 130 loss: 0.0056012186771962295\n",
      "  batch 140 loss: 0.0018626414883030407\n",
      "  batch 150 loss: 0.006226780768315621\n",
      "  batch 160 loss: 0.004467278854752799\n",
      "  batch 170 loss: 0.005384363260492364\n",
      "  batch 180 loss: 0.004916865833182804\n",
      "  batch 190 loss: 0.0029008728047173805\n",
      "LOSS train 0.0029008728047173805 valid 0.3040470387100413\n",
      "EPOCH 391:\n",
      "  batch 10 loss: 0.003117952300520699\n",
      "  batch 20 loss: 0.007052724221783535\n",
      "  batch 30 loss: 0.00500190981475015\n",
      "  batch 40 loss: 0.0030089925424078958\n",
      "  batch 50 loss: 0.0015947350786587934\n",
      "  batch 60 loss: 0.008810701926147146\n",
      "  batch 70 loss: 0.0028519847734285618\n",
      "  batch 80 loss: 0.004889173582276918\n",
      "  batch 90 loss: 0.00418784750192529\n",
      "  batch 100 loss: 0.0034303111875201965\n",
      "  batch 110 loss: 0.0021255657917208736\n",
      "  batch 120 loss: 0.008704120225746514\n",
      "  batch 130 loss: 0.0011642988257154485\n",
      "  batch 140 loss: 0.0040368342214321725\n",
      "  batch 150 loss: 0.005881947116082387\n",
      "  batch 160 loss: 0.0043893150462821495\n",
      "  batch 170 loss: 0.009358880872097686\n",
      "  batch 180 loss: 0.005810071470796175\n",
      "  batch 190 loss: 0.0018916242047851029\n",
      "LOSS train 0.0018916242047851029 valid 0.3061372251298747\n",
      "EPOCH 392:\n",
      "  batch 10 loss: 0.0017123169468746368\n",
      "  batch 20 loss: 0.0016919859266181446\n",
      "  batch 30 loss: 0.010047222052376802\n",
      "  batch 40 loss: 0.006158756524474995\n",
      "  batch 50 loss: 0.0036169972990435896\n",
      "  batch 60 loss: 0.0016513830444239374\n",
      "  batch 70 loss: 0.009264367118885097\n",
      "  batch 80 loss: 0.0045140717433234554\n",
      "  batch 90 loss: 0.00432880517961678\n",
      "  batch 100 loss: 0.004886606617227329\n",
      "  batch 110 loss: 0.00432342952808451\n",
      "  batch 120 loss: 0.004047724168248124\n",
      "  batch 130 loss: 0.003919431771856807\n",
      "  batch 140 loss: 0.008201388662570253\n",
      "  batch 150 loss: 0.0033559154536801474\n",
      "  batch 160 loss: 0.001809516562011737\n",
      "  batch 170 loss: 0.004772674950623746\n",
      "  batch 180 loss: 0.005791017798523512\n",
      "  batch 190 loss: 0.0038744800897937636\n",
      "LOSS train 0.0038744800897937636 valid 0.29640621984281634\n",
      "EPOCH 393:\n",
      "  batch 10 loss: 0.0025252828888142178\n",
      "  batch 20 loss: 0.0045470210084396\n",
      "  batch 30 loss: 0.003725917341463969\n",
      "  batch 40 loss: 0.0049113987761781885\n",
      "  batch 50 loss: 0.0016090499010161352\n",
      "  batch 60 loss: 0.0027314272514102457\n",
      "  batch 70 loss: 0.004143962563236414\n",
      "  batch 80 loss: 0.004667011989513981\n",
      "  batch 90 loss: 0.004024617013408083\n",
      "  batch 100 loss: 0.004759430150146749\n",
      "  batch 110 loss: 0.0031658392620215635\n",
      "  batch 120 loss: 0.003518766482957858\n",
      "  batch 130 loss: 0.006226228798379729\n",
      "  batch 140 loss: 0.003724539533312132\n",
      "  batch 150 loss: 0.012960260122483191\n",
      "  batch 160 loss: 0.0046909876236497896\n",
      "  batch 170 loss: 0.0042084485658426376\n",
      "  batch 180 loss: 0.004927663352550837\n",
      "  batch 190 loss: 0.0020815047316709466\n",
      "LOSS train 0.0020815047316709466 valid 0.29766526225757683\n",
      "EPOCH 394:\n",
      "  batch 10 loss: 0.005920152971252257\n",
      "  batch 20 loss: 0.002598142741305054\n",
      "  batch 30 loss: 0.0031347398228447785\n",
      "  batch 40 loss: 0.002371859529439746\n",
      "  batch 50 loss: 0.006343444694748257\n",
      "  batch 60 loss: 0.004739139745893617\n",
      "  batch 70 loss: 0.004363385596667335\n",
      "  batch 80 loss: 0.0012412968463689112\n",
      "  batch 90 loss: 0.003775157404780316\n",
      "  batch 100 loss: 0.0021787577597642384\n",
      "  batch 110 loss: 0.0014048791850171938\n",
      "  batch 120 loss: 0.0072342115822941365\n",
      "  batch 130 loss: 0.0034560497379636955\n",
      "  batch 140 loss: 0.003852332557555371\n",
      "  batch 150 loss: 0.004285690255519015\n",
      "  batch 160 loss: 0.0023799637038200315\n",
      "  batch 170 loss: 0.007548697993006215\n",
      "  batch 180 loss: 0.002314760490429535\n",
      "  batch 190 loss: 0.003591819201572122\n",
      "LOSS train 0.003591819201572122 valid 0.30079024092897955\n",
      "EPOCH 395:\n",
      "  batch 10 loss: 0.002801524652152665\n",
      "  batch 20 loss: 0.012029531432061091\n",
      "  batch 30 loss: 0.0017327085008055575\n",
      "  batch 40 loss: 0.004878715393288502\n",
      "  batch 50 loss: 0.002502683820102725\n",
      "  batch 60 loss: 0.00258378541334352\n",
      "  batch 70 loss: 0.004339407241144499\n",
      "  batch 80 loss: 0.005772282818736585\n",
      "  batch 90 loss: 0.003416458096305064\n",
      "  batch 100 loss: 0.004167264443465335\n",
      "  batch 110 loss: 0.0029241897987546395\n",
      "  batch 120 loss: 0.004269831124222812\n",
      "  batch 130 loss: 0.001045988373881457\n",
      "  batch 140 loss: 0.0019510195130905573\n",
      "  batch 150 loss: 0.00651072081477988\n",
      "  batch 160 loss: 0.002933079475908329\n",
      "  batch 170 loss: 0.006900107734843885\n",
      "  batch 180 loss: 0.005858377935294356\n",
      "  batch 190 loss: 0.004324148009313689\n",
      "LOSS train 0.004324148009313689 valid 0.3024284148407755\n",
      "EPOCH 396:\n",
      "  batch 10 loss: 0.004919092131740399\n",
      "  batch 20 loss: 0.006429536848374795\n",
      "  batch 30 loss: 0.0026184140110842693\n",
      "  batch 40 loss: 0.003159362684703915\n",
      "  batch 50 loss: 0.003508580294289132\n",
      "  batch 60 loss: 0.0037531294251067493\n",
      "  batch 70 loss: 0.0026857702805997974\n",
      "  batch 80 loss: 0.0038052652591344938\n",
      "  batch 90 loss: 0.003719723190942226\n",
      "  batch 100 loss: 0.0036639456835018793\n",
      "  batch 110 loss: 0.001451672782559399\n",
      "  batch 120 loss: 0.0028279453289108856\n",
      "  batch 130 loss: 0.004244256752553355\n",
      "  batch 140 loss: 0.00415076274039663\n",
      "  batch 150 loss: 0.0036414600367905337\n",
      "  batch 160 loss: 0.0027090675849661848\n",
      "  batch 170 loss: 0.0016606549232628608\n",
      "  batch 180 loss: 0.0038346780238740054\n",
      "  batch 190 loss: 0.0022251752295195716\n",
      "LOSS train 0.0022251752295195716 valid 0.3036084920124184\n",
      "EPOCH 397:\n",
      "  batch 10 loss: 0.0016305940148949815\n",
      "  batch 20 loss: 0.00387301963843214\n",
      "  batch 30 loss: 0.003935869122299351\n",
      "  batch 40 loss: 0.002135842774523944\n",
      "  batch 50 loss: 0.0057766304488025\n",
      "  batch 60 loss: 0.0016109860813939391\n",
      "  batch 70 loss: 0.0033493362756772172\n",
      "  batch 80 loss: 0.003423569231384249\n",
      "  batch 90 loss: 0.004393871768134794\n",
      "  batch 100 loss: 0.0010273923974899902\n",
      "  batch 110 loss: 0.003773806996815665\n",
      "  batch 120 loss: 0.0025934651742431925\n",
      "  batch 130 loss: 0.0070835988499140965\n",
      "  batch 140 loss: 0.002884434063306429\n",
      "  batch 150 loss: 0.0035728290794423854\n",
      "  batch 160 loss: 0.006584898809701478\n",
      "  batch 170 loss: 0.001632122274767589\n",
      "  batch 180 loss: 0.006290313389467528\n",
      "  batch 190 loss: 0.006509058525277567\n",
      "LOSS train 0.006509058525277567 valid 0.31323952410566674\n",
      "EPOCH 398:\n",
      "  batch 10 loss: 0.0020064442762601333\n",
      "  batch 20 loss: 0.0024657625165040997\n",
      "  batch 30 loss: 0.006861568295835241\n",
      "  batch 40 loss: 0.005705808720255412\n",
      "  batch 50 loss: 0.0022706127498516083\n",
      "  batch 60 loss: 0.0034504373975618565\n",
      "  batch 70 loss: 0.0029249922955187913\n",
      "  batch 80 loss: 0.0014841061079877704\n",
      "  batch 90 loss: 0.006671268381117557\n",
      "  batch 100 loss: 0.005790496918706367\n",
      "  batch 110 loss: 0.004754129684844166\n",
      "  batch 120 loss: 0.0020437280594393314\n",
      "  batch 130 loss: 0.0011074659829047207\n",
      "  batch 140 loss: 0.0030701058859213147\n",
      "  batch 150 loss: 0.0024579183286830286\n",
      "  batch 160 loss: 0.005318420021555426\n",
      "  batch 170 loss: 0.001828465045838179\n",
      "  batch 180 loss: 0.004500660373182974\n",
      "  batch 190 loss: 0.0017932891969216769\n",
      "LOSS train 0.0017932891969216769 valid 0.30474121435573737\n",
      "EPOCH 399:\n",
      "  batch 10 loss: 0.007403117739736587\n",
      "  batch 20 loss: 0.0017831384895387714\n",
      "  batch 30 loss: 0.0038666466117149413\n",
      "  batch 40 loss: 0.00551757651371485\n",
      "  batch 50 loss: 0.004975392736968409\n",
      "  batch 60 loss: 0.00624294651226478\n",
      "  batch 70 loss: 0.003960191902716126\n",
      "  batch 80 loss: 0.0022277397000976863\n",
      "  batch 90 loss: 0.006698946132725858\n",
      "  batch 100 loss: 0.0031430370118554762\n",
      "  batch 110 loss: 0.0019423191693615394\n",
      "  batch 120 loss: 0.0028449523072721305\n",
      "  batch 130 loss: 0.0035369962905518193\n",
      "  batch 140 loss: 0.001440526214645388\n",
      "  batch 150 loss: 0.0035954079065575683\n",
      "  batch 160 loss: 0.001799094281670932\n",
      "  batch 170 loss: 0.0020158715247362126\n",
      "  batch 180 loss: 0.0035169292289488397\n",
      "  batch 190 loss: 0.0017370065000626767\n",
      "LOSS train 0.0017370065000626767 valid 0.3060718069689037\n",
      "EPOCH 400:\n",
      "  batch 10 loss: 0.003021738637629312\n",
      "  batch 20 loss: 0.001525175557386227\n",
      "  batch 30 loss: 0.0022611821805384124\n",
      "  batch 40 loss: 0.000949353900843164\n",
      "  batch 50 loss: 0.006066943161536642\n",
      "  batch 60 loss: 0.0076693321947429284\n",
      "  batch 70 loss: 0.001877296705747078\n",
      "  batch 80 loss: 0.0019602715823038965\n",
      "  batch 90 loss: 0.00324587402510943\n",
      "  batch 100 loss: 0.0035712319708819606\n",
      "  batch 110 loss: 0.005653124644956442\n",
      "  batch 120 loss: 0.009120359965970515\n",
      "  batch 130 loss: 0.0016886325441131333\n",
      "  batch 140 loss: 0.0032582946856475558\n",
      "  batch 150 loss: 0.004239396801056472\n",
      "  batch 160 loss: 0.004565307970558763\n",
      "  batch 170 loss: 0.0015020872174877908\n",
      "  batch 180 loss: 0.001112508871403861\n",
      "  batch 190 loss: 0.0027687247387389036\n",
      "LOSS train 0.0027687247387389036 valid 0.31415797299560067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Validation Loss')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "#This is doing some logging that we don't need to worry about right now.\n",
    "epoch_number = 0\n",
    "EPOCHS = 400\n",
    "best_vloss = 1_000_000.\n",
    "val_history = []\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    avg_loss = train_one_epoch(curr_model=model)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_history.append(avg_vloss.detach().numpy())\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        torch.save(model, \"../models/model9.pth\")\n",
    "    epoch_number += 1\n",
    "\n",
    "plt.plot(range(EPOCHS), val_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2432, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../models/model9.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total = len(validation_set)\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        inputs, labels = vdata\n",
    "        outputs = torch.argmax(model(inputs), dim=1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct_count += (outputs==labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741935483870968"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741935483870968"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.argmax(model(val_data).detach().numpy(), axis=1), np.argmax(val_labels.detach().numpy(), axis=1), average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5580645161290323"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_data.view(-1,21*2), train_labels)\n",
    "y_pred = knn.predict(val_data.view(-1,21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([699, 21, 2])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193548387096775"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RidgeClassifierCV()\n",
    "clf.fit(train_data.view(-1, 21*2), train_labels)\n",
    "y_pred = clf.predict(val_data.view(-1, 21*2))\n",
    "accuracy_score(val_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/slcw2lz14snfvxp49xgqmr880000gn/T/ipykernel_86164/1044316036.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4573, 0.0484, 0.0896,  ..., 0.0850, 0.0911, 0.0797],\n",
       "        [0.4173, 0.0532, 0.1257,  ..., 0.0936, 0.0882, 0.0599],\n",
       "        [0.4564, 0.0462, 0.0918,  ..., 0.0855, 0.0899, 0.0812],\n",
       "        ...,\n",
       "        [0.0744, 0.0594, 0.1351,  ..., 0.1360, 0.1033, 0.1548],\n",
       "        [0.0907, 0.0860, 0.1948,  ..., 0.1134, 0.1084, 0.0952],\n",
       "        [0.0876, 0.0802, 0.1651,  ..., 0.1132, 0.1127, 0.1029]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5193548387096775"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(val_data.view(-1, 21*2), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reetinav/anaconda3/envs/PIC16B/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(multi_class=\"ovr\")\n",
    "clf.fit(train_data.view(-1, 21*2), torch.argmax(train_labels, dim=1))\n",
    "clf.score(val_data.view(-1, 21*2), torch.argmax(val_labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5952, 0.0090, 0.0528,  ..., 0.0620, 0.0913, 0.0809],\n",
       "        [0.5499, 0.0127, 0.0850,  ..., 0.0669, 0.0768, 0.0737],\n",
       "        [0.5842, 0.0089, 0.0531,  ..., 0.0594, 0.0923, 0.0878],\n",
       "        ...,\n",
       "        [0.0690, 0.1502, 0.1629,  ..., 0.1562, 0.0389, 0.0815],\n",
       "        [0.0903, 0.0719, 0.4556,  ..., 0.0353, 0.1396, 0.1260],\n",
       "        [0.0648, 0.0662, 0.5327,  ..., 0.0554, 0.0969, 0.1004]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor(clf.decision_function(val_data.view(-1, 21*2))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
